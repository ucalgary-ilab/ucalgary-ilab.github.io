<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link rel="preload" href="/_next/static/media/dc84b505c4b06e35-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/e484fdd57b325383.css" as="style"/><link rel="preload" href="/_next/static/css/a1a0497113412518.css" as="style"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              // $('.ui.sidebar')
              //   .sidebar('attach events', '.sidebar.icon')

              $('.sidebar.icon').on('click', function(event) {
                $('.ui.sidebar')
                  .sidebar('toggle')
              })

              $('.project').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })

              $('.thesis').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><link rel="stylesheet" href="/_next/static/css/e484fdd57b325383.css" data-n-g=""/><link rel="stylesheet" href="/_next/static/css/a1a0497113412518.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-a339f4a57035852b.js" defer=""></script><script src="/_next/static/chunks/340-6d79087c62b8c852.js" defer=""></script><script src="/_next/static/chunks/main-f5a95b893c70049f.js" defer=""></script><script src="/_next/static/chunks/vendor-styles-25e9cf206ffc5a28.js" defer=""></script><script src="/_next/static/chunks/505-07248a38b06891a1.js" defer=""></script><script src="/_next/static/chunks/pages/_app-09ca72778dd825e1.js" defer=""></script><script src="/_next/static/chunks/347-6f14004506bc7107.js" defer=""></script><script src="/_next/static/chunks/590-103c60b4bff14dfe.js" defer=""></script><script src="/_next/static/chunks/pages/theses-cf39d94b878de45a.js" defer=""></script><script src="/_next/static/Ogf9KjCYustzbp1lykkxv/_buildManifest.js" defer=""></script><script src="/_next/static/Ogf9KjCYustzbp1lykkxv/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_7d1f77"><div class="ui center aligned container"><div class="ui secondary huge compact menu"><a class="item" href="/"><img loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui tiny image" style="color:transparent" srcSet="/static/images/ilab-logo-3d.gif 1x" src="/static/images/ilab-logo-3d.gif"/></a><a class="item" href="/people/">People</a><a class="item" href="/publications/">Research</a></div></div><div id="theses" class="category ui container"><h1 class="ui horizontal divider header"><svg data-prefix="far" data-icon="file-lines" class="svg-inline--fa fa-file-lines" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M64 48l112 0 0 88c0 39.8 32.2 72 72 72l88 0 0 240c0 8.8-7.2 16-16 16L64 464c-8.8 0-16-7.2-16-16L48 64c0-8.8 7.2-16 16-16zM224 67.9l92.1 92.1-68.1 0c-13.3 0-24-10.7-24-24l0-68.1zM64 0C28.7 0 0 28.7 0 64L0 448c0 35.3 28.7 64 64 64l256 0c35.3 0 64-28.7 64-64l0-261.5c0-17-6.7-33.3-18.7-45.3L242.7 18.7C230.7 6.7 214.5 0 197.5 0L64 0zm56 256c-13.3 0-24 10.7-24 24s10.7 24 24 24l144 0c13.3 0 24-10.7 24-24s-10.7-24-24-24l-144 0zm0 96c-13.3 0-24 10.7-24 24s10.7 24 24 24l144 0c13.3 0 24-10.7 24-24s-10.7-24-24-24l-144 0z"></path></svg>Theses</h1><div class="ui segment" style="margin-top:50px"><div class="thesis ui vertical segment stackable grid" data-id="phd-2025-cabral-mota"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">PhD 2025</span></p><p class="color" style="font-size:1.3em"><b>Modeling, Designing, and Evaluating Lens Visualizations for 3D and Immersive Analytics</b></p><p><a href="/people/roberta-cabral-mota/"><img alt="Roberta Cabral Mota picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/roberta-cabral-mota.jpg 1x" src="/static/images/people/roberta-cabral-mota.jpg"/><span class="author-link">Roberta Cabral Mota</span></a><span class="role"> (author)</span>, <span>Usman Alim</span><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>Mario Costa Sousa</span><span class="role"> (committee)</span>, <span>Nivan Ferreira</span><span class="role"> (committee)</span>, <a href="/people/fateme-rajabiyazdi/"><img alt="Fateme Rajabiyazdi picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/fateme-rajabiyazdi.jpg 1x" src="/static/images/people/fateme-rajabiyazdi.jpg"/><span class="author-link">Fateme Rajabiyazdi</span></a><span class="role"> (committee)</span>, <span>Parmit K. Chilana</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Focus Context Visualization</span><span class="ui brown basic label">Lens Visualization</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Immersive Analytics</span><span class="ui brown basic label">3 D Data</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2025-chulpongsatorn"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2025</span></p><p class="color" style="font-size:1.3em"><b>Physicality and Cross-Device Interaction in Augmented Reality</b></p><p><a href="/people/neil-chulpongsatorn/"><img alt="Thobthai Chulpongsatorn picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/neil-chulpongsatorn.jpg 1x" src="/static/images/people/neil-chulpongsatorn.jpg"/><span class="author-link">Thobthai Chulpongsatorn</span></a><span class="role"> (author)</span>, <a href="/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ryo-suzuki.jpg 1x" src="/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"> (supervisor)</span>, <a href="/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"> (supervisor)</span>, <a href="/people/christian-frisson/"><img alt="Christian Frisson picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/christian-frisson.jpg 1x" src="/static/images/people/christian-frisson.jpg"/><span class="author-link">Christian Frisson</span></a><span class="role"> (committee)</span>, <span>Frank Maurer</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Human Computer Interactions</span><span class="ui brown basic label">Physicality</span><span class="ui brown basic label">Mobile Devices</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Augmented Interface</span><span class="ui brown basic label">Embedded Data Visualization</span><span class="ui brown basic label">Tangible Interaction</span><span class="ui brown basic label">Cross Device Interaction</span><span class="ui brown basic label">Holographic Cross Device Interaction</span><span class="ui brown basic label">Holographic Interface</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2025-pratte"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">PhD 2025</span></p><p class="color" style="font-size:1.3em"><b>Wearing Stories: A Comparative Analysis of Design Strategies in Performative Wearable Technology for Social Awareness</b></p><p><a href="/people/sydney-pratte/"><img alt="Sydney Pratte picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sydney-pratte.jpg 1x" src="/static/images/people/sydney-pratte.jpg"/><span class="author-link">Sydney Pratte</span></a><span class="role"> (author)</span>, <a href="/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"> (supervisor)</span>, <a href="/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/anthony-tang.jpg 1x" src="/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a><span class="role"> (supervisor)</span>, <a href="/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"> (committee)</span>, <span>Christine Brubaker</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Wearable Computing</span><span class="ui brown basic label">Fashion Technology</span><span class="ui brown basic label">Performative Wearables</span><span class="ui brown basic label">Fashion</span><span class="ui brown basic label">Performance</span><span class="ui brown basic label">Empathy</span><span class="ui brown basic label">Emapthy Tools</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2025-poostchi"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">PhD 2025</span></p><p class="color" style="font-size:1.3em"><b>PASSAGE: A Computational Workflow for Architectural Material Composition</b></p><p><span>Peyman Poostchi</span><span class="role"> (author)</span>, <a href="/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"> (supervisor)</span>, <span>Branko Kolarevic</span><span class="role"> (supervisor)</span>, <span>Jason Johnson</span><span class="role"> (committee)</span>, <span>Vera Parlac</span><span class="role"> (committee)</span></p><div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2024-mckendrick"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">PhD 2024</span></p><p class="color" style="font-size:1.3em"><b>The Virtual Rehearsal Suite: Drama and Performance Approaches for Virtual Reality and Human-Computer Interaction</b></p><p><a href="/people/zachary-mckendrick/"><img alt="Zachary E. R. McKendrick picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/zachary-mckendrick.jpg 1x" src="/static/images/people/zachary-mckendrick.jpg"/><span class="author-link">Zachary E. R. McKendrick</span></a><span class="role"> (author)</span>, <span>Patrick Finn</span><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>Cosmin Munteanu</span><span class="role"> (committee)</span>, <a href="/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"> (committee)</span>, <span>April Viczko</span><span class="role"> (committee)</span>, <span>Michael Ullyot</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Drama</span><span class="ui brown basic label">Performance</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Extended Reality</span><span class="ui brown basic label">Human Comouter Interaction</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2024-friedel"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2024</span></p><p class="color" style="font-size:1.3em"><b>Large-surface Passive Haptic Interactions using Pantograph Mechanisms</b></p><p><a href="/people/marcus-friedel/"><img alt="Marcus Kenneth Ernst Friedel picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/marcus-friedel.jpg 1x" src="/static/images/people/marcus-friedel.jpg"/><span class="author-link">Marcus Kenneth Ernst Friedel</span></a><span class="role"> (author)</span>, <a href="/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ryo-suzuki.jpg 1x" src="/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>Aditya Nittala</span><span class="role"> (committee)</span>, <span>Richard Zhao</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">HCI</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Passive Haptics</span><span class="ui brown basic label">Pantographs</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2023-jadon"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2023</span></p><p class="color" style="font-size:1.3em"><b>Mixed Reality Interfaces for Augmented Text and Speech</b></p><p><a href="/people/shivesh-jadon/"><img alt="Shivesh Singh Jadon picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/shivesh-jadon.jpg 1x" src="/static/images/people/shivesh-jadon.jpg"/><span class="author-link">Shivesh Singh Jadon</span></a><span class="role"> (author)</span>, <a href="/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ryo-suzuki.jpg 1x" src="/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"> (supervisor)</span>, <a href="/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"> (supervisor)</span>, <span>Frank Maurer</span><span class="role"> (committee)</span>, <span>Ruofei Du</span><span class="role"> (committee)</span>, <span>Kangsoo Kim</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Natural Language Processing</span><span class="ui brown basic label">Speech Recognition</span><span class="ui brown basic label">Keyword Extraction</span><span class="ui brown basic label">Mixed Reality</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2023-smith"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2023</span></p><p class="color" style="font-size:1.3em"><b>Expanding the User Interactions and Design Process of Haptic Experiences in Virtual Reality</b></p><p><a href="/people/christopher-smith/"><img alt="Christopher Geoffrey Smith picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/christopher-smith.jpg 1x" src="/static/images/people/christopher-smith.jpg"/><span class="author-link">Christopher Geoffrey Smith</span></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <a href="/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"> (supervisor)</span>, <a href="/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ryo-suzuki.jpg 1x" src="/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <a href="/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"> (supervisor)</span>, <a href="/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ryo-suzuki.jpg 1x" src="/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"> (supervisor)</span>, <span>Richard Zhao</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Design Tool</span><span class="ui brown basic label">Tactile Feedback</span><span class="ui brown basic label">Design Process</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2023-dhawka"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2023</span></p><p class="color" style="font-size:1.3em"><b>Demographically Diverse Anthropographics: Exploring Equitable Visual Representations of Diversity</b></p><p><a href="/people/priya-dhawka/"><img alt="Priyadarshinee Dhawka picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/priya-dhawka.jpg 1x" src="/static/images/people/priya-dhawka.jpg"/><span class="author-link">Priyadarshinee Dhawka</span></a><span class="role"> (author)</span>, <a href="/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"> (supervisor)</span>, <span>Leanne Wu</span><span class="role"> (committee)</span>, <span>Geoffrey Messier</span><span class="role"> (committee)</span>, <span>Ryan Henry</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Diverse Anthropographics</span><span class="ui brown basic label">Equity</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Anthropographics</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2023-wei"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2023</span></p><p class="color" style="font-size:1.3em"><b>Design of Anthropomorphic Interfaces for Autonomous Vehicle-Pedestrian Interaction</b></p><p><a href="/people/wei-wei/"><img alt="Wei Wei picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wei-wei.jpg 1x" src="/static/images/people/wei-wei.jpg"/><span class="author-link">Wei Wei</span></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>Zhangxing Chen</span><span class="role"> (committee)</span>, <a href="/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"> (committee)</span>, <a href="/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Anthropomorphism</span><span class="ui brown basic label">AV Pedestrian Interaction</span><span class="ui brown basic label">Immersive Analytics</span><span class="ui brown basic label">VR</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2022-hull"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">PhD 2022</span></p><p class="color" style="font-size:1.3em"><b>Building with Data: Bridging Architectural Design Practices and Information Visualization</b></p><p><a href="/people/carmen-hull/"><img alt="Carmen Hull picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/carmen-hull.jpg 1x" src="/static/images/people/carmen-hull.jpg"/><span class="author-link">Carmen Hull</span></a><span class="role"> (author)</span>, <a href="/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"> (supervisor)</span>, <span>Gerald Hushlak</span><span class="role"> (supervisor)</span>, <a href="/people/sheelagh-carpendale/"><img alt="Sheelagh Carpendale picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sheelagh-carpendale.jpg 1x" src="/static/images/people/sheelagh-carpendale.jpg"/><span class="author-link">Sheelagh Carpendale</span></a><span class="role"> (committee)</span>, <span>Barrett Ens</span><span class="role"> (committee)</span>, <span>Laleh Bejat</span><span class="role"> (committee)</span>, <span>Daniel Keefe</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Architectural Methods</span><span class="ui brown basic label">Data Physicalization</span><span class="ui brown basic label">Generative Design</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2021-hung"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2021</span></p><p class="color" style="font-size:1.3em"><b>Supporting Self-Teaching, Hobbyist Musicians: Technology Design Guidelines for Developing Musical Literacy and Perception</b></p><p><a href="/people/michael-hung/"><img alt="Michael Y-S Hung picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/michael-hung.jpg 1x" src="/static/images/people/michael-hung.jpg"/><span class="author-link">Michael Y-S Hung</span></a><span class="role"> (author)</span>, <a href="/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"> (supervisor)</span>, <span>Adam Bell</span><span class="role"> (committee)</span>, <span>Christian Jacob</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Music Theory</span><span class="ui brown basic label">Self Directed Learning</span><span class="ui brown basic label">Informal Music Learning</span><span class="ui brown basic label">Human Computer Interactions</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2021-asha"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2021</span></p><p class="color" style="font-size:1.3em"><b>Designing Interaction with Autonomous Vehicles: External Displays and Interfaces for Vulnerable Road Users</b></p><p><a href="/people/ashratuz-zavin-asha/"><img alt="Ashratuz Zavin Asha picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ashratuz-zavin-asha.jpg 1x" src="/static/images/people/ashratuz-zavin-asha.jpg"/><span class="author-link">Ashratuz Zavin Asha</span></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>Michael John Jacobson Jr.</span><span class="role"> (committee)</span>, <span>Barry Wylant</span><span class="role"> (committee)</span>, <span>Patrick Finn</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Autonomous Vehicles</span><span class="ui brown basic label">External Automotive Displays</span><span class="ui brown basic label">Pedestrians With Hearing Aids</span><span class="ui brown basic label">Pedestrians In Wheelchairs</span><span class="ui brown basic label">Co Design</span><span class="ui brown basic label">VR Simulation</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2021-wannamaker"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2021</span></p><p class="color" style="font-size:1.3em"><b>Situated Self-Tracking: Ideating, Designing, and Deploying Dedicated User-driven Personal Informatics Systems</b></p><p><a href="/people/kendra-wannamaker/"><img alt="Kendra Wannamaker picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/kendra-wannamaker.jpg 1x" src="/static/images/people/kendra-wannamaker.jpg"/><span class="author-link">Kendra Wannamaker</span></a><span class="role"> (author)</span>, <a href="/people/wesley-willett/"><img alt="Wesley J. Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley J. Willett</span></a><span class="role"> (supervisor)</span>, <a href="/people/anthony-tang/"><img alt="Tony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/anthony-tang.jpg 1x" src="/static/images/people/anthony-tang.jpg"/><span class="author-link">Tony Tang</span></a><span class="role"> (committee)</span>, <a href="/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ryo-suzuki.jpg 1x" src="/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"> (committee)</span>, <a href="/people/wesley-willett/"><img alt="Wesley J. Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley J. Willett</span></a><span class="role"> (supervisor)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Situated Visualization</span><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Design Workshops</span><span class="ui brown basic label">Small Displays</span><span class="ui brown basic label">Ideation</span><span class="ui brown basic label">Sketching</span><span class="ui brown basic label">Personal Data Tracking</span><span class="ui brown basic label">Ambient Devices Internet Of Things</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2020-ledo-maira"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">PhD 2020</span></p><p class="color" style="font-size:1.3em"><b>Designing Interactive Behaviours for Smart Objects</b></p><p><a href="/people/david-ledo/"><img alt="David Ledo Maira picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/david-ledo.jpg 1x" src="/static/images/people/david-ledo.jpg"/><span class="author-link">David Ledo Maira</span></a><span class="role"> (author)</span>, <a href="/people/lora-oehlberg/"><img alt="Lora A. Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora A. Oehlberg</span></a><span class="role"> (supervisor)</span>, <a href="/people/saul-greenberg/"><img alt="Saul Greenberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/saul-greenberg.jpg 1x" src="/static/images/people/saul-greenberg.jpg"/><span class="author-link">Saul Greenberg</span></a><span class="role"> (supervisor)</span>, <span>Jo Vermeulen</span><span class="role"> (committee)</span>, <span>Carey L. Williamson</span><span class="role"> (committee)</span>, <span>Barry Wylant</span><span class="role"> (committee)</span>, <span>Bj√∂rn D. Hartmann</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">HCI</span><span class="ui brown basic label">Interaction Design</span><span class="ui brown basic label">Prototyping</span><span class="ui brown basic label">Prototyping Tools</span><span class="ui brown basic label">Toolkits</span><span class="ui brown basic label">Mobile Devices</span><span class="ui brown basic label">Smart Objects</span><span class="ui brown basic label">Interactive Behaviour</span><span class="ui brown basic label">Interactive Behavior</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2019-kuzabaviciute"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2019</span></p><p class="color" style="font-size:1.3em"><b>Exploring Tactile Interface Aesthetics through Computational Media Design</b></p><p><span>Gabriele Kuzabaviciute</span><span class="role"> (author)</span>, <span>Vera Parlac</span><span class="role"> (supervisor)</span>, <a href="/people/lora-oehlberg/"><img alt="Lora A. Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora A. Oehlberg</span></a><span class="role"> (supervisor)</span>, <span>Gerald Hushlak</span><span class="role"> (committee)</span>, <span>John Aycock</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Tactile Interfaces</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Computational Media Design</span><span class="ui brown basic label">Tactility</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2019-mahadevan"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2019</span></p><p class="color" style="font-size:1.3em"><b>Exploring the Design of Autonomous Vehicle-Pedestrian Interaction</b></p><p><a href="/people/karthik-mahadevan/"><img alt="Karthik Mahadevan picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/karthik-mahadevan.jpg 1x" src="/static/images/people/karthik-mahadevan.jpg"/><span class="author-link">Karthik Mahadevan</span></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <a href="/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Autonomous Vehicle Pedestrian Interaction</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Interaction Design</span><span class="ui brown basic label">Virtual Reality</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2019-kollannur"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2019</span></p><p class="color" style="font-size:1.3em"><b>Leveraging Neuroscience to Improve Haptic Rendering</b></p><p><span>Sandeep Zechariah George Kollannur</span><span class="role"> (author)</span>, <span>Sonny Chan</span><span class="role"> (supervisor)</span>, <a href="/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"> (supervisor)</span>, <span>Ryan Peters</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Neuroscience</span><span class="ui brown basic label">HCI</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2019-mikalauskas"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2019</span></p><p class="color" style="font-size:1.3em"><b>Technology Augmented Props: Tangible User Interfaces for Performer-Controlled Technical Elements in Improvised Theatre</b></p><p><span>Claire Mikalauskas</span><span class="role"> (author)</span>, <a href="/people/lora-oehlberg/"><img alt="Lora A. Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora A. Oehlberg</span></a><span class="role"> (supervisor)</span>, <span>April Viczko</span><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (committee)</span>, <span>Patrick Finn</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Improvised Theater</span><span class="ui brown basic label">Performer Controlled Technology</span><span class="ui brown basic label">Tangible User Interfaces</span><span class="ui brown basic label">Interaction Design</span><span class="ui brown basic label">Technical Theatre</span><span class="ui brown basic label">Props</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2019-li"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">PhD 2019</span></p><p class="color" style="font-size:1.3em"><b>Applications of Interactive Topographic Maps: Tangibility with Improved Spatial Awareness and Readability</b></p><p><span>Hao Li</span><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>Mario Costa Sousa</span><span class="role"> (supervisor)</span>, <span>Kazuki Takashima</span><span class="role"> (committee)</span>, <span>Zhangxing Chen</span><span class="role"> (committee)</span>, <span>Pablo Figueroa</span><span class="role"> (committee)</span>, <a href="/people/wesley-willett/"><img alt="Wesley J. Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley J. Willett</span></a><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Tangible User Interface</span><span class="ui brown basic label">Topographic Map</span><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Physicalization</span><span class="ui brown basic label">Physical Visualization</span><span class="ui brown basic label">Spatial Awareness</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2019-danyluk"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2019</span></p><p class="color" style="font-size:1.3em"><b>Designing Camera Controls for Map Environments</b></p><p><a href="/people/kurtis-danyluk/"><img alt="Kurtis Danyluk picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/kurtis-danyluk.jpg 1x" src="/static/images/people/kurtis-danyluk.jpg"/><span class="author-link">Kurtis Danyluk</span></a><span class="role"> (author)</span>, <a href="/people/wesley-willett/"><img alt="Wesley J. Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley J. Willett</span></a><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (committee)</span>, <span>Faramarz Samavati</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Human Computer Interactions</span><span class="ui brown basic label">Interactive Camera Control</span><span class="ui brown basic label">User Interfaces</span><span class="ui brown basic label">Input Devices And Stratagies</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2019-wun"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2019</span></p><p class="color" style="font-size:1.3em"><b>Authoring Data Visualizations with Physical Template Tools</b></p><p><span>Tiffany Wun</span><span class="role"> (author)</span>, <a href="/people/sheelagh-carpendale/"><img alt="Sheelagh Carpendale picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sheelagh-carpendale.jpg 1x" src="/static/images/people/sheelagh-carpendale.jpg"/><span class="author-link">Sheelagh Carpendale</span></a><span class="role"> (supervisor)</span>, <a href="/people/lora-oehlberg/"><img alt="Lora A. Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora A. Oehlberg</span></a><span class="role"> (supervisor)</span>, <a href="/people/nelson-wong/"><img alt="Nelson Wong picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/nelson-wong.jpg 1x" src="/static/images/people/nelson-wong.jpg"/><span class="author-link">Nelson Wong</span></a><span class="role"> (committee)</span>, <span>Joel Reardon</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Visualization Authoring Tools</span><span class="ui brown basic label">Tangible Tools</span><span class="ui brown basic label">Block Printing</span><span class="ui brown basic label">Physical Template Tools</span><span class="ui brown basic label">Potato</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2018-cartwright"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2018</span></p><p class="color" style="font-size:1.3em"><b>Secure Collaboration Across the Reality-Virtuality Continuum Using Reservoir Data</b></p><p><span>Stephen Cartwright</span><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>Mario Costa Sousa</span><span class="role"> (supervisor)</span>, <span>Zhangxing Chen</span><span class="role"> (committee)</span>, <span>Naser El-Sheimy</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Extended Reality</span><span class="ui brown basic label">Collaboration</span><span class="ui brown basic label">Information Security</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2018-rajabiyazdi"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">PhD 2018</span></p><p class="color" style="font-size:1.3em"><b>Exploring the Design of Visualizations to Facilitate Patient-Provider Communication</b></p><p><a href="/people/fateme-rajabiyazdi/"><img alt="Fatemeh Rajabiyazdi picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/fateme-rajabiyazdi.jpg 1x" src="/static/images/people/fateme-rajabiyazdi.jpg"/><span class="author-link">Fatemeh Rajabiyazdi</span></a><span class="role"> (author)</span>, <a href="/people/sheelagh-carpendale/"><img alt="Sheelagh Carpendale picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sheelagh-carpendale.jpg 1x" src="/static/images/people/sheelagh-carpendale.jpg"/><span class="author-link">Sheelagh Carpendale</span></a><span class="role"> (supervisor)</span>, <a href="/people/lora-oehlberg/"><img alt="Lora A. Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora A. Oehlberg</span></a><span class="role"> (supervisor)</span>, <span>Diane Gromala</span><span class="role"> (committee)</span>, <span>Charles Perin</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Patient Provider Communication</span><span class="ui brown basic label">Technology Design</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Patient Generated Data</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2018-ta"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2018</span></p><p class="color" style="font-size:1.3em"><b>Exploring Prototyping Tools for Interactive Fashion Design</b></p><p><span>Kevin Ta</span><span class="role"> (author)</span>, <a href="/people/lora-oehlberg/"><img alt="Lora A. Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora A. Oehlberg</span></a><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>Anthony Tony</span><span class="role"> (committee)</span>, <span>Joshua M. Taron</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Prototyping Tools</span><span class="ui brown basic label">Electronic Fashion</span><span class="ui brown basic label">Augmented Reality</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2018-mostafa"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">PhD 2018</span></p><p class="color" style="font-size:1.3em"><b>Mediating Experiential Learning in Interactive Immersive Environments</b></p><p><span>Ahmed Mostafa</span><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>M√°rio Costa Sousa</span><span class="role"> (supervisor)</span>, <span>Sonny Chan</span><span class="role"> (committee)</span>, <span>Kazuki Takashima</span><span class="role"> (committee)</span>, <span>Pierre Boulanger</span><span class="role"> (committee)</span>, <span>Naser El-Sheimy</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Virtual Environments</span><span class="ui brown basic label">Interactive</span><span class="ui brown basic label">Education</span><span class="ui brown basic label">Learning</span><span class="ui brown basic label">Simulation</span><span class="ui brown basic label">Immersion</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2017-hu"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2017</span></p><p class="color" style="font-size:1.3em"><b>Designing and Evaluating a Lightweight Video Player for Language Learning</b></p><p><span>Sathaporn Hu</span><span class="role"> (author)</span>, <a href="/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"> (supervisor)</span>, <span>Usman Alim</span><span class="role"> (committee)</span>, <span>Parmit Chilana</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Computer Science</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2017-mok"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2017</span></p><p class="color" style="font-size:1.3em"><b>Critiquing Physical Prototypes for a Remote Audience</b></p><p><a href="/people/terrance-mok/"><img alt="Terrance Mok picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/terrance-mok.jpg 1x" src="/static/images/people/terrance-mok.jpg"/><span class="author-link">Terrance Mok</span></a><span class="role"> (author)</span>, <a href="/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"> (supervisor)</span>, <a href="/people/anthony-tang/"><img alt="Tony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/anthony-tang.jpg 1x" src="/static/images/people/anthony-tang.jpg"/><span class="author-link">Tony Tang</span></a><span class="role"> (committee)</span>, <span>Penelope Pexman</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Computer Science</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2017-payne"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2017</span></p><p class="color" style="font-size:1.3em"><b>Examining the Utility of Constructing Physical Representations of Data</b></p><p><span>Jennifer Payne</span><span class="role"> (author)</span>, <a href="/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"> (supervisor)</span>, <span>Jason Johnson</span><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (committee)</span>, <span>Barry Wylant</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Computer Science</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2017-somanath"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">PhD 2017</span></p><p class="color" style="font-size:1.3em"><b>&#x27;Making&#x27; within Material, Cultural, and Emotional Constraints</b></p><p><a href="/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>M√°rio Costa Sousa</span><span class="role"> (supervisor)</span>, <a href="/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"> (committee)</span>, <span>Janette Hughes</span><span class="role"> (committee)</span>, <span>Vera Parlac</span><span class="role"> (committee)</span>, <span>Oscar Meruvia Pastor</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Computer Science</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2015-li"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2015</span></p><p class="color" style="font-size:1.3em"><b>Two-Sided Transparent Display as a Collaborative Medium</b></p><p><a href="/people/jiannan-li/"><img alt="Jiannan Li picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/jiannan-li.jpg 1x" src="/static/images/people/jiannan-li.jpg"/><span class="author-link">Jiannan Li</span></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <a href="/people/saul-greenberg/"><img alt="Saul Greenberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/saul-greenberg.jpg 1x" src="/static/images/people/saul-greenberg.jpg"/><span class="author-link">Saul Greenberg</span></a><span class="role"> (supervisor)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Computer Science</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="mmus-2012-pon"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MMus 2012</span></p><p class="color" style="font-size:1.3em"><b>Vuzik: Exploring a Medium for Painting Music</b></p><p><span>Aura Pon</span><span class="role"> (author)</span>, <span>David Eagle</span><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Music</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2012-lapides"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2012</span></p><p class="color" style="font-size:1.3em"><b>Designing video games with social, physical, and authorship gameplay</b></p><p><a href="/people/paul-lapides/"><img alt="Paul Lapides picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/paul-lapides.jpg 1x" src="/static/images/people/paul-lapides.jpg"/><span class="author-link">Paul Lapides</span></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>M√°rio Costa Sousa</span><span class="role"> (supervisor)</span></p><div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2011-harris"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2011</span></p><p class="color" style="font-size:1.3em"><b>Exploring the affect of emotive motion in social human robot interaction</b></p><p><span>John J. R. Harris</span><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span></p><div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2011-sultanum"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2011</span></p><p class="color" style="font-size:1.3em"><b>Exploring novel interfaces for 3d visualization of reservoir simulation post-processing data</b></p><p><span>Nicole Barbosa Sultanum</span><span class="role"> (author)</span>, <span>M√°rio Costa Sousa</span><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span></p><div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2010-young"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">PhD 2010</span></p><p class="color" style="font-size:1.3em"><b>Exploring social interaction between robots and people</b></p><p><span>James E. Young</span><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span></p><div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2008-guo"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2008</span></p><p class="color" style="font-size:1.3em"><b>New paradigms for human-robot interaction using tangible user interfaces</b></p><p><span>Cheng Guo</span><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span></p><div></div></div></div></div><div id="theses-modal"><div id="phd-2025-cabral-mota" class="ui large modal"><div class="header"><a target="_blank" href="/theses/phd-2025-cabral-mota/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2025-cabral-mota</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">PhD 2025</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2025-cabral-mota" target="_blank">Modeling, Designing, and Evaluating Lens Visualizations for 3D and Immersive Analytics</a></h1><p class="meta"><a href="/people/roberta-cabral-mota"><img alt="roberta-cabral-mota photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/roberta-cabral-mota.jpg 1x" src="/static/images/people/roberta-cabral-mota.jpg"/><strong>Roberta Cabral Mota</strong></a><span class="role"> (author)</span>, <span>Usman Alim<!-- --> <span class="role"> (supervisor)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>Mario Costa Sousa<!-- --> <span class="role"> (committee)</span></span>, <span>Nivan Ferreira<!-- --> <span class="role"> (committee)</span></span>, <a href="/people/fateme-rajabiyazdi"><img alt="fateme-rajabiyazdi photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/fateme-rajabiyazdi.jpg 1x" src="/static/images/people/fateme-rajabiyazdi.jpg"/><strong>Fateme Rajabiyazdi</strong></a><span class="role"> (committee)</span>, <span>Parmit K. Chilana<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Lens visualization has been a prominent research area in the visualization community, fueled by the continuous need to mitigate visual clutter and occlusion resulting from the increasingly large datasets. Interactive lenses for 3D data, particularly, challenge visualization designers to conceive design strategies that facilitates the analysis of dense, multifaceted data with spatial referents. Given their relevance, the overarching research goal of this dissertation is to investigate how visualization lenses may support 3D data exploration and analysis‚Äîacross both conventional and immersive environments. To this end, we begin with (1) modeling lenses by conducting a systematic review of existing lenses operating within spatial contexts. From this survey, we derive a design space that captures core design dimensions and choices involved in constructing spatially-embedded visualization lenses. Building upon this theoretical foundation, we proceed with (2) designing lenses, through the design, development, and evaluation of four immersive lenses tailored to support distinct forms of 3D data analysis: i) a deformable quadric lens for heterogeneous feature analysis, ii) a dual-imagery lens for multivariate, multi-geometry analysis, iii) a view-dependent lens for reservoir uncertainty analysis, and iv) a shape-conformal lens for spatiotemporal urban data analysis. Afterwards, we probe (3) evaluating lenses by complementing our prior theoretical and practical endeavors with empirical evidence from a controlled study comparing the efficacy of four lens-based designs in supporting time-varying urban data analysis. Throughout the course of our research agenda, we document domain-specific lessons learned and translate them into generalizable design guidelines intended to inform the broader visualization community. Finally, this dissertation concludes by suggesting future research directions for advancing visualization lenses in 3D data exploration and analysis‚Äîacross both traditional and immersive environments. In the long term, we hope that this dissertation serve valuable reference points for visualization researchers and practitioners operating at either a theoretical, design, or empirical level.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Focus Context Visualization</span><span class="ui brown basic label">Lens Visualization</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Immersive Analytics</span><span class="ui brown basic label">3 D Data</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Roberta Cabral Mota<!-- -->.¬†<b>Modeling, Designing, and Evaluating Lens Visualizations for 3D and Immersive Analytics</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2025-09-22<!-- -->. <!-- -->DOI: <a href="https://dx.doi.org/10.11575/PRISM/50555" target="_blank">https://dx.doi.org/10.11575/PRISM/50555</a>URL: <a href="https://hdl.handle.net/1880/122961" target="_blank">https://hdl.handle.net/1880/122961</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2025-chulpongsatorn" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2025-chulpongsatorn/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2025-chulpongsatorn</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2025</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2025-chulpongsatorn" target="_blank">Physicality and Cross-Device Interaction in Augmented Reality</a></h1><p class="meta"><a href="/people/neil-chulpongsatorn"><img alt="neil-chulpongsatorn photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/neil-chulpongsatorn.jpg 1x" src="/static/images/people/neil-chulpongsatorn.jpg"/><strong>Thobthai Chulpongsatorn</strong></a><span class="role"> (author)</span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ryo-suzuki.jpg 1x" src="/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"> (supervisor)</span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"> (supervisor)</span>, <a href="/people/christian-frisson"><img alt="christian-frisson photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/christian-frisson.jpg 1x" src="/static/images/people/christian-frisson.jpg"/><strong>Christian Frisson</strong></a><span class="role"> (committee)</span>, <span>Frank Maurer<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>This thesis explores novel uses of physicality and cross-device interaction with augmented reality through three different research prototypes. (1) Augmented Math leverages the physical world as a basis for generating augmentations through an AI-assisted method that allows educators to create interactive math content from static diagrams. (2) HoloTouch enables users to physically interact with holograms by repurposing personal devices like smartphones as tangible input tools. (3) HoloDevice explores how AR can transform remote collaboration by simulating co-located interactions through holographic representations of users and their devices. These systems were developed through iterative prototyping, collaborative design discussions, user studies, and interviews. Together, they demonstrate how novel interaction techniques grounded in physical and cross-device affordances can enhance usability and engagement in AR environments.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Human Computer Interactions</span><span class="ui brown basic label">Physicality</span><span class="ui brown basic label">Mobile Devices</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Augmented Interface</span><span class="ui brown basic label">Embedded Data Visualization</span><span class="ui brown basic label">Tangible Interaction</span><span class="ui brown basic label">Cross Device Interaction</span><span class="ui brown basic label">Holographic Cross Device Interaction</span><span class="ui brown basic label">Holographic Interface</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Thobthai Chulpongsatorn<!-- -->.¬†<b>Physicality and Cross-Device Interaction in Augmented Reality</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2025-09-18<!-- -->. <!-- -->DOI: <a href="https://dx.doi.org/10.11575/PRISM/50597" target="_blank">https://dx.doi.org/10.11575/PRISM/50597</a>URL: <a href="https://hdl.handle.net/1880/123003" target="_blank">https://hdl.handle.net/1880/123003</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2025-pratte" class="ui large modal"><div class="header"><a target="_blank" href="/theses/phd-2025-pratte/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2025-pratte</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">PhD 2025</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2025-pratte" target="_blank">Wearing Stories: A Comparative Analysis of Design Strategies in Performative Wearable Technology for Social Awareness</a></h1><p class="meta"><a href="/people/sydney-pratte"><img alt="sydney-pratte photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sydney-pratte.jpg 1x" src="/static/images/people/sydney-pratte.jpg"/><strong>Sydney Pratte</strong></a><span class="role"> (author)</span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"> (supervisor)</span>, <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/anthony-tang.jpg 1x" src="/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a><span class="role"> (supervisor)</span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"> (committee)</span>, <span>Christine Brubaker<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Performative wearable technology has become a powerful medium for storytelling, advocacy, and social change. At the intersection of fashion and human-computer interaction (HCI), performative wearable technology explores how wearable technology can be utilized as an expressive tool, enabling designers to convey embodied and personal experiences. This thesis explores two distinct categories of performative wearables: (1) wearable empathy tools, designed to immerse the wearer in an experience of another, and (2) fashion-technology garments, which utilize performance and spectacle to engage audiences with social issues visually. By examining these two approaches, this research aims to understand how performative contexts influence the design process of performative wearables addressing social issues and how contextual factors impact the final designs. To achieve this, I employ qualitative research methods to explore designers&#x27; design strategies to create empathy tools and fashion-technology runway garments. I focus on the use of Research through Design (RtD) methodologies, where I incorporate first-person autobiographical accounts, soma design, and artifact evaluation to document and critically reflect on the design and impact of two performative wearable projects: HACKLES.empathy}and HACKLES.runway. The first, an empathy tool, is designed to simulate and communicate the lived experience of anxiety that I, as a woman, feel when walking alone at night, aiming to foster a deeper understanding through embodied interaction. The second, a runway garment, translates the same narrative into a spectacle-driven fashion-technology piece intended for audience engagement and public discourse on safety and gendered experiences in urban spaces. Through comparative analysis, my research shows that the design process for performative wearables is heavily affected by and informed by how the garment will be experienced. In addition, first-person experiences embedded in the design of empathy tools and fashion-technology garments lead to rich expressions of social issues. Personal experiences are crucial in these designs, and designers who are not inspired by personal experiences should collaborate with individuals who have. This thesis contributes to ongoing discourse in HCI, design research, and interactive fashion by synthesizing insights from fashion activism, embodied interaction, and first-person HCI design. The work proposes a design framework for performative wearables that helps designers navigate the trade-offs between individual experience and visual communication in wearable technology for social awareness. Ultimately, this research highlights the potential of wearable technologies to transcend their functional utility, instead positioning performative wearables as a critical medium for storytelling, activism, and empathy-building.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Wearable Computing</span><span class="ui brown basic label">Fashion Technology</span><span class="ui brown basic label">Performative Wearables</span><span class="ui brown basic label">Fashion</span><span class="ui brown basic label">Performance</span><span class="ui brown basic label">Empathy</span><span class="ui brown basic label">Emapthy Tools</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sydney Pratte<!-- -->.¬†<b>Wearing Stories: A Comparative Analysis of Design Strategies in Performative Wearable Technology for Social Awareness</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2025-08-24<!-- -->. <!-- -->DOI: <a href="https://dx.doi.org/10.11575/PRISM/50148" target="_blank">https://dx.doi.org/10.11575/PRISM/50148</a>URL: <a href="https://hdl.handle.net/1880/122555" target="_blank">https://hdl.handle.net/1880/122555</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2025-poostchi" class="ui large modal"><div class="header"><a target="_blank" href="/theses/phd-2025-poostchi/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2025-poostchi</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">PhD 2025</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2025-poostchi" target="_blank">PASSAGE: A Computational Workflow for Architectural Material Composition</a></h1><p class="meta"><span>Peyman Poostchi<!-- --> <span class="role"> (author)</span></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"> (supervisor)</span>, <span>Branko Kolarevic<!-- --> <span class="role"> (supervisor)</span></span>, <span>Jason Johnson<!-- --> <span class="role"> (committee)</span></span>, <span>Vera Parlac<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Materials have long served as a cornerstone of human creativity and the evolution of our built environment. However, the limited diversity of available materials and the increasing depletion of finite resources, exacerbated by fragmented modeling, analysis, and production processes, highlight the urgent need for integrated and selective methodologies. The emergence of computational media and advanced materials introduces new possibilities, enabling architects to interactively and efficiently conceive, evaluate, and produce well-performing products and processes. This thesis introduces PASSAGE, a comprehensive computational workflow designed to digitally configure, select, and produce material compositions for architectural applications based on customized performance criteria. At the center of PASSAGE lies an interactive workflow that automates the configuration and production of architectural objects through the strategic manipulation of material compositions. This workflow supports designers by an interactive interface, allowing for generation of performance datasets to meet specific user requirements in near-real time. The research presented in this thesis encompasses the design, development, and prototyping of the workflow using existing commercial software and fabrication technologies. The work includes the parametric and simulation modeling of geometry, material properties, and fabrication processes, all of which interact dynamically with a database, the designer, and fabrication machinery to ensure comprehensive control over the design, analysis, and production of architectural object models. The effectiveness of PASSAGE is demonstrated through the development of a visual programing code and a case study focused on the creation of a freeform-heterogeneous chair. This case study, realized through the workflow and a Grasshopper code, serves to evaluate and verify the integrated process from design to production using a 3D printer. The dual goals of this research are to empower architects with an interactive, efficient, and scalable computational workflow and to establish a holistic, integrated approach to material composition of architectural products.</p></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Peyman Poostchi<!-- -->.¬†<b>PASSAGE: A Computational Workflow for Architectural Material Composition</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2025-01-27<!-- -->. <!-- -->DOI: <a href="https://dx.doi.org/10.11575/PRISM/48222" target="_blank">https://dx.doi.org/10.11575/PRISM/48222</a>URL: <a href="https://hdl.handle.net/1880/120613" target="_blank">https://hdl.handle.net/1880/120613</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2024-mckendrick" class="ui large modal"><div class="header"><a target="_blank" href="/theses/phd-2024-mckendrick/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2024-mckendrick</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">PhD 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2024-mckendrick" target="_blank">The Virtual Rehearsal Suite: Drama and Performance Approaches for Virtual Reality and Human-Computer Interaction</a></h1><p class="meta"><a href="/people/zachary-mckendrick"><img alt="zachary-mckendrick photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/zachary-mckendrick.jpg 1x" src="/static/images/people/zachary-mckendrick.jpg"/><strong>Zachary E. R. McKendrick</strong></a><span class="role"> (author)</span>, <span>Patrick Finn<!-- --> <span class="role"> (supervisor)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>Cosmin Munteanu<!-- --> <span class="role"> (committee)</span></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"> (committee)</span>, <span>April Viczko<!-- --> <span class="role"> (committee)</span></span>, <span>Michael Ullyot<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>This dissertation explores the intersection of Drama, virtual reality (VR), and Human-Computer Interaction (HCI), examining their parallels and collective impact on shaping immersive digital experiences. Throughout our research, we ask: how can VR support traditional performance practices? And how can we leverage existing performance practices to support wholistic user engagement? From our interdisciplinary position we identify gaps in existing research and offer novel solutions and innovative frameworks for navigating and structuring experiences within VR. In pursuit of answers to our research questions, we contribute C1) Performance-Based Multimodal Methodological Approaches for HCI, and C2) A Demonstration of Interdisciplinary Possibilities to the landscape of HCI VR research. We propose a symbiotic relationship between the user&#x27;s cognitive engagement and the digital environment&#x27;s architectural and interaction design can be enhanced through the application of practices and principles from Drama and performance. The practical application of these theoretical constructs is showcased in C3) Thresholding Protocols for Digital State Change, and C4) The Virtual Rehearsal Suite (VRS), an immersive VR environment that supports solo performance training, demonstrating how Drama and performance methodologies can enhance the user&#x27;s experience, offering tested perspectives and techniques that promote interaction, presence, and embodiment. Starting with our related works, we identify our interdisciplinary position with a foundation that draws from both the academic and artistic communities interested in VR as a domain, research, and performance tool. We then cast a wide net to understand the dimensions of virtual technologies and their impact on user experience with subsequent chapters investigating the layers of reality, immersion, embodiment, performance rituals, and thresholding concepts. Each chapter contributes to the identification of gaps and parallels across research domains and the discussion of how Drama and performance can elevate the understanding and advancement of VR and HCI systems. The dissertation concludes that the confluence of Drama and performance practice with Interaction Design holds the potential to shape the future aesthetics and experiential facets of virtual environments. The embrace of VR as both a tool and a medium for creative expression is positioned as a transformative leap forward in both HCI and Drama, heralding a new era of digital interaction that needs to embrace the full spectrum of human experience for success and longevity. Our work positions actors as interaction specialists, capable of existing in multiple realities at once and providing insightful reflections on their experiences in iterative processes. The VRS study demonstrates this ability, while emphasizing that virtual environments are not merely technological constructs but complex experiential spaces where the physical and digital converge, challenging traditional perceptions of reality. It highlights the importance of centralizing the user as key to creating compelling virtual experiences that con only be achieved through meticulously designed interactions that resonate with the user&#x27;s sensory and cognitive faculties. Our study underscores the efficacy of VR in supporting actor training with minimal digital interventions, facilitating a seamless transition into and out of VR, enhancing focus during VR engagement, and addressing issues such as VR sickness. It highlights the centralization of the human element as the pivotal factor in VR creation, emphasizing that VR environments should cater to the nuanced spectrum of human emotions, behaviours, and social interactions.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Drama</span><span class="ui brown basic label">Performance</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Extended Reality</span><span class="ui brown basic label">Human Comouter Interaction</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Zachary E. R. McKendrick<!-- -->.¬†<b>The Virtual Rehearsal Suite: Drama and Performance Approaches for Virtual Reality and Human-Computer Interaction</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2024-05-14<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.11575/PRISM/46371" target="_blank">https://doi.org/10.11575/PRISM/46371</a>URL: <a href="https://hdl.handle.net/1880/118774" target="_blank">https://hdl.handle.net/1880/118774</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2024-friedel" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2024-friedel/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2024-friedel</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2024-friedel" target="_blank">Large-surface Passive Haptic Interactions using Pantograph Mechanisms</a></h1><p class="meta"><a href="/people/marcus-friedel"><img alt="marcus-friedel photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/marcus-friedel.jpg 1x" src="/static/images/people/marcus-friedel.jpg"/><strong>Marcus Kenneth Ernst Friedel</strong></a><span class="role"> (author)</span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ryo-suzuki.jpg 1x" src="/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>Aditya Nittala<!-- --> <span class="role"> (committee)</span></span>, <span>Richard Zhao<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Dexterous and natural haptic interaction with the environment in Virtual Reality promises a new era of embodied and intuitive computing. But among the remaining challenges stands the difficulty of natural wall interactions. Personal haptic devices for natural wall interaction in virtual reality should be portable and should provide passive, body-scale interactions. However, existing techniques fall short: Room-scale proxies lack portability, wearable robotic arms are energy-intensive and induce friction, and existing hand-scale passive interaction techniques are unsuitable for continuous large-scale renders. In this thesis, we introduce PantographHaptics, a technique which uses the scaling properties of a pantograph to passively render body-scale surfaces. A pantograph is a classical linkage mechanism which can enlarge or shrink designs by coordinating nodes to move in scaled, geometrically similar paths. To our knowledge, no prior work has applied the pantograph mechanism to large-scale immersive haptics. PantographHaptics is a novel method for passively achieving body-scale haptics which uses a pantograph to scale up a small positional constraint into an encounterable midair render. We present the conceptual foundation underpinning of PantographHaptics by describing the operation of the pantograph mechanism and detailing how we apply it for haptics. Then we verify the PantographHaptics technique through two prototypes: HapticLever, a grounded system, and Feedbackpack, a wearable device. We detail the designs, implementations, and technical evaluations of both prototypes, and we highlight the challenges and solutions involved in their development. We evaluate these prototypes with user evaluations, which contribute assessments of their interaction fidelity, investigations of their usability, comparisons of their performance against other haptic modalities, and recorded participant experiences of using the devices. By introducing and verifying PantographHaptics, we show that this novel technique is a viable and promising approach for interactions with large surfaces. By documenting the development of our prototype artifacts and reporting user experiences with the devices, we contribute a foundation for future research.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Haptics</span><span class="ui brown basic label">HCI</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Passive Haptics</span><span class="ui brown basic label">Pantographs</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Marcus Kenneth Ernst Friedel<!-- -->.¬†<b>Large-surface Passive Haptic Interactions using Pantograph Mechanisms</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2024-01-17<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.11575/PRISM/42844" target="_blank">https://doi.org/10.11575/PRISM/42844</a>URL: <a href="https://hdl.handle.net/1880/118000" target="_blank">https://hdl.handle.net/1880/118000</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2023-jadon" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2023-jadon/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2023-jadon</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2023-jadon" target="_blank">Mixed Reality Interfaces for Augmented Text and Speech</a></h1><p class="meta"><a href="/people/shivesh-jadon"><img alt="shivesh-jadon photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/shivesh-jadon.jpg 1x" src="/static/images/people/shivesh-jadon.jpg"/><strong>Shivesh Singh Jadon</strong></a><span class="role"> (author)</span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ryo-suzuki.jpg 1x" src="/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"> (supervisor)</span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"> (supervisor)</span>, <span>Frank Maurer<!-- --> <span class="role"> (committee)</span></span>, <span>Ruofei Du<!-- --> <span class="role"> (committee)</span></span>, <span>Kangsoo Kim<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>While technology plays a vital role in human communication, there still remain many significant challenges when using them in everyday life. Modern computing technologies, such as smartphones, offer convenient and swift access to information, facilitating tasks like reading documents or communicating with friends. However, these tools frequently lack adaptability, become distracting, consume excessive time, and impede interactions with people and contextual information. Furthermore, they often require numerous steps and significant time investment to gather pertinent information. We want to explore an efficient process of contextual information gathering for mixed reality (MR) interfaces that provide information directly in the user‚Äôs view. This approach allows for a seamless and flexible transition between language and subsequent contextual references, without disrupting the flow of communication. ‚ÄôAugmented Language‚Äô can be defined as the integration of language and communication with mixed reality to enhance, transform, or manipulate language-related aspects and various forms of linguistic augmentations (such as annotation/referencing, aiding social interactions, translation, localization, etc.). In this thesis, our broad objective is to explore mixed reality interfaces and their potential to enhance augmented language, particularly in the domains of speech and text. Our aim is to create interfaces that offer a more natural, generalizable, on-demand, and real-time experience of accessing contextually relevant information and providing adaptive interactions. To better address this broader objective, we systematically break it down to focus on two instances of augmented language. First, enhancing augmented conversation to support on-the-fly, co-located in-person conversations using embedded references. And second, enhancing digital and physical documents using MR to provide on-demand reading support in the form of different summarization techniques. To examine the effectiveness of these speech and text interfaces, we conducted two studies in which we asked the participants to evaluate our system prototype in different use cases. The exploratory usability study for the first exploration confirms that our system decreases distraction and friction in conversation compared to smartphone search while providing highly useful and relevant information. For the second project, we conducted an exploratory design workshop to identify categories of document enhancements. We later conducted a user study with a mixed-reality prototype to highlight five board themes to discuss the benefits of MR document enhancement.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Natural Language Processing</span><span class="ui brown basic label">Speech Recognition</span><span class="ui brown basic label">Keyword Extraction</span><span class="ui brown basic label">Mixed Reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Shivesh Singh Jadon<!-- -->.¬†<b>Mixed Reality Interfaces for Augmented Text and Speech</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2023-08<!-- -->. <!-- -->DOI: <a href="https://dx.doi.org/10.11575/PRISM/41696" target="_blank">https://dx.doi.org/10.11575/PRISM/41696</a>URL: <a href="https://hdl.handle.net/1880/116854" target="_blank">https://hdl.handle.net/1880/116854</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2023-smith" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2023-smith/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2023-smith</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2023-smith" target="_blank">Expanding the User Interactions and Design Process of Haptic Experiences in Virtual Reality</a></h1><p class="meta"><a href="/people/christopher-smith"><img alt="christopher-smith photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/christopher-smith.jpg 1x" src="/static/images/people/christopher-smith.jpg"/><strong>Christopher Geoffrey Smith</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"> (supervisor)</span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ryo-suzuki.jpg 1x" src="/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"> (supervisor)</span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ryo-suzuki.jpg 1x" src="/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"> (supervisor)</span>, <span>Richard Zhao<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Virtual reality can be a highly immersive experience due to its realistic visual presentation. This immersive state is useful for applications including education, training, and entertainment. To enhance the state of immersion provided by virtual reality further, devices capable of simulating touch and force have been researched to allow not only a visual and audio experience but a haptic experience as well. Such research has investigated many approaches to generating haptics for virtual reality but often does not explore how to create an immersive haptic experience using them. In this thesis, we present a discussion on four proposed areas of the virtual reality haptic experience design process using a demonstration methodology. To investigate the application of haptic devices, we designed a modular ungrounded haptic system which was used to create a general-purpose device capable of force-based feedback and used it in the three topics of exploration. The first area explored is the application of existing haptic theory for aircraft control to the field of virtual reality drone control. The second area explored is the presence of the size-weight sensory illusion within virtual reality when using a simulated haptic force.  The third area explored is how authoring within a virtual reality medium can be used by a designer to create VR haptic experiences. From these explorations, we begin a higher-level discussion of the broader process of creating a virtual reality haptic experience. Using the results of each project as a representation of our proposed design steps, we discuss not only the broader concepts the steps contribute to the process and their importance, but also draw connections between them. By doing this, we present a more holistic approach to the large-scale design of virtual reality haptic experiences and the benefits we believe it provides.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Design Tool</span><span class="ui brown basic label">Tactile Feedback</span><span class="ui brown basic label">Design Process</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Christopher Geoffrey Smith<!-- -->.¬†<b>Expanding the User Interactions and Design Process of Haptic Experiences in Virtual Reality</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2023-08<!-- -->. <!-- -->DOI: <a href="https://dx.doi.org/10.11575/PRISM/41738" target="_blank">https://dx.doi.org/10.11575/PRISM/41738</a>URL: <a href="https://hdl.handle.net/1880/116896" target="_blank">https://hdl.handle.net/1880/116896</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2023-dhawka" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2023-dhawka/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2023-dhawka</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2023-dhawka" target="_blank">Demographically Diverse Anthropographics: Exploring Equitable Visual Representations of Diversity</a></h1><p class="meta"><a href="/people/priya-dhawka"><img alt="priya-dhawka photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/priya-dhawka.jpg 1x" src="/static/images/people/priya-dhawka.jpg"/><strong>Priyadarshinee Dhawka</strong></a><span class="role"> (author)</span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"> (supervisor)</span>, <span>Leanne Wu<!-- --> <span class="role"> (committee)</span></span>, <span>Geoffrey Messier<!-- --> <span class="role"> (committee)</span></span>, <span>Ryan Henry<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this thesis, we explore the design of demographically diverse anthropographics from demographic data. Anthropographics are visualizations that often use generic human-shaped symbols as abstract representations of humans. These visualizations are frequently used to convey the human importance of data related to people‚Äôs experiences, usually focusing on demographic data such as age, gender, race among others. However, most current anthropographics employ generic human shapes to represent data about distinct demographic groups, which can hide important demographic and physical differences between these groups. The use of generic human shapes in current anthropographics highlights the lack of inclusive approaches for representing human physical diversity in data visualizations. In response,we explore the creation of demographically diverse anthropographics that communicate the visible physical diversity of demographically-distinct populations. Our contributions stem from a set of critical design explorations for visualizing demographic data with a focus on representing human physical diversity and a study exploring how viewers perceive visual representations of diversity in anthropographics. We make three contributions in this work. First, we describe critical design explorations from two prototypes for representing racial demographic data as physical characteristics of diversity (such as skin tones) in diverse anthropographics. Second, we explore how viewers may perceive visual representations of demographic diversity in anthropographics through an interview study on contemporary examples of homogeneous anthropographics from popular news media and our own set of diverse anthropographics. Finally, we identify a set of social and technical challenges in the creation of anthropographics and contribute a collection of forward-looking opportunities for advancing this line of research on equitable visual representations of diversity through demographically diverse anthropographics.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Diverse Anthropographics</span><span class="ui brown basic label">Equity</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Anthropographics</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Priyadarshinee Dhawka<!-- -->.¬†<b>Demographically Diverse Anthropographics: Exploring Equitable Visual Representations of Diversity</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2023-07<!-- -->. <!-- -->DOI: <a href="https://dx.doi.org/10.11575/PRISM/41666" target="_blank">https://dx.doi.org/10.11575/PRISM/41666</a>URL: <a href="https://hdl.handle.net/1880/116824" target="_blank">https://hdl.handle.net/1880/116824</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2023-wei" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2023-wei/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2023-wei</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2023-wei" target="_blank">Design of Anthropomorphic Interfaces for Autonomous Vehicle-Pedestrian Interaction</a></h1><p class="meta"><a href="/people/wei-wei"><img alt="wei-wei photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wei-wei.jpg 1x" src="/static/images/people/wei-wei.jpg"/><strong>Wei Wei</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>Zhangxing Chen<!-- --> <span class="role"> (committee)</span></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"> (committee)</span>, <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"> (committee)</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Autonomous Vehicle (AV) technology promises to revolutionize human life. The promise of AVs includes reduced highway congestion, more efficient energy usage, and cheaper goods and services. However, without careful design, removing human drivers from vehicles will eliminate the natural communication channels which enable pedestrians to navigate safely. This thesis aims to design, present, and study anthropomorphic interfaces for autonomous vehicles, with the objective of enabling AVs to communicate with pedestrians through non-verbal cues. Non-verbal human communication is vital in human relationships. People use non-verbal communication when speech is impractical, such as when interacting with vehicles. When looking into ways in which AVs can use non-verbal communication to interact with pedestrians, we were inspired by the prospect of using anthropomorphic interfaces. This concept is well explored in Human-Robot Interaction (HRI) but has not been investigated in the context of AVs. For this thesis, we explored the design of anthropomorphic interfaces for autonomous vehicles. First, we proposed three types of anthropomorphic interfaces for AVs: facial expressions, hand gestures, and humanoid torsos. We developed a design space for each category using sketches and a low-fi prototype. Then, to research the benefits and limitations of anthropomorphic AVs, we implemented our AV interfaces in a Virtual Reality (VR) environment and developed two testbeds to evaluate their feasibility and scalability. Finally, we conducted two studies using the two testbeds. We investigated the study results using immersive analytics alongside traditional methods and revealed that anthropomorphic AVs could be helpful in AV-pedestrian interaction when designed by specific guidelines. Since we studied anthropomorphic AVs in VR, we were interested in the possibilities of analyzing the data of our study in an immersive environment. We designed a VR prototype specifically to analyze the data collected from the anthropomorphic AV study. The prototype provided basic immersive analytics features for the AV study data. We conducted an expert session with two domain experts to evaluate our immersive analytics prototype. The study contributed insights into the opportunities and challenges of utilizing immersive analytics to analyze AV studies.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Anthropomorphism</span><span class="ui brown basic label">AV Pedestrian Interaction</span><span class="ui brown basic label">Immersive Analytics</span><span class="ui brown basic label">VR</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Wei Wei<!-- -->.¬†<b>Design of Anthropomorphic Interfaces for Autonomous Vehicle-Pedestrian Interaction</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2023-01<!-- -->. <!-- -->DOI: <a href="https://dx.doi.org/10.11575/PRISM/40689" target="_blank">https://dx.doi.org/10.11575/PRISM/40689</a>URL: <a href="http://hdl.handle.net/1880/115776" target="_blank">http://hdl.handle.net/1880/115776</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2022-hull" class="ui large modal"><div class="header"><a target="_blank" href="/theses/phd-2022-hull/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2022-hull</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">PhD 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2022-hull" target="_blank">Building with Data: Bridging Architectural Design Practices and Information Visualization</a></h1><p class="meta"><a href="/people/carmen-hull"><img alt="carmen-hull photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/carmen-hull.jpg 1x" src="/static/images/people/carmen-hull.jpg"/><strong>Carmen Hull</strong></a><span class="role"> (author)</span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"> (supervisor)</span>, <span>Gerald Hushlak<!-- --> <span class="role"> (supervisor)</span></span>, <a href="/people/sheelagh-carpendale"><img alt="sheelagh-carpendale photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sheelagh-carpendale.jpg 1x" src="/static/images/people/sheelagh-carpendale.jpg"/><strong>Sheelagh Carpendale</strong></a><span class="role"> (committee)</span>, <span>Barrett Ens<!-- --> <span class="role"> (committee)</span></span>, <span>Laleh Bejat<!-- --> <span class="role"> (committee)</span></span>, <span>Daniel Keefe<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Our work seeks to augment new information visualization research with strategies and workflows from the fields of design and architecture. To this end, this research explores how to adopt tools and methods that can integrate the best of physical and digital modalities to multiple contexts and scales in HCI and data visualization. Designing information visualization systems creates a need for a design approach that addresses and ties together two main threads ‚Äì 1) how we as humans interact with and make sense of our environment and 2) how we as designers create meaning through geometry, form, and material encodings. While the research community within data visualization has primarily focused on screen-based data visualizations, there is now an opportunity to study how we can create insight with hybrid physical and digital representations of data through the lens of architectural practice. My colleagues and I have conducted this research at the intersection of model building, diagrams, and generative design, applying this knowledge to the design of multifaceted digital environments, from micro to macro scale, in two- and three- dimensional worlds. To develop this research, we first observe and characterize the architectural methods of model making and their potential to facilitate the design process of interactive systems. Next, we describe how physical hand-crafted and digitally fabricated models of different types assist in various stages of the design process. To illustrate how model building could support fluid exploration of multiple data sets, we built a 3D interactive campus model visualizing multiple layers of building-specific data. The system uses physical models as tangible tokens on an interactive touch surface, visualizing energy use and weather data daily over a two-year period. As an extension of our design, we developed a conceptual framework from this project to highlight the potential of physical models for supporting embodied exploration of spatial and non-spatial visualizations through fluid interaction. We then examine the use of diagrams in architecture and develop a conceptual framework based on the concept of data tectonics to organize and structure the design process of physical and immersive data systems. To further study the use of diagrams and generative design for data visualization, I collaborated with researchers at Tableau Software to develop a patented Tableau extension that self-generates and evolves up to thirty different design permutations at a time. The system randomly assigns a pre-specified palette of mark types to a chosen dataset giving designers the option of adding or deleting options that they deem promising. As a final project for this research, we brought the three principles of model making, diagramming, and generative design together to create a large-scale physical and immersive data visualization. In collaboration with the Department of Social Work at the University of Calgary, the project uses diagrams and generative design to prototype a series of three-dimensional encodings visualizing Global Gender Gap statistics from the World Economic Forum. The tent-like forms evoke sheltering structures that can be registered, experienced, and measured with the whole body. For this project, we applied the diagrammatic approach used in parametric design to traditional information visualization design principles and identified workflows that support rapid exploration and fabrication of multiple data design alternatives. There is no doubt that data and digital technologies, including machine learning and AI, will be part of our human fabric in the future, but what that looks like and how it is structured is still up to us. We need artists, and more diversity in general, in order to do this to the best of our potential as humans. In determining which practices encourage the creation of rich data-driven environments, this research underscores the fundamental need of humans to make sense of the world, inspiring designers to develop new spatial constructs that integrate both the art and science of the built environment.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Architectural Methods</span><span class="ui brown basic label">Data Physicalization</span><span class="ui brown basic label">Generative Design</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Carmen Hull<!-- -->.¬†<b>Building with Data: Bridging Architectural Design Practices and Information Visualization</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2022-01-28<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/114360" target="_blank">http://hdl.handle.net/1880/114360</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2021-hung" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2021-hung/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2021-hung</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2021-hung" target="_blank">Supporting Self-Teaching, Hobbyist Musicians: Technology Design Guidelines for Developing Musical Literacy and Perception</a></h1><p class="meta"><a href="/people/michael-hung"><img alt="michael-hung photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/michael-hung.jpg 1x" src="/static/images/people/michael-hung.jpg"/><strong>Michael Y-S Hung</strong></a><span class="role"> (author)</span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"> (supervisor)</span>, <span>Adam Bell<!-- --> <span class="role"> (committee)</span></span>, <span>Christian Jacob<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>I propose a persona framework and five guidelines for designing interactive technology tools that help self-teaching, hobbyist musicians independently develop musical understanding by leveraging and augmenting their existing musical behaviour. Using perspectives from informal music education, self-directed learning, and the serious leisure perspective, I define &#x27;self-teaching, hobbyist musicians&#x27; as self-directed, experiential learners who build musical understanding and skills in opportunistic, constructionist ways, primarily for leisure. However, self-directed music learning inevitably produces gaps in music theory understanding, affecting musical literacy and perception, which can overall increase the difficulty of picking up new musical knowledge and skills over time. Previous HCI-Music research focuses on motor skill acquisition or augmenting expressivity, while theory-teaching systems neglect practical musical behaviours, learning context, and practices in music pedagogy. To better inform this domain, I interviewed eight musicians to learn about their resources, strategies, and challenges in their idiosyncratic learning processes of music and music theory. Given my findings, this thesis contributes (1) a discussion of hobbyist musicians as technology users and a persona framework to guide design: &quot;The user is a {musician} who learns {music theory competency} while doing {practical musical behaviour}&quot;; and (2) guidelines for designing technology for self-teaching, hobbyist musicians. For each guideline, I offer and discuss speculative, concept sketches of example interactive tools. Finally, I discuss the limitations of the research presented in this thesis and opportunities for future work.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Music Theory</span><span class="ui brown basic label">Self Directed Learning</span><span class="ui brown basic label">Informal Music Learning</span><span class="ui brown basic label">Human Computer Interactions</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Michael Y-S Hung<!-- -->.¬†<b>Supporting Self-Teaching, Hobbyist Musicians: Technology Design Guidelines for Developing Musical Literacy and Perception</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2021-09-27<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/113999" target="_blank">http://hdl.handle.net/1880/113999</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2021-asha" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2021-asha/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2021-asha</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2021-asha" target="_blank">Designing Interaction with Autonomous Vehicles: External Displays and Interfaces for Vulnerable Road Users</a></h1><p class="meta"><a href="/people/ashratuz-zavin-asha"><img alt="ashratuz-zavin-asha photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ashratuz-zavin-asha.jpg 1x" src="/static/images/people/ashratuz-zavin-asha.jpg"/><strong>Ashratuz Zavin Asha</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>Michael John Jacobson Jr.<!-- --> <span class="role"> (committee)</span></span>, <span>Barry Wylant<!-- --> <span class="role"> (committee)</span></span>, <span>Patrick Finn<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>In the near future, mixed traffic consisting of manual and autonomous vehicles (AVs) will be common. Autonomous vehicles with advanced technology offer opportunities for innovative designs and introduce communication challenges for vulnerable road users such as pedestrians and cyclists. Our goal is to explore the emerging new domain of interaction between different road users and autonomous vehicles in a future AV transportation ecosystem. This led us to conduct the thesis following these two themes: 1) understanding design opportunities for external automotive displays (EADs) of AVs; 2) exploring the design of interactions between vulnerable road users (VRUs) and AVs. In theme 1, our work extends contemporary research into visualizations and related applications for autonomous vehicles. Focusing on external car bodies as a design space we introduce a set of EADs. EADs show visualizations to share context and user-specific information and offer opportunities for interaction between users and AVs. We conducted a design study to explore design concepts for EADs to provide services to different road users: pedestrians, passengers, and drivers of other vehicles. Based on the design study, we prototyped four EADs in virtual reality (VR) to demonstrate the potential of our approach. This exploration contributes to our vision for EADs, a design critique of the prototypes, and a discussion of the possible impact and future usage of external automotive displays. In theme 2, we are interested in the ways pedestrians will interact with autonomous vehicles in the absence of non-verbal cues from the driver (such as eye movements, hand gestures, etc.). Crossing streets in these new situations could be more dangerous for VRUs without a proper communication medium. We examined a subset of this challenge with two groups of pedestrians: interaction between AVs and pedestrians with hearing aids (PHAs), and pedestrians in wheelchairs (PWs). First, we worked with hearing aid users as a preliminary exploration of this research. We conduct a co-design study with a co-designer with hearing impairment who has lived experience of wearing hearing aid enhancements. This study contributes several insights and design recommendations on how potential audio cues can be designed to enhance direct communications between PHAs and AVs. For the second part of our research, we designed interactions between pedestrians in wheelchairs and AVs. From an early exploration of potential interface designs through a design study with interaction designers, we prototyped different interfaces in VR. Then, we evaluated the implemented simulations during a co-design study with a powered wheelchair user following inclusive design practices. We identify and reflect on interface design ideas that can help PWs make safe crossing decisions at intersections and discuss design insights for implementing different inclusive interfaces.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Autonomous Vehicles</span><span class="ui brown basic label">External Automotive Displays</span><span class="ui brown basic label">Pedestrians With Hearing Aids</span><span class="ui brown basic label">Pedestrians In Wheelchairs</span><span class="ui brown basic label">Co Design</span><span class="ui brown basic label">VR Simulation</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ashratuz Zavin Asha<!-- -->.¬†<b>Designing Interaction with Autonomous Vehicles: External Displays and Interfaces for Vulnerable Road Users</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2021-09-02<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/113831" target="_blank">http://hdl.handle.net/1880/113831</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2021-wannamaker" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2021-wannamaker/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2021-wannamaker</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2021-wannamaker" target="_blank">Situated Self-Tracking: Ideating, Designing, and Deploying Dedicated User-driven Personal Informatics Systems</a></h1><p class="meta"><a href="/people/kendra-wannamaker"><img alt="kendra-wannamaker photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/kendra-wannamaker.jpg 1x" src="/static/images/people/kendra-wannamaker.jpg"/><strong>Kendra Wannamaker</strong></a><span class="role"> (author)</span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><strong>Wesley J. Willett</strong></a><span class="role"> (supervisor)</span>, <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/anthony-tang.jpg 1x" src="/static/images/people/anthony-tang.jpg"/><strong>Tony Tang</strong></a><span class="role"> (committee)</span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ryo-suzuki.jpg 1x" src="/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"> (committee)</span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><strong>Wesley J. Willett</strong></a><span class="role"> (supervisor)</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this thesis, we examine the intersection between personal informatics and situated visualization. Personal Informatics systems aim to help people collect and utilize their own data. Situated visualizations aim to decentralize data consumption and support people in making data-driven decisions in-situ. We present I/O Bits, a prototype personal informatics system that explores the potential for situated self-tracking. With simple tactile inputs and small e-paper visualizations, I/O Bits are dedicated physical devices that allow individuals to track and visualize different kinds of personal activities in-situ. This is in contrast to most self-tracking systems, which automate data collection, centralize information displays, or integrate into multi-purpose devices like smartwatches or mobile phones. Our contributions stem from a set of situated ideation workshops, an e-paper visualization workshop, the development of I/O Bits, and a prototype deployment where participants constructed their own I/O Bits and used them to track a range of personal data. We make three contributions with this work. First, we report on methodologies from seven design workshops that used ideation and sketching activities to prototype new situated visualizations. Based on our diverse set of workshops, we identify challenges and opportunities for sketching and ideating situated visualizations and highlight promising methods for both designers and researchers. Second, we use our design workshop results to design our novel situated self-tracking system, I/O Bits. We discuss the tensions experienced during our iterative design and development process and explore the design space of small situated visualizations on e-paper displays. Finally, we examine our findings from the situated ideation workshops, e-paper visualization workshop, development process, and prototype deployment. Using sketches, photos, hardware, audio recordings, and transcripts, we distill a set of insights and opportunities for future research on situated self-tracking.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Situated Visualization</span><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Design Workshops</span><span class="ui brown basic label">Small Displays</span><span class="ui brown basic label">Ideation</span><span class="ui brown basic label">Sketching</span><span class="ui brown basic label">Personal Data Tracking</span><span class="ui brown basic label">Ambient Devices Internet Of Things</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kendra Wannamaker<!-- -->.¬†<b>Situated Self-Tracking: Ideating, Designing, and Deploying Dedicated User-driven Personal Informatics Systems</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2021-01-20<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/113022" target="_blank">http://hdl.handle.net/1880/113022</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2020-ledo-maira" class="ui large modal"><div class="header"><a target="_blank" href="/theses/phd-2020-ledo-maira/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2020-ledo-maira</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">PhD 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2020-ledo-maira" target="_blank">Designing Interactive Behaviours for Smart Objects</a></h1><p class="meta"><a href="/people/david-ledo"><img alt="david-ledo photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/david-ledo.jpg 1x" src="/static/images/people/david-ledo.jpg"/><strong>David Ledo Maira</strong></a><span class="role"> (author)</span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora A. Oehlberg</strong></a><span class="role"> (supervisor)</span>, <a href="/people/saul-greenberg"><img alt="saul-greenberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/saul-greenberg.jpg 1x" src="/static/images/people/saul-greenberg.jpg"/><strong>Saul Greenberg</strong></a><span class="role"> (supervisor)</span>, <span>Jo Vermeulen<!-- --> <span class="role"> (committee)</span></span>, <span>Carey L. Williamson<!-- --> <span class="role"> (committee)</span></span>, <span>Barry Wylant<!-- --> <span class="role"> (committee)</span></span>, <span>Bj√∂rn D. Hartmann<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this thesis, I propose methods for repurposing existing hardware and software to enable designers to create live interactive prototypes for smart interactive objects without the need to write code or create custom circuitry. The advent of ubiquitous computing brought the promise of interactive artifacts that integrate into our everyday lives. While this has led to a myriad of ‚Äúsmart objects‚Äù, the problem is that it is difficult for interaction designers to devise interactive behaviours for such objects. For example, how might an interaction designer prototype behaviours for a smart speaker? How can they go beyond voice responses and, for instance, animate lights to show that the speaker is listening, or searching for an answer on the web? Designers today face three challenges: (1) needing multiple expertise of designing behaviour, form, circuitry, and programming the functionality; (2) lacking software tools to author fine-tuned dynamic behaviours; and (3) needing closer-to-product representations to physically manipulate the prototype. I overcome this gap through a method and two interactive systems. I propose a design metaphor: Soul‚ÄìBody Prototyping, which suggests leveraging off-the-shelf mobile phones and watches to create smart object prototypes. By enclosing the mobile device (‚Äúsoul‚Äù) into a physical enclosure (‚Äúbody‚Äù), the designer can exploit the mobile device‚Äôs rich sensing, outputs, and internet connectivity. I then operationalize Soul‚ÄìBody Prototyping through two proof-of-concept prototyping tools. Pineal features trigger-action behaviours which automatically generate 3D models for physical forms. These forms fit a mobile device and expose the necessary inputs and outputs. Astral is a tool where designers can mirror a portion of the desktop‚Äôs screen onto a mobile device, and create mappings that convert live mobile sensor data into mouse or keyboard events. Thus, the mobile device remote controls (and repurposes) familiar desktop applications for dynamic behaviour prototyping. Overall, my work contributes an alternative way to prototype smart interactive objects, which informs the design of future prototyping tools. Moreover, I investigate fundamental questions such as the meaning of interactive behaviour, as well as evaluation methods for prototyping tools and toolkits in HCI research.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">HCI</span><span class="ui brown basic label">Interaction Design</span><span class="ui brown basic label">Prototyping</span><span class="ui brown basic label">Prototyping Tools</span><span class="ui brown basic label">Toolkits</span><span class="ui brown basic label">Mobile Devices</span><span class="ui brown basic label">Smart Objects</span><span class="ui brown basic label">Interactive Behaviour</span><span class="ui brown basic label">Interactive Behavior</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Ledo Maira<!-- -->.¬†<b>Designing Interactive Behaviours for Smart Objects</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2020-09-01<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/112512" target="_blank">http://hdl.handle.net/1880/112512</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2019-kuzabaviciute" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2019-kuzabaviciute/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2019-kuzabaviciute</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2019-kuzabaviciute" target="_blank">Exploring Tactile Interface Aesthetics through Computational Media Design</a></h1><p class="meta"><span>Gabriele Kuzabaviciute<!-- --> <span class="role"> (author)</span></span>, <span>Vera Parlac<!-- --> <span class="role"> (supervisor)</span></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora A. Oehlberg</strong></a><span class="role"> (supervisor)</span>, <span>Gerald Hushlak<!-- --> <span class="role"> (committee)</span></span>, <span>John Aycock<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>The tactile qualities of technology are often neglected by interaction designers who favour digital functionality, even though materiality plays a huge role in defining humans‚Äô surroundings. Multiple benefits emerge from tactile interfaces and this thesis concentrates on three of them: 1. tactile interfaces can communicate visual, emotional, or sensorial information when visual and hearing senses are already overloaded with information; 2. they can guide users‚Äô mobility through an environment, such as using hands to assist in walking through a dark room; and, 3. they can benefit computer-based applications for visually-impaired people. I focus on three themes ‚Äì materiality, physical computing, and human touch behaviour ‚Äì which I investigate through a collaborative project with four other design students. I conduct experime nts to explore three ways of using different tactile materials to create (computationally) interactive tactile interfaces (for example, Conductive Silicone, Touch-Sound Synthesizers, Reactive Chair). This led to two final installations, the first (‚ÄúTactile Room‚Äù) focusing on how to create tactile cues to guide the people through a non-visual interactive layer of the space, and the second (‚ÄúGrowth‚Äù) oriented to designing tactile cues to invite the people to interact with the interactive system. This Research through Design approach enables me to demonstrate how tactile interactivity enhances engagement with digital information. This research expands designers‚Äô range of materials and fabrication techniques, including the elements of physical computing, to prototype new tactile interfaces.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Tactile Interfaces</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Computational Media Design</span><span class="ui brown basic label">Tactility</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Gabriele Kuzabaviciute<!-- -->.¬†<b>Exploring Tactile Interface Aesthetics through Computational Media Design</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2019-11<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/111279" target="_blank">http://hdl.handle.net/1880/111279</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2019-mahadevan" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2019-mahadevan/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2019-mahadevan</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2019-mahadevan" target="_blank">Exploring the Design of Autonomous Vehicle-Pedestrian Interaction</a></h1><p class="meta"><a href="/people/karthik-mahadevan"><img alt="karthik-mahadevan photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/karthik-mahadevan.jpg 1x" src="/static/images/people/karthik-mahadevan.jpg"/><strong>Karthik Mahadevan</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"> (committee)</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Autonomous vehicle research today places an emphasis on developing better sensors and algorithms to enable the vehicle to localize itself in the environment, plan routes, and control its movement. Surveying the general public reveals optimism about the technology but also some skepticism about its ability to communicate with vulnerable road users such as pedestrians and cyclists. In today&#x27;s interaction with vehicles at crosswalks, pedestrians rely on cues originating from the vehicle and the driver. Vehicle cues relate to its kinematics such as speed and stopping distance while driver cues are concerned with communication such as eye gaze and contact, head and body movement, and hand gestures. In autonomous vehicles, however, a driver is not expected to be on-board to provide cues to pedestrians. We attempted to tackle the problem of designing novel ways to facilitate autonomous vehicle-pedestrian interaction at crosswalks. We propose interfaces which communicate an autonomous vehicle&#x27;s awareness and intent as a means of helping pedestrians make safe crossing decisions. Through our exploration, we make several contributions. First, we propose a design space for building interfaces using different cue modalities and cue locations. From an early exploration of this design space, we prototype interfaces designed to facilitate autonomous vehicle-pedestrian interaction. The interaction between vehicles and pedestrians will become more challenging during the transition period until all vehicles on the road are fully autonomous. During this period which we term mixed traffic, vehicles of varying levels of autonomy will occupy roads, some of which will have drivers, others such as semi-autonomous which may have distracted drivers, and fully autonomous vehicles which may or may not have drivers. To study this problem, we contribute a virtual reality-based pedestrian simulator. Our final contribution relates to the evaluation of interfaces in the real and virtual world where we found their inclusion helped pedestrians make safe crossing decisions.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Autonomous Vehicle Pedestrian Interaction</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Interaction Design</span><span class="ui brown basic label">Virtual Reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Karthik Mahadevan<!-- -->.¬†<b>Exploring the Design of Autonomous Vehicle-Pedestrian Interaction</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2019-09-12<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/110924" target="_blank">http://hdl.handle.net/1880/110924</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2019-kollannur" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2019-kollannur/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2019-kollannur</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2019-kollannur" target="_blank">Leveraging Neuroscience to Improve Haptic Rendering</a></h1><p class="meta"><span>Sandeep Zechariah George Kollannur<!-- --> <span class="role"> (author)</span></span>, <span>Sonny Chan<!-- --> <span class="role"> (supervisor)</span></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"> (supervisor)</span>, <span>Ryan Peters<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>In the evolving world of virtual reality (VR), haptics plays an important role. Haptics enables individuals to feel and experience the virtual world and to immerse fully in the virtual environment. Haptics has a long history in which researchers have created various types of devices. Virtual reality brings in the need for highly portable and wearable devices which are limited in weight and grounding. Within the restrictive weight and size limitations of mobile and wearable devices limits the haptic rendering capabilities.  This thesis attempts to overcome the limitations by developing a better understanding of the biology of touch via sensorimotor neuroscience to improve touch perception in virtual textures. In this research, we explore the receptor cells that encode touch information from mechanical stimuli and send them to the brain. Sensorimotor neuroscience explores the functional role of the different type of receptor cells in the human body. We use this biological perspective to create a classification of wearable haptic devices for the fingertip and the hand. The second part of the research explores the neuroscience concept of stochastic resonance that is proved to improve light touch sensation. We create a system comprising of hardware and software to evaluate the impact of stochastic resonance in a virtual texture discrimination task. I conclude this thesis by exploring the future directions of the research presented in this thesis and summarizing the two main contributions of this thesis: first is to develop a neuroscience-based classification of wearable haptic devices for the fingertip and the hand and second is to build a platform to evaluate the effect of mechanical SR in discriminating virtual textures.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Neuroscience</span><span class="ui brown basic label">HCI</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sandeep Zechariah George Kollannur<!-- -->.¬†<b>Leveraging Neuroscience to Improve Haptic Rendering</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2019-07-26<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/110675" target="_blank">http://hdl.handle.net/1880/110675</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2019-mikalauskas" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2019-mikalauskas/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2019-mikalauskas</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2019-mikalauskas" target="_blank">Technology Augmented Props: Tangible User Interfaces for Performer-Controlled Technical Elements in Improvised Theatre</a></h1><p class="meta"><span>Claire Mikalauskas<!-- --> <span class="role"> (author)</span></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora A. Oehlberg</strong></a><span class="role"> (supervisor)</span>, <span>April Viczko<!-- --> <span class="role"> (supervisor)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (committee)</span>, <span>Patrick Finn<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>While improvised theatre (improv) is often performed on a bare stage, improvisers sometimes incorporate physical props and technical elements to inspire new directions for a scene and to enrich their performance. For improvising technical elements such as light and sound, a control booth is used in a theatre space. However, coordinating with improvisers‚Äô action on-stage is challenging as there is a disconnect between the technicians and the performers. My goal is to inform the design of a technology augmented prop that brings the capabilities of the control booth directly to the hands of the performers as a tangible user interface. I interviewed five professional improvisers about their use of physical props and technical elements in improv, and their expectations of performer-controlled technology. I propose a set of guidelines for the design of a technology augmented prop that is integrated into the existing world of improvised theatre.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Improvised Theater</span><span class="ui brown basic label">Performer Controlled Technology</span><span class="ui brown basic label">Tangible User Interfaces</span><span class="ui brown basic label">Interaction Design</span><span class="ui brown basic label">Technical Theatre</span><span class="ui brown basic label">Props</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Claire Mikalauskas<!-- -->.¬†<b>Technology Augmented Props: Tangible User Interfaces for Performer-Controlled Technical Elements in Improvised Theatre</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2019-07-24<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/110668" target="_blank">http://hdl.handle.net/1880/110668</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2019-li" class="ui large modal"><div class="header"><a target="_blank" href="/theses/phd-2019-li/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2019-li</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">PhD 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2019-li" target="_blank">Applications of Interactive Topographic Maps: Tangibility with Improved Spatial Awareness and Readability</a></h1><p class="meta"><span>Hao Li<!-- --> <span class="role"> (author)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>Mario Costa Sousa<!-- --> <span class="role"> (supervisor)</span></span>, <span>Kazuki Takashima<!-- --> <span class="role"> (committee)</span></span>, <span>Zhangxing Chen<!-- --> <span class="role"> (committee)</span></span>, <span>Pablo Figueroa<!-- --> <span class="role"> (committee)</span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><strong>Wesley J. Willett</strong></a><span class="role"> (committee)</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Traditional flat topographic maps are difficult to understand due to the distortion and compromise of the 3-dimensional (3D) spatial representation when it is folded into lower-dimension media (e.g. 2D). During the process, the x-y coordinate of a location can be captured but its physical elevation must be transformed using some visualization techniques, resulting in noticeable cognitive effort in comprehending the original geometric and geographic properties of the original terrain. In this manuscript-based dissertation, I present a collection of my past publications that aim to increase the readability of topographic maps by restoring the original spatiality of the terrain - including the elevations - with a physical map representation and then superimpose additional data visualization on top of it. In this way, the entire terrain topology is kept in a scaled physical representation, allowing users to view it with natural human perceptions. Additionally, user gestures can be tracked in real-time as a sketch-based input to allow novel dynamic interaction of the map interface and data manipulation of the spatial information. Through the chapters, I present the aforementioned concept, named interactive topographic interface, along with a few applications of it in different academic and industrial environments. I also report the design and results of a user study that compares the interface with traditional flat topographic maps. In the long-term, I hope that research mentioned in this dissertation inspires future interactive physical cartography to not only improve map comprehension but also facilitate better spatial and situational awareness over the map interface, resulting in an evolved map usefulness.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Tangible User Interface</span><span class="ui brown basic label">Topographic Map</span><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Physicalization</span><span class="ui brown basic label">Physical Visualization</span><span class="ui brown basic label">Spatial Awareness</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Hao Li<!-- -->.¬†<b>Applications of Interactive Topographic Maps: Tangibility with Improved Spatial Awareness and Readability</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2019-07-02<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/110577" target="_blank">http://hdl.handle.net/1880/110577</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2019-danyluk" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2019-danyluk/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2019-danyluk</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2019-danyluk" target="_blank">Designing Camera Controls for Map Environments</a></h1><p class="meta"><a href="/people/kurtis-danyluk"><img alt="kurtis-danyluk photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/kurtis-danyluk.jpg 1x" src="/static/images/people/kurtis-danyluk.jpg"/><strong>Kurtis Danyluk</strong></a><span class="role"> (author)</span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><strong>Wesley J. Willett</strong></a><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (committee)</span>, <span>Faramarz Samavati<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present an exploration of two classes of navigation techniques designed for representations of real-world terrain. The first introduces look-from camera controls, a new style of camera control for touch devices designed with representations of real world-terrain in mind and provides an evaluation of three different implementations of this style of control. The second looks to virtual reality and compares the effectiveness of four existing and common camera control techniques within the context of a representations of real world-terrain. Effective camera controls greatly increase a user‚Äôs ability to engage with a virtual environment, and virtual map environments are no different. However, current camera controls are difficult to use within map-like environments, requiring burdensome sequences of interactions or performing poorly within ragged terrain. To examine the effectiveness of different camera controls in this space we conducted two studies in which we asked participants to perform map reading and interaction tasks. In both studies the camera control technique greatly influenced participant engagement and enjoyment within a scene. The first study highlights the effectiveness of look-from camera controls as light-weight additions to direct manipulation controls and provides design guidelines for the construction of look-from camera controls. The second study highlights which existing common navigation techniques are most appropriate within a map-like environment presented in immersive virtual reality and how combinations of these controls can combine the strengths of the controls to cover for the weaknesses of others.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Human Computer Interactions</span><span class="ui brown basic label">Interactive Camera Control</span><span class="ui brown basic label">User Interfaces</span><span class="ui brown basic label">Input Devices And Stratagies</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kurtis Danyluk<!-- -->.¬†<b>Designing Camera Controls for Map Environments</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2019-01-16<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/109480" target="_blank">http://hdl.handle.net/1880/109480</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2019-wun" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2019-wun/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2019-wun</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2019-wun" target="_blank">Authoring Data Visualizations with Physical Template Tools</a></h1><p class="meta"><span>Tiffany Wun<!-- --> <span class="role"> (author)</span></span>, <a href="/people/sheelagh-carpendale"><img alt="sheelagh-carpendale photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sheelagh-carpendale.jpg 1x" src="/static/images/people/sheelagh-carpendale.jpg"/><strong>Sheelagh Carpendale</strong></a><span class="role"> (supervisor)</span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora A. Oehlberg</strong></a><span class="role"> (supervisor)</span>, <a href="/people/nelson-wong"><img alt="nelson-wong photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/nelson-wong.jpg 1x" src="/static/images/people/nelson-wong.jpg"/><strong>Nelson Wong</strong></a><span class="role"> (committee)</span>, <span>Joel Reardon<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>In our data-rich society, it is increasingly important that all people are able to use and understand data. Large data sets commonly require expert knowledge to design and disseminate accessible and information-rich visualizations, resulting in visualizations that work well for experts but are less accessible for the general public. My research question addresses how we can support the use of visualization to increase data accessibility for the general public. One approach is to encourage people to self-author data representations suited to their own comprehension needs using simple techniques. However, providing data visualization authoring tools for the general public remains an ongoing challenge. My thesis explores the use of physical tools‚Äîspecifically, rulers and block-printing stamps‚Äîas novel methods of authoring data visualizations, leveraging the advantages of ready-made visualization templates while providing freedom to personalize visual elements. To first explore the possibility of designing physical tools for authoring data visualizations, I present prototypes for several modifiable, computationally-fabricated ruler and stamp designs, created with the goal of allowing users to quickly create repeating visual elements when authoring visual elements on paper. From my design efforts, stamps show promise as low-effort, easy-to-create tools; I therefore conducted a workshop study to understand how people approach visualization authoring when given the ability to create their own physical template tools. In this study, participants authored visualizations on paper using hand-carved stamps made from potatoes and sponges. My results show that participants  were able to author meaningful data visualizations from their self-created stamps, as well as several unique traits and uses of block-printing stamps. I conclude the thesis by discussing issues around expressivity and effectiveness of personalizing physical authoring tools, identify implications for the design and assembly of primitives in potential visualization authoring kits, and applications for physical authoring tools in the bigger scope of data democratization.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Visualization Authoring Tools</span><span class="ui brown basic label">Tangible Tools</span><span class="ui brown basic label">Block Printing</span><span class="ui brown basic label">Physical Template Tools</span><span class="ui brown basic label">Potato</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Tiffany Wun<!-- -->.¬†<b>Authoring Data Visualizations with Physical Template Tools</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2019-01-14<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/109475" target="_blank">http://hdl.handle.net/1880/109475</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2018-cartwright" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2018-cartwright/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2018-cartwright</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2018-cartwright" target="_blank">Secure Collaboration Across the Reality-Virtuality Continuum Using Reservoir Data</a></h1><p class="meta"><span>Stephen Cartwright<!-- --> <span class="role"> (author)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>Mario Costa Sousa<!-- --> <span class="role"> (supervisor)</span></span>, <span>Zhangxing Chen<!-- --> <span class="role"> (committee)</span></span>, <span>Naser El-Sheimy<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Acquiring, storing, processing, synthesizing, visualizing, and interpreting data are core to scientific knowledge discovery. This data life cycle is common to many diverse fields such as medicine and petroleum engineering. A wide range of techniques may be used for interacting with and visualizing data. Immersive technologies such as augmented reality and virtual reality show great potential to enhance important workplace activities such as collaboration. To this end an immersive, collaborative tool for visualizing reservoir data is discussed. Some collaborative scenarios using these technologies are then described. It is important to carefully consider how these technologies will be incorporated into a professional setting to ensure tools based on these technologies will provide a high quality user experience while meeting the security needs of industry. In order to further this goal, some of the architectural considerations of a collaboration tool that uses a variety of technologies from the reality-virtuality continuum are explored. A prototype tool is then presented that has been developed for collaborating over petroleum reservoir scenarios involving sensitive data. This tool incorporates visual protection mechanisms to facilitate collaboration while providing enhanced control over information disclosure. A user feedback session was performed with reservoir engineering subject matter experts, and the results from this exploratory evaluation are reported.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Extended Reality</span><span class="ui brown basic label">Collaboration</span><span class="ui brown basic label">Information Security</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Stephen Cartwright<!-- -->.¬†<b>Secure Collaboration Across the Reality-Virtuality Continuum Using Reservoir Data</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2018-12-20<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/109396" target="_blank">http://hdl.handle.net/1880/109396</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2018-rajabiyazdi" class="ui large modal"><div class="header"><a target="_blank" href="/theses/phd-2018-rajabiyazdi/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2018-rajabiyazdi</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">PhD 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2018-rajabiyazdi" target="_blank">Exploring the Design of Visualizations to Facilitate Patient-Provider Communication</a></h1><p class="meta"><a href="/people/fateme-rajabiyazdi"><img alt="fateme-rajabiyazdi photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/fateme-rajabiyazdi.jpg 1x" src="/static/images/people/fateme-rajabiyazdi.jpg"/><strong>Fatemeh Rajabiyazdi</strong></a><span class="role"> (author)</span>, <a href="/people/sheelagh-carpendale"><img alt="sheelagh-carpendale photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sheelagh-carpendale.jpg 1x" src="/static/images/people/sheelagh-carpendale.jpg"/><strong>Sheelagh Carpendale</strong></a><span class="role"> (supervisor)</span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora A. Oehlberg</strong></a><span class="role"> (supervisor)</span>, <span>Diane Gromala<!-- --> <span class="role"> (committee)</span></span>, <span>Charles Perin<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Establishing strong patient-provider communication during clinical visits can have positive impacts on patient health outcomes. On the other hand, miscommunication between patients and healthcare providers can cause harm or in extreme cases cause death to patients. Factors such as healthcare providers&#x27; limited time, inefficient clinical environments, and patients&#x27; difficulties in communicating their concerns can be the causes of this miscommunication. In this thesis, I explore the design of visualizations to facilitate communication between healthcare providers and patients during clinical visits. In the first part (i) of this thesis, I present the results of a literature review I have conducted to expand our understanding of patients&#x27; and providers&#x27; communication challenges during in-clinic visits. In the second phase (ii) I discuss the results of interviews with healthcare providers and I contrast and compare patients&#x27; and providers&#x27; perspectives in the context of each other to unveil the roots of their communication challenges. Among the communication challenges we identified, I focus on exploring the challenges and the realities patients and healthcare providers face tracking and sharing patient-generated health data. In the third part of this dissertation (iii), I discuss the results of a series of interviews and focus groups with patients and healthcare providers I have conducted to gain a better understanding of patient-generated data communication challenges. I leverage this understanding to propose potential visualization designs representing patient-generated data collections to improve the process of reviewing and communicating these data between patients and healthcare providers. In the fourth part of this dissertation (iv), I discuss the results of the interviews with healthcare providers seeking their reflection on the proposed visualization designs. Finally, in collaboration with our healthcare provider team in Alberta Healthcare Services, I implement the prototypes of a number of carefully selected visualization designs. In the last part of this dissertation (v), I outline insights, lessons learned, and future research directions that arise from these studies and the design process. I hope this research provides more support for considering patients&#x27; and healthcare providers&#x27; individualities when designing technologies and visualizations in healthcare settings.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Patient Provider Communication</span><span class="ui brown basic label">Technology Design</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Patient Generated Data</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Fatemeh Rajabiyazdi<!-- -->.¬†<b>Exploring the Design of Visualizations to Facilitate Patient-Provider Communication</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2018-12-04<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/109237" target="_blank">http://hdl.handle.net/1880/109237</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2018-ta" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2018-ta/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2018-ta</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2018-ta" target="_blank">Exploring Prototyping Tools for Interactive Fashion Design</a></h1><p class="meta"><span>Kevin Ta<!-- --> <span class="role"> (author)</span></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora A. Oehlberg</strong></a><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>Anthony Tony<!-- --> <span class="role"> (committee)</span></span>, <span>Joshua M. Taron<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Interactive garments enable new forms of communication between our bodies and with other people. In electronic fashion (eFashion) design, interactive garments on high fashion runways envision how people might use interactive technologies to enhance our clothing with new sensing and output capabilities. Researchers and fashion designers have since explored new interactive textiles that enable aesthetics-driven, interactive, and new material properties to explore on clothing. While there exist physical tools to implement interactive garments and software tools to create the visual aesthetic of a garment, these tools cannot yet enable designers to use new eFashion technologies in their garments because they require engineering expertise and specialized laboratory equipment. In this thesis, I explore the use of computer-aided prototyping tools to develop interactive eFashion garments. I present case studies with makers and two experienced eFashion designers about their design practices and formulate design guidelines for prototyping tools. I then present two prototyping tools for implementation and exploration of interactive garments. Finally, I discuss future work for physical and virtual prototyping tools in eFashion.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Prototyping Tools</span><span class="ui brown basic label">Electronic Fashion</span><span class="ui brown basic label">Augmented Reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kevin Ta<!-- -->.¬†<b>Exploring Prototyping Tools for Interactive Fashion Design</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2018-09-21<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/108718" target="_blank">http://hdl.handle.net/1880/108718</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2018-mostafa" class="ui large modal"><div class="header"><a target="_blank" href="/theses/phd-2018-mostafa/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2018-mostafa</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">PhD 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2018-mostafa" target="_blank">Mediating Experiential Learning in Interactive Immersive Environments</a></h1><p class="meta"><span>Ahmed Mostafa<!-- --> <span class="role"> (author)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>M√°rio Costa Sousa<!-- --> <span class="role"> (supervisor)</span></span>, <span>Sonny Chan<!-- --> <span class="role"> (committee)</span></span>, <span>Kazuki Takashima<!-- --> <span class="role"> (committee)</span></span>, <span>Pierre Boulanger<!-- --> <span class="role"> (committee)</span></span>, <span>Naser El-Sheimy<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Simulation and immersive environments are gaining popularity in various contexts. Arguably, such interactive systems have the potential to benefit many users in a variety of education and training scenarios. However, some of these systems especially with the lack of skilled instructors are still faced by challenges of operational complexity, the incorporation of different technologies and features, and the limited availability of performance measures and feedback. Therefore, the design of these systems would benefit from integrating experiential aspects and essential educational aids. For example, users of such learning systems, especially the novice ones, can be better supported by a smoother learning curve, detailed guidance features, the availability of feedback and performance reporting, and the integration of engaging &amp; reflective capabilities. In essence, we recognize a need to re-explore learning aids and how they impact design, usage, and overall learning experience in interactive immersive environments.<br/>The goal of this dissertation is to mediate experiential learning in interactive immersive environments. This includes exploring existing and novel learning aids that would facilitate learning with improved engagement and immersion, enrich learners with insightful reflections, better support novice users‚Äô learning and training needs, and ultimately enhance the overall experience.<br/>To achieve this goal, we utilized existing learning models and simulation-based training approaches and proposed a framework of learning aids to mediate learning in interactive immersive environments. Working closely with domain expert collaborators, we designed, implemented and evaluated four new interactive immersive prototypes in an attempt to validate the practicality of our aids. The first prototype, NeuroSimVR, is a stereoscopic visualization augmented with educational aids to support how medical users learn about a common back surgery procedure. The second prototype, ReflectiveSpineVR, is an immersive virtual reality surgical simulation with innovative<br/>interaction history capabilities that aim to empower users‚Äô memories and enable deliberate repetitive practice as needed. The third prototype, JackVR, is an interactive immersive training system, utilizing novel gamification elements, and aims to support oil-and-gas experts in the process of landing oil rigs. Our fourth prototype, RoboTeacher, involves a humanoid robot instructor for teaching people industrial assembly tasks. In our prototypes, we presented novel learning aids, visualization, and interaction techniques that are new to many of the current immersive learning tools. We conclude this dissertation with lessons learned and guidelines for designing with learning aids in future research directions that target interactive experiential environments.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Virtual Environments</span><span class="ui brown basic label">Interactive</span><span class="ui brown basic label">Education</span><span class="ui brown basic label">Learning</span><span class="ui brown basic label">Simulation</span><span class="ui brown basic label">Immersion</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ahmed Mostafa<!-- -->.¬†<b>Mediating Experiential Learning in Interactive Immersive Environments</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2018-01-22<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/106342" target="_blank">http://hdl.handle.net/1880/106342</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2017-hu" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2017-hu/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2017-hu</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2017-hu" target="_blank">Designing and Evaluating a Lightweight Video Player for Language Learning</a></h1><p class="meta"><span>Sathaporn Hu<!-- --> <span class="role"> (author)</span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"> (supervisor)</span>, <span>Usman Alim<!-- --> <span class="role"> (committee)</span></span>, <span>Parmit Chilana<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Watching foreign language videos is a popular and convenient strategy used by many people for learning a new language. However, traditional video players, such as the YouTube player, are not designed to support language learning. We created two video players to explore and to address the issues of using traditional players as a language learning tool. Our players specifically target casual language learners. After evaluating the first player, we found that a traditional player makes it difficult for learners to (1) adjust the level of difficulty, (2) recover missed information, and (3) assess learning progress. We then created the second player to address these issues. The results of the evaluation of the second player demonstrate that people found the player to be helpful for language learning. We also found common usage patterns in the results and opportunities for future improvement.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Computer Science</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sathaporn Hu<!-- -->.¬†<b>Designing and Evaluating a Lightweight Video Player for Language Learning</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2017<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/11023/4265" target="_blank">http://hdl.handle.net/11023/4265</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2017-mok" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2017-mok/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2017-mok</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2017-mok" target="_blank">Critiquing Physical Prototypes for a Remote Audience</a></h1><p class="meta"><a href="/people/terrance-mok"><img alt="terrance-mok photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/terrance-mok.jpg 1x" src="/static/images/people/terrance-mok.jpg"/><strong>Terrance Mok</strong></a><span class="role"> (author)</span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"> (supervisor)</span>, <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/anthony-tang.jpg 1x" src="/static/images/people/anthony-tang.jpg"/><strong>Tony Tang</strong></a><span class="role"> (committee)</span>, <span>Penelope Pexman<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>I present an observational study of physical prototype critique that highlights some of the challenges of communicating physical behaviors and materiality at a distance. Geographically distributed open hardware communities often conduct user feedback and peer critique sessions via video conference. However, people have difficulty using current video conferencing tools to demonstrate and critique physical designs. To examine the challenges of remote critique, I conducted an observational lab study in which participants critiqued pairs of physical prototypes (prosthetic hands) for a face-to-face or remote collaborator. In both conditions, participants‚Äô material experiences were an important part of their critique, however their attention was divided between interacting with the prototype and finding strategies to communicate ‚Äòinvisible‚Äô features. Based on my findings, I propose design implications for remote collaboration tools that support the sharing of material experiences and prototype critique.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Computer Science</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Terrance Mok<!-- -->.¬†<b>Critiquing Physical Prototypes for a Remote Audience</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2017<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/11023/4248" target="_blank">http://hdl.handle.net/11023/4248</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2017-payne" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2017-payne/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2017-payne</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2017-payne" target="_blank">Examining the Utility of Constructing Physical Representations of Data</a></h1><p class="meta"><span>Jennifer Payne<!-- --> <span class="role"> (author)</span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"> (supervisor)</span>, <span>Jason Johnson<!-- --> <span class="role"> (supervisor)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (committee)</span>, <span>Barry Wylant<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>For millennia, people have constructed physicalizations---physical representations of information---by hand. Recent studies have shown that physicalizations can be more efficient for transmitting information than on-screen visualizations. In addition, innovations like shape-changing interfaces and digital fabrication now make it possible to create physicalizations with little manual effort. Yet many physicalizations are still constructed by hand. In this thesis, we explore how manual construction of physicalizations influences the way people approach and comprehend data, through two studies.<br/>One study compares bar chart authoring through physical construction to authoring using template-based chart creation software. A second study compares participant behaviour when constructing physicalizations to that when exploring previously-built physicalizations. Through comparison of these processes, we derive implications for the design of visualization authoring tools, and for the exploration of data.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Computer Science</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jennifer Payne<!-- -->.¬†<b>Examining the Utility of Constructing Physical Representations of Data</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2017<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/11023/3613" target="_blank">http://hdl.handle.net/11023/3613</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2017-somanath" class="ui large modal"><div class="header"><a target="_blank" href="/theses/phd-2017-somanath/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2017-somanath</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">PhD 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2017-somanath" target="_blank">&#x27;Making&#x27; within Material, Cultural, and Emotional Constraints</a></h1><p class="meta"><a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>M√°rio Costa Sousa<!-- --> <span class="role"> (supervisor)</span></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"> (committee)</span>, <span>Janette Hughes<!-- --> <span class="role"> (committee)</span></span>, <span>Vera Parlac<!-- --> <span class="role"> (committee)</span></span>, <span>Oscar Meruvia Pastor<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>The Maker Movement aims to democratize technological practices and promises many benefits for people including improved technical literacy, a means for self-expression and agency, and an opportunity to become more than consumers of technology. As part of the Maker Movement, people build hobbyist and utilitarian projects by themselves using programmable electronics (e.g., microcontroller, sensors, actuators) and software tools. While the Maker Movement is gaining momentum globally, some people are left out. Constraints such as material limitations, educational culture restrictions, and emotional or behavioral difficulties can often limit people from taking part in the Maker Movement. We refer to the systematic investigation of how diverse people respond to making-centered activities within constraints as an exploration of making within constraints.<br/>In this dissertation, we (1) study how people respond to creating physical objects by themselves within constraints and, (2) investigate how to design technology that can help makers within constraints.  We conducted an observational study in an impoverished school in India and identified the students&#x27; challenges and their strategies for making within material and educational culture constraints. We conducted a second study with at-promise youth in Canada and identified a set of lessons learned to engage youth within emotional and behavioral constraints in making-centered activities. Leveraging our observations, we proposed Augmented Reality (AR)-mediated prototyping as a way to address material constraints. AR-mediated prototyping can help makers to build, program, interact with and iterate on physical computing projects that combine both real-world and stand-in virtual electronic components. We designed, implemented, and evaluated a technology probe, Polymorphic Cube (PMC), as an instance of our vision. Our results show that PMC helped participants prototype despite missing I/O electronic components, and highlighted how AR-mediated prototyping extends to exploring project ideas, tinkering with implementation, and making with others.<br/>Informed by our empirical and design explorations, we suggest a set of characteristics of constraints and implications for designing future technologies for makers within constraints. In the long-term, we hope that this research will inspire interaction designers to develop new tools that can help resolve constraints for making.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Computer Science</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sowmya Somanath<!-- -->.¬†<b>&#x27;Making&#x27; within Material, Cultural, and Emotional Constraints</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2017<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/11023/4237" target="_blank">http://hdl.handle.net/11023/4237</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2015-li" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2015-li/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2015-li</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2015</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2015-li" target="_blank">Two-Sided Transparent Display as a Collaborative Medium</a></h1><p class="meta"><a href="/people/jiannan-li"><img alt="jiannan-li photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/jiannan-li.jpg 1x" src="/static/images/people/jiannan-li.jpg"/><strong>Jiannan Li</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <a href="/people/saul-greenberg"><img alt="saul-greenberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/saul-greenberg.jpg 1x" src="/static/images/people/saul-greenberg.jpg"/><strong>Saul Greenberg</strong></a><span class="role"> (supervisor)</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Transparent displays are ‚Äòsee-through‚Äô screens: a person can simultaneously view both the graphics on the screen and real-world content visible through the screen. Interactive transparent displays can serve as an important medium supporting face-to-face collaboration, where people interact with both sides of the display and work together. Such displays enhance workspace awareness, which smooths collaboration: when a person is working on one side of a transparent display, the person on the other side can see the other&#x27;s hand gestures, gaze, and what s/he is currently manipulating on the shared screen. Even so, we argue that in order to provide effective support for collaboration, designing such transparent displays must go beyond current offerings. We propose using two-sided transparent displays, which can present different content on both sides. The displays should also accept interactive input on both sides and visually augment users‚Äô actions when display transparency is reduced. We operationalized these design requirements with our two-sided transparent display prototype, FACINGBOARD-II, and devised a palette of supportive interaction techniques. Through empirical studies, we found that the workspace awareness provided by transparent displays is compromised with degrading display transparency, and that visually enhancing user actions can compensate for this awareness loss.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Computer Science</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jiannan Li<!-- -->.¬†<b>Two-Sided Transparent Display as a Collaborative Medium</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2015-01-28<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/11023/2033" target="_blank">http://hdl.handle.net/11023/2033</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="mmus-2012-pon" class="ui large modal"><div class="header"><a target="_blank" href="/theses/mmus-2012-pon/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>mmus-2012-pon</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MMus 2012</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/mmus-2012-pon" target="_blank">Vuzik: Exploring a Medium for Painting Music</a></h1><p class="meta"><span>Aura Pon<!-- --> <span class="role"> (author)</span></span>, <span>David Eagle<!-- --> <span class="role"> (supervisor)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>What if one could paint music? Music is often shaped by the medium we use to interact with it, and thus the development of different ways to compose and experience music can open up new creative possibilities to people. This somewhat whimsical proposition of painting music, and a quest to find a new medium to create, experience, and interact with music, gave impetus and shape to the development of a musical interface known as Vuzik. Vuzik is an interface for creating and visualizing music through painting gestures on a large interactive surface. This thesis presents the vision, design and implementation of Vuzik. It then describes explorations of it as a tool for music education, and for the performance and creation of electronic music. Finally, it presents evaluation efforts of Vuzik in its performance of each of these explorations, and reflects on their implications regarding the nature and potential of Vuzik.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Music</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Aura Pon<!-- -->.¬†<b>Vuzik: Exploring a Medium for Painting Music</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Music (MMus)<!-- -->. <!-- -->2012-10-04<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/11023/302" target="_blank">http://hdl.handle.net/11023/302</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2012-lapides" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2012-lapides/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2012-lapides</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2012</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2012-lapides" target="_blank">Designing video games with social, physical, and authorship gameplay</a></h1><p class="meta"><a href="/people/paul-lapides"><img alt="paul-lapides photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/paul-lapides.jpg 1x" src="/static/images/people/paul-lapides.jpg"/><strong>Paul Lapides</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>M√°rio Costa Sousa<!-- --> <span class="role"> (supervisor)</span></span></p></div></div></div><div class="block"></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Paul Lapides<!-- -->.¬†<b>Designing video games with social, physical, and authorship gameplay</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2012<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/105630" target="_blank">http://hdl.handle.net/1880/105630</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2011-harris" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2011-harris/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2011-harris</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2011</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2011-harris" target="_blank">Exploring the affect of emotive motion in social human robot interaction</a></h1><p class="meta"><span>John J. R. Harris<!-- --> <span class="role"> (author)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span></p></div></div></div><div class="block"></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">John J. R. Harris<!-- -->.¬†<b>Exploring the affect of emotive motion in social human robot interaction</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2011<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/105508" target="_blank">http://hdl.handle.net/1880/105508</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2011-sultanum" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2011-sultanum/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2011-sultanum</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2011</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2011-sultanum" target="_blank">Exploring novel interfaces for 3d visualization of reservoir simulation post-processing data</a></h1><p class="meta"><span>Nicole Barbosa Sultanum<!-- --> <span class="role"> (author)</span></span>, <span>M√°rio Costa Sousa<!-- --> <span class="role"> (supervisor)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span></p></div></div></div><div class="block"></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nicole Barbosa Sultanum<!-- -->.¬†<b>Exploring novel interfaces for 3d visualization of reservoir simulation post-processing data</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2011<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/105516" target="_blank">http://hdl.handle.net/1880/105516</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2010-young" class="ui large modal"><div class="header"><a target="_blank" href="/theses/phd-2010-young/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2010-young</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">PhD 2010</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2010-young" target="_blank">Exploring social interaction between robots and people</a></h1><p class="meta"><span>James E. Young<!-- --> <span class="role"> (author)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span></p></div></div></div><div class="block"></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">James E. Young<!-- -->.¬†<b>Exploring social interaction between robots and people</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2010<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/104850" target="_blank">http://hdl.handle.net/1880/104850</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2008-guo" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2008-guo/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2008-guo</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2008</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2008-guo" target="_blank">New paradigms for human-robot interaction using tangible user interfaces</a></h1><p class="meta"><span>Cheng Guo<!-- --> <span class="role"> (author)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span></p></div></div></div><div class="block"></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Cheng Guo<!-- -->.¬†<b>New paradigms for human-robot interaction using tangible user interfaces</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2008<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/103564" target="_blank">http://hdl.handle.net/1880/103564</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><div class="content"><a href="https://ucalgary.ca"><img alt="University of Calgary logo" loading="lazy" width="200" height="0" decoding="async" data-nimg="1" style="color:transparent;max-width:200px;margin:0px auto;height:auto" srcSet="/static/images/logo-4.png 1x, /static/images/logo-4.png 2x" src="/static/images/logo-4.png"/></a><div class="sub header"><a class="item" href="https://cpsc.ucalgary.ca">Department of Computer Science</a></div></div></div></footer></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{},"__N_SSG":true},"page":"/theses","query":{},"buildId":"Ogf9KjCYustzbp1lykkxv","runtimeConfig":{"basePath":""},"isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>