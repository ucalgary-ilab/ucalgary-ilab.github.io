<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><title data-next-head="">Data Experience Lab | Interactions Lab - University of Calgary HCI Group</title><meta name="keywords" content="Human-Computer Interaction, HCI, Information Visualization, University of Calgary, CHI, UIST" data-next-head=""/><meta name="description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" data-next-head=""/><meta property="og:title" content="Data Experience Lab | Interactions Lab - University of Calgary HCI Group" data-next-head=""/><meta property="og:description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" data-next-head=""/><meta property="og:site_name" content="University of Calgary Interactions Lab" data-next-head=""/><meta property="og:url" content="https://ilab.ucalgary.ca/" data-next-head=""/><meta property="og:image" content="https://ilab.ucalgary.ca/static/images/labs/data-experience.png" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:title" content="Data Experience Lab | Interactions Lab - University of Calgary HCI Group" data-next-head=""/><meta name="twitter:description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" data-next-head=""/><meta name="twitter:image" content="https://ilab.ucalgary.ca/static/images/labs/data-experience.png" data-next-head=""/><meta name="twitter:card" content="summary" data-next-head=""/><meta name="twitter:site" content="@ucalgary" data-next-head=""/><meta name="twitter:url" content="https://ilab.ucalgary.ca/" data-next-head=""/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link rel="preload" href="/pr-preview/pr-140/_next/static/media/dc84b505c4b06e35-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/pr-preview/pr-140/_next/static/css/02b3b06775ed22db.css" as="style"/><link rel="preload" href="/pr-preview/pr-140/_next/static/css/a1a0497113412518.css" as="style"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              // $('.ui.sidebar')
              //   .sidebar('attach events', '.sidebar.icon')

              $('.sidebar.icon').on('click', function(event) {
                $('.ui.sidebar')
                  .sidebar('toggle')
              })

              $('.project').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })

              $('.thesis').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><link rel="stylesheet" href="/pr-preview/pr-140/_next/static/css/02b3b06775ed22db.css" data-n-g=""/><link rel="stylesheet" href="/pr-preview/pr-140/_next/static/css/a1a0497113412518.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/pr-preview/pr-140/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/pr-preview/pr-140/_next/static/chunks/webpack-675a0260897ef0bc.js" defer=""></script><script src="/pr-preview/pr-140/_next/static/chunks/340-338e30cba42d90e4.js" defer=""></script><script src="/pr-preview/pr-140/_next/static/chunks/main-237fefe93cf728ed.js" defer=""></script><script src="/pr-preview/pr-140/_next/static/chunks/vendor-styles-505bee37018c87d4.js" defer=""></script><script src="/pr-preview/pr-140/_next/static/chunks/505-e99ce8b1306408b7.js" defer=""></script><script src="/pr-preview/pr-140/_next/static/chunks/pages/_app-ef1f9bbf0259e57f.js" defer=""></script><script src="/pr-preview/pr-140/_next/static/chunks/347-501ace96f6678428.js" defer=""></script><script src="/pr-preview/pr-140/_next/static/chunks/590-2708568a316c6731.js" defer=""></script><script src="/pr-preview/pr-140/_next/static/chunks/718-8576d61d8b65683f.js" defer=""></script><script src="/pr-preview/pr-140/_next/static/chunks/404-8046d737a7cb0047.js" defer=""></script><script src="/pr-preview/pr-140/_next/static/chunks/pages/labs/%5Bid%5D-0fa79cee1926e1af.js" defer=""></script><script src="/pr-preview/pr-140/_next/static/wamj8UQpl0oV0xKoxX0hb/_buildManifest.js" defer=""></script><script src="/pr-preview/pr-140/_next/static/wamj8UQpl0oV0xKoxX0hb/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_3c049d"><div class="ui center aligned container"><div class="ui secondary huge compact menu"><a class="item" href="/pr-preview/pr-140/"><img loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui tiny image" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/ilab-logo-3d.gif 1x" src="/pr-preview/pr-140/static/images/ilab-logo-3d.gif"/></a><a class="item" href="/pr-preview/pr-140/people/">People</a><a class="item" href="/pr-preview/pr-140/publications/">Research</a></div></div><div class="pusher"><div class="ui stackable grid"><div class="one wide column"></div><div class="eleven wide column centered"><div id="lab" class="category" style="text-align:center"><div style="display:flex"><div style="background:#c14824;z-index:2;border-radius:50%;min-height:150px;height:150px;min-width:150px;width:150px;margin:auto;display:flex;align-items:center"><img alt="data-experience" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;height:auto;width:100%" srcSet="/pr-preview/pr-140/static/images/labs/data-experience.png 1x" src="/pr-preview/pr-140/static/images/labs/data-experience.png"/></div></div><h1>Data Experience Lab</h1><p></p><p><a target="_blank" href="https://dataexperience.cpsc.ucalgary.ca/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>https://dataexperience.cpsc.ucalgary.ca/</a></p><div style="display:flex;align-items:center"><p style="margin-left:5px"><span style="font-size:1.15em">The </span><a href="https://dataexperience.cpsc.ucalgary.ca/"><span style="color:#c14824;font-weight:700;font-size:1.7em"> <!-- -->Data Experience Lab<!-- --> </span></a><span style="font-size:1.15em"> (<a href="/pr-preview/pr-140/people/wesley-willett/"><span style="font-weight:600">Prof. <!-- -->Wesley Willett</span></a>) <!-- -->develops new data visualizations, interactions, and experiences.<!-- --> </span> </p></div><div class="ui horizontal small divided link list"></div></div><div id="people" class="category ui container"><h1 class="ui horizontal divider header"><svg data-prefix="fas" data-icon="child-reaching" class="svg-inline--fa fa-child-reaching" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M256 64a64 64 0 1 0 -128 0 64 64 0 1 0 128 0zM152.9 169.3c-23.7-8.4-44.5-24.3-58.8-45.8L74.6 94.2C64.8 79.5 45 75.6 30.3 85.4S11.6 115 21.4 129.8L40.9 159c18.1 27.1 42.8 48.4 71.1 62.4L112 480c0 17.7 14.3 32 32 32s32-14.3 32-32l0-96 32 0 0 96c0 17.7 14.3 32 32 32s32-14.3 32-32l0-258.4c29.1-14.2 54.4-36.2 72.7-64.2l18.2-27.9c9.6-14.8 5.4-34.6-9.4-44.3s-34.6-5.5-44.3 9.4L291 122.4c-21.8 33.4-58.9 53.6-98.8 53.6-12.6 0-24.9-2-36.6-5.8-.9-.3-1.8-.7-2.7-.9z"></path></svg>Researchers</h1><div class="people-category eleven wide column centered"><h2>Faculty</h2><div class="ui grid"><a class="five wide column person" href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><p><b>Wesley Willett</b></p><p></p><div class="ui large basic labels"><span class="ui large inverted label label-brown-color">Data Visualization</span><span class="ui large inverted label label-brown-color">Data Phyz</span><span class="ui large inverted label label-brown-color">AR</span></div></a></div></div><div class="people-category eleven wide column centered"><h2>Master&#x27;s Students</h2><div class="ui grid"><a class="four wide column person" href="/pr-preview/pr-140/people/ben-pearman/"><img alt="Ben Pearman photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/ben-pearman.jpg 1x" src="/pr-preview/pr-140/static/images/people/ben-pearman.jpg"/><p><b>Ben Pearman</b></p><p>MSc</p></a><a class="four wide column person" href="/pr-preview/pr-140/people/bonnie-wu/"><img alt="Bonnie Wu photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/bonnie-wu.jpg 1x" src="/pr-preview/pr-140/static/images/people/bonnie-wu.jpg"/><p><b>Bonnie Wu</b></p><p>MSc</p></a><a class="four wide column person" href="/pr-preview/pr-140/people/carson-witts/"><img alt="Carson Witts photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/carson-witts.jpg 1x" src="/pr-preview/pr-140/static/images/people/carson-witts.jpg"/><p><b>Carson Witts</b></p><p>MSc</p></a><a class="four wide column person" href="/pr-preview/pr-140/people/karly-ross/"><img alt="Karly Ross photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/karly-ross.jpg 1x" src="/pr-preview/pr-140/static/images/people/karly-ross.jpg"/><p><b>Karly Ross</b></p><p>MSc</p></a><a class="four wide column person" href="/pr-preview/pr-140/people/shanna-hollingworth/"><img alt="Shanna Hollingworth photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/shanna-hollingworth.jpg 1x" src="/pr-preview/pr-140/static/images/people/shanna-hollingworth.jpg"/><p><b>Shanna Hollingworth</b></p><p>MSc</p></a></div></div><div class="people-category eleven wide column centered"><h2>Undergrad Students</h2><div class="ui grid"><a class="four wide column person" href="/pr-preview/pr-140/people/godwin-saure/"><img alt="Godwin Saure photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/godwin-saure.jpg 1x" src="/pr-preview/pr-140/static/images/people/godwin-saure.jpg"/><p><b>Godwin Saure</b></p><p>Ugrad</p></a><a class="four wide column person" href="/pr-preview/pr-140/people/lychelle-pham/"><img alt="Lychelle Pham photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/lychelle-pham.jpg 1x" src="/pr-preview/pr-140/static/images/people/lychelle-pham.jpg"/><p><b>Lychelle Pham</b></p><p>Ugrad</p></a><a class="four wide column person" href="/pr-preview/pr-140/people/victoria-wong/"><img alt="Victoria Wong photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/victoria-wong.jpg 1x" src="/pr-preview/pr-140/static/images/people/victoria-wong.jpg"/><p><b>Victoria Wong</b></p><p>Ugrad</p></a></div></div><div class="people-category eleven wide column centered"><h2>Alumni</h2><div class="ui grid"><a class="four wide column person" href="/pr-preview/pr-140/people/ahmed-elshabasi/"><img alt="Ahmed Elshabasi photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/ahmed-elshabasi.jpg 1x" src="/pr-preview/pr-140/static/images/people/ahmed-elshabasi.jpg"/><p><b>Ahmed Elshabasi</b></p><p>Alumni (MSc)</p></a><a class="four wide column person" href="/pr-preview/pr-140/people/carmen-hull/"><img alt="Carmen Hull photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/carmen-hull.jpg 1x" src="/pr-preview/pr-140/static/images/people/carmen-hull.jpg"/><p><b>Carmen Hull</b></p><p>Alumni (PhD)<span><br/>Northeastern University</span></p></a><a class="four wide column person" href="/pr-preview/pr-140/people/helen-ai-he/"><img alt="Helen Ai He photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/helen-ai-he.jpg 1x" src="/pr-preview/pr-140/static/images/people/helen-ai-he.jpg"/><p><b>Helen Ai He</b></p><p>Alumni ()</p></a><a class="four wide column person" href="/pr-preview/pr-140/people/jagoda-walny/"><img alt="Jagoda Walny photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/jagoda-walny.jpg 1x" src="/pr-preview/pr-140/static/images/people/jagoda-walny.jpg"/><p><b>Jagoda Walny</b></p><p>Alumni (PhD)<span><br/>Canada Energy Regulator (CER)</span></p></a><a class="four wide column person" href="/pr-preview/pr-140/people/kendra-wannamaker/"><img alt="Kendra Wannamaker photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kendra-wannamaker.jpg 1x" src="/pr-preview/pr-140/static/images/people/kendra-wannamaker.jpg"/><p><b>Kendra Wannamaker</b></p><p>Alumni (MSc)<span><br/>Autodesk Research</span></p></a><a class="four wide column person" href="/pr-preview/pr-140/people/kurtis-danyluk/"><img alt="Kurtis Danyluk photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg"/><p><b>Kurtis Danyluk</b></p><p>Alumni (PhD)</p></a><a class="four wide column person" href="/pr-preview/pr-140/people/mackenzie-hisako-dalton/"><img alt="Mackenzie Hisako Dalton photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/mackenzie-hisako-dalton.jpg 1x" src="/pr-preview/pr-140/static/images/people/mackenzie-hisako-dalton.jpg"/><p><b>Mackenzie Hisako Dalton</b></p><p>Alumni (Ugrad)</p></a><a class="four wide column person" href="/pr-preview/pr-140/people/nathalie-bressa/"><img alt="Nathalie Bressa photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/nathalie-bressa.jpg 1x" src="/pr-preview/pr-140/static/images/people/nathalie-bressa.jpg"/><p><b>Nathalie Bressa</b></p><p>Alumni (Visiting)<span><br/>Aarhus University</span></p></a><a class="four wide column person" href="/pr-preview/pr-140/people/neil-chulpongsatorn/"><img alt="Neil Chulpongsatorn photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-140/static/images/people/neil-chulpongsatorn.jpg"/><p><b>Neil Chulpongsatorn</b></p><p>Alumni (MSc)</p></a><a class="four wide column person" href="/pr-preview/pr-140/people/paige-sobrien/"><img alt="Paige So&#x27;Brien photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/paige-sobrien.jpg 1x" src="/pr-preview/pr-140/static/images/people/paige-sobrien.jpg"/><p><b>Paige So&#x27;Brien</b></p><p>Alumni (Ugrad)</p></a><a class="four wide column person" href="/pr-preview/pr-140/people/priya-dhawka/"><img alt="Priya Dhawka photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/priya-dhawka.jpg 1x" src="/pr-preview/pr-140/static/images/people/priya-dhawka.jpg"/><p><b>Priya Dhawka</b></p><p>Alumni (MSc)</p></a><a class="four wide column person" href="/pr-preview/pr-140/people/sasha-ivanov/"><img alt="Sasha Ivanov photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/sasha-ivanov.jpg 1x" src="/pr-preview/pr-140/static/images/people/sasha-ivanov.jpg"/><p><b>Sasha Ivanov</b></p><p>Alumni (MSc)<span><br/>Meta Reality Labs</span></p></a><a class="four wide column person" href="/pr-preview/pr-140/people/shamim-seyson/"><img alt="Shamim Seyson photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/shamim-seyson.jpg 1x" src="/pr-preview/pr-140/static/images/people/shamim-seyson.jpg"/><p><b>Shamim Seyson</b></p><p>Alumni (Ugrad)</p></a><a class="four wide column person" href="/pr-preview/pr-140/people/shivesh-jadon/"><img alt="Shivesh Jadon photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/shivesh-jadon.jpg 1x" src="/pr-preview/pr-140/static/images/people/shivesh-jadon.jpg"/><p><b>Shivesh Jadon</b></p><p>Alumni (MSc)<span><br/>Apple</span></p></a><a class="four wide column person" href="/pr-preview/pr-140/people/simon-kluber/"><img alt="Simon Klüber photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/no-profile.jpg 1x" src="/pr-preview/pr-140/static/images/people/no-profile.jpg"/><p><b>Simon Klüber</b></p><p>Alumni (Visiting)</p></a><a class="four wide column person" href="/pr-preview/pr-140/people/soren-knudsen/"><img alt="Søren Knudsen photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/soren-knudsen.jpg 1x" src="/pr-preview/pr-140/static/images/people/soren-knudsen.jpg"/><p><b>Søren Knudsen</b></p><p>Alumni (Postdoc)<span><br/>University of Copenhagen</span></p></a><a class="four wide column person" href="/pr-preview/pr-140/people/wei-wei/"><img alt="Wei Wei photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wei-wei.jpg 1x" src="/pr-preview/pr-140/static/images/people/wei-wei.jpg"/><p><b>Wei Wei</b></p><p>Alumni (MSc)</p></a></div></div></div><div id="publications" class="category ui container"><h1 class="ui horizontal divider header"><svg data-prefix="far" data-icon="file-lines" class="svg-inline--fa fa-file-lines" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M64 48l112 0 0 88c0 39.8 32.2 72 72 72l88 0 0 240c0 8.8-7.2 16-16 16L64 464c-8.8 0-16-7.2-16-16L48 64c0-8.8 7.2-16 16-16zM224 67.9l92.1 92.1-68.1 0c-13.3 0-24-10.7-24-24l0-68.1zM64 0C28.7 0 0 28.7 0 64L0 448c0 35.3 28.7 64 64 64l256 0c35.3 0 64-28.7 64-64l0-261.5c0-17-6.7-33.3-18.7-45.3L242.7 18.7C230.7 6.7 214.5 0 197.5 0L64 0zm56 256c-13.3 0-24 10.7-24 24s10.7 24 24 24l144 0c13.3 0 24-10.7 24-24s-10.7-24-24-24l-144 0zm0 96c-13.3 0-24 10.7-24 24s10.7 24 24 24l144 0c13.3 0 24-10.7 24-24s-10.7-24-24-24l-144 0z"></path></svg>Publications</h1><div class="ui segment" style="margin-top:50px"><div class="publication ui vertical segment stackable grid" data-id="dis-2024-danyluk"><div class="three wide column" style="margin:auto"><img alt="dis-2024-danyluk cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/dis-2024-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/dis-2024-danyluk.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2024</span></p><p class="color" style="font-size:1.3em"><b>Understanding Gesture and Microgesture Inputs for Augmented Reality Maps</b></p><p><a href="/pr-preview/pr-140/people/kurtis-danyluk/"><img alt="Kurtis Danyluk picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg"/><span class="author-link">Kurtis Danyluk</span></a><span class="role"></span>, <span>Simon Klueber</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/aditya-shekhar-nittala/"><img alt="Aditya Shekhar Nittala picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-140/static/images/people/aditya-shekhar-nittala.jpg"/><span class="author-link">Aditya Shekhar Nittala</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Gestural Input</span><span class="ui brown basic label">Microgestures</span><span class="ui brown basic label">AR</span><span class="ui brown basic label">Maps</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2024-bressa"><div class="three wide column" style="margin:auto"><img alt="chi-2024-bressa cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2024-bressa.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2024-bressa.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2024</span></p><p class="color" style="font-size:1.3em"><b>Input Visualization: Collecting and Modifying Data with Visual Representations</b></p><p><a href="/pr-preview/pr-140/people/nathalie-bressa/"><img alt="Nathalie Bressa picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/nathalie-bressa.jpg 1x" src="/pr-preview/pr-140/static/images/people/nathalie-bressa.jpg"/><span class="author-link">Nathalie Bressa</span></a><span class="role"></span>, <span>Jordan Louis</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span>, <span>Samuel Huron</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Input Visualization</span><span class="ui brown basic label">Data Physicalization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2024-dhawka"><div class="three wide column" style="margin:auto"><img alt="chi-2024-dhawka cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2024-dhawka.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2024-dhawka.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2024</span></p><p class="color" style="font-size:1.3em"><b>Better Little People Pictures: Generative Creation of Demographically Diverse Anthropographics</b></p><p><a href="/pr-preview/pr-140/people/priya-dhawka/"><img alt="Priya Dhawka picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/priya-dhawka.jpg 1x" src="/pr-preview/pr-140/static/images/people/priya-dhawka.jpg"/><span class="author-link">Priya Dhawka</span></a><span class="role"></span>, <span>Lauren Perera</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Anthropographics</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Marginalized Populations</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2023-chulpongsatorn"><div class="three wide column" style="margin:auto"><img alt="uist-2023-chulpongsatorn cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/uist-2023-chulpongsatorn.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/uist-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2023</span></p><p class="color" style="font-size:1.3em"><b>Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks</b></p><p><a href="/pr-preview/pr-140/people/neil-chulpongsatorn/"><img alt="Neil Chulpongsatorn picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-140/static/images/people/neil-chulpongsatorn.jpg"/><span class="author-link">Neil Chulpongsatorn</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/mille-skovhus-lunding/"><img alt="Mille Skovhus Lunding picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/mille-skovhus-lunding.jpg 1x" src="/pr-preview/pr-140/static/images/people/mille-skovhus-lunding.jpg"/><span class="author-link">Mille Skovhus Lunding</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/nishan-soni/"><img alt="Nishan Soni picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/no-profile-2.jpg 1x" src="/pr-preview/pr-140/static/images/people/no-profile-2.jpg"/><span class="author-link">Nishan Soni</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-140/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Authoring Interfaces</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2023-dhawka"><div class="three wide column" style="margin:auto"><img alt="chi-2023-dhawka cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2023-dhawka.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2023-dhawka.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2023</span></p><p class="color" style="font-size:1.3em"><b>We are the Data: Challenges and Opportunities for Creating Demographically Diverse Anthropographics</b></p><p><a href="/pr-preview/pr-140/people/priya-dhawka/"><img alt="Priya Dhawka picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/priya-dhawka.jpg 1x" src="/pr-preview/pr-140/static/images/people/priya-dhawka.jpg"/><span class="author-link">Priya Dhawka</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/helen-ai-he/"><img alt="Helen Ai He picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/helen-ai-he.jpg 1x" src="/pr-preview/pr-140/static/images/people/helen-ai-he.jpg"/><span class="author-link">Helen Ai He</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Anthropographics</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Marginalized Populations</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2023-monteiro"><div class="three wide column" style="margin:auto"><img alt="chi-2023-monteiro cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2023-monteiro.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2023-monteiro.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2023</span></p><p class="color" style="font-size:1.3em"><b>Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching</b></p><p><a href="/pr-preview/pr-140/people/kyzyl-monteiro/"><img alt="Kyzyl Monteiro picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kyzyl-monteiro.jpg 1x" src="/pr-preview/pr-140/static/images/people/kyzyl-monteiro.jpg"/><span class="author-link">Kyzyl Monteiro</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/ritik-vatsal/"><img alt="Ritik Vatsal picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/ritik-vatsal.jpg 1x" src="/pr-preview/pr-140/static/images/people/ritik-vatsal.jpg"/><span class="author-link">Ritik Vatsal</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/neil-chulpongsatorn/"><img alt="Neil Chulpongsatorn picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-140/static/images/people/neil-chulpongsatorn.jpg"/><span class="author-link">Neil Chulpongsatorn</span></a><span class="role"></span>, <span>Aman Parnami</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-140/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Prototyping Tools</span><span class="ui brown basic label">Tangible Interactions</span><span class="ui brown basic label">Everyday Objects</span><span class="ui brown basic label">Interactive Machine Teaching</span><span class="ui brown basic label">Human Centered Machine Learning</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-ea-2023-chulpongsatorn"><div class="three wide column" style="margin:auto"><img alt="chi-ea-2023-chulpongsatorn cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-ea-2023-chulpongsatorn.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-ea-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI EA 2023</span></p><p class="color" style="font-size:1.3em"><b>HoloTouch: Interacting with Mixed Reality Visualizations Through Smartphone Proxies</b></p><p><a href="/pr-preview/pr-140/people/neil-chulpongsatorn/"><img alt="Neil Chulpongsatorn picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-140/static/images/people/neil-chulpongsatorn.jpg"/><span class="author-link">Neil Chulpongsatorn</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-140/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Embedded Data Visualization</span><span class="ui brown basic label">Tangible Interaction</span><span class="ui brown basic label">Cross Device Interaction</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2022-liao"><div class="three wide column" style="margin:auto"><img alt="uist-2022-liao cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/uist-2022-liao.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/uist-2022-liao.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2022</span></p><p class="color" style="font-size:1.3em"><b>RealityTalk: Real-time Speech-driven Augmented Presentation for AR Live Storytelling</b></p><p><a href="/pr-preview/pr-140/people/jian-liao/"><img alt="Jian Liao picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/jian-liao.jpg 1x" src="/pr-preview/pr-140/static/images/people/jian-liao.jpg"/><span class="author-link">Jian Liao</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/adnan-karim/"><img alt="Adnan Karim picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/adnan-karim.jpg 1x" src="/pr-preview/pr-140/static/images/people/adnan-karim.jpg"/><span class="author-link">Adnan Karim</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/shivesh-jadon/"><img alt="Shivesh Jadon picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/shivesh-jadon.jpg 1x" src="/pr-preview/pr-140/static/images/people/shivesh-jadon.jpg"/><span class="author-link">Shivesh Jadon</span></a><span class="role"></span>, <span>Rubaiat Habib Kazi</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-140/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Augmented Presentation</span><span class="ui brown basic label">Natural Language Processing</span><span class="ui brown basic label">Gestural And Speech Input</span><span class="ui brown basic label">Video</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="gecco-2022-ivanov"><div class="three wide column" style="margin:auto"><img alt="gecco-2022-ivanov cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/gecco-2022-ivanov.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/gecco-2022-ivanov.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">GECCO 2022</span></p><p class="color" style="font-size:1.3em"><b>EvoIsland: Interactive Evolution via an Island-Inspired Spatial User Interface Framework</b></p><p><a href="/pr-preview/pr-140/people/sasha-ivanov/"><img alt="Sasha Ivanov picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/sasha-ivanov.jpg 1x" src="/pr-preview/pr-140/static/images/people/sasha-ivanov.jpg"/><span class="author-link">Sasha Ivanov</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span>, <span>Christian Jacob</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Interactive Evolutionary Systems</span><span class="ui brown basic label">User Interfaces</span><span class="ui brown basic label">Visualization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="gi-2022-hull"><div class="three wide column" style="margin:auto"><img alt="gi-2022-hull cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/gi-2022-hull.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/gi-2022-hull.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">GI 2022</span></p><p class="color" style="font-size:1.3em"><b>Simultaneous Worlds: Supporting Fluid Exploration of Multiple Data Sets via Physical Models</b></p><p><a href="/pr-preview/pr-140/people/carmen-hull/"><img alt="Carmen Hull picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/carmen-hull.jpg 1x" src="/pr-preview/pr-140/static/images/people/carmen-hull.jpg"/><span class="author-link">Carmen Hull</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/soren-knudsen/"><img alt="Søren Knudsen picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/soren-knudsen.jpg 1x" src="/pr-preview/pr-140/static/images/people/soren-knudsen.jpg"/><span class="author-link">Søren Knudsen</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/sheelagh-carpendale/"><img alt="Sheelagh Carpendale picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-140/static/images/people/sheelagh-carpendale.jpg"/><span class="author-link">Sheelagh Carpendale</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Interactive Surfaces</span><span class="ui brown basic label">Data Physicalization</span><span class="ui brown basic label">Architectural Models</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2022-bressa"><div class="three wide column" style="margin:auto"><img alt="chi-2022-bressa cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2022-bressa.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2022-bressa.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2022</span></p><p class="color" style="font-size:1.3em"><b>Data Every Day: Designing and Living with Personal Situated Visualizations</b></p><p><a href="/pr-preview/pr-140/people/nathalie-bressa/"><img alt="Nathalie Bressa picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/nathalie-bressa.jpg 1x" src="/pr-preview/pr-140/static/images/people/nathalie-bressa.jpg"/><span class="author-link">Nathalie Bressa</span></a><span class="role"></span>, <span>Jo Vermeulen</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Self Tracking</span><span class="ui brown basic label">Situated Visualization</span><span class="ui brown basic label">Personal Data</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2022-ivanov"><div class="three wide column" style="margin:auto"><img alt="chi-2022-ivanov cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2022-ivanov.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2022-ivanov.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2022</span></p><p class="color" style="font-size:1.3em"><b>One Week in the Future: Previs Design Futuring for HCI Research</b></p><p><a href="/pr-preview/pr-140/people/sasha-ivanov/"><img alt="Sasha Ivanov picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/sasha-ivanov.jpg 1x" src="/pr-preview/pr-140/static/images/people/sasha-ivanov.jpg"/><span class="author-link">Sasha Ivanov</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/tim-au-yeung/"><img alt="Tim Au Yeung picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/tim-au-yeung.jpg 1x" src="/pr-preview/pr-140/static/images/people/tim-au-yeung.jpg"/><span class="author-link">Tim Au Yeung</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/kathryn-blair/"><img alt="Kathryn Blair picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kathryn-blair.jpg 1x" src="/pr-preview/pr-140/static/images/people/kathryn-blair.jpg"/><span class="author-link">Kathryn Blair</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/kurtis-danyluk/"><img alt="Kurtis Danyluk picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg"/><span class="author-link">Kurtis Danyluk</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/georgina-freeman/"><img alt="Georgina Freeman picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/georgina-freeman.jpg 1x" src="/pr-preview/pr-140/static/images/people/georgina-freeman.jpg"/><span class="author-link">Georgina Freeman</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/marcus-friedel/"><img alt="Marcus Friedel picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/marcus-friedel.jpg 1x" src="/pr-preview/pr-140/static/images/people/marcus-friedel.jpg"/><span class="author-link">Marcus Friedel</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/carmen-hull/"><img alt="Carmen Hull picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/carmen-hull.jpg 1x" src="/pr-preview/pr-140/static/images/people/carmen-hull.jpg"/><span class="author-link">Carmen Hull</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/michael-hung/"><img alt="Michael Hung picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/michael-hung.jpg 1x" src="/pr-preview/pr-140/static/images/people/michael-hung.jpg"/><span class="author-link">Michael Hung</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/sydney-pratte/"><img alt="Sydney Pratte picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/sydney-pratte.jpg 1x" src="/pr-preview/pr-140/static/images/people/sydney-pratte.jpg"/><span class="author-link">Sydney Pratte</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Design Futuring</span><span class="ui brown basic label">Prototyping</span><span class="ui brown basic label">Previsualization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="ieee-2021-willett"><div class="three wide column" style="margin:auto"><img alt="ieee-2021-willett cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/ieee-2021-willett.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/ieee-2021-willett.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IEEE 2021</span><span class="ui big basic pink label"><b><svg data-prefix="fas" data-icon="trophy" class="svg-inline--fa fa-trophy" role="img" viewBox="0 0 512 512" aria-hidden="true"><path fill="currentColor" d="M144.3 0l224 0c26.5 0 48.1 21.8 47.1 48.2-.2 5.3-.4 10.6-.7 15.8l49.6 0c26.1 0 49.1 21.6 47.1 49.8-7.5 103.7-60.5 160.7-118 190.5-15.8 8.2-31.9 14.3-47.2 18.8-20.2 28.6-41.2 43.7-57.9 51.8l0 73.1 64 0c17.7 0 32 14.3 32 32s-14.3 32-32 32l-192 0c-17.7 0-32-14.3-32-32s14.3-32 32-32l64 0 0-73.1c-16-7.7-35.9-22-55.3-48.3-18.4-4.8-38.4-12.1-57.9-23.1-54.1-30.3-102.9-87.4-109.9-189.9-1.9-28.1 21-49.7 47.1-49.7l49.6 0c-.3-5.2-.5-10.4-.7-15.8-1-26.5 20.6-48.2 47.1-48.2zM101.5 112l-52.4 0c6.2 84.7 45.1 127.1 85.2 149.6-14.4-37.3-26.3-86-32.8-149.6zM380 256.8c40.5-23.8 77.1-66.1 83.3-144.8L411 112c-6.2 60.9-17.4 108.2-31 144.8z"></path></svg> Best Paper</b></span></p><p class="color" style="font-size:1.3em"><b>Perception! Immersion! Empowerment!: Superpowers as Inspiration for Visualization</b></p><p><a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/bon-adriel-aseniero/"><img alt="Bon Adriel Aseniero picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/bon-adriel-aseniero.jpg 1x" src="/pr-preview/pr-140/static/images/people/bon-adriel-aseniero.jpg"/><span class="author-link">Bon Adriel Aseniero</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/sheelagh-carpendale/"><img alt="Sheelagh Carpendale picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-140/static/images/people/sheelagh-carpendale.jpg"/><span class="author-link">Sheelagh Carpendale</span></a><span class="role"></span>, <span>Pierre Dragicevic</span><span class="role"></span>, <span>Yvonne Jansen</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-140/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/petra-isenberg/"><img alt="Petra Isenberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/petra-isenberg.jpg 1x" src="/pr-preview/pr-140/static/images/people/petra-isenberg.jpg"/><span class="author-link">Petra Isenberg</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Cognition</span><span class="ui brown basic label">Interactive Systems</span><span class="ui brown basic label">Tools</span><span class="ui brown basic label">Pragmatics</span><span class="ui brown basic label">Pattern Recognition</span><span class="ui brown basic label">Superpowers</span><span class="ui brown basic label">Empowerment</span><span class="ui brown basic label">Vision</span><span class="ui brown basic label">Perception</span><span class="ui brown basic label">Fiction</span><span class="ui brown basic label">Situated Visualization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2021-wannamaker"><div class="three wide column" style="margin:auto"><img alt="dis-2021-wannamaker cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/dis-2021-wannamaker.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/dis-2021-wannamaker.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2021</span></p><p class="color" style="font-size:1.3em"><b>I/O Bits: User-Driven, Situated, and Dedicated Self-Tracking</b></p><p><a href="/pr-preview/pr-140/people/kendra-wannamaker/"><img alt="Kendra Wannamaker picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kendra-wannamaker.jpg 1x" src="/pr-preview/pr-140/static/images/people/kendra-wannamaker.jpg"/><span class="author-link">Kendra Wannamaker</span></a><span class="role"></span>, <span>Sandeep Kollannur</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/marian-doerk/"><img alt="Marian Dörk picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/marian-doerk.jpg 1x" src="/pr-preview/pr-140/static/images/people/marian-doerk.jpg"/><span class="author-link">Marian Dörk</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Personal Informatics</span><span class="ui brown basic label">Situated Visualization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2021-danyluk"><div class="three wide column" style="margin:auto"><img alt="chi-2021-danyluk cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2021-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2021-danyluk.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2021</span></p><p class="color" style="font-size:1.3em"><b>A Design Space Exploration of Worlds in Miniature</b></p><p><a href="/pr-preview/pr-140/people/kurtis-danyluk/"><img alt="Kurtis Danyluk picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg"/><span class="author-link">Kurtis Danyluk</span></a><span class="role"></span>, <span>Barrett Ens</span><span class="role"></span>, <span>Bernhard Jenny</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Virtual Augmented Reality</span><span class="ui brown basic label">Meta Analysis Literature Survey</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2021-ens"><div class="three wide column" style="margin:auto"><img alt="chi-2021-ens cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2021-ens.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2021-ens.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2021</span></p><p class="color" style="font-size:1.3em"><b>Grand Challenges in Immersive Analytics</b></p><p><span>Barrett Ens</span><span class="role"></span>, <span>Benjamin Bach</span><span class="role"></span>, <span>Maxime Cordeil</span><span class="role"></span>, <span>Ulrich Engelke</span><span class="role"></span>, <span>Marcos Serrano</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span>, <span>Arnaud Prouzeau</span><span class="role"></span>, <span>Christoph Anthes</span><span class="role"></span>, <span>Wolfgang Büschel</span><span class="role"></span>, <span>Cody Dunne</span><span class="role"></span>, <span>Tim Dwyer</span><span class="role"></span>, <span>Jens Grubert</span><span class="role"></span>, <span>Jason H. Haga</span><span class="role"></span>, <span>Nurit Kishenbaum</span><span class="role"></span>, <span>Dylan Kobayashi</span><span class="role"></span>, <span>Tica Lin</span><span class="role"></span>, <span>Monsurat Olaosebikan</span><span class="role"></span>, <span>Fabian Pointecker</span><span class="role"></span>, <span>David Saffo</span><span class="role"></span>, <span>Nazmus Saquib</span><span class="role"></span>, <span>Dieter Schmalsteig</span><span class="role"></span>, <span>Danielle Albers Szafir</span><span class="role"></span>, <span>Matthew Whitlock</span><span class="role"></span>, <span>Yalong Yang</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Immersive Analytics</span><span class="ui brown basic label">Grand Research Challenges</span><span class="ui brown basic label">Data Visualisation</span><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Virtual Reality</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="gi-2021-mactavish"><div class="three wide column" style="margin:auto"><img alt="gi-2021-mactavish cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/gi-2021-mactavish.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/gi-2021-mactavish.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">GI 2021</span></p><p class="color" style="font-size:1.3em"><b>Perspective Charts</b></p><p><span>Mia MacTavish</span><span class="role"></span>, <span>Katayoon Etemad</span><span class="role"></span>, <span>Faramarz Samavati</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Information Visualization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="cupum-2021-rout"><div class="three wide column" style="margin:auto"><img alt="cupum-2021-rout cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/cupum-2021-rout.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/cupum-2021-rout.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Urban Informatics and Future Cities</span></p><p class="color" style="font-size:1.3em"><b>(Big) Data in Urban Design Practice: Supporting High-Level Design Tasks Using a Visualization of Human Movement Data from Smartphones</b></p><p><span>Angela Rout</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Smartphone Data</span><span class="ui brown basic label">GPS</span><span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Architecture</span><span class="ui brown basic label">Urban Design</span><span class="ui brown basic label">Task Based Framework</span><span class="ui brown basic label">High Level Tasks</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="tvcg-2020-danyluk"><div class="three wide column" style="margin:auto"><img alt="tvcg-2020-danyluk cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/tvcg-2020-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/tvcg-2020-danyluk.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TVCG 2020</span></p><p class="color" style="font-size:1.3em"><b>Touch and Beyond: Comparing Physical and Virtual Reality Visualizations</b></p><p><a href="/pr-preview/pr-140/people/kurtis-danyluk/"><img alt="Kurtis Thorvald Danyluk picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg"/><span class="author-link">Kurtis Thorvald Danyluk</span></a><span class="role"></span>, <span>Teoman Tomo Ulusoy</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/wei-wei/"><img alt="Wei Wei picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wei-wei.jpg 1x" src="/pr-preview/pr-140/static/images/people/wei-wei.jpg"/><span class="author-link">Wei Wei</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley J. Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley J. Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Physicalization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-goffin"><div class="three wide column" style="margin:auto"><img alt="chi-2020-goffin cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2020-goffin.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2020-goffin.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020</span></p><p class="color" style="font-size:1.3em"><b>Interaction Techniques for Visual Exploration Using Embedded Word-Scale Visualizations</b></p><p><span>Pascal Goffin</span><span class="role"></span>, <span>Tanja Blascheck</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/petra-isenberg/"><img alt="Petra Isenberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/petra-isenberg.jpg 1x" src="/pr-preview/pr-140/static/images/people/petra-isenberg.jpg"/><span class="author-link">Petra Isenberg</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Glyphs</span><span class="ui brown basic label">Word Scale Visualization</span><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Interaction Techniques</span><span class="ui brown basic label">Text Visualization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="tvcg-2019-walny"><div class="three wide column" style="margin:auto"><img alt="tvcg-2019-walny cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/tvcg-2019-walny.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/tvcg-2019-walny.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TVCG 2019</span><span class="ui big basic pink label"><b><svg data-prefix="fas" data-icon="trophy" class="svg-inline--fa fa-trophy" role="img" viewBox="0 0 512 512" aria-hidden="true"><path fill="currentColor" d="M144.3 0l224 0c26.5 0 48.1 21.8 47.1 48.2-.2 5.3-.4 10.6-.7 15.8l49.6 0c26.1 0 49.1 21.6 47.1 49.8-7.5 103.7-60.5 160.7-118 190.5-15.8 8.2-31.9 14.3-47.2 18.8-20.2 28.6-41.2 43.7-57.9 51.8l0 73.1 64 0c17.7 0 32 14.3 32 32s-14.3 32-32 32l-192 0c-17.7 0-32-14.3-32-32s14.3-32 32-32l64 0 0-73.1c-16-7.7-35.9-22-55.3-48.3-18.4-4.8-38.4-12.1-57.9-23.1-54.1-30.3-102.9-87.4-109.9-189.9-1.9-28.1 21-49.7 47.1-49.7l49.6 0c-.3-5.2-.5-10.4-.7-15.8-1-26.5 20.6-48.2 47.1-48.2zM101.5 112l-52.4 0c6.2 84.7 45.1 127.1 85.2 149.6-14.4-37.3-26.3-86-32.8-149.6zM380 256.8c40.5-23.8 77.1-66.1 83.3-144.8L411 112c-6.2 60.9-17.4 108.2-31 144.8z"></path></svg> Best Paper</b></span></p><p class="color" style="font-size:1.3em"><b>Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff</b></p><p><a href="/pr-preview/pr-140/people/jagoda-walny/"><img alt="Jagoda Walny picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/jagoda-walny.jpg 1x" src="/pr-preview/pr-140/static/images/people/jagoda-walny.jpg"/><span class="author-link">Jagoda Walny</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/christian-frisson/"><img alt="Christian Frisson picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-140/static/images/people/christian-frisson.jpg"/><span class="author-link">Christian Frisson</span></a><span class="role"></span>, <span>Mieka West</span><span class="role"></span>, <span>Doris Kosminsky</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/soren-knudsen/"><img alt="Søren Knudsen picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/soren-knudsen.jpg 1x" src="/pr-preview/pr-140/static/images/people/soren-knudsen.jpg"/><span class="author-link">Søren Knudsen</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/sheelagh-carpendale/"><img alt="Sheelagh Carpendale picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-140/static/images/people/sheelagh-carpendale.jpg"/><span class="author-link">Sheelagh Carpendale</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Design Handoff</span><span class="ui brown basic label">Data Mapping</span><span class="ui brown basic label">Design Process</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2019-bressa"><div class="three wide column" style="margin:auto"><img alt="dis-2019-bressa cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/dis-2019-bressa.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/dis-2019-bressa.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2019</span></p><p class="color" style="font-size:1.3em"><b>Sketching and Ideation Activities for Situated Visualization Design</b></p><p><a href="/pr-preview/pr-140/people/nathalie-bressa/"><img alt="Nathalie Bressa picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/nathalie-bressa.jpg 1x" src="/pr-preview/pr-140/static/images/people/nathalie-bressa.jpg"/><span class="author-link">Nathalie Bressa</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/kendra-wannamaker/"><img alt="Kendra Wannamaker picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kendra-wannamaker.jpg 1x" src="/pr-preview/pr-140/static/images/people/kendra-wannamaker.jpg"/><span class="author-link">Kendra Wannamaker</span></a><span class="role"></span>, <span>Henrik Korsgaard</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span>, <span>Jo Vermeulen</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Ideation</span><span class="ui brown basic label">Design Workshops</span><span class="ui brown basic label">Situated Visualization</span><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Small Displays</span><span class="ui brown basic label">Sketching</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="tvcg-2019-blascheck"><div class="three wide column" style="margin:auto"><img alt="tvcg-2019-blascheck cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/tvcg-2019-blascheck.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/tvcg-2019-blascheck.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TVCG 2019</span></p><p class="color" style="font-size:1.3em"><b>Exploration Strategies for Discovery of Interactivity in Visualizations</b></p><p><span>Tanja Blascheck</span><span class="role"></span>, <span>Lindsay MacDonald Vermeulen</span><span class="role"></span>, <span>Jo Vermeulen</span><span class="role"></span>, <span>Charles Perin</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span>, <span>Thomas Ertl</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/sheelagh-carpendale/"><img alt="Sheelagh Carpendale picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-140/static/images/people/sheelagh-carpendale.jpg"/><span class="author-link">Sheelagh Carpendale</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Discovery</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Open Data</span><span class="ui brown basic label">Evaluation</span><span class="ui brown basic label">Eye Tracking</span><span class="ui brown basic label">Interaction Logs</span><span class="ui brown basic label">Think Aloud</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2019-danyluk"><div class="three wide column" style="margin:auto"><img alt="chi-2019-danyluk cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2019-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2019-danyluk.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2019</span><span class="ui big basic pink label"><b><svg data-prefix="fas" data-icon="award" class="svg-inline--fa fa-award" role="img" viewBox="0 0 448 512" aria-hidden="true"><path fill="currentColor" d="M245.9-25.9c-13.4-8.2-30.3-8.2-43.7 0-24.4 14.9-39.5 18.9-68.1 18.3-15.7-.4-30.3 8.1-37.9 21.9-13.7 25.1-24.8 36.2-49.9 49.9-13.8 7.5-22.2 22.2-21.9 37.9 .7 28.6-3.4 43.7-18.3 68.1-8.2 13.4-8.2 30.3 0 43.7 14.9 24.4 18.9 39.5 18.3 68.1-.4 15.7 8.1 30.3 21.9 37.9 22.1 12.1 33.3 22.1 45.1 41.5L42.7 458.5c-5.9 11.9-1.1 26.3 10.7 32.2l86 43c11.5 5.7 25.5 1.4 31.7-9.8l52.8-95.1 52.8 95.1c6.2 11.2 20.2 15.6 31.7 9.8l86-43c11.9-5.9 16.7-20.3 10.7-32.2l-48.6-97.2c11.7-19.4 23-29.4 45.1-41.5 13.8-7.5 22.2-22.2 21.9-37.9-.7-28.6 3.4-43.7 18.3-68.1 8.2-13.4 8.2-30.3 0-43.7-14.9-24.4-18.9-39.5-18.3-68.1 .4-15.7-8.1-30.3-21.9-37.9-25.1-13.7-36.2-24.8-49.9-49.9-7.5-13.8-22.2-22.2-37.9-21.9-28.6 .7-43.7-3.4-68.1-18.3zM224 96a96 96 0 1 1 0 192 96 96 0 1 1 0-192z"></path></svg> Honorable Mention</b></span></p><p class="color" style="font-size:1.3em"><b>Look-From Camera Control for 3D Terrain Maps</b></p><p><a href="/pr-preview/pr-140/people/kurtis-danyluk/"><img alt="Kurtis Danyluk picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg"/><span class="author-link">Kurtis Danyluk</span></a><span class="role"></span>, <span>Bernhard Jenny</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Terrain</span><span class="ui brown basic label">Touch</span><span class="ui brown basic label">Map Interaction</span><span class="ui brown basic label">Look From Camera Control</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="vr-2019-satriadi"><div class="three wide column" style="margin:auto"><img alt="vr-2019-satriadi cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/vr-2019-satriadi.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/vr-2019-satriadi.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IEEE VR 2019</span></p><p class="color" style="font-size:1.3em"><b>Augmented Reality Map Navigation with Freehand Gestures</b></p><p><span>Kadek Ananta Satriadi</span><span class="role"></span>, <span>Barrett Ens</span><span class="role"></span>, <span>Maxime Cordeil</span><span class="role"></span>, <span>Bernhard Jenny</span><span class="role"></span>, <span>Tobias Czauderna</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Gesture Recognition</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Interactive Devices</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="cga-2019-ivanov"><div class="three wide column" style="margin:auto"><img alt="cga-2019-ivanov cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/cga-2019-ivanov.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/cga-2019-ivanov.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IEEE CG&amp;A 2019</span></p><p class="color" style="font-size:1.3em"><b>A Walk Among the Data</b></p><p><a href="/pr-preview/pr-140/people/sasha-ivanov/"><img alt="Alexander Ivanov picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/sasha-ivanov.jpg 1x" src="/pr-preview/pr-140/static/images/people/sasha-ivanov.jpg"/><span class="author-link">Alexander Ivanov</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/kurtis-danyluk/"><img alt="Kurtis Danyluk picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg"/><span class="author-link">Kurtis Danyluk</span></a><span class="role"></span>, <span>Christian Jacob</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Art</span><span class="ui brown basic label">Tools</span><span class="ui brown basic label">Virtual Environments</span><span class="ui brown basic label">Two Dimensional Displays</span><span class="ui brown basic label">Anthropomorphism</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="sui-2017-li"><div class="three wide column" style="margin:auto"><img alt="sui-2017-li cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/sui-2017-li.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/sui-2017-li.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">SUI 2017</span></p><p class="color" style="font-size:1.3em"><b>Visibility Perception and Dynamic Viewsheds for Topographic Maps and Models</b></p><p><span>Nico Li</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-140/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"></span>, <span>Mario Costa Sousa</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Terrain Visualization</span><span class="ui brown basic label">Geospatial Visualization</span><span class="ui brown basic label">Dynamic Viewshed</span><span class="ui brown basic label">Topographic Maps</span><span class="ui brown basic label">Tangible User Interfaces</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2017-aoki"><div class="three wide column" style="margin:auto"><img alt="chi-2017-aoki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2017-aoki.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2017-aoki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2017</span></p><p class="color" style="font-size:1.3em"><b>Environmental Protection and Agency: Motivations, Capacity, and Goals in Participatory Sensing</b></p><p><span>Paul Aoki</span><span class="role"></span>, <span>Allison Woodruff</span><span class="role"></span>, <span>Baladitya Yellapragada</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Citizen Science</span><span class="ui brown basic label">Environmental Sensing</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2017-hull"><div class="three wide column" style="margin:auto"><img alt="chi-2017-hull cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2017-hull.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2017-hull.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2017</span></p><p class="color" style="font-size:1.3em"><b>Building with Data: Architectural Models as Inspiration for Data Physicalization</b></p><p><a href="/pr-preview/pr-140/people/carmen-hull/"><img alt="Carmen Hull picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/carmen-hull.jpg 1x" src="/pr-preview/pr-140/static/images/people/carmen-hull.jpg"/><span class="author-link">Carmen Hull</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Design Process</span><span class="ui brown basic label">Architectural Models</span><span class="ui brown basic label">Data Physicalization</span><span class="ui brown basic label">Embodied Interaction</span><span class="ui brown basic label">Data Visualization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="tvcg-2017-goffin"><div class="three wide column" style="margin:auto"><img alt="tvcg-2017-goffin cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/tvcg-2017-goffin.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/tvcg-2017-goffin.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TVCG 2017</span></p><p class="color" style="font-size:1.3em"><b>An Exploratory Study of Word-Scale Graphics in Data-Rich Text Documents</b></p><p><span>Pascal Goffin</span><span class="role"></span>, <span>Jeremy Boy</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/petra-isenberg/"><img alt="Petra Isenberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/petra-isenberg.jpg 1x" src="/pr-preview/pr-140/static/images/people/petra-isenberg.jpg"/><span class="author-link">Petra Isenberg</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Word Scale Visualization</span><span class="ui brown basic label">Word Scale Graphic</span><span class="ui brown basic label">Text Visualization</span><span class="ui brown basic label">Sparklines</span><span class="ui brown basic label">Authoring Tool</span><span class="ui brown basic label">Information Visualization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="tvcg-2017-willett"><div class="three wide column" style="margin:auto"><img alt="tvcg-2017-willett cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/tvcg-2017-willett.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/tvcg-2017-willett.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TVCG 2017</span></p><p class="color" style="font-size:1.3em"><b>Embedded Data Representations</b></p><p><a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span>, <span>Yvonne Jansen</span><span class="role"></span>, <span>Pierre Dragicevic</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Data Physicalization</span><span class="ui brown basic label">Ambient Displays</span><span class="ui brown basic label">Ubiquitous Computing</span><span class="ui brown basic label">Augmented Reality</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2015-oehlberg"><div class="three wide column" style="margin:auto"><img alt="chi-2015-oehlberg cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2015-oehlberg.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2015-oehlberg.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2015</span></p><p class="color" style="font-size:1.3em"><b>Patterns of Physical Design Remixing in Online Maker Communities</b></p><p><a href="/pr-preview/pr-140/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-140/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"></span>, <a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span>, <span>Wendy E. Mackay</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Customization</span><span class="ui brown basic label">Maker Communities</span><span class="ui brown basic label">User Innovation</span><span class="ui brown basic label">Collaboration</span><span class="ui brown basic label">Hacking</span><span class="ui brown basic label">Remixing</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2015-willett"><div class="three wide column" style="margin:auto"><img alt="chi-2015-willett cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2015-willett.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2015-willett.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2015</span></p><p class="color" style="font-size:1.3em"><b>Lightweight Relief Shearing for Enhanced Terrain Perception on Interactive Maps</b></p><p><a href="/pr-preview/pr-140/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span>, <span>Bernhard Jenny</span><span class="role"></span>, <a href="/pr-preview/pr-140/people/tobias-isenberg/"><img alt="Tobias Isenberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/no-profile-2.jpg 1x" src="/pr-preview/pr-140/static/images/people/no-profile-2.jpg"/><span class="author-link">Tobias Isenberg</span></a><span class="role"></span>, <span>Pierre Dragicevic</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Plan Oblique Relief</span><span class="ui brown basic label">Interaction</span><span class="ui brown basic label">Depth Perception</span><span class="ui brown basic label">Terrain Maps</span><span class="ui brown basic label">Relief Shearing</span></div></div></div></div></div><div id="publications-modal"><div id="dis-2024-danyluk" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/dis-2024-danyluk/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2024-danyluk</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2024-danyluk cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/dis-2024-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/dis-2024-danyluk.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2024-danyluk" target="_blank">Understanding Gesture and Microgesture Inputs for Augmented Reality Maps</a></h1><p class="meta"><a href="/people/kurtis-danyluk"><img alt="kurtis-danyluk photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg"/><strong>Kurtis Danyluk</strong></a><span class="role"></span>, <span>Simon Klueber<!-- --> <span class="role"></span></span>, <a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-140/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a><span class="role"></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>We explore the potential for subtle on-hand gesture and microgesture interactions for map navigation with augmented reality (AR) devices. We describe a design exercise and follow-up elicitation study in which we identified on-hand gestures for cartographic interaction primitives. Microgestures and on-hand interactions are a promising space for AR map navigation as they offers always-available, tactile, and memorable spaces for interaction. Our findings show a clear set of microgesture interaction patterns that are well suited for supporting map navigation and manipulation. In particular, we highlight how the properties of various microgestures align with particular cartographic interaction tasks. We also describe our experience creating an exploratory proof-of-concept AR map prototype which helped us identify new opportunities and practical challenges for microgesture control. Finally, we discuss how future AR map systems could benefit from on-hand and microgesture input schemes.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Gestural Input</span><span class="ui brown basic label">Microgestures</span><span class="ui brown basic label">AR</span><span class="ui brown basic label">Maps</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kurtis Danyluk<!-- -->, <!-- -->Simon Klueber<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Understanding Gesture and Microgesture Inputs for Augmented Reality Maps</b>. <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- --> <!-- -->(<!-- -->DIS 2024<!-- -->)</i>ACM, New York, NY, USA<!-- --> <!-- -->DOI: <a href="https://doi.org/10.1145/3643834.3661630" target="_blank">https://doi.org/10.1145/3643834.3661630</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2024-bressa" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/chi-2024-bressa/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2024-bressa</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2024-bressa cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2024-bressa.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2024-bressa.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2024-bressa" target="_blank">Input Visualization: Collecting and Modifying Data with Visual Representations</a></h1><p class="meta"><a href="/people/nathalie-bressa"><img alt="nathalie-bressa photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/nathalie-bressa.jpg 1x" src="/pr-preview/pr-140/static/images/people/nathalie-bressa.jpg"/><strong>Nathalie Bressa</strong></a><span class="role"></span>, <span>Jordan Louis<!-- --> <span class="role"></span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span>, <span>Samuel Huron<!-- --> <span class="role"></span></span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/RAfv2quE6nA" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/RAfv2quE6nA?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/RAfv2quE6nA/maxresdefault.jpg src=https://img.youtube.com/vi/RAfv2quE6nA/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We examine input visualizations, visual representations that are designed to collect (and represent) new data rather than encode preexisting datasets. Information visualization is commonly used to reveal insights and stories within existing data. As a result, most contemporary visualization approaches assume existing datasets as the starting point for design, through which that data is mapped to visual encodings. Meanwhile, the implications of visualizations as inputs and as data sources have received little attention—despite the existence of visual and physical examples stretching back centuries. In this paper, we present a design space of 50 input visualizations analyzing their visual representation, data, artifact, context, and input. Based on this, we identify input modalities, purposes of input visualizations, and a set of design considerations. Finally, we discuss the relationship between input visualization and traditional visualization design and suggest opportunities for future research to better understand these visual representations and their potential.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Input Visualization</span><span class="ui brown basic label">Data Physicalization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nathalie Bressa<!-- -->, <!-- -->Jordan Louis<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Samuel Huron<!-- -->. <b>Input Visualization: Collecting and Modifying Data with Visual Representations</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2024<!-- -->)</i>ACM, New York, NY, USA<!-- --> <!-- -->DOI: <a href="https://doi.org/10.1145/3613904.3642808" target="_blank">https://doi.org/10.1145/3613904.3642808</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2024-dhawka" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/chi-2024-dhawka/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2024-dhawka</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2024-dhawka cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2024-dhawka.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2024-dhawka.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2024-dhawka" target="_blank">Better Little People Pictures: Generative Creation of Demographically Diverse Anthropographics</a></h1><p class="meta"><a href="/people/priya-dhawka"><img alt="priya-dhawka photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/priya-dhawka.jpg 1x" src="/pr-preview/pr-140/static/images/people/priya-dhawka.jpg"/><strong>Priya Dhawka</strong></a><span class="role"></span>, <span>Lauren Perera<!-- --> <span class="role"></span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/dCEFvx4AqIo" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/dCEFvx4AqIo?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/dCEFvx4AqIo/maxresdefault.jpg src=https://img.youtube.com/vi/dCEFvx4AqIo/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We explore the potential of generative AI text-to-image models to help designers efficiently craft unique, representative, and demographically diverse anthropographics that visualize data about people. Currently, creating data-driven iconic images to represent individuals in a dataset often requires considerable design effort. Generative text-to-image models can streamline the process of creating these images, but risk perpetuating designer biases in addition to stereotypes latent in the models. In response, we outline a conceptual workflow for crafting anthropographic assets for visualizations, highlighting possible sources of risk and bias as well as opportunities for reflection and refinement by a human designer. Using an implementation of this workflow with Stable Diffusion and Google Colab, we illustrate a variety of new anthropographic designs that showcase the visual expressiveness and scalability of these generative approaches. Based on our experiments, we also identify challenges and research opportunities for new AI-enabled anthropographic visualization tools.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Anthropographics</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Marginalized Populations</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Priya Dhawka<!-- -->, <!-- -->Lauren Perera<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Better Little People Pictures: Generative Creation of Demographically Diverse Anthropographics</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2024<!-- -->)</i>ACM, New York, NY, USA<!-- --> <!-- -->DOI: <a href="https://doi.org/10.1145/3613904.3641957" target="_blank">https://doi.org/10.1145/3613904.3641957</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2023-chulpongsatorn" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/uist-2023-chulpongsatorn/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2023-chulpongsatorn</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2023-chulpongsatorn cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/uist-2023-chulpongsatorn.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/uist-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2023-chulpongsatorn" target="_blank">Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks</a></h1><p class="meta"><a href="/people/neil-chulpongsatorn"><img alt="neil-chulpongsatorn photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-140/static/images/people/neil-chulpongsatorn.jpg"/><strong>Neil Chulpongsatorn</strong></a><span class="role"></span>, <a href="/people/mille-skovhus-lunding"><img alt="mille-skovhus-lunding photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/mille-skovhus-lunding.jpg 1x" src="/pr-preview/pr-140/static/images/people/mille-skovhus-lunding.jpg"/><strong>Mille Skovhus Lunding</strong></a><span class="role"></span>, <a href="/people/nishan-soni"><img alt="nishan-soni photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/nishan-soni.jpg 1x" src="/pr-preview/pr-140/static/images/people/nishan-soni.jpg"/><strong>Nishan Soni</strong></a><span class="role"></span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-140/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2023-chulpongsatorn.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2023-chulpongsatorn.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Zv6JQ5T-qn0" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Zv6JQ5T-qn0?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/Zv6JQ5T-qn0/maxresdefault.jpg src=https://img.youtube.com/vi/Zv6JQ5T-qn0/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce Augmented Math, a machine learning-based approach to authoring AR explorable explanations by augmenting static math textbooks without programming. To augment a static document, our system first extracts mathematical formulas and figures from a given document using optical character recognition (OCR) and computer vision. By binding and manipulating these extracted contents, the user can see the interactive animation overlaid onto the document through mobile AR interfaces. This empowers non-technical users, such as teachers or students, to transform existing math textbooks and handouts into on-demand and personalized explorable explanations. To design our system, we first analyzed existing explorable math explanations to identify common design strategies. Based on the findings, we developed a set of augmentation techniques that can be automatically generated based on the extracted content, which are 1) dynamic values, 2) interactive figures, 3) relationship highlights, 4) concrete examples, and 5) step-by-step hints. To evaluate our system, we conduct two user studies: preliminary user testing and expert interviews. The study results confirm that our system allows more engaging experiences for learning math concepts.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Authoring Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Neil Chulpongsatorn<!-- -->, <!-- -->Mille Skovhus Lunding<!-- -->, <!-- -->Nishan Soni<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks</b>. <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- --> <!-- -->(<!-- -->UIST 2023<!-- -->)</i>ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->16<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3586183.3606827" target="_blank">https://doi.org/10.1145/3586183.3606827</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2023-dhawka" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/chi-2023-dhawka/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2023-dhawka</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2023-dhawka cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2023-dhawka.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2023-dhawka.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2023-dhawka" target="_blank">We are the Data: Challenges and Opportunities for Creating Demographically Diverse Anthropographics</a></h1><p class="meta"><a href="/people/priya-dhawka"><img alt="priya-dhawka photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/priya-dhawka.jpg 1x" src="/pr-preview/pr-140/static/images/people/priya-dhawka.jpg"/><strong>Priya Dhawka</strong></a><span class="role"></span>, <a href="/people/helen-ai-he"><img alt="helen-ai-he photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/helen-ai-he.jpg 1x" src="/pr-preview/pr-140/static/images/people/helen-ai-he.jpg"/><strong>Helen Ai He</strong></a><span class="role"></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2023-dhawka.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2023-dhawka.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/iBzv2jS3ECM" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/iBzv2jS3ECM?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/iBzv2jS3ECM/maxresdefault.jpg src=https://img.youtube.com/vi/iBzv2jS3ECM/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Anthropographics are human-shaped visualizations that aim to emphasize the human importance of datasets and the people behind them. However, current anthropographics tend to employ homogeneous human shapes to encode data about diverse demographic groups. Such anthropographics can obscure important differences between groups and contemporary designs exemplify the lack of inclusive approaches for representing human diversity in visualizations. In response, we explore the creation of demographically diverse anthropographics that communicate the visible diversity of demographically distinct populations. Building on previous anthropographics research, we explore strategies for visualizing datasets about people in ways that explicitly encode diversity—illustrating these approaches with examples in a variety of visual styles. We also critically reflect on strategies for creating diverse anthropographics, identifying social and technical challenges that can result in harmful representations. Finally, we highlight a set of forward-looking research opportunities for advancing the design and understanding of diverse anthropographics.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Anthropographics</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Marginalized Populations</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Priya Dhawka<!-- -->, <!-- -->Helen Ai He<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>We are the Data: Challenges and Opportunities for Creating Demographically Diverse Anthropographics</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2023<!-- -->)</i>ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->14<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3544548.3581086" target="_blank">https://doi.org/10.1145/3544548.3581086</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2023-monteiro" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/chi-2023-monteiro/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2023-monteiro</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2023-monteiro cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2023-monteiro.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2023-monteiro.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2023-monteiro" target="_blank">Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching</a></h1><p class="meta"><a href="/people/kyzyl-monteiro"><img alt="kyzyl-monteiro photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kyzyl-monteiro.jpg 1x" src="/pr-preview/pr-140/static/images/people/kyzyl-monteiro.jpg"/><strong>Kyzyl Monteiro</strong></a><span class="role"></span>, <a href="/people/ritik-vatsal"><img alt="ritik-vatsal photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/ritik-vatsal.jpg 1x" src="/pr-preview/pr-140/static/images/people/ritik-vatsal.jpg"/><strong>Ritik Vatsal</strong></a><span class="role"></span>, <a href="/people/neil-chulpongsatorn"><img alt="neil-chulpongsatorn photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-140/static/images/people/neil-chulpongsatorn.jpg"/><strong>Neil Chulpongsatorn</strong></a><span class="role"></span>, <span>Aman Parnami<!-- --> <span class="role"></span></span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-140/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2023-monteiro.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2023-monteiro.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/JssiyfrhIJw" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/JssiyfrhIJw?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/JssiyfrhIJw/maxresdefault.jpg src=https://img.youtube.com/vi/JssiyfrhIJw/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces Teachable Reality, an augmented reality (AR) prototyping tool for creating interactive tangible AR applications with arbitrary everyday objects. Teachable Reality leverages vision-based interactive machine teaching (e.g., Teachable Machine), which captures real-world interactions for AR prototyping. It identifies the user-defined tangible and gestural interactions using an on-demand computer vision model. Based on this, the user can easily create functional AR prototypes without programming, enabled by a trigger-action authoring interface. Therefore, our approach allows the flexibility, customizability, and generalizability of tangible AR applications that can address the limitation of current marker-based approaches. We explore the design space and demonstrate various AR prototypes, which include tangible and deformable interfaces, context-aware assistants, and body-driven AR applications. The results of our user study and expert interviews confirm that our approach can lower the barrier to creating functional AR prototypes while also allowing flexible and general-purpose prototyping experiences.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Prototyping Tools</span><span class="ui brown basic label">Tangible Interactions</span><span class="ui brown basic label">Everyday Objects</span><span class="ui brown basic label">Interactive Machine Teaching</span><span class="ui brown basic label">Human Centered Machine Learning</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kyzyl Monteiro<!-- -->, <!-- -->Ritik Vatsal<!-- -->, <!-- -->Neil Chulpongsatorn<!-- -->, <!-- -->Aman Parnami<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2023<!-- -->)</i>ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->15<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3544548.3581449" target="_blank">https://doi.org/10.1145/3544548.3581449</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-ea-2023-chulpongsatorn" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/chi-ea-2023-chulpongsatorn/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-ea-2023-chulpongsatorn</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI EA 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-ea-2023-chulpongsatorn cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-ea-2023-chulpongsatorn.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-ea-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-ea-2023-chulpongsatorn" target="_blank">HoloTouch: Interacting with Mixed Reality Visualizations Through Smartphone Proxies</a></h1><p class="meta"><a href="/people/neil-chulpongsatorn"><img alt="neil-chulpongsatorn photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-140/static/images/people/neil-chulpongsatorn.jpg"/><strong>Neil Chulpongsatorn</strong></a><span class="role"></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-140/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-ea-2023-chulpongsatorn.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-ea-2023-chulpongsatorn.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We contribute interaction techniques for augmenting mixed reality (MR) visualizations with smartphone proxies. By combining head-mounted displays (HMDs) with mobile touchscreens, we can augment low-resolution holographic 3D charts with precise touch input, haptics feedback, high-resolution 2D graphics, and physical manipulation. Our approach aims to complement both MR and physical visualizations. Most current MR visualizations suffer from unreliable tracking, low visual resolution, and imprecise input. Data physicalizations on the other hand, although allowing for natural physical manipulation, are limited in dynamic and interactive modification. We demonstrate how mobile devices such as smartphones or tablets can serve as physical proxies for MR data interactions, creating dynamic visualizations that support precise manipulation and rich input and output. We describe 6 interaction techniques that leverage the combined physicality, sensing, and output capabilities of HMDs and smartphones, and demonstrate those interactions via a prototype system. Based on an evaluation, we outline opportunities for combining the advantages of both MR and physical charts.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Embedded Data Visualization</span><span class="ui brown basic label">Tangible Interaction</span><span class="ui brown basic label">Cross Device Interaction</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Neil Chulpongsatorn<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>HoloTouch: Interacting with Mixed Reality Visualizations Through Smartphone Proxies</b>. <i>In <!-- -->Extended Abstracts of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI EA 2023<!-- -->)</i>ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->8<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3544549.3585738" target="_blank">https://doi.org/10.1145/3544549.3585738</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2022-liao" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/uist-2022-liao/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2022-liao</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2022-liao cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/uist-2022-liao.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/uist-2022-liao.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2022-liao" target="_blank">RealityTalk: Real-time Speech-driven Augmented Presentation for AR Live Storytelling</a></h1><p class="meta"><a href="/people/jian-liao"><img alt="jian-liao photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/jian-liao.jpg 1x" src="/pr-preview/pr-140/static/images/people/jian-liao.jpg"/><strong>Jian Liao</strong></a><span class="role"></span>, <a href="/people/adnan-karim"><img alt="adnan-karim photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/adnan-karim.jpg 1x" src="/pr-preview/pr-140/static/images/people/adnan-karim.jpg"/><strong>Adnan Karim</strong></a><span class="role"></span>, <a href="/people/shivesh-jadon"><img alt="shivesh-jadon photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/shivesh-jadon.jpg 1x" src="/pr-preview/pr-140/static/images/people/shivesh-jadon.jpg"/><strong>Shivesh Jadon</strong></a><span class="role"></span>, <span>Rubaiat Habib Kazi<!-- --> <span class="role"></span></span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-140/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2022-liao.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2022-liao.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/vfIMeICV-7c" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/vfIMeICV-7c?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/vfIMeICV-7c/maxresdefault.jpg src=https://img.youtube.com/vi/vfIMeICV-7c/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present RealityTalk, a system that augments real-time live presentations with speech-driven interactive virtual elements. Augmented presentations leverage embedded visuals and animation for engaging and expressive storytelling. However, existing tools for live presentations often lack interactivity and improvisation, while creating such effects in video editing tools require significant time and expertise. RealityTalk enables users to create live augmented presentations with real-time speech-driven interactions. The user can interactively prompt, move, and manipulate graphical elements through real-time speech and supporting modalities. Based on our analysis of 177 existing video-edited augmented presentations, we propose a novel set of interaction techniques and then incorporated them into RealityTalk. We evaluate our tool from a presenter’s perspective to demonstrate the effectiveness of our system.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Augmented Presentation</span><span class="ui brown basic label">Natural Language Processing</span><span class="ui brown basic label">Gestural And Speech Input</span><span class="ui brown basic label">Video</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jian Liao<!-- -->, <!-- -->Adnan Karim<!-- -->, <!-- -->Shivesh Jadon<!-- -->, <!-- -->Rubaiat Habib Kazi<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>RealityTalk: Real-time Speech-driven Augmented Presentation for AR Live Storytelling</b>. <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- --> <!-- -->(<!-- -->UIST 2022<!-- -->)</i>ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->12<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3526113.3545702" target="_blank">https://doi.org/10.1145/3526113.3545702</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="gecco-2022-ivanov" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/gecco-2022-ivanov/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>gecco-2022-ivanov</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">GECCO 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="gecco-2022-ivanov cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/gecco-2022-ivanov.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/gecco-2022-ivanov.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/gecco-2022-ivanov" target="_blank">EvoIsland: Interactive Evolution via an Island-Inspired Spatial User Interface Framework</a></h1><p class="meta"><a href="/people/sasha-ivanov"><img alt="sasha-ivanov photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/sasha-ivanov.jpg 1x" src="/pr-preview/pr-140/static/images/people/sasha-ivanov.jpg"/><strong>Sasha Ivanov</strong></a><span class="role"></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span>, <span>Christian Jacob<!-- --> <span class="role"></span></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/gecco-2022-ivanov.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>gecco-2022-ivanov.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present EvoIsland, a scalable interactive evolutionary user interface framework inspired by the spatially isolated land masses seen on Earth. Our generalizable interaction system encourages creators to spatially explore a wide range of design possibilities through the combination, separation, and rearrangement of hexagonal tiles on a grid. As these tiles are grouped into islandlike clusters, localized populations of designs form through an underlying evolutionary system. The interactions that take place within EvoIsland provide content creators with new ways to shape, display and assess populations in evolutionary systems that produce a wide range of solutions with visual phenotype outputs.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Interactive Evolutionary Systems</span><span class="ui brown basic label">User Interfaces</span><span class="ui brown basic label">Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sasha Ivanov<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Christian Jacob<!-- -->. <b>EvoIsland: Interactive Evolution via an Island-Inspired Spatial User Interface Framework</b>. <i>(<!-- -->GECCO 2022<!-- -->)</i> <!-- -->Page: 1-<!-- -->8<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3512290.3528722" target="_blank">https://doi.org/10.1145/3512290.3528722</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="gi-2022-hull" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/gi-2022-hull/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>gi-2022-hull</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">GI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="gi-2022-hull cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/gi-2022-hull.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/gi-2022-hull.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/gi-2022-hull" target="_blank">Simultaneous Worlds: Supporting Fluid Exploration of Multiple Data Sets via Physical Models</a></h1><p class="meta"><a href="/people/carmen-hull"><img alt="carmen-hull photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/carmen-hull.jpg 1x" src="/pr-preview/pr-140/static/images/people/carmen-hull.jpg"/><strong>Carmen Hull</strong></a><span class="role"></span>, <a href="/people/soren-knudsen"><img alt="soren-knudsen photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/soren-knudsen.jpg 1x" src="/pr-preview/pr-140/static/images/people/soren-knudsen.jpg"/><strong>Søren Knudsen</strong></a><span class="role"></span>, <a href="/people/sheelagh-carpendale"><img alt="sheelagh-carpendale photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-140/static/images/people/sheelagh-carpendale.jpg"/><strong>Sheelagh Carpendale</strong></a><span class="role"></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/gi-2022-hull.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>gi-2022-hull.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We take the well-established use of physical scale models in architecture and identify new opportunities for using them to interactively visualize and examine multiple streams of geospatial data. Overlaying, comparing, or integrating visualizations of complementary data sets in the same physical space is often challenging given the constraints of various data types and the limited design space of possible visual encodings. Our vision of “simultaneous worlds” uses physical models as a substrate upon which visualizations of multiple data streams can be dynamically and concurrently integrated. To explore the potential of this concept, we created three design explorations that use an illuminated campus model to integrate visualizations about building energy use, climate, and movement paths on a university campus. We use a research through design approach, documenting how our interdisciplinary collaborations with domain experts, students, and architects informed our designs. Based on our observations, we characterize the benefits of models for 1) situating visualizations, 2) composing visualizations, and 3) manipulating and authoring visualizations. Our work highlights the potential of physical models to support embodied exploration of spatial and non-spatial visualizations through fluid interactions.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Interactive Surfaces</span><span class="ui brown basic label">Data Physicalization</span><span class="ui brown basic label">Architectural Models</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Carmen Hull<!-- -->, <!-- -->Søren Knudsen<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Simultaneous Worlds: Supporting Fluid Exploration of Multiple Data Sets via Physical Models</b>. <i>(<!-- -->GI 2022<!-- -->)</i> <!-- -->Page: 1-<!-- -->10<!-- -->. <!-- -->DOI: <a href="http://hdl.handle.net/1880/114742" target="_blank">http://hdl.handle.net/1880/114742</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2022-bressa" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/chi-2022-bressa/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2022-bressa</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2022-bressa cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2022-bressa.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2022-bressa.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2022-bressa" target="_blank">Data Every Day: Designing and Living with Personal Situated Visualizations</a></h1><p class="meta"><a href="/people/nathalie-bressa"><img alt="nathalie-bressa photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/nathalie-bressa.jpg 1x" src="/pr-preview/pr-140/static/images/people/nathalie-bressa.jpg"/><strong>Nathalie Bressa</strong></a><span class="role"></span>, <span>Jo Vermeulen<!-- --> <span class="role"></span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2022-bressa.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2022-bressa.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/B0bKMgDd1xY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/B0bKMgDd1xY?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/B0bKMgDd1xY/maxresdefault.jpg src=https://img.youtube.com/vi/B0bKMgDd1xY/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We explore the design and utility of situated manual self-tracking visualizations on dedicated displays that integrate data tracking into existing practices and physical environments. Situating self-tracking tools in relevant locations is a promising approach to enable reflection on and awareness of data without needing to rely on sensorized tracking or personal devices. In both a long-term autobiographical design process and a co-design study with six participants, we rapidly prototyped and deployed 30 situated self-tracking applications over a ten month period. Grounded in the experience of designing and living with these trackers, we contribute findings on logging and data entry, the use of situated displays, and the visual design and customization of trackers. Our results demonstrate the potential of customizable dedicated self-tracking visualizations that are situated in relevant physical spaces, and suggest future research opportunities and new potential applications for situated visualizations.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Self Tracking</span><span class="ui brown basic label">Situated Visualization</span><span class="ui brown basic label">Personal Data</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nathalie Bressa<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Data Every Day: Designing and Living with Personal Situated Visualizations</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2022<!-- -->)</i>ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->18<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3491102.3517737" target="_blank">https://doi.org/10.1145/3491102.3517737</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2022-ivanov" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/chi-2022-ivanov/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2022-ivanov</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2022-ivanov cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2022-ivanov.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2022-ivanov.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2022-ivanov" target="_blank">One Week in the Future: Previs Design Futuring for HCI Research</a></h1><p class="meta"><a href="/people/sasha-ivanov"><img alt="sasha-ivanov photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/sasha-ivanov.jpg 1x" src="/pr-preview/pr-140/static/images/people/sasha-ivanov.jpg"/><strong>Sasha Ivanov</strong></a><span class="role"></span>, <a href="/people/tim-au-yeung"><img alt="tim-au-yeung photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/tim-au-yeung.jpg 1x" src="/pr-preview/pr-140/static/images/people/tim-au-yeung.jpg"/><strong>Tim Au Yeung</strong></a><span class="role"></span>, <a href="/people/kathryn-blair"><img alt="kathryn-blair photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kathryn-blair.jpg 1x" src="/pr-preview/pr-140/static/images/people/kathryn-blair.jpg"/><strong>Kathryn Blair</strong></a><span class="role"></span>, <a href="/people/kurtis-danyluk"><img alt="kurtis-danyluk photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg"/><strong>Kurtis Danyluk</strong></a><span class="role"></span>, <a href="/people/georgina-freeman"><img alt="georgina-freeman photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/georgina-freeman.jpg 1x" src="/pr-preview/pr-140/static/images/people/georgina-freeman.jpg"/><strong>Georgina Freeman</strong></a><span class="role"></span>, <a href="/people/marcus-friedel"><img alt="marcus-friedel photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/marcus-friedel.jpg 1x" src="/pr-preview/pr-140/static/images/people/marcus-friedel.jpg"/><strong>Marcus Friedel</strong></a><span class="role"></span>, <a href="/people/carmen-hull"><img alt="carmen-hull photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/carmen-hull.jpg 1x" src="/pr-preview/pr-140/static/images/people/carmen-hull.jpg"/><strong>Carmen Hull</strong></a><span class="role"></span>, <a href="/people/michael-hung"><img alt="michael-hung photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/michael-hung.jpg 1x" src="/pr-preview/pr-140/static/images/people/michael-hung.jpg"/><strong>Michael Hung</strong></a><span class="role"></span>, <a href="/people/sydney-pratte"><img alt="sydney-pratte photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/sydney-pratte.jpg 1x" src="/pr-preview/pr-140/static/images/people/sydney-pratte.jpg"/><strong>Sydney Pratte</strong></a><span class="role"></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2022-ivanov.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2022-ivanov.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/qoIwYW83iSU" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/qoIwYW83iSU?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/qoIwYW83iSU/maxresdefault.jpg src=https://img.youtube.com/vi/qoIwYW83iSU/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We explore the use of cinematic “pre-visualization” (previs) techniques as a rapid ideation and design futuring method for human computer interaction (HCI) research. Previs approaches, which are widely used in animation and film production, use digital design tools to create medium-fidelity videos that capture richer interaction, motion, and context than sketches or static illustrations. When used as a design futuring method, previs can facilitate rapid, iterative discussions that reveal tensions, challenges, and opportunities for new research. We performed eight one-week design futuring sprints, in which individual HCI researchers collaborated with a lead designer to produce concept sketches, storyboards, and videos that examined future applications of their research. From these experiences, we identify recurring themes and challenges and present a One Week Futuring Workbook that other researchers can use to guide their own futuring sprints. We also highlight how variations of our approach could support other speculative design practices.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Design Futuring</span><span class="ui brown basic label">Prototyping</span><span class="ui brown basic label">Previsualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sasha Ivanov<!-- -->, <!-- -->Tim Au Yeung<!-- -->, <!-- -->Kathryn Blair<!-- -->, <!-- -->Kurtis Danyluk<!-- -->, <!-- -->Georgina Freeman<!-- -->, <!-- -->Marcus Friedel<!-- -->, <!-- -->Carmen Hull<!-- -->, <!-- -->Michael Hung<!-- -->, <!-- -->Sydney Pratte<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>One Week in the Future: Previs Design Futuring for HCI Research</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2022<!-- -->)</i>ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->15<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3491102.3517584" target="_blank">https://doi.org/10.1145/3491102.3517584</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="ieee-2021-willett" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/ieee-2021-willett/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>ieee-2021-willett</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">IEEE 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="ieee-2021-willett cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/ieee-2021-willett.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/ieee-2021-willett.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/ieee-2021-willett" target="_blank">Perception! Immersion! Empowerment!: Superpowers as Inspiration for Visualization</a></h1><p class="meta"><a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span>, <a href="/people/bon-adriel-aseniero"><img alt="bon-adriel-aseniero photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/bon-adriel-aseniero.jpg 1x" src="/pr-preview/pr-140/static/images/people/bon-adriel-aseniero.jpg"/><strong>Bon Adriel Aseniero</strong></a><span class="role"></span>, <a href="/people/sheelagh-carpendale"><img alt="sheelagh-carpendale photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-140/static/images/people/sheelagh-carpendale.jpg"/><strong>Sheelagh Carpendale</strong></a><span class="role"></span>, <span>Pierre Dragicevic<!-- --> <span class="role"></span></span>, <span>Yvonne Jansen<!-- --> <span class="role"></span></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-140/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"></span>, <a href="/people/petra-isenberg"><img alt="petra-isenberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/petra-isenberg.jpg 1x" src="/pr-preview/pr-140/static/images/people/petra-isenberg.jpg"/><strong>Petra Isenberg</strong></a><span class="role"></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>We explore how the lens of fictional superpowers can help characterize how visualizations empower people and provide inspiration for new visualization systems. Researchers and practitioners often tout visualizations&#x27; ability to “make the invisible visible” and to “enhance cognitive abilities.” Meanwhile superhero comics and other modern fiction often depict characters with similarly fantastic abilities that allow them to see and interpret the world in ways that transcend traditional human perception. We investigate the intersection of these domains, and show how the language of superpowers can be used to characterize existing visualization systems and suggest opportunities for new and empowering ones. We introduce two frameworks: The first characterizes seven underlying mechanisms that form the basis for a variety of visual superpowers portrayed in fiction. The second identifies seven ways in which visualization tools and interfaces can instill a sense of empowerment in the people who use them. Building on these observations, we illustrate a diverse set of “visualization superpowers” and highlight opportunities for the visualization community to create new systems and interactions that empower new experiences with data. Material and illustrations are available under CC-BY 4.0 at osf.io/8yhfz.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Cognition</span><span class="ui brown basic label">Interactive Systems</span><span class="ui brown basic label">Tools</span><span class="ui brown basic label">Pragmatics</span><span class="ui brown basic label">Pattern Recognition</span><span class="ui brown basic label">Superpowers</span><span class="ui brown basic label">Empowerment</span><span class="ui brown basic label">Vision</span><span class="ui brown basic label">Perception</span><span class="ui brown basic label">Fiction</span><span class="ui brown basic label">Situated Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Wesley Willett<!-- -->, <!-- -->Bon Adriel Aseniero<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->, <!-- -->Pierre Dragicevic<!-- -->, <!-- -->Yvonne Jansen<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Petra Isenberg<!-- -->. <b>Perception! Immersion! Empowerment!: Superpowers as Inspiration for Visualization</b>. <i>(<!-- -->IEEE 2021<!-- -->)</i> <!-- -->Page: 1-<!-- -->11<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1109/TVCG.2021.3114844" target="_blank">10.1109/TVCG.2021.3114844</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2021-wannamaker" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/dis-2021-wannamaker/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2021-wannamaker</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2021-wannamaker cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/dis-2021-wannamaker.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/dis-2021-wannamaker.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2021-wannamaker" target="_blank">I/O Bits: User-Driven, Situated, and Dedicated Self-Tracking</a></h1><p class="meta"><a href="/people/kendra-wannamaker"><img alt="kendra-wannamaker photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kendra-wannamaker.jpg 1x" src="/pr-preview/pr-140/static/images/people/kendra-wannamaker.jpg"/><strong>Kendra Wannamaker</strong></a><span class="role"></span>, <span>Sandeep Kollannur<!-- --> <span class="role"></span></span>, <a href="/people/marian-doerk"><img alt="marian-doerk photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/marian-doerk.jpg 1x" src="/pr-preview/pr-140/static/images/people/marian-doerk.jpg"/><strong>Marian Dörk</strong></a><span class="role"></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present I/O Bits, a prototype personal informatics system that explores the potential for user-driven and situated self-tracking. With simple tactile inputs and small e-paper visualizations, I/O Bits are dedicated physical devices that allow individuals to track and visualize different kinds of personal activities in-situ. This is in contrast to most self-tracking systems, which automate data collection, centralize information displays, or integrate into multi-purpose devices like smartwatches or mobile phones. We report findings from an e-paper visualization workshop and a prototype deployment where participants constructed their own I/O Bits and used them to track a range of personal data. Based on these experiences, we contribute insights and opportunities for situated and user-driven personal informatics.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Personal Informatics</span><span class="ui brown basic label">Situated Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kendra Wannamaker<!-- -->, <!-- -->Sandeep Kollannur<!-- -->, <!-- -->Marian Dörk<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>I/O Bits: User-Driven, Situated, and Dedicated Self-Tracking</b>. <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- --> <!-- -->(<!-- -->DIS 2021<!-- -->)</i>ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->10<!-- -->. <!-- -->DOI: <a href="http://hdl.handle.net/1880/113555" target="_blank">http://hdl.handle.net/1880/113555</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/yhMKURtgFZ0" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/yhMKURtgFZ0?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/yhMKURtgFZ0/maxresdefault.jpg src=https://img.youtube.com/vi/yhMKURtgFZ0/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2021-danyluk" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/chi-2021-danyluk/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2021-danyluk</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2021-danyluk cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2021-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2021-danyluk.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2021-danyluk" target="_blank">A Design Space Exploration of Worlds in Miniature</a></h1><p class="meta"><a href="/people/kurtis-danyluk"><img alt="kurtis-danyluk photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg"/><strong>Kurtis Danyluk</strong></a><span class="role"></span>, <span>Barrett Ens<!-- --> <span class="role"></span></span>, <span>Bernhard Jenny<!-- --> <span class="role"></span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Worlds-in-Miniature (WiMs) are interactive worlds within a world and combine the advantages of an input space, a cartographicmap, and an overview+detail interface. They have been used across the extended virtuality spectrum for a variety of applications.Building on an analysis of examples of WiMs from the research literature we contribute a design space for WiMs based on sevendesign dimensions. Further, we expand upon existing definitions of WiMs to provide a definition that applies across the extendedreality spectrum. We identify the design dimensions of size-scope-scale, abstraction, geometry, reference frame, links, multiples, andvirtuality. Using our framework we describe existing Worlds-in-Miniature from the research literature and reveal unexplored researchareas. Finally, we generate new examples of WiMs using our framework to fill some of these gaps. With our findings, we identifyopportunities that can guide future research into WiMs.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Virtual Augmented Reality</span><span class="ui brown basic label">Meta Analysis Literature Survey</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kurtis Danyluk<!-- -->, <!-- -->Barrett Ens<!-- -->, <!-- -->Bernhard Jenny<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>A Design Space Exploration of Worlds in Miniature</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2021<!-- -->)</i>ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->20<!-- -->. </p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2021-ens" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/chi-2021-ens/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2021-ens</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2021-ens cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2021-ens.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2021-ens.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2021-ens" target="_blank">Grand Challenges in Immersive Analytics</a></h1><p class="meta"><span>Barrett Ens<!-- --> <span class="role"></span></span>, <span>Benjamin Bach<!-- --> <span class="role"></span></span>, <span>Maxime Cordeil<!-- --> <span class="role"></span></span>, <span>Ulrich Engelke<!-- --> <span class="role"></span></span>, <span>Marcos Serrano<!-- --> <span class="role"></span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span>, <span>Arnaud Prouzeau<!-- --> <span class="role"></span></span>, <span>Christoph Anthes<!-- --> <span class="role"></span></span>, <span>Wolfgang Büschel<!-- --> <span class="role"></span></span>, <span>Cody Dunne<!-- --> <span class="role"></span></span>, <span>Tim Dwyer<!-- --> <span class="role"></span></span>, <span>Jens Grubert<!-- --> <span class="role"></span></span>, <span>Jason H. Haga<!-- --> <span class="role"></span></span>, <span>Nurit Kishenbaum<!-- --> <span class="role"></span></span>, <span>Dylan Kobayashi<!-- --> <span class="role"></span></span>, <span>Tica Lin<!-- --> <span class="role"></span></span>, <span>Monsurat Olaosebikan<!-- --> <span class="role"></span></span>, <span>Fabian Pointecker<!-- --> <span class="role"></span></span>, <span>David Saffo<!-- --> <span class="role"></span></span>, <span>Nazmus Saquib<!-- --> <span class="role"></span></span>, <span>Dieter Schmalsteig<!-- --> <span class="role"></span></span>, <span>Danielle Albers Szafir<!-- --> <span class="role"></span></span>, <span>Matthew Whitlock<!-- --> <span class="role"></span></span>, <span>Yalong Yang<!-- --> <span class="role"></span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Immersive Analytics is a quickly evolving field that unites several areas such as visualisation, immersive environments, and human-computer interaction to support human data analysis with emerging technologies. This research has thrived over the past years with multiple workshops, seminars, and a growing body of publications, spanning several conferences. Given the rapid advancement of interaction technologies and novel application domains, this paper aims toward a broader research agenda to enable widespread adoption. We present 17 key research challenges developed over multiple sessions by a diverse group of 24 international experts, initiated from a virtual scientific workshop at ACM CHI 2020. These challenges aim to coordinate future work by providing a systematic roadmap of current directions and impending hurdles to facilitate productive and effective applications for Immersive Analytics.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Immersive Analytics</span><span class="ui brown basic label">Grand Research Challenges</span><span class="ui brown basic label">Data Visualisation</span><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Virtual Reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Barrett Ens<!-- -->, <!-- -->Benjamin Bach<!-- -->, <!-- -->Maxime Cordeil<!-- -->, <!-- -->Ulrich Engelke<!-- -->, <!-- -->Marcos Serrano<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Arnaud Prouzeau<!-- -->, <!-- -->Christoph Anthes<!-- -->, <!-- -->Wolfgang Büschel<!-- -->, <!-- -->Cody Dunne<!-- -->, <!-- -->Tim Dwyer<!-- -->, <!-- -->Jens Grubert<!-- -->, <!-- -->Jason H. Haga<!-- -->, <!-- -->Nurit Kishenbaum<!-- -->, <!-- -->Dylan Kobayashi<!-- -->, <!-- -->Tica Lin<!-- -->, <!-- -->Monsurat Olaosebikan<!-- -->, <!-- -->Fabian Pointecker<!-- -->, <!-- -->David Saffo<!-- -->, <!-- -->Nazmus Saquib<!-- -->, <!-- -->Dieter Schmalsteig<!-- -->, <!-- -->Danielle Albers Szafir<!-- -->, <!-- -->Matthew Whitlock<!-- -->, <!-- -->Yalong Yang<!-- -->. <b>Grand Challenges in Immersive Analytics</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2021<!-- -->)</i>ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->17<!-- -->. </p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="gi-2021-mactavish" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/gi-2021-mactavish/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>gi-2021-mactavish</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">GI 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="gi-2021-mactavish cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/gi-2021-mactavish.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/gi-2021-mactavish.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/gi-2021-mactavish" target="_blank">Perspective Charts</a></h1><p class="meta"><span>Mia MacTavish<!-- --> <span class="role"></span></span>, <span>Katayoon Etemad<!-- --> <span class="role"></span></span>, <span>Faramarz Samavati<!-- --> <span class="role"></span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Sp4Vt8mMhCs&amp;list=PLZQ0ePfDvRH4Ac7xfBdPlMQX0d0NYQ7Y-" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Sp4Vt8mMhCs&amp;list=PLZQ0ePfDvRH4Ac7xfBdPlMQX0d0NYQ7Y-?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/Sp4Vt8mMhCs&amp;list=PLZQ0ePfDvRH4Ac7xfBdPlMQX0d0NYQ7Y-/maxresdefault.jpg src=https://img.youtube.com/vi/Sp4Vt8mMhCs&amp;list=PLZQ0ePfDvRH4Ac7xfBdPlMQX0d0NYQ7Y-/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce three novel data visualizations, called perspective charts, based on the concept of size constancy in linear perspective projection. Bar charts are a popular and commonly used tool for the interpretation of datasets, however, representing datasets with multi-scale variation is challenging in a bar chart due to limitations in viewing space. Each of our designs focuses on the static representation of datasets with large ranges with respect to important variations in the data. Through a user study, we measure the effectiveness of our designs for representing these datasets in comparison to traditional methods, such as a standard bar chart or a broken-axis bar chart, and state-of-the-art methods, such as a scale-stack bar chart. The evaluation reveals that our designs allow pieces of data to be visually compared at a level of accuracy similar to traditional visualizations. Our designs demonstrate advantages when compared to state-of-the-art visualizations designed to represent datasets with large outliers.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Information Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Mia MacTavish<!-- -->, <!-- -->Katayoon Etemad<!-- -->, <!-- -->Faramarz Samavati<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Perspective Charts</b>. <i>(<!-- -->GI 2021<!-- -->)</i> <!-- -->Page: 1-<!-- -->10<!-- -->. <!-- -->DOI: <a href="http://hdl.handle.net/1880/113671" target="_blank">http://hdl.handle.net/1880/113671</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="cupum-2021-rout" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/cupum-2021-rout/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>cupum-2021-rout</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Urban Informatics and Future Cities</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="cupum-2021-rout cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/cupum-2021-rout.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/cupum-2021-rout.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/cupum-2021-rout" target="_blank">(Big) Data in Urban Design Practice: Supporting High-Level Design Tasks Using a Visualization of Human Movement Data from Smartphones</a></h1><p class="meta"><span>Angela Rout<!-- --> <span class="role"></span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Me8cU6RoCiA" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Me8cU6RoCiA?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/Me8cU6RoCiA/maxresdefault.jpg src=https://img.youtube.com/vi/Me8cU6RoCiA/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present the SmartCampus visualization tool, representing spatiotemporal data of over 200 student pathways and restpoints on a university campus. Based on our experiences with SmartCampus, we also propose a task-based framework that de-scribes how practicing urban designers (specifically, architects) can use human movement data visualizations in their work. Although extensive amounts of location data are produced daily by smartphones, existing geospatial tools are not customized to specifically support high-level urban design tasks. To help identify opportunities in urban design for visualizing human movement data from devices such as smartphones, we used our SmartCampus prototype to facilitate a series of 3 participatory design sessions (3 participants), a targeted online survey (14 participants), and semi-structured interviews (6 participants) with architectural experts. Our findings showcase the need for location analysis tools tailored to concrete urban design practices, and also highlight opportunities for Smart City researchers interested in developing domain specific, visualization tools.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Smartphone Data</span><span class="ui brown basic label">GPS</span><span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Architecture</span><span class="ui brown basic label">Urban Design</span><span class="ui brown basic label">Task Based Framework</span><span class="ui brown basic label">High Level Tasks</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Angela Rout<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>(Big) Data in Urban Design Practice: Supporting High-Level Design Tasks Using a Visualization of Human Movement Data from Smartphones</b>. <i>(<!-- -->Urban Informatics and Future Cities<!-- -->)</i> <!-- -->Page: 1-<!-- -->301-318<!-- -->. <!-- -->DOI: <a href="http://hdl.handle.net/1880/113114" target="_blank">http://hdl.handle.net/1880/113114</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tvcg-2020-danyluk" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/tvcg-2020-danyluk/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>tvcg-2020-danyluk</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TVCG 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="tvcg-2020-danyluk cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/tvcg-2020-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/tvcg-2020-danyluk.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tvcg-2020-danyluk" target="_blank">Touch and Beyond: Comparing Physical and Virtual Reality Visualizations</a></h1><p class="meta"><a href="/people/kurtis-danyluk"><img alt="kurtis-danyluk photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg"/><strong>Kurtis Thorvald Danyluk</strong></a><span class="role"></span>, <span>Teoman Tomo Ulusoy<!-- --> <span class="role"></span></span>, <a href="/people/wei-wei"><img alt="wei-wei photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wei-wei.jpg 1x" src="/pr-preview/pr-140/static/images/people/wei-wei.jpg"/><strong>Wei Wei</strong></a><span class="role"></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley J. Willett</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/tvcg-2020-danyluk.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>tvcg-2020-danyluk.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We compare physical and virtual reality (VR) versions of simple data visualizations and explore how the addition of virtual annotation and filtering tools affects how viewers solve basic data analysis tasks. We report on two studies, inspired by previous examinations of data physicalizations. The first study examines differences in how viewers interact with physical hand-scale, virtual hand-scale, and virtual table-scale visualizations and the impact that the different forms had on viewer’s problem solving behavior. A second study examines how interactive annotation and filtering tools might support new modes of use that transcend the limitations of physical representations. Our results highlight challenges associated with virtual reality representations and hint at the potential of interactive annotation and filtering tools in VR visualizations.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Physicalization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kurtis Thorvald Danyluk<!-- -->, <!-- -->Teoman Tomo Ulusoy<!-- -->, <!-- -->Wei Wei<!-- -->, <!-- -->Wesley J. Willett<!-- -->. <b>Touch and Beyond: Comparing Physical and Virtual Reality Visualizations</b>. <i>In <!-- -->IEEE Transactions on Visualization and Computer Graphics<!-- --> <!-- -->(<!-- -->TVCG 2020<!-- -->)</i>IEEE, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->12<!-- -->. <!-- -->DOI: <a href="http://dx.doi.org/10.1109/TVCG.2020.3023336" target="_blank">http://dx.doi.org/10.1109/TVCG.2020.3023336</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-goffin" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/chi-2020-goffin/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2020-goffin</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2020-goffin cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2020-goffin.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2020-goffin.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-goffin" target="_blank">Interaction Techniques for Visual Exploration Using Embedded Word-Scale Visualizations</a></h1><p class="meta"><span>Pascal Goffin<!-- --> <span class="role"></span></span>, <span>Tanja Blascheck<!-- --> <span class="role"></span></span>, <a href="/people/petra-isenberg"><img alt="petra-isenberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/petra-isenberg.jpg 1x" src="/pr-preview/pr-140/static/images/people/petra-isenberg.jpg"/><strong>Petra Isenberg</strong></a><span class="role"></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2020-goffin.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2020-goffin.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/wPaVdSWM8hU" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/wPaVdSWM8hU?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/wPaVdSWM8hU/maxresdefault.jpg src=https://img.youtube.com/vi/wPaVdSWM8hU/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We describe a design space of view manipulation interactions for small data-driven contextual visualizations (word-scale visualizations). These interaction techniques support an active reading experience and engage readers through exploration of embedded visualizations whose placement and content connect them to specific terms in a document. A reader could, for example, use our proposed interaction techniques to explore word-scale visualizations of stock market trends for companies listed in a market overview article. When readers wish to engage more deeply with the data, they can collect, arrange, compare, and navigate the document using the embedded word-scale visualizations, permitting more visualization-centric analyses. We support our design space with a concrete implementation, illustrate it with examples from three application domains, and report results from two experiments. The experiments show how view manipulation interactions helped readers examine embedded visualizations more quickly and with less scrolling and yielded qualitative feedback on usability and future opportunities.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Glyphs</span><span class="ui brown basic label">Word Scale Visualization</span><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Interaction Techniques</span><span class="ui brown basic label">Text Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Pascal Goffin<!-- -->, <!-- -->Tanja Blascheck<!-- -->, <!-- -->Petra Isenberg<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Interaction Techniques for Visual Exploration Using Embedded Word-Scale Visualizations</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2020<!-- -->)</i>ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3313831.3376842" target="_blank">https://doi.org/10.1145/3313831.3376842</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tvcg-2019-walny" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/tvcg-2019-walny/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>tvcg-2019-walny</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TVCG 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="tvcg-2019-walny cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/tvcg-2019-walny.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/tvcg-2019-walny.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tvcg-2019-walny" target="_blank">Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff</a></h1><p class="meta"><a href="/people/jagoda-walny"><img alt="jagoda-walny photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/jagoda-walny.jpg 1x" src="/pr-preview/pr-140/static/images/people/jagoda-walny.jpg"/><strong>Jagoda Walny</strong></a><span class="role"></span>, <a href="/people/christian-frisson"><img alt="christian-frisson photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-140/static/images/people/christian-frisson.jpg"/><strong>Christian Frisson</strong></a><span class="role"></span>, <span>Mieka West<!-- --> <span class="role"></span></span>, <span>Doris Kosminsky<!-- --> <span class="role"></span></span>, <a href="/people/soren-knudsen"><img alt="soren-knudsen photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/soren-knudsen.jpg 1x" src="/pr-preview/pr-140/static/images/people/soren-knudsen.jpg"/><strong>Søren Knudsen</strong></a><span class="role"></span>, <a href="/people/sheelagh-carpendale"><img alt="sheelagh-carpendale photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-140/static/images/people/sheelagh-carpendale.jpg"/><strong>Sheelagh Carpendale</strong></a><span class="role"></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/tvcg-2019-walny.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>tvcg-2019-walny.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/360483702" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/360483702?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://i.vimeocdn.com/video/814665539_640.webp src=https://i.vimeocdn.com/video/814665539_640.webp&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Complex data visualization design projects often entail collaboration between people with different visualization-related skills. For example, many teams include both designers who create new visualization designs and developers who implement the resulting visualization software. We identify gaps between data characterization tools, visualization design tools, and development platforms that pose challenges for designer-developer teams working to create new data visualizations. While it is common for commercial interaction design tools to support collaboration between designers and developers, creating data visualizations poses several unique challenges that are not supported by current tools. In particular, visualization designers must characterize and build an understanding of the underlying data, then specify layouts, data encodings, and other data-driven parameters that will be robust across many different data values. In larger teams, designers must also clearly communicate these mappings and their dependencies to developers, clients, and other collaborators. We report observations and reflections from five large multidisciplinary visualization design projects and highlight six data-specific visualization challenges for design specification and handoff. These challenges include adapting to changing data, anticipating edge cases in data, understanding technical challenges, articulating data-dependent interactions, communicating data mappings, and preserving the integrity of data mappings across iterations. Based on these observations, we identify opportunities for future tools for prototyping, testing, and communicating data-driven designs, which might contribute to more successful and collaborative data visualization design.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Design Handoff</span><span class="ui brown basic label">Data Mapping</span><span class="ui brown basic label">Design Process</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jagoda Walny<!-- -->, <!-- -->Christian Frisson<!-- -->, <!-- -->Mieka West<!-- -->, <!-- -->Doris Kosminsky<!-- -->, <!-- -->Søren Knudsen<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff</b>. <i>In <!-- -->IEEE Transactions on Visualization and Computer Graphics<!-- --> <!-- -->(<!-- -->TVCG 2019<!-- -->)</i>IEEE, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->10<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1109/TVCG.2019.2934538" target="_blank">https://doi.org/10.1109/TVCG.2019.2934538</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/368703151" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/368703151?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://i.vimeocdn.com/video/825448765_640.webp src=https://i.vimeocdn.com/video/825448765_640.webp&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2019-bressa" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/dis-2019-bressa/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2019-bressa</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2019-bressa cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/dis-2019-bressa.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/dis-2019-bressa.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2019-bressa" target="_blank">Sketching and Ideation Activities for Situated Visualization Design</a></h1><p class="meta"><a href="/people/nathalie-bressa"><img alt="nathalie-bressa photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/nathalie-bressa.jpg 1x" src="/pr-preview/pr-140/static/images/people/nathalie-bressa.jpg"/><strong>Nathalie Bressa</strong></a><span class="role"></span>, <a href="/people/kendra-wannamaker"><img alt="kendra-wannamaker photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kendra-wannamaker.jpg 1x" src="/pr-preview/pr-140/static/images/people/kendra-wannamaker.jpg"/><strong>Kendra Wannamaker</strong></a><span class="role"></span>, <span>Henrik Korsgaard<!-- --> <span class="role"></span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span>, <span>Jo Vermeulen<!-- --> <span class="role"></span></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/dis-2019-bressa.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>dis-2019-bressa.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We report on findings from seven design workshops that used ideation and sketching activities to prototype new situated visualizations - representations of data that are displayed in proximity to the physical referents (such as people, objects, and locations) to which the data is related. Designing situated visualizations requires a fine-grained understanding of the context in which the visualizations are placed, as well as an exploration of different options for placement and form factors, which existing methods for visualization design do not account for. Focusing on small displays as a target platform, we reflect on our experiences of using a diverse range of sketching activities, materials, and prompts. Based on these observations, we identify challenges and opportunities for sketching and ideating situated visualizations. We also outline the space of design activities for situated visualization and highlight promising methods for both designers and researchers.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Ideation</span><span class="ui brown basic label">Design Workshops</span><span class="ui brown basic label">Situated Visualization</span><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Small Displays</span><span class="ui brown basic label">Sketching</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nathalie Bressa<!-- -->, <!-- -->Kendra Wannamaker<!-- -->, <!-- -->Henrik Korsgaard<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Jo Vermeulen<!-- -->. <b>Sketching and Ideation Activities for Situated Visualization Design</b>. <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- --> <!-- -->(<!-- -->DIS 2019<!-- -->)</i>ACM, New York, NY, USA<!-- --> <!-- -->DOI: <a href="https://doi.org/10.1145/3322276.3322326" target="_blank">https://doi.org/10.1145/3322276.3322326</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tvcg-2019-blascheck" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/tvcg-2019-blascheck/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>tvcg-2019-blascheck</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TVCG 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="tvcg-2019-blascheck cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/tvcg-2019-blascheck.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/tvcg-2019-blascheck.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tvcg-2019-blascheck" target="_blank">Exploration Strategies for Discovery of Interactivity in Visualizations</a></h1><p class="meta"><span>Tanja Blascheck<!-- --> <span class="role"></span></span>, <span>Lindsay MacDonald Vermeulen<!-- --> <span class="role"></span></span>, <span>Jo Vermeulen<!-- --> <span class="role"></span></span>, <span>Charles Perin<!-- --> <span class="role"></span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span>, <span>Thomas Ertl<!-- --> <span class="role"></span></span>, <a href="/people/sheelagh-carpendale"><img alt="sheelagh-carpendale photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-140/static/images/people/sheelagh-carpendale.jpg"/><strong>Sheelagh Carpendale</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/tvcg-2019-blascheck.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>tvcg-2019-blascheck.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/289789025" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/289789025?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://i.vimeocdn.com/video/725516359_640.webp src=https://i.vimeocdn.com/video/725516359_640.webp&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We investigate how people discover the functionality of an interactive visualization that was designed for the general public. While interactive visualizations are increasingly available for public use, we still know little about how the general public discovers what they can do with these visualizations and what interactions are available. Developing a better understanding of this discovery process can help inform the design of visualizations for the general public, which in turn can help make data more accessible. To unpack this problem, we conducted a lab study in which participants were free to use their own methods to discover the functionality of a connected set of interactive visualizations of public energy data. We collected eye movement data and interaction logs as well as video and audio recordings. By analyzing this combined data, we extract exploration strategies that the participants employed to discover the functionality in these interactive visualizations. These exploration strategies illuminate possible design directions for improving the discoverability of a visualization&#x27;s functionality.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Discovery</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Open Data</span><span class="ui brown basic label">Evaluation</span><span class="ui brown basic label">Eye Tracking</span><span class="ui brown basic label">Interaction Logs</span><span class="ui brown basic label">Think Aloud</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Tanja Blascheck<!-- -->, <!-- -->Lindsay MacDonald Vermeulen<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Charles Perin<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Thomas Ertl<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->. <b>Exploration Strategies for Discovery of Interactivity in Visualizations</b>. <i>In <!-- -->IEEE Transactions on Visualization and Computer Graphics<!-- --> <!-- -->(<!-- -->TVCG 2019<!-- -->)</i>IEEE, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1109/TVCG.2018.2802520" target="_blank">https://doi.org/10.1109/TVCG.2018.2802520</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2019-danyluk" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/chi-2019-danyluk/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2019-danyluk</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2019-danyluk cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2019-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2019-danyluk.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2019-danyluk" target="_blank">Look-From Camera Control for 3D Terrain Maps</a></h1><p class="meta"><a href="/people/kurtis-danyluk"><img alt="kurtis-danyluk photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg"/><strong>Kurtis Danyluk</strong></a><span class="role"></span>, <span>Bernhard Jenny<!-- --> <span class="role"></span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2019-danyluk.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2019-danyluk.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We introduce three lightweight interactive camera control techniques for 3D terrain maps on touch devices based on a look-from metaphor (Discrete Look-From-At, Continuous Look-From-Forwards, and Continuous Look-From-Towards). These techniques complement traditional touch screen pan, zoom, rotate, and pitch controls allowing viewers to quickly transition between top-down, oblique, and ground-level views. We present the results of a study in which we asked participants to perform elevation comparison and line-of-sight determination tasks using each technique. Our results highlight how look-from techniques can be integrated on top of current direct manipulation navigation approaches by combining several direct manipulation operations into a single look-from operation. Additionally, they show how look-from techniques help viewers complete a variety of common and challenging map-based tasks.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Terrain</span><span class="ui brown basic label">Touch</span><span class="ui brown basic label">Map Interaction</span><span class="ui brown basic label">Look From Camera Control</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kurtis Danyluk<!-- -->, <!-- -->Bernhard Jenny<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Look-From Camera Control for 3D Terrain Maps</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2019<!-- -->)</i>ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->12<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3290605.3300594" target="_blank">https://doi.org/10.1145/3290605.3300594</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="vr-2019-satriadi" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/vr-2019-satriadi/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>vr-2019-satriadi</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">IEEE VR 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="vr-2019-satriadi cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/vr-2019-satriadi.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/vr-2019-satriadi.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/vr-2019-satriadi" target="_blank">Augmented Reality Map Navigation with Freehand Gestures</a></h1><p class="meta"><span>Kadek Ananta Satriadi<!-- --> <span class="role"></span></span>, <span>Barrett Ens<!-- --> <span class="role"></span></span>, <span>Maxime Cordeil<!-- --> <span class="role"></span></span>, <span>Bernhard Jenny<!-- --> <span class="role"></span></span>, <span>Tobias Czauderna<!-- --> <span class="role"></span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/vr-2019-satriadi.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>vr-2019-satriadi.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/TE6AJEu8zdY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/TE6AJEu8zdY?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/TE6AJEu8zdY/maxresdefault.jpg src=https://img.youtube.com/vi/TE6AJEu8zdY/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Freehand gesture interaction has long been proposed as a `natural&#x27; input method for Augmented Reality (AR) applications, yet has been little explored for intensive applications like multiscale navigation. In multiscale navigation, such as digital map navigation, pan and zoom are the predominant interactions. A position-based input mapping (e.g. grabbing metaphor) is intuitive for such interactions, but is prone to arm fatigue. This work focuses on improving digital map navigation in AR with mid-air hand gestures, using a horizontal intangible map display. First, we conducted a user study to explore the effects of handedness (unimanual and bimanual) and input mapping (position-based and rate-based). From these findings we designed DiveZoom and TerraceZoom, two novel hybrid techniques that smoothly transition between position- and rate-based mappings. A second user study evaluated these designs. Our results indicate that the introduced input-mapping transitions can reduce perceived arm fatigue with limited impact on performance.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Gesture Recognition</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Interactive Devices</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kadek Ananta Satriadi<!-- -->, <!-- -->Barrett Ens<!-- -->, <!-- -->Maxime Cordeil<!-- -->, <!-- -->Bernhard Jenny<!-- -->, <!-- -->Tobias Czauderna<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Augmented Reality Map Navigation with Freehand Gestures</b>. <i>(<!-- -->IEEE VR 2019<!-- -->)</i> <!-- -->Page: 1-<!-- -->11<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1109/VR.2019.8798340" target="_blank">https://doi.org/10.1109/VR.2019.8798340</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/jNeEbB3sTn0" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/jNeEbB3sTn0?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/jNeEbB3sTn0/maxresdefault.jpg src=https://img.youtube.com/vi/jNeEbB3sTn0/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="cga-2019-ivanov" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/cga-2019-ivanov/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>cga-2019-ivanov</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">IEEE CG&amp;A 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="cga-2019-ivanov cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/cga-2019-ivanov.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/cga-2019-ivanov.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/cga-2019-ivanov" target="_blank">A Walk Among the Data</a></h1><p class="meta"><a href="/people/sasha-ivanov"><img alt="sasha-ivanov photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/sasha-ivanov.jpg 1x" src="/pr-preview/pr-140/static/images/people/sasha-ivanov.jpg"/><strong>Alexander Ivanov</strong></a><span class="role"></span>, <a href="/people/kurtis-danyluk"><img alt="kurtis-danyluk photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-140/static/images/people/kurtis-danyluk.jpg"/><strong>Kurtis Danyluk</strong></a><span class="role"></span>, <span>Christian Jacob<!-- --> <span class="role"></span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/cga-2019-ivanov.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>cga-2019-ivanov.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We examine the potential for immersive unit visualizations—interactive virtual environments populated with objects representing individual items in a dataset. Our virtual reality prototype highlights how immersive unit visualizations can allow viewers to examine data at multiple scales, support immersive exploration, and create affective personal experiences with data.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Art</span><span class="ui brown basic label">Tools</span><span class="ui brown basic label">Virtual Environments</span><span class="ui brown basic label">Two Dimensional Displays</span><span class="ui brown basic label">Anthropomorphism</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Alexander Ivanov<!-- -->, <!-- -->Kurtis Danyluk<!-- -->, <!-- -->Christian Jacob<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>A Walk Among the Data</b>. <i>(<!-- -->IEEE CG&amp;A 2019<!-- -->)</i> <!-- -->Page: 1-<!-- -->9<!-- -->. <!-- -->DOI: <a href="http://dx.doi.org/10.1109/MCG.2019.2898941" target="_blank">http://dx.doi.org/10.1109/MCG.2019.2898941</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="sui-2017-li" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/sui-2017-li/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>sui-2017-li</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">SUI 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="sui-2017-li cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/sui-2017-li.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/sui-2017-li.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/sui-2017-li" target="_blank">Visibility Perception and Dynamic Viewsheds for Topographic Maps and Models</a></h1><p class="meta"><span>Nico Li<!-- --> <span class="role"></span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-140/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"></span>, <span>Mario Costa Sousa<!-- --> <span class="role"></span></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/sui-2017-li.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>sui-2017-li.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/275404995" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/275404995?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://i.vimeocdn.com/video/707686230_640.webp src=https://i.vimeocdn.com/video/707686230_640.webp&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We compare the effectiveness of 2D maps and 3D terrain models for visibility tasks and demonstrate how interactive dynamic viewsheds can improve performance for both types of terrain representations. In general, the two-dimensional nature of classic topographic maps limits their legibility and can make complex yet typical cartographic tasks like determining the visibility between locations difficult. Both 3D physical models and interactive techniques like dynamic viewsheds have the potential to improve viewers&#x27; understanding of topography, but their impact has not been deeply explored. We evaluate the effectiveness of 2D maps, 3D models, and interactive viewsheds for both simple and complex visibility tasks. Our results demonstrate the benefits of the dynamic viewshed technique and highlight opportunities for additional tactile interactions. Based on these findings we present guidelines for improving the design and usability of future topographic maps and models.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Terrain Visualization</span><span class="ui brown basic label">Geospatial Visualization</span><span class="ui brown basic label">Dynamic Viewshed</span><span class="ui brown basic label">Topographic Maps</span><span class="ui brown basic label">Tangible User Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nico Li<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Mario Costa Sousa<!-- -->. <b>Visibility Perception and Dynamic Viewsheds for Topographic Maps and Models</b>. <i>(<!-- -->SUI 2017<!-- -->)</i> <!-- -->Page: 1-<!-- -->9<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3131277.3132178" target="_blank">https://doi.org/10.1145/3131277.3132178</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/aVXUojoQF60" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/aVXUojoQF60?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/aVXUojoQF60/maxresdefault.jpg src=https://img.youtube.com/vi/aVXUojoQF60/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2017-aoki" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/chi-2017-aoki/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2017-aoki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2017-aoki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2017-aoki.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2017-aoki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2017-aoki" target="_blank">Environmental Protection and Agency: Motivations, Capacity, and Goals in Participatory Sensing</a></h1><p class="meta"><span>Paul Aoki<!-- --> <span class="role"></span></span>, <span>Allison Woodruff<!-- --> <span class="role"></span></span>, <span>Baladitya Yellapragada<!-- --> <span class="role"></span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2017-aoki.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2017-aoki.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this paper we consider various genres of citizen science from the perspective of citizen participants. As a mode of scientific inquiry, citizen science has the potential to &quot;scale up&quot; scientific data collection efforts and increase lay engagement with science. However, current technological directions risk losing sight of the ways in which citizen science is actually practiced. As citizen science is increasingly used to describe a wide range of activities, we begin by presenting a framework of citizen science genres. We then present findings from four interlocking qualitative studies and technological interventions of community air quality monitoring efforts, examining the motivations and capacities of citizen participants and characterizing their alignment with different types of citizen science. Based on these studies, we suggest that data acquisition involves complex multi-dimensional tradeoffs, and the commonly held view that citizen science systems are a win-win for citizens and science may be overstated.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Citizen Science</span><span class="ui brown basic label">Environmental Sensing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Paul Aoki<!-- -->, <!-- -->Allison Woodruff<!-- -->, <!-- -->Baladitya Yellapragada<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Environmental Protection and Agency: Motivations, Capacity, and Goals in Participatory Sensing</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2017<!-- -->)</i>ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3025453.3025667" target="_blank">https://doi.org/10.1145/3025453.3025667</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2017-hull" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/chi-2017-hull/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2017-hull</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2017-hull cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2017-hull.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2017-hull.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2017-hull" target="_blank">Building with Data: Architectural Models as Inspiration for Data Physicalization</a></h1><p class="meta"><a href="/people/carmen-hull"><img alt="carmen-hull photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/carmen-hull.jpg 1x" src="/pr-preview/pr-140/static/images/people/carmen-hull.jpg"/><strong>Carmen Hull</strong></a><span class="role"></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/bkqLNgYIXek" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/bkqLNgYIXek?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/bkqLNgYIXek/maxresdefault.jpg src=https://img.youtube.com/vi/bkqLNgYIXek/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>In this paper we analyze the role of physical scale models in the architectural design process and apply insights from architecture for the creation and use of data physicalizations. Based on a survey of the architecture literature on model making and ten interviews with practicing architects, we describe the role of physical models as a tool for exploration and communication. From these observations, we identify trends in the use of physical models in architecture, which have the potential to inform the design of data physicalizations. We identify four functions of architectural modeling that can be directly adapted for use in the process of building rich data models. Finally, we discuss how the visualization community can apply observations from architecture to the design of new data physicalizations.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Design Process</span><span class="ui brown basic label">Architectural Models</span><span class="ui brown basic label">Data Physicalization</span><span class="ui brown basic label">Embodied Interaction</span><span class="ui brown basic label">Data Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Carmen Hull<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Building with Data: Architectural Models as Inspiration for Data Physicalization</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2017<!-- -->)</i>ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->12<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3025453.3025850" target="_blank">https://doi.org/10.1145/3025453.3025850</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tvcg-2017-goffin" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/tvcg-2017-goffin/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>tvcg-2017-goffin</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TVCG 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="tvcg-2017-goffin cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/tvcg-2017-goffin.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/tvcg-2017-goffin.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tvcg-2017-goffin" target="_blank">An Exploratory Study of Word-Scale Graphics in Data-Rich Text Documents</a></h1><p class="meta"><span>Pascal Goffin<!-- --> <span class="role"></span></span>, <span>Jeremy Boy<!-- --> <span class="role"></span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span>, <a href="/people/petra-isenberg"><img alt="petra-isenberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/petra-isenberg.jpg 1x" src="/pr-preview/pr-140/static/images/people/petra-isenberg.jpg"/><strong>Petra Isenberg</strong></a><span class="role"></span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/230834366" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/230834366?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://i.vimeocdn.com/video/651490206_640.webp src=https://i.vimeocdn.com/video/651490206_640.webp&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We contribute an investigation of the design and function of word-scale graphics and visualizations embedded in text documents. Word-scale graphics include both data-driven representations such as word-scale visualizations and sparklines, and non-data-driven visual marks. Their design, function, and use has so far received little research attention. We present the results of an open ended exploratory study with nine graphic designers. The study resulted in a rich collection of different types of graphics, data provenance, and relationships between text, graphics, and data. Based on this corpus, we present a systematic overview of word-scale graphic designs, and examine how designers used them. We also discuss the designers&#x27; goals in creating their graphics, and characterize how they used word-scale graphics to visualize data, add emphasis, and create alternative narratives. Building on these examples, we discuss implications for the design of authoring tools for word-scale graphics and visualizations, and explore how new authoring environments could make it easier for designers to integrate them into documents.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Word Scale Visualization</span><span class="ui brown basic label">Word Scale Graphic</span><span class="ui brown basic label">Text Visualization</span><span class="ui brown basic label">Sparklines</span><span class="ui brown basic label">Authoring Tool</span><span class="ui brown basic label">Information Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Pascal Goffin<!-- -->, <!-- -->Jeremy Boy<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Petra Isenberg<!-- -->. <b>An Exploratory Study of Word-Scale Graphics in Data-Rich Text Documents</b>. <i>In <!-- -->IEEE Transactions on Visualization and Computer Graphics<!-- --> <!-- -->(<!-- -->TVCG 2017<!-- -->)</i>IEEE, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1109/TVCG.2016.2618797" target="_blank">https://doi.org/10.1109/TVCG.2016.2618797</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tvcg-2017-willett" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/tvcg-2017-willett/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>tvcg-2017-willett</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TVCG 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="tvcg-2017-willett cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/tvcg-2017-willett.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/tvcg-2017-willett.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tvcg-2017-willett" target="_blank">Embedded Data Representations</a></h1><p class="meta"><a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span>, <span>Yvonne Jansen<!-- --> <span class="role"></span></span>, <span>Pierre Dragicevic<!-- --> <span class="role"></span></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/tvcg-2017-willett.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>tvcg-2017-willett.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/182971005" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/182971005?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://i.vimeocdn.com/video/592033369_640.webp src=https://i.vimeocdn.com/video/592033369_640.webp&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce embedded data representations, the use of visual and physical representations of data that are deeply integrated with the physical spaces, objects, and entities to which the data refers. Technologies like lightweight wireless displays, mixed reality hardware, and autonomous vehicles are making it increasingly easier to display data in-context. While researchers and artists have already begun to create embedded data representations, the benefits, trade-offs, and even the language necessary to describe and compare these approaches remain unexplored. In this paper, we formalize the notion of physical data referents – the real-world entities and spaces to which data corresponds – and examine the relationship between referents and the visual and physical representations of their data. We differentiate situated representations, which display data in proximity to data referents, and embedded representations, which display data so that it spatially coincides with data referents. Drawing on examples from visualization, ubiquitous computing, and art, we explore the role of spatial indirection, scale, and interaction for embedded representations. We also examine the tradeoffs between non-situated, situated, and embedded data displays, including both visualizations and physicalizations. Based on our observations, we identify a variety of design challenges for embedded data representation, and suggest opportunities for future research and applications.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Data Physicalization</span><span class="ui brown basic label">Ambient Displays</span><span class="ui brown basic label">Ubiquitous Computing</span><span class="ui brown basic label">Augmented Reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Wesley Willett<!-- -->, <!-- -->Yvonne Jansen<!-- -->, <!-- -->Pierre Dragicevic<!-- -->. <b>Embedded Data Representations</b>. <i>In <!-- -->IEEE Transactions on Visualization and Computer Graphics<!-- --> <!-- -->(<!-- -->TVCG 2017<!-- -->)</i>IEEE, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->10<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1109/TVCG.2016.2598608" target="_blank">https://doi.org/10.1109/TVCG.2016.2598608</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/ZS7lU60xChI" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/ZS7lU60xChI?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/ZS7lU60xChI/maxresdefault.jpg src=https://img.youtube.com/vi/ZS7lU60xChI/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2015-oehlberg" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/chi-2015-oehlberg/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2015-oehlberg</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2015</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2015-oehlberg cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2015-oehlberg.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2015-oehlberg.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2015-oehlberg" target="_blank">Patterns of Physical Design Remixing in Online Maker Communities</a></h1><p class="meta"><a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-140/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span>, <span>Wendy E. Mackay<!-- --> <span class="role"></span></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2015-oehlberg.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2015-oehlberg.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Makers participate in remixing culture by drawing inspiration from, combining, and adapting designs for physical objects. To examine how makers remix each others&#x27; designs on a community scale, we analyzed metadata from over 175,000 digital designs from Thingiverse, the largest online design community for digital fabrication. Remixed designs on Thingiverse are predominantly generated designs from Customizer a built-in web app for adjusting parametric designs. However, we find that these designs do not elicit subsequent user activity and the authors who generate them tend not to contribute additional content to Thingiverse. Outside of Customizer, influential sources of remixing include complex assemblies and design primitives, as well as non-physical resources posing as physical designs. Building on our findings, we discuss ways in which online maker communities could become more than just design repositories and better support collaborative remixing.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Customization</span><span class="ui brown basic label">Maker Communities</span><span class="ui brown basic label">User Innovation</span><span class="ui brown basic label">Collaboration</span><span class="ui brown basic label">Hacking</span><span class="ui brown basic label">Remixing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Lora Oehlberg<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Wendy E. Mackay<!-- -->. <b>Patterns of Physical Design Remixing in Online Maker Communities</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2015<!-- -->)</i>ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->12<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/2702123.2702175" target="_blank">https://doi.org/10.1145/2702123.2702175</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/vJrVjH04nGc" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/vJrVjH04nGc?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/vJrVjH04nGc/maxresdefault.jpg src=https://img.youtube.com/vi/vJrVjH04nGc/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2015-willett" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-140/publications/chi-2015-willett/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2015-willett</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-140/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2015</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2015-willett cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/publications/cover/chi-2015-willett.jpg 1x" src="/pr-preview/pr-140/static/images/publications/cover/chi-2015-willett.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2015-willett" target="_blank">Lightweight Relief Shearing for Enhanced Terrain Perception on Interactive Maps</a></h1><p class="meta"><a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-140/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span>, <span>Bernhard Jenny<!-- --> <span class="role"></span></span>, <a href="/people/tobias-isenberg"><img alt="tobias-isenberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-140/static/images/people/tobias-isenberg.jpg 1x" src="/pr-preview/pr-140/static/images/people/tobias-isenberg.jpg"/><strong>Tobias Isenberg</strong></a><span class="role"></span>, <span>Pierre Dragicevic<!-- --> <span class="role"></span></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2015-willett.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2015-willett.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/YW31lmzQzpc" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/YW31lmzQzpc?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/YW31lmzQzpc/maxresdefault.jpg src=https://img.youtube.com/vi/YW31lmzQzpc/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We explore interactive relief shearing, a set of non-intrusive, direct manipulation interactions that expose depth and shape information in terrain maps using ephemeral animations. Reading and interpreting topography and relief on terrain maps is an important aspect of map use, but extracting depth information from 2D maps is notoriously difficult. Modern mapping software attempts to alleviate this limitation by presenting digital terrain using 3D views. However, 3D views introduce occlusion, complicate distance estimations, and typically require more complex interactions. In contrast, our approach reveals depth information via shearing animations on 2D maps, and can be paired with existing interactions such as pan and zoom. We examine explicit, integrated, and hybrid interactions for triggering relief shearing and present a version that uses device tilt to control depth effects. Our evaluation shows that these interactive techniques improve depth perception when compared to standard 2D and perspective views.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Plan Oblique Relief</span><span class="ui brown basic label">Interaction</span><span class="ui brown basic label">Depth Perception</span><span class="ui brown basic label">Terrain Maps</span><span class="ui brown basic label">Relief Shearing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Wesley Willett<!-- -->, <!-- -->Bernhard Jenny<!-- -->, <!-- -->Tobias Isenberg<!-- -->, <!-- -->Pierre Dragicevic<!-- -->. <b>Lightweight Relief Shearing for Enhanced Terrain Perception on Interactive Maps</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2015<!-- -->)</i>ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->10<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/2702123.2702172" target="_blank">https://doi.org/10.1145/2702123.2702172</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div></div></div></div><div class="one wide column"></div></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><div class="content"><a href="https://ucalgary.ca"><img alt="University of Calgary logo" loading="lazy" width="200" height="0" decoding="async" data-nimg="1" style="color:transparent;max-width:200px;margin:0px auto;height:auto" srcSet="/pr-preview/pr-140/static/images/logo-4.png 1x, /pr-preview/pr-140/static/images/logo-4.png 2x" src="/pr-preview/pr-140/static/images/logo-4.png"/></a><div class="sub header"><a class="item" href="https://cpsc.ucalgary.ca">Department of Computer Science</a></div></div></div></footer></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"lab":{"id":"data-experience","name":"Data Experience Lab","description":"Visual Data-driven Tools and Experiences","statement":"develops new data visualizations, interactions, and experiences.","prof":"wesley-willett","url":"https://dataexperience.cpsc.ucalgary.ca/","colour":"#c14824","index":1,"logo":"/static/images/labs/data-experience.png","person":{"name":"Wesley Willett","alias":"Wesley J. Willett","type":"faculty","title":"Associate Professor","labs":["data-experience"],"keywords":["Data Visualization","Data Phyz","AR"],"order":3,"url":"https://dataexperience.cpsc.ucalgary.ca/","scholar":"https://scholar.google.ca/citations?user=Q17-rckAAAAJ","dir":"content/output/people","base":"wesley-willett.json","ext":".json","sourceBase":"wesley-willett.yaml","sourceExt":".yaml"}},"peopleStaticProps":{"people":[{"name":"Ehud Sharlin","type":"faculty","title":"Professor","labs":["utouch"],"keywords":["HRI","Robots","Drones"],"order":1,"url":"http://contacts.ucalgary.ca/info/cpsc/profiles/102-3264","scholar":"https://scholar.google.ca/citations?hl=en\u0026user=eAFxlZIAAAAJ","dir":"content/output/people","base":"ehud-sharlin.json","ext":".json","sourceBase":"ehud-sharlin.yaml","sourceExt":".yaml","id":"ehud-sharlin","photo":"/static/images/people/ehud-sharlin.jpg"},{"name":"Lora Oehlberg","alias":"Lora A. Oehlberg","type":"faculty","title":"Associate Professor","labs":["curio"],"keywords":["Tangible","Design Tools"],"order":2,"url":"https://pages.cpsc.ucalgary.ca/~lora.oehlberg/","scholar":"https://scholar.google.ca/citations?hl=en\u0026user=8GzaBdwAAAAJ","dir":"content/output/people","base":"lora-oehlberg.json","ext":".json","sourceBase":"lora-oehlberg.yaml","sourceExt":".yaml","id":"lora-oehlberg","photo":"/static/images/people/lora-oehlberg.jpg"},{"name":"Wesley Willett","alias":"Wesley J. Willett","type":"faculty","title":"","labs":["data-experience"],"keywords":["Data Visualization","Data Phyz","AR"],"order":3,"url":"https://dataexperience.cpsc.ucalgary.ca/","scholar":"https://scholar.google.ca/citations?user=Q17-rckAAAAJ","dir":"content/output/people","base":"wesley-willett.json","ext":".json","sourceBase":"wesley-willett.yaml","sourceExt":".yaml","id":"wesley-willett","photo":"/static/images/people/wesley-willett.jpg"},{"name":"Aditya Shekhar Nittala","type":"faculty","title":"Assistant Professor","labs":["diff"],"keywords":["Wearable Computing","Fabrication","Interaction Techniques"],"order":4,"url":"https://sites.google.com/site/adityanittala/","scholar":"https://scholar.google.com/citations?user=pDSbjBsAAAAJ","email":"anittala@ucalgary.ca","linkedin":"https://www.linkedin.com/in/adityashekharn","dir":"content/output/people","base":"aditya-shekhar-nittala.json","ext":".json","sourceBase":"aditya-shekhar-nittala.yaml","sourceExt":".yaml","id":"aditya-shekhar-nittala","photo":"/static/images/people/aditya-shekhar-nittala.jpg"},{"name":"Christian Frisson","type":"faculty","title":"Assistant Professor","labs":["shivers"],"keywords":["Haptics","Multisensory \u0026 Multimedia","Immersive Arts"],"order":5,"url":"https://frisson.re","cv":"https://frisson.re","scholar":"https://scholar.google.com/citations?user=sZVn1V4AAAAJ","twitter":"https://twitter.com/TuyleriDikenli","facebook":"https://www.facebook.com/christian.frisson","linkedin":"https://www.linkedin.com/in/christianfrisson","github":"https://github.com/ChristianFrisson","gitlab":"https://gitlab.com/ChristianFrisson","email":"christian.frisson@ucalgary.ca","dir":"content/output/people","base":"christian-frisson.json","ext":".json","sourceBase":"christian-frisson.yaml","sourceExt":".yaml","id":"christian-frisson","photo":"/static/images/people/christian-frisson.jpg"},{"name":"Fateme Rajabiyazdi","alias":"Fatemeh Rajabiyazdi","type":"faculty","title":"Assistant Professor","labs":["health-vis"],"keywords":["Data Visualization","Health"],"order":6,"url":"https://fatemerajabi.github.io/HealthVisFutures/","scholar":"https://scholar.google.com/citations?user=mvoc_5AAAAAJ\u0026hl=en","email":"fateme.rajabiyazdi@ucalgary.ca","github":"https://github.com/FatemeRajabi","linkedin":"https://www.linkedin.com/in/rajabiyazdi","dir":"content/output/people","base":"fateme-rajabiyazdi.json","ext":".json","sourceBase":"fateme-rajabiyazdi.yaml","sourceExt":".yaml","id":"fateme-rajabiyazdi","photo":"/static/images/people/fateme-rajabiyazdi.jpg"},{"name":"Matthew Lakier","type":"faculty","title":"Assistant Professor","labs":["prpl"],"keywords":["Playful Interfaces","Games \u0026 Play","VR/AR"],"order":7,"url":"https://matthewlakier.com","scholar":"https://scholar.google.ca/citations?hl=en\u0026user=RYrb8kwAAAAJ","email":"matthew.lakier@ucalgary.ca","github":"https://github.com/spamalot","dir":"content/output/people","base":"matthew-lakier.json","ext":".json","sourceBase":"matthew-lakier.yaml","sourceExt":".yaml","id":"matthew-lakier","photo":"/static/images/people/matthew-lakier.jpg"},{"name":"Saul Greenberg","type":"faculty","title":"Emeritus Professor","keywords":["UbiComp","CSCW"],"order":10,"url":"http://saul.cpsc.ucalgary.ca/","scholar":"https://scholar.google.com/citations?user=TthhUuoAAAAJ","dir":"content/output/people","base":"saul-greenberg.json","ext":".json","sourceBase":"saul-greenberg.yaml","sourceExt":".yaml","id":"saul-greenberg","photo":"/static/images/people/saul-greenberg.jpg"},{"name":"Ryo Suzuki","type":"faculty","title":"Adjunct Assistant Professor (CU Boulder)","keywords":["Tangible","AR x AI","Robots"],"order":11,"url":"https://ryosuzuki.org","scholar":"https://scholar.google.com/citations?user=klWjaQIAAAAJ","twitter":"https://twitter.com/ryosuzk","facebook":"https://www.facebook.com/ryosuzk","email":"ryo.suzuki@ucalgary.ca","github":"https://github.com/ryosuzuki","linkedin":"https://www.linkedin.com/in/ryosuzuki/","dir":"content/output/people","base":"ryo-suzuki.json","ext":".json","sourceBase":"ryo-suzuki.yaml","sourceExt":".yaml","id":"ryo-suzuki","photo":"/static/images/people/ryo-suzuki.jpg"},{"name":"Anthony Tang","alias":"Tony Tang","type":"faculty","title":"Adjunct Associate Professor (Singapore Management University)","keywords":["Mixed Reality","CSCW"],"order":12,"url":"https://hcitang.github.io/","scholar":"https://scholar.google.com/citations?user=RG1EQowAAAAJ","twitter":"https://twitter.com/proclubboy","github":"http://github.com/hcitang","dir":"content/output/people","base":"anthony-tang.json","ext":".json","sourceBase":"anthony-tang.yaml","sourceExt":".yaml","id":"anthony-tang","photo":"/static/images/people/anthony-tang.jpg"},{"name":"Sheelagh Carpendale","type":"faculty","title":"Adjunct Professor (Simon Fraser University)","keywords":["Data Viz","Data Phyz"],"order":13,"url":"https://www.cs.sfu.ca/~sheelagh/","scholar":"https://scholar.google.com/citations?user=43LLX2kAAAAJ","dir":"content/output/people","base":"sheelagh-carpendale.json","ext":".json","sourceBase":"sheelagh-carpendale.yaml","sourceExt":".yaml","id":"sheelagh-carpendale","photo":"/static/images/people/sheelagh-carpendale.jpg"},{"name":"Abhinav Pillai","type":"alumni","past":"undergrad","email":"abhinav.arunpillai@ucalgary.ca","linkedin":"https://www.linkedin.com/in/abhinav-pillai/","dir":"content/output/people","base":"abhinav-pillai.json","ext":".json","sourceBase":"abhinav-pillai.yaml","sourceExt":".yaml","id":"abhinav-pillai","photo":"/static/images/people/abhinav-pillai.jpg"},{"name":"Aditya Gunturu","type":"master","url":"https://adigunturu.com/","email":"aditya.gunturu@ucalgary.ca","linkedin":"https://www.linkedin.com/in/adigunturu/","github":"https://github.com/adigunturu","labs":["programmable-reality"],"dir":"content/output/people","base":"aditya-gunturu.json","ext":".json","sourceBase":"aditya-gunturu.yaml","sourceExt":".yaml","id":"aditya-gunturu","photo":"/static/images/people/aditya-gunturu.jpg"},{"name":"Adnan Karim","type":"alumni","past":"master","now":"Attabotics","url":"https://sites.google.com/view/adnankarim/","email":"adnan.karim@ucalgary.ca","linkedin":"https://www.linkedin.com/in/adnan-karim-757b98101/","dir":"content/output/people","base":"adnan-karim.json","ext":".json","sourceBase":"adnan-karim.yaml","sourceExt":".yaml","id":"adnan-karim","photo":"/static/images/people/adnan-karim.jpg"},{"name":"Ahmed Elshabasi","type":"alumni","past":"master","labs":["utouch","data-experience"],"dir":"content/output/people","base":"ahmed-elshabasi.json","ext":".json","sourceBase":"ahmed-elshabasi.yaml","sourceExt":".yaml","id":"ahmed-elshabasi","photo":"/static/images/people/ahmed-elshabasi.jpg","title":"Alumni (MSc)"},{"name":"Aidan Gaede-Janke","type":"undergrad","labs":["shivers"],"linkedin":"https://www.linkedin.com/in/aidan-gaede-janke-9202991b2","github":"https://github.com/AidanGJ","email":"aidan.gaedejanke@ucalgary.ca","dir":"content/output/people","base":"aidan-gaede-janke.json","ext":".json","sourceBase":"aidan-gaede-janke.yaml","sourceExt":".yaml","id":"aidan-gaede-janke","photo":"/static/images/people/aidan-gaede-janke.jpg"},{"name":"Akashdeep Singh","type":"alumni","past":"undergrad","now":"Hexagon AB","email":"akashdeep.singh4@ucalgary.ca","linkedin":"https://www.linkedin.com/in/akash02ita/","labs":["diff"],"dir":"content/output/people","base":"akashdeep-singh.json","ext":".json","sourceBase":"akashdeep-singh.yaml","sourceExt":".yaml","id":"akashdeep-singh","photo":"/static/images/people/akashdeep-singh.jpg"},{"name":"Anand Kumar","type":"master","url":"https://anandkumarrajpal.github.io/","email":"anand.kumar1@ucalgary.ca","linkedin":"https://www.linkedin.com/in/anand-kumar-rajpal/","github":"https://github.com/AnandKumarRajpal/","scholar":"https://scholar.google.com/citations?user=1_qfoBUAAAAJ\u0026hl=en","labs":["diff"],"dir":"content/output/people","base":"anand-kumar.json","ext":".json","sourceBase":"anand-kumar.yaml","sourceExt":".yaml","id":"anand-kumar","photo":"/static/images/people/anand-kumar.jpg"},{"name":"Annie Tat","type":"alumni","past":"master","linkedin":"https://www.linkedin.com/in/annie-tat/","now":"Canada Energy Regulator","dir":"content/output/people","base":"annie-tat.json","ext":".json","sourceBase":"annie-tat.yaml","sourceExt":".yaml","id":"annie-tat","photo":"/static/images/people/annie-tat.jpg"},{"name":"April Zhang","type":"alumni","past":"master","url":"https://aprilzhang.design/","linkedin":"https://www.linkedin.com/in/aprilxczhang/","dir":"content/output/people","base":"april-zhang.json","ext":".json","sourceBase":"april-zhang.yaml","sourceExt":".yaml","id":"april-zhang","photo":"/static/images/people/april-zhang.jpg"},{"name":"Ashratuz Zavin Asha","type":"phd","url":"https://sites.google.com/view/ashratuzzavinasha","scholar":"https://scholar.google.com/citations?user=E7gtMMoAAAAJ","labs":["utouch"],"dir":"content/output/people","base":"ashratuz-zavin-asha.json","ext":".json","sourceBase":"ashratuz-zavin-asha.yaml","sourceExt":".yaml","id":"ashratuz-zavin-asha","photo":"/static/images/people/ashratuz-zavin-asha.jpg"},{"name":"Ben Pearman","type":"master","url":"https://benpearman.xyz","linkedin":"https://www.linkedin.com/in/benpearman","scholar":"https://scholar.google.com/citations?user=QJ5G87UAAAAJ\u0026hl=en","labs":["data-experience"],"dir":"content/output/people","base":"ben-pearman.json","ext":".json","sourceBase":"ben-pearman.yaml","sourceExt":".yaml","id":"ben-pearman","photo":"/static/images/people/ben-pearman.jpg","title":"MSc"},{"name":"Bheesha Kumari","type":"alumni","past":"undergrad","email":"bheesha.kumari@ucalgary.ca","linkedin":"https://www.linkedin.com/in/bheesha-kumari-/","dir":"content/output/people","base":"bheesha-kumari.json","ext":".json","sourceBase":"bheesha-kumari.yaml","sourceExt":".yaml","id":"bheesha-kumari","photo":"/static/images/people/bheesha-kumari.jpg"},{"name":"Bon Adriel Aseniero","type":"alumni","past":"phd","now":"Autodesk Research","url":"http://bonadriel.com/","scholar":"https://scholar.google.com/citations?user=V4nRMoMAAAAJ","twitter":"https://twitter.com/HexenKoenig","facebook":"https://www.facebook.com/bonadriel","linkedin":"https://www.linkedin.com/in/bon-adriel-aseniero-47140560/","labs":["utouch"],"dir":"content/output/people","base":"bon-adriel-aseniero.json","ext":".json","sourceBase":"bon-adriel-aseniero.yaml","sourceExt":".yaml","id":"bon-adriel-aseniero","photo":"/static/images/people/bon-adriel-aseniero.jpg"},{"name":"Bonnie Wu","type":"master","linkedin":"https://www.linkedin.com/in/bonniewfwu","labs":["data-experience"],"dir":"content/output/people","base":"bonnie-wu.json","ext":".json","sourceBase":"bonnie-wu.yaml","sourceExt":".yaml","id":"bonnie-wu","photo":"/static/images/people/bonnie-wu.jpg","title":"MSc"},{"name":"Brennan Jones","type":"alumni","past":"phd","now":"Xi'an Jiaotong-Liverpool University","url":"https://brennanjones.com/","scholar":"https://scholar.google.ca/citations?user=yzxiadIAAAAJ","linkedin":"https://www.linkedin.com/in/bdgjones/","dir":"content/output/people","base":"brennan-jones.json","ext":".json","sourceBase":"brennan-jones.yaml","sourceExt":".yaml","id":"brennan-jones","photo":"/static/images/people/brennan-jones.jpg"},{"name":"Carl Gutwin","type":"alumni","past":"phd","now":"University of Saskatchewan","url":"https://www.cs.usask.ca/~gutwin/","scholar":"https://scholar.google.ca/citations?user=ZWsIEYcAAAAJ","email":"gutwin@cs.usask.ca","dir":"content/output/people","base":"carl-gutwin.json","ext":".json","sourceBase":"carl-gutwin.yaml","sourceExt":".yaml","id":"carl-gutwin","photo":"/static/images/people/carl-gutwin.jpg"},{"name":"Carman Neustaedter","type":"alumni","past":"phd","now":"Simon Fraser University","url":"https://carmster.com/","twitter":"https://mobile.twitter.com/dr_carmster","scholar":"https://scholar.google.ca/citations?user=krQJJwIAAAAJ","linkedin":"https://www.linkedin.com/in/dr-carman-neustaedter-3666591","dir":"content/output/people","base":"carman-neustaedter.json","ext":".json","sourceBase":"carman-neustaedter.yaml","sourceExt":".yaml","id":"carman-neustaedter","photo":"/static/images/people/carman-neustaedter.jpg"},{"name":"Carmen Hull","type":"alumni","past":"phd","now":"Northeastern University","url":"https://www.carmenhull.com/","linkedin":"https://www.linkedin.com/in/carmen-hull-2083ab20/","scholar":"https://scholar.google.com/citations?user=eJ-d2wsAAAAJ\u0026hl=en","labs":["data-experience"],"dir":"content/output/people","base":"carmen-hull.json","ext":".json","sourceBase":"carmen-hull.yaml","sourceExt":".yaml","id":"carmen-hull","photo":"/static/images/people/carmen-hull.jpg","title":"Alumni (PhD)"},{"name":"Carson Witts","type":"master","email":"carson.witts@ucalgary.ca","linkedin":"https://www.linkedin.com/in/carson-witts","labs":["data-experience"],"dir":"content/output/people","base":"carson-witts.json","ext":".json","sourceBase":"carson-witts.yaml","sourceExt":".yaml","id":"carson-witts","photo":"/static/images/people/carson-witts.jpg","title":"MSc"},{"name":"Charlotte Tang","type":"alumni","past":"phd","now":"University of Michigan","url":"https://charlottetang.ca/","scholar":"https://scholar.google.com/citations?user=RYkK_owAAAAJ","linkedin":"https://www.linkedin.com/in/charlottetang/","dir":"content/output/people","base":"charlotte-tang.json","ext":".json","sourceBase":"charlotte-tang.yaml","sourceExt":".yaml","id":"charlotte-tang","photo":"/static/images/people/charlotte-tang.jpg"},{"name":"Christian Salvador","type":"master","url":"https://www.csalt.dev/","email":"christian.salvador@ucalgary.ca","linkedin":"https://www.linkedin.com/in/cl-salvador/","labs":["curio"],"dir":"content/output/people","base":"christian-salvador.json","ext":".json","sourceBase":"christian-salvador.yaml","sourceExt":".yaml","id":"christian-salvador","photo":"/static/images/people/christian-salvador.jpg"},{"name":"Christopher Collins","type":"alumni","past":"phd","now":"Ontario Tech University","url":"https://vialab.ca/","scholar":"https://scholar.google.ca/citations?user=yHRw4eMAAAAJ\u0026hl=en","linkedin":"https://www.linkedin.com/in/christophercollins/","dir":"content/output/people","base":"christopher-collins.json","ext":".json","sourceBase":"christopher-collins.yaml","sourceExt":".yaml","id":"christopher-collins","photo":"/static/images/people/christopher-collins.jpg"},{"name":"Christopher Rodriguez","type":"alumni","past":"undergrad","email":"christopher.rodrigue@ucalgary.ca","linkedin":"https://www.linkedin.com/in/christopher-rodriguez-74259217a/","dir":"content/output/people","base":"christopher-rodriguez.json","ext":".json","sourceBase":"christopher-rodriguez.yaml","sourceExt":".yaml","id":"christopher-rodriguez","photo":"/static/images/people/christopher-rodriguez.jpg"},{"name":"Christopher Smith","type":"phd","program":"cs","linkedin":"https://www.linkedin.com/in/christopher-smith-uofc/","labs":["utouch"],"dir":"content/output/people","base":"christopher-smith.json","ext":".json","sourceBase":"christopher-smith.yaml","sourceExt":".yaml","id":"christopher-smith","photo":"/static/images/people/christopher-smith.jpg"},{"name":"Clara Xi","alias":"Clara Y. Xi","type":"phd","labs":["curio"],"url":"https://cspages.ucalgary.ca/~clara.xi/","email":"clara.xi@ucalgary.ca","scholar":"https://scholar.google.ca/citations?user=4iYXktYAAAAJ","dir":"content/output/people","base":"clara-xi.json","ext":".json","sourceBase":"clara-xi.yaml","sourceExt":".yaml","id":"clara-xi","photo":"/static/images/people/clara-xi.jpg"},{"name":"Colin Au Yeung","type":"master","url":"https://colinauyeung.github.io/","email":"colinauyeung@ucalgary.ca","twitter":"https://x.com/yandereyuuko","linkedin":"https://www.linkedin.com/in/colin-au-yeung-348293312","labs":["curio"],"dir":"content/output/people","base":"colin-auyeung.json","ext":".json","sourceBase":"colin-auyeung.yaml","sourceExt":".yaml","id":"colin-auyeung","photo":"/static/images/people/colin-auyeung.jpg"},{"name":"Dane Bertram","type":"alumni","past":"master","now":"YNAB","url":"https://danebertram.com/","linkedin":"https://www.linkedin.com/in/danebertram/","github":"https://github.com/dbertram","dir":"content/output/people","base":"dane-bertram.json","ext":".json","sourceBase":"dane-bertram.yaml","sourceExt":".yaml","id":"dane-bertram","photo":"/static/images/people/dane-bertram.jpg"},{"name":"Danissa Sandykbayeva","past":"master","type":"alumni","email":"danissa.sandykbay1@ucalgary.ca","scholar":"https://scholar.google.com/citations?user=tWhlB7YAAAAJ","linkedin":"https://www.linkedin.com/in/danissa-sandykbayeva/","labs":["diff"],"dir":"content/output/people","base":"danissa-sandykbayeva.json","ext":".json","sourceBase":"danissa-sandykbayeva.yaml","sourceExt":".yaml","id":"danissa-sandykbayeva","photo":"/static/images/people/danissa-sandykbayeva.jpg"},{"name":"D'Arcy Norman","type":"alumni","past":"phd","now":"University of Calgary","labs":["utouch"],"url":"https://darcynorman.net/","twitter":"https://twitter.com/realdlnorman","linkedin":"https://www.linkedin.com/in/d-arcy-norman-phd-48648272/","scholar":"https://scholar.google.ca/citations?user=s6C2YEIAAAAJ\u0026hl=en","dir":"content/output/people","base":"darcy-norman.json","ext":".json","sourceBase":"darcy-norman.yaml","sourceExt":".yaml","id":"darcy-norman","photo":"/static/images/people/darcy-norman.jpg"},{"name":"David Ledo","alias":"David Ledo Maira","type":"alumni","past":"phd","now":"Autodesk Research","url":"https://www.davidledo.com/","scholar":"https://scholar.google.com/citations?user=V_2BZDoAAAAJ","linkedin":"https://www.linkedin.com/in/david-ledo-75914249","labs":["curio"],"dir":"content/output/people","base":"david-ledo.json","ext":".json","sourceBase":"david-ledo.yaml","sourceExt":".yaml","id":"david-ledo","photo":"/static/images/people/david-ledo.jpg"},{"name":"Desmond Larsen-Rosner","type":"alumni","past":"master","github":"https://github.com/desmondlr","linkedin":"https://www.linkedin.com/in/desmondlr","labs":["utouch"],"dir":"content/output/people","base":"desmond-larsen-rosner.json","ext":".json","sourceBase":"desmond-larsen-rosner.yaml","sourceExt":".yaml","id":"desmond-larsen-rosner","photo":"/static/images/people/desmond-larsen-rosner.jpg"},{"name":"Dinmukhammed Mukashev","type":"alumni","past":"master","email":"dimash.mukashev@ucalgary.ca","scholar":"https://scholar.google.com/citations?user=rUEDTOsAAAAJ","dir":"content/output/people","base":"dinmukhammed-mukashev.json","ext":".json","sourceBase":"dinmukhammed-mukashev.yaml","sourceExt":".yaml","id":"dinmukhammed-mukashev","photo":"/static/images/people/dinmukhammed-mukashev.jpg"},{"name":"Donald Cox","type":"alumni","past":"master","now":"Ping Identity","linkedin":"https://www.linkedin.com/in/donald-cox-6455435/","dir":"content/output/people","base":"donald-cox.json","ext":".json","sourceBase":"donald-cox.yaml","sourceExt":".yaml","id":"donald-cox","photo":"/static/images/people/donald-cox.jpg"},{"name":"Doug Schaeffer","type":"alumni","past":"master","now":"R2 Solutions","linkedin":"https://www.linkedin.com/in/dougschaffer/","dir":"content/output/people","base":"doug-schaeffer.json","ext":".json","sourceBase":"doug-schaeffer.yaml","sourceExt":".yaml","id":"doug-schaeffer","photo":"/static/images/people/doug-schaeffer.jpg"},{"name":"Ebube Anachebe","type":"undergrad","labs":["shivers"],"linkedin":"https://www.linkedin.com/in/ebubea","github":"https://github.com/ebubea1","email":"chukwudiebube.anache@ucalgary.ca","dir":"content/output/people","base":"ebube-anachebe.json","ext":".json","sourceBase":"ebube-anachebe.yaml","sourceExt":".yaml","id":"ebube-anachebe","photo":"/static/images/people/ebube-anachebe.jpg"},{"name":"Edward Tse","type":"alumni","past":"phd","now":"Ai Parenting","url":"https://edwardtse.com/","linkedin":"https://ca.linkedin.com/in/edward-tse","twitter":"https://twitter.com/DoctorET","dir":"content/output/people","base":"edward-tse.json","ext":".json","sourceBase":"edward-tse.yaml","sourceExt":".yaml","id":"edward-tse","photo":"/static/images/people/edward-tse.jpg"},{"name":"Faiz Marsad","type":"alumni","past":"undergrad","email":"faiz.marsad@ucalgary.ca","dir":"content/output/people","base":"faiz-marsad.json","ext":".json","sourceBase":"faiz-marsad.yaml","sourceExt":".yaml","id":"faiz-marsad","photo":"/static/images/people/no-profile.jpg"},{"name":"Georgina Freeman","type":"phd","url":"https://www.ginafreeman.com/about","cv":"https://static1.squarespace.com/static/5d6eff51c4807e0001982813/t/611401518943c1218b053432/1628701009534/Georgina+Freeman+CV+AUG+2021+NO+ID.pdf","scholar":"https://scholar.google.com/citations?user=kkZQiusAAAAJ\u0026hl=en","linkedin":"https://www.linkedin.com/in/georgina-gina-freeman-1072b081","labs":["curio"],"dir":"content/output/people","base":"georgina-freeman.json","ext":".json","sourceBase":"georgina-freeman.yaml","sourceExt":".yaml","id":"georgina-freeman","photo":"/static/images/people/georgina-freeman.jpg"},{"name":"Godwin Saure","type":"undergrad","email":"godwin.saure@gmail.com","linkedin":"https://www.linkedin.com/in/godwin-saure/","labs":["data-experience"],"dir":"content/output/people","base":"godwin-saure.json","ext":".json","sourceBase":"godwin-saure.yaml","sourceExt":".yaml","id":"godwin-saure","photo":"/static/images/people/godwin-saure.jpg","title":"Ugrad"},{"name":"Grace Ferguson","type":"alumni","past":"undergrad","now":"Insignia Software","linkedin":"https://www.linkedin.com/in/grace-ferguson-219982193","dir":"content/output/people","base":"grace-ferguson.json","ext":".json","sourceBase":"grace-ferguson.yaml","sourceExt":".yaml","id":"grace-ferguson","photo":"/static/images/people/grace-ferguson.jpg"},{"name":"Gregor McEwan","type":"alumni","past":"master","now":"Modail Mara","url":"https://www.modailmara.ca/whoweare","scholar":"https://scholar.google.com/citations?user=6xdmrPgAAAAJ","linkedin":"https://www.linkedin.com/in/gregor-mcewan-5076992/","dir":"content/output/people","base":"gregor-mcewan.json","ext":".json","sourceBase":"gregor-mcewan.yaml","sourceExt":".yaml","id":"gregor-mcewan","photo":"/static/images/people/gregor-mcewan.jpg"},{"name":"Harrison Chen","type":"alumni","past":"undergrad","url":"https://harrisoneverettchen.com","email":"hechen@ucalgary.ca","github":"https://github.com/Hechen93","linkedin":"https://www.linkedin.com/in/harrison-chen-a90b5569/","dir":"content/output/people","base":"harrison-chen.json","ext":".json","sourceBase":"harrison-chen.yaml","sourceExt":".yaml","id":"harrison-chen","photo":"/static/images/people/harrison-chen.jpg"},{"name":"Helen Ai He","type":"alumni","past":"faculty","title":"Alumni ()","keywords":null,"url":"https://helenaihe.com/research","scholar":"https://scholar.google.com/citations?user=FlMKf0gAAAAJ","twitter":"https://twitter.com/helenaihe","email":"helen.he1@ucalgary.ca","labs":["data-experience"],"dir":"content/output/people","base":"helen-ai-he.json","ext":".json","sourceBase":"helen-ai-he.yaml","sourceExt":".yaml","id":"helen-ai-he","photo":"/static/images/people/helen-ai-he.jpg"},{"name":"Hiroki Kaimoto","type":"alumni","past":"visiting","now":"University of Tokyo","url":"https://hirokikaimoto.com","scholar":"https://scholar.google.co.jp/citations?user=BPL36T0AAAAJ","twitter":"https://twitter.com/open_origin","email":"hkaimoto@xlab.iii.u-tokyo.ac.jp","dir":"content/output/people","base":"hiroki-kaimoto.json","ext":".json","sourceBase":"hiroki-kaimoto.yaml","sourceExt":".yaml","id":"hiroki-kaimoto","photo":"/static/images/people/hiroki-kaimoto.jpg"},{"name":"Huann Zhao","type":"master","url":"https://contacts.ucalgary.ca/info/cpsc/profiles/1-12783408","email":"huanjun.zhao@ucalgary.ca","linkedin":"https://www.linkedin.com/in/huanjun-zhao-00b281230/","labs":["diff"],"dir":"content/output/people","base":"huanjun-zhao.json","ext":".json","sourceBase":"huanjun-zhao.yaml","sourceExt":".yaml","id":"huanjun-zhao","photo":"/static/images/people/huanjun-zhao.jpg"},{"name":"Iryna Luchak","type":"alumni","past":"master","linkedin":"https://www.linkedin.com/in/irynaluchak/","labs":["utouch"],"dir":"content/output/people","base":"iryna-luchak.json","ext":".json","sourceBase":"iryna-luchak.yaml","sourceExt":".yaml","id":"iryna-luchak","photo":"/static/images/people/iryna-luchak.jpg"},{"name":"Isaac Ng","type":"master","labs":["shivers"],"email":"isaac.ng@ucalgary.ca","linkedin":"https://www.linkedin.com/in/isaac-ng-273810254/","github":"https://github.com/nkh6601","dir":"content/output/people","base":"isaac-ng.json","ext":".json","sourceBase":"isaac-ng.yaml","sourceExt":".yaml","id":"isaac-ng","photo":"/static/images/people/isaac-ng.jpg"},{"name":"Isabella Huang","type":"undergrad","labs":["shivers"],"linkedin":"https://www.linkedin.com/in/isabella-huang-667a02269/","github":"https://github.com/IsabellaH537","email":"isabella.huang@ucalgary.ca","dir":"content/output/people","base":"isabella-huang.json","ext":".json","sourceBase":"isabella-huang.yaml","sourceExt":".yaml","id":"isabella-huang","photo":"/static/images/people/isabella-huang.jpg"},{"name":"Jagoda Walny","type":"alumni","past":"phd","now":"Canada Energy Regulator (CER)","url":"https://www.fwd50.com/speaker/2395/jagoda-walny-nix","scholar":"https://scholar.google.ca/citations?user=EK99E-8AAAAJ\u0026hl=en","linkedin":"https://www.linkedin.com/in/jagoda/?originalSubdomain=ca","labs":["data-experience"],"dir":"content/output/people","base":"jagoda-walny.json","ext":".json","sourceBase":"jagoda-walny.yaml","sourceExt":".yaml","id":"jagoda-walny","photo":"/static/images/people/jagoda-walny.jpg","title":"Alumni (PhD)"},{"name":"James Tam","type":"alumni","past":"master","now":"University of Calgary","url":"http://pages.cpsc.ucalgary.ca/~tamj/index.html","email":"tamj@cpsc.ucalgary.ca","dir":"content/output/people","base":"james-tam.json","ext":".json","sourceBase":"james-tam.yaml","sourceExt":".yaml","id":"james-tam","photo":"/static/images/people/no-profile.jpg"},{"name":"Jane Shen","type":"master","linkedin":"https://www.linkedin.com/in/shen-jane/","email":"jane.shen1@ucalgary.ca","labs":["curio"],"dir":"content/output/people","base":"jane-shen.json","ext":".json","sourceBase":"jane-shen.yaml","sourceExt":".yaml","id":"jane-shen","photo":"/static/images/people/jane-shen.jpg"},{"name":"Jarin Thundathil","type":"alumni","past":"undergrad","email":"jarin.thundathil@ucalgary.ca","linkedin":"https://www.linkedin.com/in/jarin-thundathil-170616150/","labs":["programmable-reality"],"dir":"content/output/people","base":"jarin-thundathil.json","ext":".json","sourceBase":"jarin-thundathil.yaml","sourceExt":".yaml","id":"jarin-thundathil","photo":"/static/images/people/jarin-thundathil.jpg"},{"name":"Jessi Stark","type":"alumni","past":"master","now":"University of Toronto","url":"https://jtstark.com/","scholar":"https://scholar.google.com/citations?user=aRkKN5UAAAAJ","twitter":"https://twitter.com/_jessistark","labs":["utouch"],"dir":"content/output/people","base":"jessi-stark.json","ext":".json","sourceBase":"jessi-stark.yaml","sourceExt":".yaml","id":"jessi-stark","photo":"/static/images/people/jessi-stark.jpg"},{"name":"Jian Liao","type":"alumni","past":"undergrad","email":"jian.liao1@ucalgary.ca","twitter":"https://twitter.com/imjliao","linkedin":"https://www.linkedin.com/in/jian-liao/","github":"https://github.com/jlia0","dir":"content/output/people","base":"jian-liao.json","ext":".json","sourceBase":"jian-liao.yaml","sourceExt":".yaml","id":"jian-liao","photo":"/static/images/people/jian-liao.jpg"},{"name":"Jiannan Li","type":"alumni","past":"master","now":"University of Toronto","url":"https://www.dgp.toronto.edu/~jiannanli/","scholar":"https://scholar.google.com/citations?user=wyW0rJQAAAAJ","labs":["utouch"],"dir":"content/output/people","base":"jiannan-li.json","ext":".json","sourceBase":"jiannan-li.yaml","sourceExt":".yaml","id":"jiannan-li","photo":"/static/images/people/jiannan-li.jpg"},{"name":"Joshua Lee","type":"alumni","past":"undergrad","email":"chang.lee@ucalgary.ca","linkedin":"https://www.linkedin.com/in/chang-lee/","labs":["diff"],"dir":"content/output/people","base":"joshua-lee.json","ext":".json","sourceBase":"joshua-lee.yaml","sourceExt":".yaml","id":"joshua-lee","photo":"/static/images/people/joshua-lee.jpg"},{"name":"Karly Ross","type":"master","labs":["data-experience"],"linkedin":"https://www.linkedin.com/in/karly-j-ross/","dir":"content/output/people","base":"karly-ross.json","ext":".json","sourceBase":"karly-ross.yaml","sourceExt":".yaml","id":"karly-ross","photo":"/static/images/people/karly-ross.jpg","title":"MSc"},{"name":"Karthik Mahadevan","type":"alumni","past":"master","now":"University of Toronto","url":"https://karthikm0.github.io/","scholar":"https://scholar.google.ca/citations?user=aK4siPkAAAAJ","labs":["utouch"],"dir":"content/output/people","base":"karthik-mahadevan.json","ext":".json","sourceBase":"karthik-mahadevan.yaml","sourceExt":".yaml","id":"karthik-mahadevan","photo":"/static/images/people/karthik-mahadevan.jpg"},{"name":"Katherine Currier","type":"alumni","past":"master","now":"Insulet Corporation","dir":"content/output/people","base":"katherine-currier.json","ext":".json","sourceBase":"katherine-currier.yaml","sourceExt":".yaml","id":"katherine-currier","photo":"/static/images/people/katherine-currier.jpg"},{"name":"Kathryn Blair","alias":"Kathryn Marie Blair","type":"phd","program":"cmd","url":"http://kathrynblair.com/","twitter":"https://twitter.com/kathblair","email":"kathryn.blair@ucalgary.ca","linkedin":"https://www.linkedin.com/in/kmblair/","labs":["curio"],"dir":"content/output/people","base":"kathryn-blair.json","ext":".json","sourceBase":"kathryn-blair.yaml","sourceExt":".yaml","id":"kathryn-blair","photo":"/static/images/people/kathryn-blair.jpg"},{"name":"Kathryn Elliot-Rounding","type":"alumni","past":"master","now":"YNAB","linkedin":"https://www.linkedin.com/in/kathrynkrounding","dir":"content/output/people","base":"kathryn-elliot-rounding.json","ext":".json","sourceBase":"kathryn-elliot-rounding.yaml","sourceExt":".yaml","id":"kathryn-elliot-rounding","photo":"/static/images/people/kathryn-elliot-rounding.jpg"},{"name":"Kaynen Mitchell","type":"alumni","past":"undergrad","email":"kaynen.mitchell1@ucalgary.ca","linkedin":"https://www.linkedin.com/in/kaynen-mitchell/","dir":"content/output/people","base":"kaynen-mitchell.json","ext":".json","sourceBase":"kaynen-mitchell.yaml","sourceExt":".yaml","id":"kaynen-mitchell","photo":"/static/images/people/kaynen-mitchell.jpg"},{"name":"Keiichi Ihara","type":"alumni","past":"visiting","url":"https://www.iplab.cs.tsukuba.ac.jp/~kihara/","email":"kihara@iplab.cs.tsukuba.ac.jp","dir":"content/output/people","base":"keiichi-ihara.json","ext":".json","sourceBase":"keiichi-ihara.yaml","sourceExt":".yaml","id":"keiichi-ihara","photo":"/static/images/people/keiichi-ihara.jpg"},{"name":"Kendra Wannamaker","type":"alumni","past":"master","now":"Autodesk Research","url":"http://kendrawannamaker.com","scholar":"https://scholar.google.com/citations?user=fC1WI8cAAAAJ\u0026hl=en","linkedin":"https://www.linkedin.com/in/kendra-wannamaker-88622aa4/","labs":["data-experience"],"dir":"content/output/people","base":"kendra-wannamaker.json","ext":".json","sourceBase":"kendra-wannamaker.yaml","sourceExt":".yaml","id":"kendra-wannamaker","photo":"/static/images/people/kendra-wannamaker.jpg","title":"Alumni (MSc)"},{"name":"Kevin Van","type":"alumni","past":"undergrad","email":"kevin.van@ucalgary.ca","url":"https://kevin-van.github.io/Website/","github":"https://github.com/kevin-van","linkedin":"https://www.linkedin.com/in/kevin-van-a66130204/","labs":["utouch"],"dir":"content/output/people","base":"kevin-van.json","ext":".json","sourceBase":"kevin-van.yaml","sourceExt":".yaml","id":"kevin-van","photo":"/static/images/people/kevin-van.jpg"},{"name":"Kimberly Tee","type":"alumni","past":"master","now":"Shopify","scholar":"https://scholar.google.com/citations?user=srTy2voAAAAJ","linkedin":"https://www.linkedin.com/in/kimberly-tee-7305b159/","dir":"content/output/people","base":"kimberly-tee.json","ext":".json","sourceBase":"kimberly-tee.yaml","sourceExt":".yaml","id":"kimberly-tee","photo":"/static/images/people/kimberly-tee.jpg"},{"name":"Kurtis Danyluk","alias":"Kurtis Thorvald Danyluk","type":"alumni","past":"phd","scholar":"https://scholar.google.com/citations?user=vr-EF5IAAAAJ","labs":["data-experience"],"dir":"content/output/people","base":"kurtis-danyluk.json","ext":".json","sourceBase":"kurtis-danyluk.yaml","sourceExt":".yaml","id":"kurtis-danyluk","photo":"/static/images/people/kurtis-danyluk.jpg","title":"Alumni (PhD)"},{"name":"Kyzyl Monteiro","type":"alumni","past":"visiting","now":"CMU","url":"https://kyzyl.me","scholar":"https://scholar.google.ca/citations?user=A9IqYNoAAAAJ","twitter":"https://twitter.com/kyzylmonteiro","email":"kyzylmonteiro@gmail.com","github":"https://github.com/kyzylmonteiro","linkedin":"https://www.linkedin.com/in/kyzylmonteiro/","dir":"content/output/people","base":"kyzyl-monteiro.json","ext":".json","sourceBase":"kyzyl-monteiro.yaml","sourceExt":".yaml","id":"kyzyl-monteiro","photo":"/static/images/people/kyzyl-monteiro.jpg"},{"name":"Linda Tauscher","type":"alumni","past":"master","now":"NetStart Consulting Ltd.","linkedin":"https://www.linkedin.com/in/ltauscher","url":"https://netstart.com","dir":"content/output/people","base":"linda-tauscher.json","ext":".json","sourceBase":"linda-tauscher.yaml","sourceExt":".yaml","id":"linda-tauscher","photo":"/static/images/people/linda-tauscher.jpg"},{"name":"Lychelle Pham","type":"undergrad","labs":["shivers","data-experience"],"linkedin":"https://www.linkedin.com/in/lychelle-pham-4b70332b9","github":"https://github.com/LychellePham","email":"lychelle.pham@ucalgary.ca","dir":"content/output/people","base":"lychelle-pham.json","ext":".json","sourceBase":"lychelle-pham.yaml","sourceExt":".yaml","id":"lychelle-pham","photo":"/static/images/people/lychelle-pham.jpg","title":"Ugrad"},{"name":"Mackenzie Bowal","type":"alumni","past":"undergrad","email":"mackenzie.bowal@ucalgary.ca","linkedin":"https://www.linkedin.com/in/mackenzie-bowal/","labs":["utouch"],"dir":"content/output/people","base":"mackenzie-bowal.json","ext":".json","sourceBase":"mackenzie-bowal.yaml","sourceExt":".yaml","id":"mackenzie-bowal","photo":"/static/images/people/mackenzie-bowal.jpg"},{"name":"Mackenzie Hisako Dalton","type":"alumni","past":"undergrad","url":"https://hisak00.weebly.com/","labs":["data-experience"],"dir":"content/output/people","base":"mackenzie-hisako-dalton.json","ext":".json","sourceBase":"mackenzie-hisako-dalton.yaml","sourceExt":".yaml","id":"mackenzie-hisako-dalton","photo":"/static/images/people/mackenzie-hisako-dalton.jpg","title":"Alumni (Ugrad)"},{"name":"Maham Fatima","type":"alumni","past":"undergrad","now":"University of Calgary","cv":"https://linktr.ee/mahamf","github":"https://github.com/maham000","linkedin":"https://www.linkedin.com/in/maham-f/","email":"maham.fatima2@ucalgary.ca","url":"https://mahamf567.wixsite.com/maham-2","labs":["curio"],"dir":"content/output/people","base":"maham-fatima.json","ext":".json","sourceBase":"maham-fatima.yaml","sourceExt":".yaml","id":"maham-fatima","photo":"/static/images/people/maham-fatima.jpg"},{"name":"Manjot Khangura","type":"alumni","past":"undergrad","email":"manjot.khangura@ucalgary.ca","linkedin":"https://www.linkedin.com/in/manjot-khangura/","dir":"content/output/people","base":"manjot-khangura.json","ext":".json","sourceBase":"manjot-khangura.yaml","sourceExt":".yaml","id":"manjot-khangura","photo":"/static/images/people/manjot-khangura.jpg"},{"name":"Manuel Rodriguez","type":"alumni","past":"undergrad","email":"manuel.rodriguez@ucalgary.ca","linkedin":"https://www.linkedin.com/in/manuel-rodriguez/","dir":"content/output/people","base":"manuel-rodriguez.json","ext":".json","sourceBase":"manuel-rodriguez.yaml","sourceExt":".yaml","id":"manuel-rodriguez","photo":"/static/images/people/manuel-rodriguez.jpg"},{"name":"Marcus Friedel","type":"alumni","past":"master","labs":["utouch"],"url":"https://marcusfriedel.com","facebook":"https://www.facebook.com/marcus.friedel.3","email":"marcus.friedel@ucalgary.ca","linkedin":"https://www.linkedin.com/in/marcusfriedel/","dir":"content/output/people","base":"marcus-friedel.json","ext":".json","sourceBase":"marcus-friedel.yaml","sourceExt":".yaml","id":"marcus-friedel","photo":"/static/images/people/marcus-friedel.jpg"},{"name":"Marian Dörk","type":"alumni","past":"phd","now":"University of Applied Sciences Potsdam","url":"https://mariandoerk.de/","email":"marian.doerk@fh-potsdam.de","scholar":"https://scholar.google.com/citations?user=pDg8ZMAAAAAJ\u0026hl=en","dir":"content/output/people","base":"marian-doerk.json","ext":".json","sourceBase":"marian-doerk.yaml","sourceExt":".yaml","id":"marian-doerk","photo":"/static/images/people/marian-doerk.jpg"},{"name":"Mark Hancock","type":"alumni","past":"phd","now":"University of Waterloo","url":"https://uwaterloo.ca/touchlab","email":"mark.hancock@uwaterloo.ca","linkedin":"https://www.linkedin.com/in/mark-hancock-55439925","scholar":"https://scholar.google.ca/citations?user=PYvcN3cAAAAJ\u0026hl=en","dir":"content/output/people","base":"mark-hancock.json","ext":".json","sourceBase":"mark-hancock.yaml","sourceExt":".yaml","id":"mark-hancock","photo":"/static/images/people/mark-hancock.jpg"},{"name":"Mark Roseman","type":"alumni","past":"master","url":"https://markroseman.com/","linkedin":"https://www.linkedin.com/in/mroseman/","dir":"content/output/people","base":"mark-roseman.json","ext":".json","sourceBase":"mark-roseman.yaml","sourceExt":".yaml","id":"mark-roseman","photo":"/static/images/people/mark-roseman.jpg"},{"name":"Martin Feick","type":"alumni","past":"phd","now":"Saarland University","url":"http://martinfeick.com/","scholar":"https://scholar.google.de/citations?user=az0GkfQAAAAJ","github":"https://github.com/MartinFk","twitter":"https://twitter.com/mafeick","labs":["utouch"],"dir":"content/output/people","base":"martin-feick.json","ext":".json","sourceBase":"martin-feick.yaml","sourceExt":".yaml","id":"martin-feick","photo":"/static/images/people/martin-feick.jpg"},{"name":"Matthew Dunlap","type":"alumni","past":"master","now":"UNC Chapel Hill","url":"https://odum.unc.edu/people/dunlap/","linkedin":"https://www.linkedin.com/in/matthew-dunlap-47a7b3b3/","github":"https://github.com/matthew-a-dunlap","dir":"content/output/people","base":"matthew-dunlap.json","ext":".json","sourceBase":"matthew-dunlap.yaml","sourceExt":".yaml","id":"matthew-dunlap","photo":"/static/images/people/matthew-dunlap.jpg"},{"name":"Mehrad Faridan","type":"alumni","past":"undergrad","now":"University of Tokyo and MaKami College","url":"https://www.mehradfaridan.com/","email":"mehrad.faridan1@ucalgary.ca","scholar":"https://scholar.google.com/citations?user=amh7v2EAAAAJ","twitter":"https://twitter.com/MehradFaridan","github":"https://github.com/mehradFaridan","linkedin":"https://www.linkedin.com/in/mehrad-f-a34462164/","labs":["programmable-reality"],"dir":"content/output/people","base":"mehrad-faridan.json","ext":".json","sourceBase":"mehrad-faridan.yaml","sourceExt":".yaml","id":"mehrad-faridan","photo":"/static/images/people/mehrad-faridan.jpg"},{"name":"Melissa Hoang","type":"alumni","past":"undergrad","email":"melissa.hoang@ucalgary.ca","linkedin":"https://www.linkedin.com/in/melissa-e-hoang/","dir":"content/output/people","base":"melissa-hoang.json","ext":".json","sourceBase":"melissa-hoang.yaml","sourceExt":".yaml","id":"melissa-hoang","photo":"/static/images/people/melissa-hoang.jpg"},{"name":"Miaosen Wang","type":"alumni","past":"master","now":"DeepMind","scholar":"https://scholar.google.com/citations?user=j99nvycAAAAJ\u0026hl=en","linkedin":"https://www.linkedin.com/in/miaosen-wang-2607a317/","twitter":"https://x.com/MiaosenWang","dir":"content/output/people","base":"miaosen-wang.json","ext":".json","sourceBase":"miaosen-wang.yaml","sourceExt":".yaml","id":"miaosen-wang","photo":"/static/images/people/miaosen-wang.jpg"},{"name":"Michael Hung","alias":"Michael Y-S Hung","type":"alumni","past":"master","now":"Open Cycle Technologies Inc.","url":"https://michael-hung.ca","email":"myshung@ucalgary.ca","github":"https://github.com/murrrkle","linkedin":"https://www.linkedin.com/in/hungyukshing","labs":["curio"],"dir":"content/output/people","base":"michael-hung.json","ext":".json","sourceBase":"michael-hung.yaml","sourceExt":".yaml","id":"michael-hung","photo":"/static/images/people/michael-hung.jpg"},{"name":"Micheal Boyle","type":"alumni","past":"phd","dir":"content/output/people","base":"micheal-boyle.json","ext":".json","sourceBase":"micheal-boyle.yaml","sourceExt":".yaml","id":"micheal-boyle","photo":"/static/images/people/no-profile.jpg"},{"name":"Micheal Nunes","type":"alumni","past":"master","dir":"content/output/people","base":"micheal-nunes.json","ext":".json","sourceBase":"micheal-nunes.yaml","sourceExt":".yaml","id":"micheal-nunes","photo":"/static/images/people/no-profile.jpg"},{"name":"Micheal Roudning","type":"alumni","past":"master","now":"Curve Dental","linkedin":"https://www.linkedin.com/in/mlroundi/","dir":"content/output/people","base":"micheal-rounding.json","ext":".json","sourceBase":"micheal-rounding.yaml","sourceExt":".yaml","id":"micheal-rounding","photo":"/static/images/people/micheal-rounding.jpg"},{"name":"Mille Skovhus Lunding","type":"alumni","past":"visiting","url":"https://pure.au.dk/portal/en/persons/mille-skovhus-lunding(806b2f20-b7ae-4ebc-a32f-98ee39a53380).html","email":"milledsk@cs.au.dk","scholar":"https://scholar.google.com/citations?user=kt6cN_UAAAAJ","linkedin":"https://www.linkedin.com/in/mille-skovhus-lunding-1b419583/","dir":"content/output/people","base":"mille-skovhus-lunding.json","ext":".json","sourceBase":"mille-skovhus-lunding.yaml","sourceExt":".yaml","id":"mille-skovhus-lunding","photo":"/static/images/people/mille-skovhus-lunding.jpg"},{"name":"Muhammad Mahian","type":"alumni","past":"undergrad","email":"muhammad.mahian@ucalgary.ca","dir":"content/output/people","base":"muhammad-mahian.json","ext":".json","sourceBase":"muhammad-mahian.yaml","sourceExt":".yaml","id":"muhammad-mahian","photo":"/static/images/people/no-profile.jpg"},{"name":"Nadeem Moosa","type":"undergrad","labs":["shivers"],"linkedin":"https://www.linkedin.com/in/nadeem-moosa","github":"https://github.com/DeemDeem52","email":"nadeem.moosa@ucalgary.ca","dir":"content/output/people","base":"nadeem-moosa.json","ext":".json","sourceBase":"nadeem-moosa.yaml","sourceExt":".yaml","id":"nadeem-moosa","photo":"/static/images/people/nadeem-moosa.jpg"},{"name":"Nandi Zhang","type":"alumni","past":"master","now":"University of Rochester","email":"nandi.zhang@ucalgary.ca","url":"https://nandi-zhang.github.io/","linkedin":"https://www.linkedin.com/in/nandi-zhang-022ab8184/","github":"https://github.com/nandi-zhang","labs":["programmable-reality"],"dir":"content/output/people","base":"nandi-zhang.json","ext":".json","sourceBase":"nandi-zhang.yaml","sourceExt":".yaml","id":"nandi-zhang","photo":"/static/images/people/nandi-zhang.jpg"},{"name":"Nathalie Bressa","type":"alumni","past":"visiting","now":"Aarhus University","labs":["data-experience"],"dir":"content/output/people","base":"nathalie-bressa.json","ext":".json","sourceBase":"nathalie-bressa.yaml","sourceExt":".yaml","id":"nathalie-bressa","photo":"/static/images/people/nathalie-bressa.jpg","title":"Alumni (Visiting)"},{"name":"Neil Chulpongsatorn","type":"alumni","past":"master","url":"https://thobthai.wixsite.com/neilchulpongsatorn","scholar":"https://scholar.google.com/citations?user=D0KbikIAAAAJ\u0026hl=en","linkedin":"https://www.linkedin.com/in/neil-chulpongsatorn-187273204/","email":"thobthai.chulpongsat@ucalgary.ca","labs":["data-experience","programmable-reality"],"dir":"content/output/people","base":"neil-chulpongsatorn.json","ext":".json","sourceBase":"neil-chulpongsatorn.yaml","sourceExt":".yaml","id":"neil-chulpongsatorn","photo":"/static/images/people/neil-chulpongsatorn.jpg","title":"Alumni (MSc)"},{"name":"Nelson Wong","type":"alumni","past":"master","dir":"content/output/people","base":"nelson-wong.json","ext":".json","sourceBase":"nelson-wong.yaml","sourceExt":".yaml","id":"nelson-wong","photo":"/static/images/people/nelson-wong.jpg"},{"name":"Nicolai Marquardt","type":"alumni","past":"phd","now":"Microsoft Research","url":"http://www.nicolaimarquardt.com/","scholar":"https://scholar.google.com/citations?user=PXeN0RsAAAAJ","dir":"content/output/people","base":"nicolai-marquardt.json","ext":".json","sourceBase":"nicolai-marquardt.yaml","sourceExt":".yaml","id":"nicolai-marquardt","photo":"/static/images/people/nicolai-marquardt.jpg"},{"name":"Nishan Soni","type":"alumni","past":"undergrad","email":"nishan.soni@ucalgary.ca","linkedin":"https://www.linkedin.com/in/nishan-soni/","github":"https://github.com/nishan-soni","labs":["programmable-reality"],"dir":"content/output/people","base":"nishan-soni.json","ext":".json","sourceBase":"nishan-soni.yaml","sourceExt":".yaml","id":"nishan-soni","photo":"/static/images/people/no-profile.jpg"},{"name":"Nour Hammad","type":"alumni","past":"undergrad","labs":["utouch"],"dir":"content/output/people","base":"nour-hammad.json","ext":".json","sourceBase":"nour-hammad.yaml","sourceExt":".yaml","id":"nour-hammad","photo":"/static/images/people/nour-hammad.jpg"},{"name":"Paige So'Brien","type":"alumni","past":"undergrad","labs":["data-experience"],"dir":"content/output/people","base":"paige-sobrien.json","ext":".json","sourceBase":"paige-sobrien.yaml","sourceExt":".yaml","id":"paige-sobrien","photo":"/static/images/people/paige-sobrien.jpg","title":"Alumni (Ugrad)"},{"name":"Paul Lapides","type":"alumni","past":"phd","url":"http://www.paullapides.com/","labs":["utouch"],"dir":"content/output/people","base":"paul-lapides.json","ext":".json","sourceBase":"paul-lapides.yaml","sourceExt":".yaml","id":"paul-lapides","photo":"/static/images/people/paul-lapides.jpg"},{"name":"Paul Saulnier","type":"alumni","past":"master","now":"Bravura Security","url":"http://paulsaulnier.com/","linkedin":"https://www.linkedin.com/in/paulsaulnier/","labs":["utouch"],"dir":"content/output/people","base":"paul-saulnier.json","ext":".json","sourceBase":"paul-saulnier.yaml","sourceExt":".yaml","id":"paul-saulnier","photo":"/static/images/people/paul-saulnier.jpg"},{"name":"Petra Isenberg","type":"alumni","past":"phd","now":"Inria Saclay","url":"https://petra.isenberg.cc/","linkedin":"https://www.linkedin.com/in/petraisenberg","scholar":"https://scholar.google.com/citations?user=mw0_aLoAAAAJ\u0026hl=en","dir":"content/output/people","base":"petra-isenberg.json","ext":".json","sourceBase":"petra-isenberg.yaml","sourceExt":".yaml","id":"petra-isenberg","photo":"/static/images/people/petra-isenberg.jpg"},{"name":"Priya Dhawka","type":"alumni","past":"master","email":"priyadarshinee.dhawk@ucalgary.ca","github":"https://github.com/dhawkap","linkedin":"https://www.linkedin.com/in/priya-d-15b743112/","labs":["data-experience"],"dir":"content/output/people","base":"priya-dhawka.json","ext":".json","sourceBase":"priya-dhawka.yaml","sourceExt":".yaml","id":"priya-dhawka","photo":"/static/images/people/priya-dhawka.jpg","title":"Alumni (MSc)"},{"name":"Ran Zhou","type":"alumni","past":"visiting","url":"https://www.ranzhourobot.com/","email":"ran.zhou@colorado.edu","scholar":"https://scholar.google.com/citations?user=QDCBu-cAAAAJ","linkedin":"https://www.linkedin.com/in/ran-zhou-096b8812b/","twitter":"https://twitter.com/ranzhoubot","dir":"content/output/people","base":"ran-zhou.json","ext":".json","sourceBase":"ran-zhou.yaml","sourceExt":".yaml","id":"ran-zhou","photo":"/static/images/people/ran-zhou.jpg"},{"name":"Rasmus Lunding","type":"alumni","past":"visiting","url":"https://pure.au.dk/portal/en/persons/rasmus-skovhus-lunding(a1b1400c-dffa-4227-ac5b-0218dbd72de9).html","email":"rsl@cs.au.dk","linkedin":"https://www.linkedin.com/in/rlunding/","dir":"content/output/people","base":"rasmus-lunding.json","ext":".json","sourceBase":"rasmus-lunding.yaml","sourceExt":".yaml","id":"rasmus-lunding","photo":"/static/images/people/rasmus-lunding.jpg"},{"name":"Ritik Vatsal","type":"alumni","past":"visiting","email":"vatrit@gmail.com","github":"https://github.com/RitikVatsal-2019321","linkedin":"https://www.linkedin.com/in/ritik-vatsal/","dir":"content/output/people","base":"ritik-vatsal.json","ext":".json","sourceBase":"ritik-vatsal.yaml","sourceExt":".yaml","id":"ritik-vatsal","photo":"/static/images/people/ritik-vatsal.jpg"},{"name":"Roberta Cabral Mota","type":"alumni","past":"phd","url":"https://www.robertacrmota.com/","labs":["utouch"],"dir":"content/output/people","base":"roberta-cabral-mota.json","ext":".json","sourceBase":"roberta-cabral-mota.yaml","sourceExt":".yaml","id":"roberta-cabral-mota","photo":"/static/images/people/roberta-cabral-mota.jpg"},{"name":"Roberto Diaz Marino","type":"alumni","past":"master","now":"SMART Technologies","linkedin":"https://www.linkedin.com/in/rob-diaz-marino-b7a575bb","labs":["grouplab"],"dir":"content/output/people","base":"roberto-diaz-marino.json","ext":".json","sourceBase":"roberto-diaz-marino.yaml","sourceExt":".yaml","id":"roberto-diaz-marino","photo":"/static/images/people/roberto-diaz-marino.jpg"},{"name":"Ryota Gomi","type":"alumni","past":"visiting","url":"https://www.icd.riec.tohoku.ac.jp/en/about/member/","email":"ryota.gomi.s7@dc.tohoku.ac.jp","linkedin":"https://www.linkedin.com/in/ryota-gomi-6a6029203/","dir":"content/output/people","base":"ryota-gomi.json","ext":".json","sourceBase":"ryota-gomi.yaml","sourceExt":".yaml","id":"ryota-gomi","photo":"/static/images/people/ryota-gomi.jpg"},{"name":"Sabrina Lakhdhir","type":"alumni","past":"undergrad","now":"University of Victoria","labs":["utouch"],"dir":"content/output/people","base":"sabrina-lakhdhir.json","ext":".json","sourceBase":"sabrina-lakhdhir.yaml","sourceExt":".yaml","id":"sabrina-lakhdhir","photo":"/static/images/people/sabrina-lakhdhir.jpg"},{"name":"Saja Abufarha","type":"alumni","past":"undergrad","email":"saja.abufarha@ucalgary.ca","linkedin":"https://www.linkedin.com/in/saja-abufarha/","dir":"content/output/people","base":"saja-abufarha.json","ext":".json","sourceBase":"saja-abufarha.yaml","sourceExt":".yaml","id":"saja-abufarha","photo":"/static/images/people/saja-abufarha.jpg"},{"name":"Samin Farajian","type":"alumni","past":"master","url":"https://farajiansamin.github.io","linkedin":"https://www.linkedin.com/in/samin-farajian-736018140/","dir":"content/output/people","base":"samin-farajian.json","ext":".json","sourceBase":"samin-farajian.yaml","sourceExt":".yaml","id":"samin-farajian","photo":"/static/images/people/samin-farajian.jpg"},{"name":"Sasha Ivanov","alias":"Alexander Ivanov","type":"alumni","past":"master","now":"Meta Reality Labs","labs":["data-experience"],"dir":"content/output/people","base":"sasha-ivanov.json","ext":".json","sourceBase":"sasha-ivanov.yaml","sourceExt":".yaml","id":"sasha-ivanov","photo":"/static/images/people/sasha-ivanov.jpg","title":"Alumni (MSc)"},{"name":"Sebastian Gil","type":"undergrad","labs":["shivers"],"linkedin":"https://www.linkedin.com/in/sebastian-gil-268a3b22a","github":"https://github.com/EmpOfBol","email":"sebastian.gil@ucalgary.ca","dir":"content/output/people","base":"sebastian-gil.json","ext":".json","sourceBase":"sebastian-gil.yaml","sourceExt":".yaml","id":"sebastian-gil","photo":"/static/images/people/sebastian-gil.jpg"},{"name":"Setareh Manesh","type":"alumni","past":"master","now":"Maple Scan","linkedin":"https://www.linkedin.com/in/setareh-m","scholar":"https://scholar.google.com/citations?user=Cb-_dmIAAAAJ\u0026hl=en","labs":["utouch"],"dir":"content/output/people","base":"setareh-manesh.json","ext":".json","sourceBase":"setareh-manesh.yaml","sourceExt":".yaml","id":"setareh-manesh","photo":"/static/images/people/setareh-manesh.jpg"},{"name":"Shamim Seyson","type":"alumni","past":"undergrad","email":"shamim.seyson@ucalgary.ca","linkedin":"https://www.linkedin.com/in/shamimseyson/","labs":["data-experience"],"dir":"content/output/people","base":"shamim-seyson.json","ext":".json","sourceBase":"shamim-seyson.yaml","sourceExt":".yaml","id":"shamim-seyson","photo":"/static/images/people/shamim-seyson.jpg","title":"Alumni (Ugrad)"},{"name":"Shanna Hollingworth","alias":"Shanna Li Ching Hollingworth","type":"master","email":"shanna.hollingwor1@ucalgary.ca","url":"https://shanna1408.github.io/","linkedin":"https://www.linkedin.com/in/shanna-hollingworth/","labs":["data-experience"],"dir":"content/output/people","base":"shanna-hollingworth.json","ext":".json","sourceBase":"shanna-hollingworth.yaml","sourceExt":".yaml","id":"shanna-hollingworth","photo":"/static/images/people/shanna-hollingworth.jpg","title":"MSc"},{"name":"Shaun Kaasten","type":"alumni","past":"master","dir":"content/output/people","base":"shaun-kaasten.json","ext":".json","sourceBase":"shaun-kaasten.yaml","sourceExt":".yaml","id":"shaun-kaasten","photo":"/static/images/people/no-profile.jpg"},{"name":"Shivesh Jadon","type":"alumni","past":"master","now":"Apple","url":"https://shivesh.dev/","scholar":"https://scholar.google.com/citations?user=EwqYU2sAAAAJ\u0026hl=en","linkedin":"https://www.linkedin.com/in/shiveshjadon/","twitter":"https://twitter.com/ShiveshJadon","labs":["data-experience"],"dir":"content/output/people","base":"shivesh-jadon.json","ext":".json","sourceBase":"shivesh-jadon.yaml","sourceExt":".yaml","id":"shivesh-jadon","photo":"/static/images/people/shivesh-jadon.jpg","title":"Alumni (MSc)"},{"name":"Shrivatsa Mishra","type":"alumni","past":"visiting","email":"shrivatsamishra@gmail.com","github":"https://github.com/ShrivatsaMishra","linkedin":"https://www.linkedin.com/in/shrivatsa-mishra/","dir":"content/output/people","base":"shrivatsa-mishra.json","ext":".json","sourceBase":"shrivatsa-mishra.yaml","sourceExt":".yaml","id":"shrivatsa-mishra","photo":"/static/images/people/shrivatsa-mishra.jpg"},{"name":"Simon Klüber","type":"alumni","past":"visiting","labs":["data-experience"],"dir":"content/output/people","base":"simon-kluber.json","ext":".json","sourceBase":"simon-kluber.yaml","sourceExt":".yaml","id":"simon-kluber","photo":"/static/images/people/no-profile.jpg","title":"Alumni (Visiting)"},{"name":"Søren Knudsen","type":"alumni","past":"postdoc","now":"University of Copenhagen","url":"http://sorenknudsen.com/","labs":["data-experience"],"dir":"content/output/people","base":"soren-knudsen.json","ext":".json","sourceBase":"soren-knudsen.yaml","sourceExt":".yaml","id":"soren-knudsen","photo":"/static/images/people/soren-knudsen.jpg","title":"Alumni (Postdoc)"},{"name":"Sowmya Somanath","type":"alumni","past":"phd","now":"University of Victoria","labs":["utouch"],"url":"http://pages.cpsc.ucalgary.ca/~ssomanat/","scholar":"https://scholar.google.ca/citations?user=R9ar1NkAAAAJ","twitter":"https://twitter.com/sowmyasomanath","dir":"content/output/people","base":"sowmya-somanath.json","ext":".json","sourceBase":"sowmya-somanath.yaml","sourceExt":".yaml","id":"sowmya-somanath","photo":"/static/images/people/sowmya-somanath.jpg"},{"name":"Stacey Scott","type":"alumni","past":"phd","now":"University of Guelph","url":"https://csl.uwaterloo.ca/","linkedin":"https://www.linkedin.com/in/staceydscott","scholar":"https://scholar.google.ca/citations?user=fB_xzi8AAAAJ\u0026hl=en","dir":"content/output/people","base":"stacey-scott.json","ext":".json","sourceBase":"stacey-scott.yaml","sourceExt":".yaml","id":"stacey-scott","photo":"/static/images/people/stacey-scott.jpg"},{"name":"Stephanie Smale","type":"alumni","past":"master","dir":"content/output/people","base":"stephanie-smale.json","ext":".json","sourceBase":"stephanie-smale.yaml","sourceExt":".yaml","id":"stephanie-smale","photo":"/static/images/people/no-profile.jpg"},{"name":"Sutirtha Roy","type":"master","labs":["diff"],"email":"sutirtha.roy@ucalgary.ca","scholar":"https://scholar.google.com/citations?user=i_4V4oAAAAAJ","linkedin":"https://www.linkedin.com/in/sutirtha-roy-320586199/","dir":"content/output/people","base":"sutirtha-roy.json","ext":".json","sourceBase":"sutirtha-roy.yaml","sourceExt":".yaml","id":"sutirtha-roy","photo":"/static/images/people/sutirtha-roy.jpg"},{"name":"Sydney Pratte","type":"alumni","past":"phd","url":"https://www.sydneypratte.ca/","labs":["curio"],"dir":"content/output/people","base":"sydney-pratte.json","ext":".json","sourceBase":"sydney-pratte.yaml","sourceExt":".yaml","id":"sydney-pratte","photo":"/static/images/people/sydney-pratte.jpg"},{"name":"Tania Villalobos Lujan","type":"phd","linkedin":"https://www.linkedin.com/in/tania-villalobos-lujan","labs":["curio"],"dir":"content/output/people","base":"tania-villalobos-lujan.json","ext":".json","sourceBase":"tania-villalobos-lujan.yaml","sourceExt":".yaml","id":"tania-villalobos-lujan","photo":"/static/images/people/tania-villalobos-lujan.jpg"},{"name":"Teale Masrani","type":"alumni","past":"master","now":"University of Calgary","scholar":"https://scholar.google.ca/citations?user=sGEgYE0AAAAJ\u0026hl=en","email":"teale.masrani2@ucalgary.ca","linkedin":"https://www.linkedin.com/in/teale-masrani-876151127/","dir":"content/output/people","base":"teale-masrani.json","ext":".json","sourceBase":"teale-masrani.yaml","sourceExt":".yaml","id":"teale-masrani","photo":"/static/images/people/teale-masrani.jpg"},{"name":"Teddy Seyed","type":"alumni","past":"phd","now":"Microsoft Research","url":"https://www.microsoft.com/en-us/research/people/teddy/","scholar":"https://scholar.google.com/citations?user=A8VSir8AAAAJ","dir":"content/output/people","base":"teddy-seyed.json","ext":".json","sourceBase":"teddy-seyed.yaml","sourceExt":".yaml","id":"teddy-seyed","photo":"/static/images/people/teddy-seyed.jpg"},{"name":"Terrance Mok","type":"phd","url":"http://terrancemok.com/","scholar":"https://scholar.google.ca/citations?user=nHkJDSEAAAAJ","twitter":"https://twitter.com/terrancem","linkedin":"https://www.linkedin.com/in/terrance-mok-22421011","labs":["curio"],"dir":"content/output/people","base":"terrance-mok.json","ext":".json","sourceBase":"terrance-mok.yaml","sourceExt":".yaml","id":"terrance-mok","photo":"/static/images/people/terrance-mok.jpg"},{"name":"Theodore (Ted) O'Grady","type":"alumni","past":"master","now":"Fillip","linkedin":"https://www.linkedin.com/in/ogradyt","dir":"content/output/people","base":"theodore-ogrady.json","ext":".json","sourceBase":"theodore-ogrady.yaml","sourceExt":".yaml","id":"theodore-ogrady","photo":"/static/images/people/theodore-ogrady.jpg"},{"name":"Tian Xia","type":"alumni","past":"undergrad","linkedin":"https://www.linkedin.com/in/tianxiacs/","email":"tian.xia2@ucalgary.ca","labs":["utouch"],"dir":"content/output/people","base":"tian-xia.json","ext":".json","sourceBase":"tian-xia.yaml","sourceExt":".yaml","id":"tian-xia","photo":"/static/images/people/tian-xia.jpg"},{"name":"Tim Au Yeung","type":"phd","linkedin":"https://www.linkedin.com/in/tim-au-yeung-3a29816","labs":["utouch"],"dir":"content/output/people","base":"tim-au-yeung.json","ext":".json","sourceBase":"tim-au-yeung.yaml","sourceExt":".yaml","id":"tim-au-yeung","photo":"/static/images/people/tim-au-yeung.jpg"},{"name":"Tobias Isenberg","type":"alumni","past":"postdoc","now":"Inria Saclay","url":"https://tobias.isenberg.cc/","linkedin":"https://www.linkedin.com/in/tobiasisenberg","scholar":"https://scholar.google.com/citations?user=e09cpQUAAAAJ","dir":"content/output/people","base":"tobias-isenberg.json","ext":".json","sourceBase":"tobias-isenberg.yaml","sourceExt":".yaml","id":"tobias-isenberg","photo":"/static/images/people/no-profile.jpg"},{"name":"Uta Hinrichs","type":"alumni","past":"phd","now":"University of Edinburgh","url":"http://www.utahinrichs.de/","scholar":"https://scholar.google.ca/citations?user=kGPAR0YAAAAJ\u0026hl=en","linkedin":"https://www.linkedin.com/in/uta-hinrichs-506a5560","dir":"content/output/people","base":"uta-hinrichs.json","ext":".json","sourceBase":"uta-hinrichs.yaml","sourceExt":".yaml","id":"uta-hinrichs","photo":"/static/images/people/uta-hinrichs.jpg"},{"name":"Victoria Wong","type":"undergrad","linkedin":"https://www.linkedin.com/in/victoria-wong-63509723b","labs":["data-experience"],"dir":"content/output/people","base":"victoria-wong.json","ext":".json","sourceBase":"victoria-wong.yaml","sourceExt":".yaml","id":"victoria-wong","photo":"/static/images/people/victoria-wong.jpg","title":"Ugrad"},{"name":"Wei Wei","type":"alumni","past":"master","url":"http://www.weiweiff.com/","email":"wei.wei2@ucalgary.ca","linkedin":"https://www.linkedin.com/in/wei-wei-961a211ba/","labs":["utouch","data-experience"],"dir":"content/output/people","base":"wei-wei.json","ext":".json","sourceBase":"wei-wei.yaml","sourceExt":".yaml","id":"wei-wei","photo":"/static/images/people/wei-wei.jpg","title":"Alumni (MSc)"},{"name":"William Wright","type":"masters","url":"http://pages.cpsc.ucalgary.ca/~wwright/","scholar":"https://scholar.google.com/citations?user=V4nRMoMAAAAJ","twitter":"https://twitter.com/HexenKoenig","facebook":"https://www.facebook.com/bonadriel","linkedin":"https://www.linkedin.com/in/bon-adriel-aseniero-47140560/","dir":"content/output/people","base":"william-wright.json","ext":".json","sourceBase":"william-wright.yaml","sourceExt":".yaml","id":"william-wright","photo":"/static/images/people/no-profile.jpg"},{"name":"Xiang 'Anthony' Chen","type":"alumni","past":"master","now":"UCLA","url":"https://xac.is/","scholar":"https://scholar.google.com/citations?user=I2W13z0AAAAJ","twitter":"https://twitter.com/_xiang_chen_","dir":"content/output/people","base":"xiang-anthony-chen.json","ext":".json","sourceBase":"xiang-anthony-chen.yaml","sourceExt":".yaml","id":"xiang-anthony-chen","photo":"/static/images/people/xiang-anthony-chen.jpg"},{"name":"Xing-Dong Yang","type":"alumni","past":"postdoc","now":"Simon Fraser University","url":"https://www.sfu.ca/~xingdong/","scholar":"https://scholar.google.ca/citations?user=9WfDSNwAAAAJ\u0026hl=en","linkedin":"https://www.linkedin.com/in/xing-dong-yang-648a1663/","dir":"content/output/people","base":"xing-dong-yang.json","ext":".json","sourceBase":"xing-dong-yang.yaml","sourceExt":".yaml","id":"xing-dong-yang","photo":"/static/images/people/xing-dong-yang.jpg"},{"name":"Yaseen Rashid","type":"undergrad","labs":["shivers"],"linkedin":"https://www.linkedin.com/in/yaseenmrashid","github":"https://github.com/YaseeenMR","email":"yaseen.rashid@ucalgary.ca","dir":"content/output/people","base":"yaseen-rashid.json","ext":".json","sourceBase":"yaseen-rashid.yaml","sourceExt":".yaml","id":"yaseen-rashid","photo":"/static/images/people/yaseen-rashid.jpg"},{"name":"Yibo Sun","type":"alumni","past":"master","linkedin":"https://www.linkedin.com/in/yibo-sun-30800334/?originalSubdomain=ca","dir":"content/output/people","base":"yibo-sun.json","ext":".json","sourceBase":"yibo-sun.yaml","sourceExt":".yaml","id":"yibo-sun","photo":"/static/images/people/no-profile.jpg"},{"name":"Zachary McKendrick","type":"alumni","past":"phd","now":"University of Waterloo","url":"https://uwaterloo.ca/postdoctoral-scholars/blog/2024-provosts-program-interdisciplinary-postdoctoral-scholar-2","linkedin":"https://www.linkedin.com/in/zach-mckendrick-7a24bb3b","labs":["utouch"],"dir":"content/output/people","base":"zachary-mckendrick.json","ext":".json","sourceBase":"zachary-mckendrick.yaml","sourceExt":".yaml","id":"zachary-mckendrick","photo":"/static/images/people/zachary-mckendrick.jpg"},{"name":"Zhijie Xia","type":"alumni","past":"undergrad","now":"Huawei","url":"https://zhijiexia.dev/","linkedin":"https://www.linkedin.com/in/zhijie-xia-678b331b5/","dir":"content/output/people","base":"zhijie-xia.json","ext":".json","sourceBase":"zhijie-xia.yaml","sourceExt":".yaml","id":"zhijie-xia","photo":"/static/images/people/zhijie-xia.jpg"}]}},"__N_SSG":true},"page":"/labs/[id]","query":{"id":"data-experience"},"buildId":"wamj8UQpl0oV0xKoxX0hb","assetPrefix":"/pr-preview/pr-140","runtimeConfig":{"basePath":"/pr-preview/pr-140"},"isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>