<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><title data-next-head="">Electrotactile Tongue Interface for Human-Computer Interaction and Robot Teleoperation | Interactions Lab - University of Calgary HCI Group</title><meta name="keywords" content="haptics, robotics, sensory substitution, tongue interface, electrotactile" data-next-head=""/><meta name="description" content="By exploiting the tongue’s exceptional density of nerve endings and low-threshold electrical response, electrotactile tongue displays provide a precise, low-power sensory-substitution pathway for teleoperated robotic systems. This approach addresses the persistent lack of effective haptic sensation in Robot Assisted Surgery (RAS) which recognized as a critical limitation. In this thesis, we introduce TactTongue, a modular electrotactile tongue-display toolkit with a simplified high-level user interface for rapid prototyping of electrotactile tongue stimulation (ETS). Building on previous works that informed the design of a flexible electrotactile tongue display, we investigate: (1) the efficacy of ETS for delivering force feedback during teleoperated manipulation tasks; (2) the design requirements and user-interface (UI) design to support rapid prototyping; (3) the potential for tongue-gesture input and applications; and (4) how tongue-based feedback can be compared to kinesthetic haptics during robot-assisted needle-insertion task. First, we evaluated ETS in an egg-lifting teleoperation task under visual-only and tongue-stimulation conditions, demonstrating potential or higher control accuracy with tongue feedback compared to vision alone. Building on these results, we refined the hardware and UI into a compact, modular TactTongue toolkit that lowers entry barriers for both researchers and practitioners. Moreover, we show a diverse set of application demonstrators—from accessibility aids with touch input and medical‐surgery overlays to extended‐reality, taste and texture rendering—highlighting the platform’s versatility beyond the force feedback. Drawing on insights from these early studies, in ongoing work, we explore integration of electrotactile tongue stimulation with kinesthetic haptic device to enhance force perception during robot assisted needle insertion in soft tissue. These findings validate TactTongue’s design, establish UI guidelines for rapid prototyping of tongue-based interfaces, and suggest a promising multimodal feedback paradigm for preciser RAS." data-next-head=""/><meta property="og:title" content="Electrotactile Tongue Interface for Human-Computer Interaction and Robot Teleoperation | Interactions Lab - University of Calgary HCI Group" data-next-head=""/><meta property="og:description" content="By exploiting the tongue’s exceptional density of nerve endings and low-threshold electrical response, electrotactile tongue displays provide a precise, low-power sensory-substitution pathway for teleoperated robotic systems. This approach addresses the persistent lack of effective haptic sensation in Robot Assisted Surgery (RAS) which recognized as a critical limitation. In this thesis, we introduce TactTongue, a modular electrotactile tongue-display toolkit with a simplified high-level user interface for rapid prototyping of electrotactile tongue stimulation (ETS). Building on previous works that informed the design of a flexible electrotactile tongue display, we investigate: (1) the efficacy of ETS for delivering force feedback during teleoperated manipulation tasks; (2) the design requirements and user-interface (UI) design to support rapid prototyping; (3) the potential for tongue-gesture input and applications; and (4) how tongue-based feedback can be compared to kinesthetic haptics during robot-assisted needle-insertion task. First, we evaluated ETS in an egg-lifting teleoperation task under visual-only and tongue-stimulation conditions, demonstrating potential or higher control accuracy with tongue feedback compared to vision alone. Building on these results, we refined the hardware and UI into a compact, modular TactTongue toolkit that lowers entry barriers for both researchers and practitioners. Moreover, we show a diverse set of application demonstrators—from accessibility aids with touch input and medical‐surgery overlays to extended‐reality, taste and texture rendering—highlighting the platform’s versatility beyond the force feedback. Drawing on insights from these early studies, in ongoing work, we explore integration of electrotactile tongue stimulation with kinesthetic haptic device to enhance force perception during robot assisted needle insertion in soft tissue. These findings validate TactTongue’s design, establish UI guidelines for rapid prototyping of tongue-based interfaces, and suggest a promising multimodal feedback paradigm for preciser RAS." data-next-head=""/><meta property="og:site_name" content="University of Calgary Interactions Lab" data-next-head=""/><meta property="og:url" content="https://ilab.ucalgary.ca/" data-next-head=""/><meta property="og:image" content="https://ilab.ucalgary.ca/static/images/theses/cover/msc-2025-mukashev.jpg" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:title" content="Electrotactile Tongue Interface for Human-Computer Interaction and Robot Teleoperation | Interactions Lab - University of Calgary HCI Group" data-next-head=""/><meta name="twitter:description" content="By exploiting the tongue’s exceptional density of nerve endings and low-threshold electrical response, electrotactile tongue displays provide a precise, low-power sensory-substitution pathway for teleoperated robotic systems. This approach addresses the persistent lack of effective haptic sensation in Robot Assisted Surgery (RAS) which recognized as a critical limitation. In this thesis, we introduce TactTongue, a modular electrotactile tongue-display toolkit with a simplified high-level user interface for rapid prototyping of electrotactile tongue stimulation (ETS). Building on previous works that informed the design of a flexible electrotactile tongue display, we investigate: (1) the efficacy of ETS for delivering force feedback during teleoperated manipulation tasks; (2) the design requirements and user-interface (UI) design to support rapid prototyping; (3) the potential for tongue-gesture input and applications; and (4) how tongue-based feedback can be compared to kinesthetic haptics during robot-assisted needle-insertion task. First, we evaluated ETS in an egg-lifting teleoperation task under visual-only and tongue-stimulation conditions, demonstrating potential or higher control accuracy with tongue feedback compared to vision alone. Building on these results, we refined the hardware and UI into a compact, modular TactTongue toolkit that lowers entry barriers for both researchers and practitioners. Moreover, we show a diverse set of application demonstrators—from accessibility aids with touch input and medical‐surgery overlays to extended‐reality, taste and texture rendering—highlighting the platform’s versatility beyond the force feedback. Drawing on insights from these early studies, in ongoing work, we explore integration of electrotactile tongue stimulation with kinesthetic haptic device to enhance force perception during robot assisted needle insertion in soft tissue. These findings validate TactTongue’s design, establish UI guidelines for rapid prototyping of tongue-based interfaces, and suggest a promising multimodal feedback paradigm for preciser RAS." data-next-head=""/><meta name="twitter:image" content="https://ilab.ucalgary.ca/static/images/theses/cover/msc-2025-mukashev.jpg" data-next-head=""/><meta name="twitter:card" content="summary" data-next-head=""/><meta name="twitter:site" content="@ucalgary" data-next-head=""/><meta name="twitter:url" content="https://ilab.ucalgary.ca/" data-next-head=""/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link rel="preload" href="/pr-preview/pr-138/_next/static/media/dc84b505c4b06e35-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/pr-preview/pr-138/_next/static/css/983cdb9cbcda2fe3.css" as="style"/><link rel="preload" href="/pr-preview/pr-138/_next/static/css/a1a0497113412518.css" as="style"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              // $('.ui.sidebar')
              //   .sidebar('attach events', '.sidebar.icon')

              $('.sidebar.icon').on('click', function(event) {
                $('.ui.sidebar')
                  .sidebar('toggle')
              })

              $('.project').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })

              $('.thesis').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><link rel="stylesheet" href="/pr-preview/pr-138/_next/static/css/983cdb9cbcda2fe3.css" data-n-g=""/><link rel="stylesheet" href="/pr-preview/pr-138/_next/static/css/a1a0497113412518.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/pr-preview/pr-138/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/pr-preview/pr-138/_next/static/chunks/webpack-09ad242dd996c7e7.js" defer=""></script><script src="/pr-preview/pr-138/_next/static/chunks/340-949765fef7fa60ed.js" defer=""></script><script src="/pr-preview/pr-138/_next/static/chunks/main-f5a95b893c70049f.js" defer=""></script><script src="/pr-preview/pr-138/_next/static/chunks/vendor-styles-c798be15be9b9e0c.js" defer=""></script><script src="/pr-preview/pr-138/_next/static/chunks/505-c409d49df7d91ffa.js" defer=""></script><script src="/pr-preview/pr-138/_next/static/chunks/pages/_app-09ca72778dd825e1.js" defer=""></script><script src="/pr-preview/pr-138/_next/static/chunks/347-6f14004506bc7107.js" defer=""></script><script src="/pr-preview/pr-138/_next/static/chunks/590-78b9bb8ec06e0f2c.js" defer=""></script><script src="/pr-preview/pr-138/_next/static/chunks/718-521b28b62aa04ae2.js" defer=""></script><script src="/pr-preview/pr-138/_next/static/chunks/960-432469f5c07f699e.js" defer=""></script><script src="/pr-preview/pr-138/_next/static/chunks/pages/theses/%5Bid%5D-700340d9bce1832c.js" defer=""></script><script src="/pr-preview/pr-138/_next/static/J78dNJWhQjn2df1-OuO16/_buildManifest.js" defer=""></script><script src="/pr-preview/pr-138/_next/static/J78dNJWhQjn2df1-OuO16/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_bae0a2"><div class="ui center aligned container"><div class="ui secondary huge compact menu"><a class="item" href="/pr-preview/pr-138/"><img loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui tiny image" style="color:transparent" srcSet="/pr-preview/pr-138/static/images/ilab-logo-3d.gif 1x" src="/pr-preview/pr-138/static/images/ilab-logo-3d.gif"/></a><a class="item" href="/pr-preview/pr-138/people/">People</a><a class="item" href="/pr-preview/pr-138/publications/">Research</a></div></div><div class="pusher"><div class="ui stackable grid"><div class="one wide column"></div><div class="ten wide column centered" style="margin-top:30px"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-138/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2025</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1>Electrotactile Tongue Interface for Human-Computer Interaction and Robot Teleoperation</h1><p class="meta"><a href="/people/dinmukhammed-mukashev"><img alt="dinmukhammed-mukashev photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-138/static/images/people/dinmukhammed-mukashev.jpg 1x" src="/pr-preview/pr-138/static/images/people/dinmukhammed-mukashev.jpg"/><strong>Dinmukhammed Mukashev</strong></a><span class="role"> (author)</span>, <a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-138/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-138/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Nittala</strong></a><span class="role"> (supervisor)</span>, <span>Park Junho<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>By exploiting the tongue’s exceptional density of nerve endings and low-threshold electrical response, electrotactile tongue displays provide a precise, low-power sensory-substitution pathway for teleoperated robotic systems. This approach addresses the persistent lack of effective haptic sensation in Robot Assisted Surgery (RAS) which recognized as a critical limitation. In this thesis, we introduce TactTongue, a modular electrotactile tongue-display toolkit with a simplified high-level user interface for rapid prototyping of electrotactile tongue stimulation (ETS). Building on previous works that informed the design of a flexible electrotactile tongue display, we investigate: (1) the efficacy of ETS for delivering force feedback during teleoperated manipulation tasks; (2) the design requirements and user-interface (UI) design to support rapid prototyping; (3) the potential for tongue-gesture input and applications; and (4) how tongue-based feedback can be compared to kinesthetic haptics during robot-assisted needle-insertion task. First, we evaluated ETS in an egg-lifting teleoperation task under visual-only and tongue-stimulation conditions, demonstrating potential or higher control accuracy with tongue feedback compared to vision alone. Building on these results, we refined the hardware and UI into a compact, modular TactTongue toolkit that lowers entry barriers for both researchers and practitioners. Moreover, we show a diverse set of application demonstrators—from accessibility aids with touch input and medical‐surgery overlays to extended‐reality, taste and texture rendering—highlighting the platform’s versatility beyond the force feedback. Drawing on insights from these early studies, in ongoing work, we explore integration of electrotactile tongue stimulation with kinesthetic haptic device to enhance force perception during robot assisted needle insertion in soft tissue. These findings validate TactTongue’s design, establish UI guidelines for rapid prototyping of tongue-based interfaces, and suggest a promising multimodal feedback paradigm for preciser RAS.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Robotics</span><span class="ui brown basic label">Sensory Substitution</span><span class="ui brown basic label">Tongue Interface</span><span class="ui brown basic label">Electrotactile</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Dinmukhammed Mukashev<!-- -->. <b>Electrotactile Tongue Interface for Human-Computer Interaction and Robot Teleoperation</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2025-07-04<!-- -->. <!-- -->DOI: <a href="https://dx.doi.org/10.11575/PRISM/49895" target="_blank">https://dx.doi.org/10.11575/PRISM/49895</a>URL: <a href="https://hdl.handle.net/1880/122303" target="_blank">https://hdl.handle.net/1880/122303</a></p></div></div></div></div><div class="one wide column"></div></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><div class="content"><a href="https://ucalgary.ca"><img alt="University of Calgary logo" loading="lazy" width="200" height="0" decoding="async" data-nimg="1" style="color:transparent;max-width:200px;margin:0px auto;height:auto" srcSet="/pr-preview/pr-138/static/images/logo-4.png 1x, /pr-preview/pr-138/static/images/logo-4.png 2x" src="/pr-preview/pr-138/static/images/logo-4.png"/></a><div class="sub header"><a class="item" href="https://cpsc.ucalgary.ca">Department of Computer Science</a></div></div></div></footer></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"ids":[{"id":"mmus-2012-pon"},{"id":"msc-2008-guo"},{"id":"msc-2011-harris"},{"id":"msc-2011-sultanum"},{"id":"msc-2012-lapides"},{"id":"msc-2015-li"},{"id":"msc-2017-hu"},{"id":"msc-2017-payne"},{"id":"msc-2018-cartwright"},{"id":"msc-2018-ta"},{"id":"msc-2019-danyluk"},{"id":"msc-2019-mahadevan"},{"id":"msc-2021-asha"},{"id":"msc-2021-wannamaker"},{"id":"msc-2023-dhawka"},{"id":"msc-2023-jadon"},{"id":"msc-2023-smith"},{"id":"msc-2023-wei"},{"id":"msc-2024-friedel"},{"id":"msc-2025-chulpongsatorn"},{"id":"msc-2025-mukashev"},{"id":"msc-2025-roy"},{"id":"msc-2025-sandykbayeva"},{"id":"phd-2010-young"},{"id":"phd-2017-somanath"},{"id":"phd-2018-mostafa"},{"id":"phd-2019-li"},{"id":"phd-2022-hull"},{"id":"phd-2024-mckendrick"},{"id":"phd-2025-cabral-mota"}]},"__N_SSG":true},"page":"/theses/[id]","query":{"id":"msc-2025-mukashev"},"buildId":"J78dNJWhQjn2df1-OuO16","assetPrefix":"/pr-preview/pr-138","runtimeConfig":{"basePath":"/pr-preview/pr-138"},"isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>