<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><title data-next-head="">SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures | Interactions Lab - University of Calgary HCI Group</title><meta name="keywords" content="Gesture Recognition, Hand Gestures, Sensor Placement, IMU, Objects, Design Tool" data-next-head=""/><meta name="description" content="Gestural interaction with freehands and while grasping an everyday object enables always-available input. To sense such gestures, minimal instrumentation of the user’s hand is desirable. However, the choice of an effective but minimal IMU layout remains challenging, due to the complexity of the multi-factorial space that comprises diverse finger gestures, objects and grasps. We present SparseIMU, a rapid method for selecting minimal inertial sensor-based layouts for effective gesture recognition. Furthermore, we contribute a computational tool to guide designers with optimal sensor placement. Our approach builds on an extensive microgestures dataset that we collected with a dense network of 17 inertial measurement units (IMUs). We performed a series of analyses, including an evaluation of the entire combinatorial space for freehand and grasping microgestures (393K layouts), and quantified the performance across different layout choices, revealing new gesture detection opportunities with IMUs. Finally, we demonstrate the versatility of our method with four scenarios." data-next-head=""/><meta property="og:title" content="SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures | Interactions Lab - University of Calgary HCI Group" data-next-head=""/><meta property="og:description" content="Gestural interaction with freehands and while grasping an everyday object enables always-available input. To sense such gestures, minimal instrumentation of the user’s hand is desirable. However, the choice of an effective but minimal IMU layout remains challenging, due to the complexity of the multi-factorial space that comprises diverse finger gestures, objects and grasps. We present SparseIMU, a rapid method for selecting minimal inertial sensor-based layouts for effective gesture recognition. Furthermore, we contribute a computational tool to guide designers with optimal sensor placement. Our approach builds on an extensive microgestures dataset that we collected with a dense network of 17 inertial measurement units (IMUs). We performed a series of analyses, including an evaluation of the entire combinatorial space for freehand and grasping microgestures (393K layouts), and quantified the performance across different layout choices, revealing new gesture detection opportunities with IMUs. Finally, we demonstrate the versatility of our method with four scenarios." data-next-head=""/><meta property="og:site_name" content="University of Calgary Interactions Lab" data-next-head=""/><meta property="og:url" content="https://ilab.ucalgary.ca/" data-next-head=""/><meta property="og:image" content="https://ilab.ucalgary.ca/static/images/publications/cover/tochi-2022-nittala.jpg" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:title" content="SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures | Interactions Lab - University of Calgary HCI Group" data-next-head=""/><meta name="twitter:description" content="Gestural interaction with freehands and while grasping an everyday object enables always-available input. To sense such gestures, minimal instrumentation of the user’s hand is desirable. However, the choice of an effective but minimal IMU layout remains challenging, due to the complexity of the multi-factorial space that comprises diverse finger gestures, objects and grasps. We present SparseIMU, a rapid method for selecting minimal inertial sensor-based layouts for effective gesture recognition. Furthermore, we contribute a computational tool to guide designers with optimal sensor placement. Our approach builds on an extensive microgestures dataset that we collected with a dense network of 17 inertial measurement units (IMUs). We performed a series of analyses, including an evaluation of the entire combinatorial space for freehand and grasping microgestures (393K layouts), and quantified the performance across different layout choices, revealing new gesture detection opportunities with IMUs. Finally, we demonstrate the versatility of our method with four scenarios." data-next-head=""/><meta name="twitter:image" content="https://ilab.ucalgary.ca/static/images/publications/cover/tochi-2022-nittala.jpg" data-next-head=""/><meta name="twitter:card" content="summary" data-next-head=""/><meta name="twitter:site" content="@ucalgary" data-next-head=""/><meta name="twitter:url" content="https://ilab.ucalgary.ca/" data-next-head=""/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link rel="preload" href="/pr-preview/pr-91/_next/static/media/e807dee2426166ad-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/pr-preview/pr-91/_next/static/css/7954ffdb11e47135.css" as="style"/><link rel="preload" href="/pr-preview/pr-91/_next/static/css/fa1543fdff44676b.css" as="style"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              // $('.ui.sidebar')
              //   .sidebar('attach events', '.sidebar.icon')

              $('.sidebar.icon').on('click', function(event) {
                $('.ui.sidebar')
                  .sidebar('toggle')
              })

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><link rel="stylesheet" href="/pr-preview/pr-91/_next/static/css/7954ffdb11e47135.css" data-n-g=""/><link rel="stylesheet" href="/pr-preview/pr-91/_next/static/css/fa1543fdff44676b.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/pr-preview/pr-91/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/pr-preview/pr-91/_next/static/chunks/webpack-8770cbabbee4d699.js" defer=""></script><script src="/pr-preview/pr-91/_next/static/chunks/340-9cf8ea7a5f74adb3.js" defer=""></script><script src="/pr-preview/pr-91/_next/static/chunks/main-237fefe93cf728ed.js" defer=""></script><script src="/pr-preview/pr-91/_next/static/chunks/vendor-styles-06d5c558e3786b77.js" defer=""></script><script src="/pr-preview/pr-91/_next/static/chunks/110-2c2224f87be24bba.js" defer=""></script><script src="/pr-preview/pr-91/_next/static/chunks/pages/_app-a1a52e1bf24d014b.js" defer=""></script><script src="/pr-preview/pr-91/_next/static/chunks/723-9071df5d30f1ec6a.js" defer=""></script><script src="/pr-preview/pr-91/_next/static/chunks/900-b725faaba9de29ec.js" defer=""></script><script src="/pr-preview/pr-91/_next/static/chunks/230-149465347cb3e206.js" defer=""></script><script src="/pr-preview/pr-91/_next/static/chunks/829-1c30adaf55f71841.js" defer=""></script><script src="/pr-preview/pr-91/_next/static/chunks/296-4a5d5256c11fe53c.js" defer=""></script><script src="/pr-preview/pr-91/_next/static/chunks/696-738db013856df479.js" defer=""></script><script src="/pr-preview/pr-91/_next/static/chunks/969-8dde949f18a7a22d.js" defer=""></script><script src="/pr-preview/pr-91/_next/static/chunks/pages/publications/%5Bid%5D-43c48fefab462daf.js" defer=""></script><script src="/pr-preview/pr-91/_next/static/GT9S0op_Fc0slktLyLhax/_buildManifest.js" defer=""></script><script src="/pr-preview/pr-91/_next/static/GT9S0op_Fc0slktLyLhax/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_6d1703"><div class="ui right vertical sidebar menu"><a class="item" href="/pr-preview/pr-91/">Home</a><a class="item active" href="/pr-preview/pr-91/publications/">Publications</a><a class="item" href="/pr-preview/pr-91/people/">People</a><a class="item" href="/pr-preview/pr-91/courses/">Courses</a><a class="item" href="/pr-preview/pr-91/facility/">Facility</a><a class="item" href="/pr-preview/pr-91/seminar/">Seminar</a><a class="item" href="/pr-preview/pr-91/location/">Location</a></div><div class="ui stackable secondary pointing container menu" style="border-bottom:none;margin-right:15%;font-size:1.1em"><div class="left menu"><a class="item" href="/pr-preview/pr-91/"><b>UCalgary iLab</b></a></div><div class="right menu"><a class="item active" href="/pr-preview/pr-91/publications/">Publications</a><a class="item" href="/pr-preview/pr-91/people/">People</a><a class="item" href="/pr-preview/pr-91/courses/">Courses</a><a class="item" href="/pr-preview/pr-91/facility/">Facility</a><a class="item" href="/pr-preview/pr-91/seminar/">Seminar</a><a class="item" href="/pr-preview/pr-91/location/">Location</a><div class="toc item"><a href="/pr-preview/pr-91/"><b>UCalgary iLab</b></a><i style="float:right" class="sidebar icon"></i></div></div></div><div class="pusher"><div class="ui stackable grid"><div class="one wide column"></div><div class="ten wide column centered" style="margin-top:30px"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-91/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TOCHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-91/static/images/publications/cover/tochi-2022-nittala.jpg 1x" src="/pr-preview/pr-91/static/images/publications/cover/tochi-2022-nittala.jpg"/></div><div class="thirteen wide column"><h1>SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures</h1><p class="meta"><span>Adwait Sharma</span> , <span>Christina Salchow-Hömmen</span> , <span>Vimal Suresh Mollyn</span> , <a href="/people/aditya-shekhar-nittala"><img loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-91/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-91/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a> , <span>Michael A. Hedderich</span> , <span>Marion Koelle</span> , <span>Thomas Seel</span> , <span>Jürgen Steimle</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/tochi-2022-nittala.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>tochi-2022-nittala.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Gestural interaction with freehands and while grasping an everyday object enables always-available input. To sense such gestures, minimal instrumentation of the user’s hand is desirable. However, the choice of an effective but minimal IMU layout remains challenging, due to the complexity of the multi-factorial space that comprises diverse finger gestures, objects and grasps. We present SparseIMU, a rapid method for selecting minimal inertial sensor-based layouts for effective gesture recognition. Furthermore, we contribute a computational tool to guide designers with optimal sensor placement. Our approach builds on an extensive microgestures dataset that we collected with a dense network of 17 inertial measurement units (IMUs). We performed a series of analyses, including an evaluation of the entire combinatorial space for freehand and grasping microgestures (393K layouts), and quantified the performance across different layout choices, revealing new gesture detection opportunities with IMUs. Finally, we demonstrate the versatility of our method with four scenarios.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Gesture Recognition</span><span class="ui brown basic label">Hand Gestures</span><span class="ui brown basic label">Sensor Placement</span><span class="ui brown basic label">IMU</span><span class="ui brown basic label">Objects</span><span class="ui brown basic label">Design Tool</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Adwait Sharma<!-- -->, <!-- -->Christina Salchow-Hömmen<!-- -->, <!-- -->Vimal Suresh Mollyn<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Michael A. Hedderich<!-- -->, <!-- -->Marion Koelle<!-- -->, <!-- -->Thomas Seel<!-- -->, <!-- -->Jürgen Steimle<!-- -->. <b>SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures</b>. <i>In undefined (TOCHI &#x27;22)</i>. <!-- -->  Page: 1-<!-- -->40<!-- -->.  DOI: <a href="https://doi.org/10.1145/3569894" target="_blank">https://doi.org/10.1145/3569894</a></p></div></div></div></div><div class="one wide column"></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><img loading="lazy" width="180" height="0" decoding="async" data-nimg="1" style="color:transparent;max-width:180px;margin:30px auto;height:auto" srcSet="/pr-preview/pr-91/static/images/logo-6.png 1x, /pr-preview/pr-91/static/images/logo-6.png 2x" src="/pr-preview/pr-91/static/images/logo-6.png"/><div class="content"><img loading="lazy" width="200" height="0" decoding="async" data-nimg="1" style="color:transparent;max-width:200px;margin:0px auto;height:auto" srcSet="/pr-preview/pr-91/static/images/logo-4.png 1x, /pr-preview/pr-91/static/images/logo-4.png 2x" src="/pr-preview/pr-91/static/images/logo-4.png"/><div class="sub header">Department of Computer Science</div></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"ids":[{"id":"assets-2017-suzuki"},{"id":"assets-2023-mok"},{"id":"cga-2019-ivanov"},{"id":"cgi-2019-danyluk"},{"id":"chi-2015-aseniero"},{"id":"chi-2015-jones"},{"id":"chi-2015-oehlberg"},{"id":"chi-2015-willett"},{"id":"chi-2017-aoki"},{"id":"chi-2017-hull"},{"id":"chi-2017-ledo"},{"id":"chi-2017-somanath"},{"id":"chi-2018-dillman"},{"id":"chi-2018-feick"},{"id":"chi-2018-heshmat"},{"id":"chi-2018-ledo"},{"id":"chi-2018-mahadevan"},{"id":"chi-2018-neustaedter"},{"id":"chi-2018-oh"},{"id":"chi-2018-suzuki"},{"id":"chi-2018-wuertz"},{"id":"chi-2019-danyluk"},{"id":"chi-2019-george"},{"id":"chi-2020-anjani"},{"id":"chi-2020-asha"},{"id":"chi-2020-goffin"},{"id":"chi-2020-hou"},{"id":"chi-2020-suzuki"},{"id":"chi-2021-danyluk"},{"id":"chi-2021-ens"},{"id":"chi-2021-hammad"},{"id":"chi-2022-bressa"},{"id":"chi-2022-ivanov"},{"id":"chi-2022-nittala"},{"id":"chi-2022-suzuki"},{"id":"chi-2023-dhawka"},{"id":"chi-2023-faridan"},{"id":"chi-2023-monteiro"},{"id":"chi-2024-bressa"},{"id":"chi-2024-dhawka"},{"id":"chi-2024-panigrahy"},{"id":"chi-2025-ghaneezabadi"},{"id":"chi-2025-madill"},{"id":"chi-2025-shiokawa"},{"id":"chi-ea-2022-blair"},{"id":"chi-ea-2023-chulpongsatorn"},{"id":"chi-ea-2023-fang"},{"id":"cmj-2020-ko"},{"id":"cnc-2019-hammad"},{"id":"cupum-2021-rout"},{"id":"dis-2016-jones"},{"id":"dis-2017-mok"},{"id":"dis-2018-mikalauskas"},{"id":"dis-2018-pham"},{"id":"dis-2018-ta"},{"id":"dis-2019-blair"},{"id":"dis-2019-bressa"},{"id":"dis-2019-ledo"},{"id":"dis-2019-mahadevan"},{"id":"dis-2019-nakayama"},{"id":"dis-2019-seyed"},{"id":"dis-2021-asha"},{"id":"dis-2021-blair"},{"id":"dis-2021-wannamaker"},{"id":"dis-2023-li"},{"id":"dis-2024-danyluk"},{"id":"frobt-2022-suzuki"},{"id":"gecco-2022-ivanov"},{"id":"gi-2020-rajabiyazdi"},{"id":"gi-2021-mactavish"},{"id":"gi-2022-hull"},{"id":"haid-2020-frisson"},{"id":"hri-2018-feick"},{"id":"httf-2024-blair"},{"id":"ieee-2021-willett"},{"id":"ijac-2021-hosseini"},{"id":"imwut-2020-wang"},{"id":"imx-2020-mok"},{"id":"iros-2020-hedayati"},{"id":"iros-2022-suzuki"},{"id":"mdpi-actuators-2024-piao"},{"id":"mdpi-arts-2023-frisson"},{"id":"mobilehci-2015-ledo"},{"id":"mobilehci-2019-hung"},{"id":"nime-2020-kirkegaard"},{"id":"nime-2020-ko"},{"id":"nime-2022-frisson"},{"id":"siggraph-labs-2023-seta"},{"id":"sui-2017-li"},{"id":"tei-2016-somanath"},{"id":"tei-2019-mikalauskas"},{"id":"tei-2019-tolley"},{"id":"tei-2019-wun"},{"id":"tei-2020-suzuki"},{"id":"tei-2021-pratte"},{"id":"tochi-2022-nittala"},{"id":"tvcg-2016-lopez"},{"id":"tvcg-2017-goffin"},{"id":"tvcg-2017-willett"},{"id":"tvcg-2019-blascheck"},{"id":"tvcg-2019-walny"},{"id":"tvcg-2020-danyluk"},{"id":"uist-2018-suzuki"},{"id":"uist-2019-suzuki"},{"id":"uist-2020-suzuki"},{"id":"uist-2020-yixian"},{"id":"uist-2021-suzuki"},{"id":"uist-2022-kaimoto"},{"id":"uist-2022-liao"},{"id":"uist-2022-nisser"},{"id":"uist-2022-nittala"},{"id":"uist-2023-chulpongsatorn"},{"id":"uist-2023-ihara"},{"id":"uist-2023-mukashev"},{"id":"uist-2023-xia"},{"id":"uist-2023-xia2"},{"id":"uist-2024-gunturu"},{"id":"uist-2024-roy"},{"id":"uist-sic-2022-faridan"},{"id":"vr-2019-satriadi"},{"id":"vrst-2022-frisson"}]},"__N_SSG":true},"page":"/publications/[id]","query":{"id":"tochi-2022-nittala"},"buildId":"GT9S0op_Fc0slktLyLhax","assetPrefix":"/pr-preview/pr-91","runtimeConfig":{"basePath":"/pr-preview/pr-91"},"isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>