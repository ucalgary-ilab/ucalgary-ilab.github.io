<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><title data-next-head="">Exploring the Design of Autonomous Vehicle-Pedestrian Interaction | Interactions Lab - University of Calgary HCI Group</title><meta name="keywords" content="Autonomous Vehicle-Pedestrian Interaction, Human-Computer Interaction, Human-Robot Interaction, Interaction Design, Virtual Reality" data-next-head=""/><meta name="description" content="Autonomous vehicle research today places an emphasis on developing better sensors and algorithms to enable the vehicle to localize itself in the environment, plan routes, and control its movement. Surveying the general public reveals optimism about the technology but also some skepticism about its ability to communicate with vulnerable road users such as pedestrians and cyclists. In today&#x27;s interaction with vehicles at crosswalks, pedestrians rely on cues originating from the vehicle and the driver. Vehicle cues relate to its kinematics such as speed and stopping distance while driver cues are concerned with communication such as eye gaze and contact, head and body movement, and hand gestures. In autonomous vehicles, however, a driver is not expected to be on-board to provide cues to pedestrians. We attempted to tackle the problem of designing novel ways to facilitate autonomous vehicle-pedestrian interaction at crosswalks. We propose interfaces which communicate an autonomous vehicle&#x27;s awareness and intent as a means of helping pedestrians make safe crossing decisions. Through our exploration, we make several contributions. First, we propose a design space for building interfaces using different cue modalities and cue locations. From an early exploration of this design space, we prototype interfaces designed to facilitate autonomous vehicle-pedestrian interaction. The interaction between vehicles and pedestrians will become more challenging during the transition period until all vehicles on the road are fully autonomous. During this period which we term mixed traffic, vehicles of varying levels of autonomy will occupy roads, some of which will have drivers, others such as semi-autonomous which may have distracted drivers, and fully autonomous vehicles which may or may not have drivers. To study this problem, we contribute a virtual reality-based pedestrian simulator. Our final contribution relates to the evaluation of interfaces in the real and virtual world where we found their inclusion helped pedestrians make safe crossing decisions." data-next-head=""/><meta property="og:title" content="Exploring the Design of Autonomous Vehicle-Pedestrian Interaction | Interactions Lab - University of Calgary HCI Group" data-next-head=""/><meta property="og:description" content="Autonomous vehicle research today places an emphasis on developing better sensors and algorithms to enable the vehicle to localize itself in the environment, plan routes, and control its movement. Surveying the general public reveals optimism about the technology but also some skepticism about its ability to communicate with vulnerable road users such as pedestrians and cyclists. In today&#x27;s interaction with vehicles at crosswalks, pedestrians rely on cues originating from the vehicle and the driver. Vehicle cues relate to its kinematics such as speed and stopping distance while driver cues are concerned with communication such as eye gaze and contact, head and body movement, and hand gestures. In autonomous vehicles, however, a driver is not expected to be on-board to provide cues to pedestrians. We attempted to tackle the problem of designing novel ways to facilitate autonomous vehicle-pedestrian interaction at crosswalks. We propose interfaces which communicate an autonomous vehicle&#x27;s awareness and intent as a means of helping pedestrians make safe crossing decisions. Through our exploration, we make several contributions. First, we propose a design space for building interfaces using different cue modalities and cue locations. From an early exploration of this design space, we prototype interfaces designed to facilitate autonomous vehicle-pedestrian interaction. The interaction between vehicles and pedestrians will become more challenging during the transition period until all vehicles on the road are fully autonomous. During this period which we term mixed traffic, vehicles of varying levels of autonomy will occupy roads, some of which will have drivers, others such as semi-autonomous which may have distracted drivers, and fully autonomous vehicles which may or may not have drivers. To study this problem, we contribute a virtual reality-based pedestrian simulator. Our final contribution relates to the evaluation of interfaces in the real and virtual world where we found their inclusion helped pedestrians make safe crossing decisions." data-next-head=""/><meta property="og:site_name" content="University of Calgary Interactions Lab" data-next-head=""/><meta property="og:url" content="https://ilab.ucalgary.ca/" data-next-head=""/><meta property="og:image" content="https://ilab.ucalgary.ca/static/images/theses/cover/msc-2019-mahadevan.jpg" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:title" content="Exploring the Design of Autonomous Vehicle-Pedestrian Interaction | Interactions Lab - University of Calgary HCI Group" data-next-head=""/><meta name="twitter:description" content="Autonomous vehicle research today places an emphasis on developing better sensors and algorithms to enable the vehicle to localize itself in the environment, plan routes, and control its movement. Surveying the general public reveals optimism about the technology but also some skepticism about its ability to communicate with vulnerable road users such as pedestrians and cyclists. In today&#x27;s interaction with vehicles at crosswalks, pedestrians rely on cues originating from the vehicle and the driver. Vehicle cues relate to its kinematics such as speed and stopping distance while driver cues are concerned with communication such as eye gaze and contact, head and body movement, and hand gestures. In autonomous vehicles, however, a driver is not expected to be on-board to provide cues to pedestrians. We attempted to tackle the problem of designing novel ways to facilitate autonomous vehicle-pedestrian interaction at crosswalks. We propose interfaces which communicate an autonomous vehicle&#x27;s awareness and intent as a means of helping pedestrians make safe crossing decisions. Through our exploration, we make several contributions. First, we propose a design space for building interfaces using different cue modalities and cue locations. From an early exploration of this design space, we prototype interfaces designed to facilitate autonomous vehicle-pedestrian interaction. The interaction between vehicles and pedestrians will become more challenging during the transition period until all vehicles on the road are fully autonomous. During this period which we term mixed traffic, vehicles of varying levels of autonomy will occupy roads, some of which will have drivers, others such as semi-autonomous which may have distracted drivers, and fully autonomous vehicles which may or may not have drivers. To study this problem, we contribute a virtual reality-based pedestrian simulator. Our final contribution relates to the evaluation of interfaces in the real and virtual world where we found their inclusion helped pedestrians make safe crossing decisions." data-next-head=""/><meta name="twitter:image" content="https://ilab.ucalgary.ca/static/images/theses/cover/msc-2019-mahadevan.jpg" data-next-head=""/><meta name="twitter:card" content="summary" data-next-head=""/><meta name="twitter:site" content="@ucalgary" data-next-head=""/><meta name="twitter:url" content="https://ilab.ucalgary.ca/" data-next-head=""/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link rel="preload" href="/pr-preview/pr-163/_next/static/media/dc84b505c4b06e35-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/pr-preview/pr-163/_next/static/css/f4de94a4e08ecab3.css" as="style"/><link rel="preload" href="/pr-preview/pr-163/_next/static/css/a1a0497113412518.css" as="style"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              // $('.ui.sidebar')
              //   .sidebar('attach events', '.sidebar.icon')

              $('.sidebar.icon').on('click', function(event) {
                $('.ui.sidebar')
                  .sidebar('toggle')
              })

              $('.project').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })

              $('.thesis').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><link rel="stylesheet" href="/pr-preview/pr-163/_next/static/css/f4de94a4e08ecab3.css" data-n-g=""/><link rel="stylesheet" href="/pr-preview/pr-163/_next/static/css/a1a0497113412518.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/pr-preview/pr-163/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/pr-preview/pr-163/_next/static/chunks/webpack-2345941070d47ac4.js" defer=""></script><script src="/pr-preview/pr-163/_next/static/chunks/340-e48e16335d89a433.js" defer=""></script><script src="/pr-preview/pr-163/_next/static/chunks/main-f5a95b893c70049f.js" defer=""></script><script src="/pr-preview/pr-163/_next/static/chunks/vendor-styles-c12d0d7ab35b0c98.js" defer=""></script><script src="/pr-preview/pr-163/_next/static/chunks/505-9d87e35d03758351.js" defer=""></script><script src="/pr-preview/pr-163/_next/static/chunks/pages/_app-09ca72778dd825e1.js" defer=""></script><script src="/pr-preview/pr-163/_next/static/chunks/347-6f14004506bc7107.js" defer=""></script><script src="/pr-preview/pr-163/_next/static/chunks/590-8bb7f72173922b48.js" defer=""></script><script src="/pr-preview/pr-163/_next/static/chunks/104-79403ca78d551e49.js" defer=""></script><script src="/pr-preview/pr-163/_next/static/chunks/960-f1b2911e7e0a7930.js" defer=""></script><script src="/pr-preview/pr-163/_next/static/chunks/pages/theses/%5Bid%5D-5942484107cc76b8.js" defer=""></script><script src="/pr-preview/pr-163/_next/static/86xmzMl4ij6CfWFN0U-00/_buildManifest.js" defer=""></script><script src="/pr-preview/pr-163/_next/static/86xmzMl4ij6CfWFN0U-00/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_9e8ce4"><div class="ui center aligned container"><div class="ui secondary huge compact menu"><a class="item" href="/pr-preview/pr-163/"><img loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui tiny image" style="color:transparent" srcSet="/pr-preview/pr-163/static/images/ilab-logo-3d.gif 1x" src="/pr-preview/pr-163/static/images/ilab-logo-3d.gif"/></a><a class="item" href="/pr-preview/pr-163/people/">People</a><a class="item" href="/pr-preview/pr-163/publications/">Research</a></div></div><div class="pusher"><div class="ui stackable grid"><div class="one wide column"></div><div class="ten wide column centered" style="margin-top:30px"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-163/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1>Exploring the Design of Autonomous Vehicle-Pedestrian Interaction</h1><p class="meta"><a href="/people/karthik-mahadevan"><img alt="karthik-mahadevan photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-163/static/images/people/karthik-mahadevan.jpg 1x" src="/pr-preview/pr-163/static/images/people/karthik-mahadevan.jpg"/><strong>Karthik Mahadevan</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-163/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-163/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-163/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-163/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"> (committee)</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Autonomous vehicle research today places an emphasis on developing better sensors and algorithms to enable the vehicle to localize itself in the environment, plan routes, and control its movement. Surveying the general public reveals optimism about the technology but also some skepticism about its ability to communicate with vulnerable road users such as pedestrians and cyclists. In today&#x27;s interaction with vehicles at crosswalks, pedestrians rely on cues originating from the vehicle and the driver. Vehicle cues relate to its kinematics such as speed and stopping distance while driver cues are concerned with communication such as eye gaze and contact, head and body movement, and hand gestures. In autonomous vehicles, however, a driver is not expected to be on-board to provide cues to pedestrians. We attempted to tackle the problem of designing novel ways to facilitate autonomous vehicle-pedestrian interaction at crosswalks. We propose interfaces which communicate an autonomous vehicle&#x27;s awareness and intent as a means of helping pedestrians make safe crossing decisions. Through our exploration, we make several contributions. First, we propose a design space for building interfaces using different cue modalities and cue locations. From an early exploration of this design space, we prototype interfaces designed to facilitate autonomous vehicle-pedestrian interaction. The interaction between vehicles and pedestrians will become more challenging during the transition period until all vehicles on the road are fully autonomous. During this period which we term mixed traffic, vehicles of varying levels of autonomy will occupy roads, some of which will have drivers, others such as semi-autonomous which may have distracted drivers, and fully autonomous vehicles which may or may not have drivers. To study this problem, we contribute a virtual reality-based pedestrian simulator. Our final contribution relates to the evaluation of interfaces in the real and virtual world where we found their inclusion helped pedestrians make safe crossing decisions.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Autonomous Vehicle Pedestrian Interaction</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Interaction Design</span><span class="ui brown basic label">Virtual Reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Karthik Mahadevan<!-- -->. <b>Exploring the Design of Autonomous Vehicle-Pedestrian Interaction</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2019-09-12<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/110924" target="_blank">http://hdl.handle.net/1880/110924</a></p></div></div></div></div><div class="one wide column"></div></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><div class="content"><a href="https://ucalgary.ca"><img alt="University of Calgary logo" loading="lazy" width="200" height="0" decoding="async" data-nimg="1" style="color:transparent;max-width:200px;margin:0px auto;height:auto" srcSet="/pr-preview/pr-163/static/images/logo-4.png 1x, /pr-preview/pr-163/static/images/logo-4.png 2x" src="/pr-preview/pr-163/static/images/logo-4.png"/></a><div class="sub header"><a class="item" href="https://cpsc.ucalgary.ca">Department of Computer Science</a></div></div></div></footer></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"ids":[{"id":"mmus-2012-pon"},{"id":"msc-2008-guo"},{"id":"msc-2011-harris"},{"id":"msc-2011-sultanum"},{"id":"msc-2012-lapides"},{"id":"msc-2015-li"},{"id":"msc-2017-hu"},{"id":"msc-2017-mok"},{"id":"msc-2017-payne"},{"id":"msc-2018-cartwright"},{"id":"msc-2018-ta"},{"id":"msc-2019-danyluk"},{"id":"msc-2019-kollannur"},{"id":"msc-2019-kuzabaviciute"},{"id":"msc-2019-mahadevan"},{"id":"msc-2019-mikalauskas"},{"id":"msc-2019-wun"},{"id":"msc-2021-asha"},{"id":"msc-2021-hung"},{"id":"msc-2021-wannamaker"},{"id":"msc-2023-dhawka"},{"id":"msc-2023-jadon"},{"id":"msc-2023-smith"},{"id":"msc-2023-wei"},{"id":"msc-2024-friedel"},{"id":"msc-2025-chulpongsatorn"},{"id":"phd-2010-young"},{"id":"phd-2017-somanath"},{"id":"phd-2018-mostafa"},{"id":"phd-2018-rajabiyazdi"},{"id":"phd-2019-li"},{"id":"phd-2020-ledo-maira"},{"id":"phd-2022-hull"},{"id":"phd-2024-mckendrick"},{"id":"phd-2025-cabral-mota"},{"id":"phd-2025-poostchi"},{"id":"phd-2025-pratte"}]},"__N_SSG":true},"page":"/theses/[id]","query":{"id":"msc-2019-mahadevan"},"buildId":"86xmzMl4ij6CfWFN0U-00","assetPrefix":"/pr-preview/pr-163","runtimeConfig":{"basePath":"/pr-preview/pr-163"},"isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>