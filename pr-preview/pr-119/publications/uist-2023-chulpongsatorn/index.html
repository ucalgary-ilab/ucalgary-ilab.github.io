<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><title data-next-head="">Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks | Interactions Lab - University of Calgary HCI Group</title><meta name="keywords" content="augmented reality, explorable explanations, interactive paper, augmented textbook, authoring interfaces" data-next-head=""/><meta name="description" content="We introduce Augmented Math, a machine learning-based approach to authoring AR explorable explanations by augmenting static math textbooks without programming. To augment a static document, our system first extracts mathematical formulas and figures from a given document using optical character recognition (OCR) and computer vision. By binding and manipulating these extracted contents, the user can see the interactive animation overlaid onto the document through mobile AR interfaces. This empowers non-technical users, such as teachers or students, to transform existing math textbooks and handouts into on-demand and personalized explorable explanations. To design our system, we first analyzed existing explorable math explanations to identify common design strategies. Based on the findings, we developed a set of augmentation techniques that can be automatically generated based on the extracted content, which are 1) dynamic values, 2) interactive figures, 3) relationship highlights, 4) concrete examples, and 5) step-by-step hints. To evaluate our system, we conduct two user studies: preliminary user testing and expert interviews. The study results confirm that our system allows more engaging experiences for learning math concepts." data-next-head=""/><meta property="og:title" content="Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks | Interactions Lab - University of Calgary HCI Group" data-next-head=""/><meta property="og:description" content="We introduce Augmented Math, a machine learning-based approach to authoring AR explorable explanations by augmenting static math textbooks without programming. To augment a static document, our system first extracts mathematical formulas and figures from a given document using optical character recognition (OCR) and computer vision. By binding and manipulating these extracted contents, the user can see the interactive animation overlaid onto the document through mobile AR interfaces. This empowers non-technical users, such as teachers or students, to transform existing math textbooks and handouts into on-demand and personalized explorable explanations. To design our system, we first analyzed existing explorable math explanations to identify common design strategies. Based on the findings, we developed a set of augmentation techniques that can be automatically generated based on the extracted content, which are 1) dynamic values, 2) interactive figures, 3) relationship highlights, 4) concrete examples, and 5) step-by-step hints. To evaluate our system, we conduct two user studies: preliminary user testing and expert interviews. The study results confirm that our system allows more engaging experiences for learning math concepts." data-next-head=""/><meta property="og:site_name" content="University of Calgary Interactions Lab" data-next-head=""/><meta property="og:url" content="https://ilab.ucalgary.ca/" data-next-head=""/><meta property="og:image" content="https://ilab.ucalgary.ca/static/images/publications/cover/uist-2023-chulpongsatorn.jpg" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:title" content="Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks | Interactions Lab - University of Calgary HCI Group" data-next-head=""/><meta name="twitter:description" content="We introduce Augmented Math, a machine learning-based approach to authoring AR explorable explanations by augmenting static math textbooks without programming. To augment a static document, our system first extracts mathematical formulas and figures from a given document using optical character recognition (OCR) and computer vision. By binding and manipulating these extracted contents, the user can see the interactive animation overlaid onto the document through mobile AR interfaces. This empowers non-technical users, such as teachers or students, to transform existing math textbooks and handouts into on-demand and personalized explorable explanations. To design our system, we first analyzed existing explorable math explanations to identify common design strategies. Based on the findings, we developed a set of augmentation techniques that can be automatically generated based on the extracted content, which are 1) dynamic values, 2) interactive figures, 3) relationship highlights, 4) concrete examples, and 5) step-by-step hints. To evaluate our system, we conduct two user studies: preliminary user testing and expert interviews. The study results confirm that our system allows more engaging experiences for learning math concepts." data-next-head=""/><meta name="twitter:image" content="https://ilab.ucalgary.ca/static/images/publications/cover/uist-2023-chulpongsatorn.jpg" data-next-head=""/><meta name="twitter:card" content="summary" data-next-head=""/><meta name="twitter:site" content="@ucalgary" data-next-head=""/><meta name="twitter:url" content="https://ilab.ucalgary.ca/" data-next-head=""/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link rel="preload" href="/pr-preview/pr-119/_next/static/media/dc84b505c4b06e35-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/pr-preview/pr-119/_next/static/css/166b884a4fa23da5.css" as="style"/><link rel="preload" href="/pr-preview/pr-119/_next/static/css/d9e3927f9c043bc5.css" as="style"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              // $('.ui.sidebar')
              //   .sidebar('attach events', '.sidebar.icon')

              $('.sidebar.icon').on('click', function(event) {
                $('.ui.sidebar')
                  .sidebar('toggle')
              })

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><link rel="stylesheet" href="/pr-preview/pr-119/_next/static/css/166b884a4fa23da5.css" data-n-g=""/><link rel="stylesheet" href="/pr-preview/pr-119/_next/static/css/d9e3927f9c043bc5.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/pr-preview/pr-119/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/pr-preview/pr-119/_next/static/chunks/webpack-80f4257a9d9b51ec.js" defer=""></script><script src="/pr-preview/pr-119/_next/static/chunks/340-0811f861fdc48158.js" defer=""></script><script src="/pr-preview/pr-119/_next/static/chunks/main-237fefe93cf728ed.js" defer=""></script><script src="/pr-preview/pr-119/_next/static/chunks/vendor-styles-645b1da9d3c23d5f.js" defer=""></script><script src="/pr-preview/pr-119/_next/static/chunks/505-08c629510f2693f2.js" defer=""></script><script src="/pr-preview/pr-119/_next/static/chunks/pages/_app-b2c4abf66b60dfa7.js" defer=""></script><script src="/pr-preview/pr-119/_next/static/chunks/851-9ae2497a89059d76.js" defer=""></script><script src="/pr-preview/pr-119/_next/static/chunks/649-9eade54d2ab75249.js" defer=""></script><script src="/pr-preview/pr-119/_next/static/chunks/696-9cc2f457c0753d45.js" defer=""></script><script src="/pr-preview/pr-119/_next/static/chunks/969-40dea6a701629a1c.js" defer=""></script><script src="/pr-preview/pr-119/_next/static/chunks/pages/publications/%5Bid%5D-443e4bc73a80c2d3.js" defer=""></script><script src="/pr-preview/pr-119/_next/static/yCOpuKbcxZC1D-n1Ao9LZ/_buildManifest.js" defer=""></script><script src="/pr-preview/pr-119/_next/static/yCOpuKbcxZC1D-n1Ao9LZ/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_2318e8"><div class="ui center aligned container"><div class="ui secondary huge compact menu"><a class="item" href="/pr-preview/pr-119/"><img class="ui tiny image" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent" srcSet="/pr-preview/pr-119/static/images/ilab-logo-3d.gif 1x" src="/pr-preview/pr-119/static/images/ilab-logo-3d.gif"/></a><a class="item" href="/pr-preview/pr-119/people/">People</a><a class="item" href="/pr-preview/pr-119/publications/">Research</a></div></div><div class="pusher"><div class="ui stackable grid"><div class="one wide column"></div><div class="ten wide column centered" style="margin-top:30px"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-119/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2023-chulpongsatorn cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-119/static/images/publications/cover/uist-2023-chulpongsatorn.jpg 1x" src="/pr-preview/pr-119/static/images/publications/cover/uist-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><h1>Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks</h1><p class="meta"><a href="/people/neil-chulpongsatorn"><img alt="neil-chulpongsatorn photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-119/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-119/static/images/people/neil-chulpongsatorn.jpg"/><strong>Neil Chulpongsatorn</strong></a> , <a href="/people/mille-skovhus-lunding"><img alt="mille-skovhus-lunding photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-119/static/images/people/mille-skovhus-lunding.jpg 1x" src="/pr-preview/pr-119/static/images/people/mille-skovhus-lunding.jpg"/><strong>Mille Skovhus Lunding</strong></a> , <a href="/people/nishan-soni"><img alt="nishan-soni photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-119/static/images/people/nishan-soni.jpg 1x" src="/pr-preview/pr-119/static/images/people/nishan-soni.jpg"/><strong>Nishan Soni</strong></a> , <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-119/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-119/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2023-chulpongsatorn.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2023-chulpongsatorn.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Zv6JQ5T-qn0" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Zv6JQ5T-qn0?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/Zv6JQ5T-qn0/maxresdefault.jpg src=https://img.youtube.com/vi/Zv6JQ5T-qn0/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce Augmented Math, a machine learning-based approach to authoring AR explorable explanations by augmenting static math textbooks without programming. To augment a static document, our system first extracts mathematical formulas and figures from a given document using optical character recognition (OCR) and computer vision. By binding and manipulating these extracted contents, the user can see the interactive animation overlaid onto the document through mobile AR interfaces. This empowers non-technical users, such as teachers or students, to transform existing math textbooks and handouts into on-demand and personalized explorable explanations. To design our system, we first analyzed existing explorable math explanations to identify common design strategies. Based on the findings, we developed a set of augmentation techniques that can be automatically generated based on the extracted content, which are 1) dynamic values, 2) interactive figures, 3) relationship highlights, 4) concrete examples, and 5) step-by-step hints. To evaluate our system, we conduct two user studies: preliminary user testing and expert interviews. The study results confirm that our system allows more engaging experiences for learning math concepts.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Authoring Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Neil Chulpongsatorn<!-- -->, <!-- -->Mille Skovhus Lunding<!-- -->, <!-- -->Nishan Soni<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks</b>. <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- -->(<!-- -->UIST 2023<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->16<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3586183.3606827" target="_blank">https://doi.org/10.1145/3586183.3606827</a></p></div></div></div></div><div class="one wide column"></div></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><div class="content"><a href="https://ucalgary.ca"><img alt="University of Calgary logo" loading="lazy" width="200" height="0" decoding="async" data-nimg="1" style="color:transparent;max-width:200px;margin:0px auto;height:auto" srcSet="/pr-preview/pr-119/static/images/logo-4.png 1x, /pr-preview/pr-119/static/images/logo-4.png 2x" src="/pr-preview/pr-119/static/images/logo-4.png"/></a><div class="sub header"><a class="item" href="https://cpsc.ucalgary.ca">Department of Computer Science</a></div></div></div></footer></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"ids":[{"id":"assets-2017-suzuki"},{"id":"assets-2023-mok"},{"id":"capstone-2025-haptic-floor-proxy"},{"id":"cga-2019-ivanov"},{"id":"cgi-2019-danyluk"},{"id":"chi-2015-aseniero"},{"id":"chi-2015-jones"},{"id":"chi-2015-oehlberg"},{"id":"chi-2015-willett"},{"id":"chi-2017-aoki"},{"id":"chi-2017-hull"},{"id":"chi-2017-ledo"},{"id":"chi-2017-somanath"},{"id":"chi-2018-dillman"},{"id":"chi-2018-feick"},{"id":"chi-2018-heshmat"},{"id":"chi-2018-ledo"},{"id":"chi-2018-mahadevan"},{"id":"chi-2018-neustaedter"},{"id":"chi-2018-oh"},{"id":"chi-2018-suzuki"},{"id":"chi-2018-wuertz"},{"id":"chi-2019-danyluk"},{"id":"chi-2019-george"},{"id":"chi-2020-anjani"},{"id":"chi-2020-asha"},{"id":"chi-2020-goffin"},{"id":"chi-2020-hou"},{"id":"chi-2020-suzuki"},{"id":"chi-2021-danyluk"},{"id":"chi-2021-ens"},{"id":"chi-2021-hammad"},{"id":"chi-2022-bressa"},{"id":"chi-2022-ivanov"},{"id":"chi-2022-nittala"},{"id":"chi-2022-suzuki"},{"id":"chi-2023-dhawka"},{"id":"chi-2023-faridan"},{"id":"chi-2023-monteiro"},{"id":"chi-2024-bressa"},{"id":"chi-2024-dhawka"},{"id":"chi-2024-panigrahy"},{"id":"chi-2025-ghaneezabadi"},{"id":"chi-2025-madill"},{"id":"chi-2025-shiokawa"},{"id":"chi-ea-2022-blair"},{"id":"chi-ea-2023-chulpongsatorn"},{"id":"chi-ea-2023-fang"},{"id":"cmj-2020-ko"},{"id":"cnc-2019-hammad"},{"id":"cupum-2021-rout"},{"id":"dis-2016-jones"},{"id":"dis-2017-mok"},{"id":"dis-2018-mikalauskas"},{"id":"dis-2018-pham"},{"id":"dis-2018-ta"},{"id":"dis-2019-blair"},{"id":"dis-2019-bressa"},{"id":"dis-2019-ledo"},{"id":"dis-2019-mahadevan"},{"id":"dis-2019-nakayama"},{"id":"dis-2019-seyed"},{"id":"dis-2021-asha"},{"id":"dis-2021-blair"},{"id":"dis-2021-wannamaker"},{"id":"dis-2023-li"},{"id":"dis-2024-danyluk"},{"id":"frobt-2022-suzuki"},{"id":"gecco-2022-ivanov"},{"id":"gi-2020-rajabiyazdi"},{"id":"gi-2021-mactavish"},{"id":"gi-2022-hull"},{"id":"haid-2020-frisson"},{"id":"hri-2018-feick"},{"id":"httf-2024-blair"},{"id":"ieee-2021-willett"},{"id":"ijac-2021-hosseini"},{"id":"imwut-2020-wang"},{"id":"imx-2020-mok"},{"id":"iros-2020-hedayati"},{"id":"iros-2022-suzuki"},{"id":"mdpi-actuators-2024-piao"},{"id":"mdpi-arts-2023-frisson"},{"id":"mobilehci-2015-ledo"},{"id":"mobilehci-2019-hung"},{"id":"nime-2020-kirkegaard"},{"id":"nime-2020-ko"},{"id":"nime-2022-frisson"},{"id":"siggraph-labs-2023-seta"},{"id":"sui-2017-li"},{"id":"tei-2016-somanath"},{"id":"tei-2019-mikalauskas"},{"id":"tei-2019-tolley"},{"id":"tei-2019-wun"},{"id":"tei-2020-suzuki"},{"id":"tei-2021-pratte"},{"id":"tochi-2022-nittala"},{"id":"tvcg-2016-lopez"},{"id":"tvcg-2017-goffin"},{"id":"tvcg-2017-willett"},{"id":"tvcg-2019-blascheck"},{"id":"tvcg-2019-walny"},{"id":"tvcg-2020-danyluk"},{"id":"uist-2018-suzuki"},{"id":"uist-2019-suzuki"},{"id":"uist-2020-suzuki"},{"id":"uist-2020-yixian"},{"id":"uist-2021-suzuki"},{"id":"uist-2022-kaimoto"},{"id":"uist-2022-liao"},{"id":"uist-2022-nisser"},{"id":"uist-2022-nittala"},{"id":"uist-2023-chulpongsatorn"},{"id":"uist-2023-ihara"},{"id":"uist-2023-mukashev"},{"id":"uist-2023-xia"},{"id":"uist-2023-xia2"},{"id":"uist-2024-gunturu"},{"id":"uist-2024-roy"},{"id":"uist-sic-2022-faridan"},{"id":"vr-2019-satriadi"},{"id":"vrst-2022-frisson"}]},"__N_SSG":true},"page":"/publications/[id]","query":{"id":"uist-2023-chulpongsatorn"},"buildId":"yCOpuKbcxZC1D-n1Ao9LZ","assetPrefix":"/pr-preview/pr-119","runtimeConfig":{"basePath":"/pr-preview/pr-119"},"isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>