<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link rel="preload" href="/pr-preview/pr-139/_next/static/media/dc84b505c4b06e35-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/pr-preview/pr-139/_next/static/css/86d5280f1e6275fe.css" as="style"/><link rel="preload" href="/pr-preview/pr-139/_next/static/css/a1a0497113412518.css" as="style"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              // $('.ui.sidebar')
              //   .sidebar('attach events', '.sidebar.icon')

              $('.sidebar.icon').on('click', function(event) {
                $('.ui.sidebar')
                  .sidebar('toggle')
              })

              $('.project').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })

              $('.thesis').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><link rel="stylesheet" href="/pr-preview/pr-139/_next/static/css/86d5280f1e6275fe.css" data-n-g=""/><link rel="stylesheet" href="/pr-preview/pr-139/_next/static/css/a1a0497113412518.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/pr-preview/pr-139/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/pr-preview/pr-139/_next/static/chunks/webpack-feee1eabc5104d22.js" defer=""></script><script src="/pr-preview/pr-139/_next/static/chunks/340-2f2942fd68e67fb7.js" defer=""></script><script src="/pr-preview/pr-139/_next/static/chunks/main-237fefe93cf728ed.js" defer=""></script><script src="/pr-preview/pr-139/_next/static/chunks/vendor-styles-6a3af95115cc4b9d.js" defer=""></script><script src="/pr-preview/pr-139/_next/static/chunks/505-497238b18588d25d.js" defer=""></script><script src="/pr-preview/pr-139/_next/static/chunks/pages/_app-ef1f9bbf0259e57f.js" defer=""></script><script src="/pr-preview/pr-139/_next/static/chunks/347-501ace96f6678428.js" defer=""></script><script src="/pr-preview/pr-139/_next/static/chunks/590-f2bb628a1d736ca3.js" defer=""></script><script src="/pr-preview/pr-139/_next/static/chunks/pages/theses-5c3cc556a89eadad.js" defer=""></script><script src="/pr-preview/pr-139/_next/static/eiqJHKxDgc2G2ZNhdY9n-/_buildManifest.js" defer=""></script><script src="/pr-preview/pr-139/_next/static/eiqJHKxDgc2G2ZNhdY9n-/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_0eea4f"><div class="ui center aligned container"><div class="ui secondary huge compact menu"><a class="item" href="/pr-preview/pr-139/"><img loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui tiny image" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/ilab-logo-3d.gif 1x" src="/pr-preview/pr-139/static/images/ilab-logo-3d.gif"/></a><a class="item" href="/pr-preview/pr-139/people/">People</a><a class="item" href="/pr-preview/pr-139/publications/">Research</a></div></div><div id="theses" class="category ui container"><h1 class="ui horizontal divider header"><svg data-prefix="far" data-icon="file-lines" class="svg-inline--fa fa-file-lines" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M64 48l112 0 0 88c0 39.8 32.2 72 72 72l88 0 0 240c0 8.8-7.2 16-16 16L64 464c-8.8 0-16-7.2-16-16L48 64c0-8.8 7.2-16 16-16zM224 67.9l92.1 92.1-68.1 0c-13.3 0-24-10.7-24-24l0-68.1zM64 0C28.7 0 0 28.7 0 64L0 448c0 35.3 28.7 64 64 64l256 0c35.3 0 64-28.7 64-64l0-261.5c0-17-6.7-33.3-18.7-45.3L242.7 18.7C230.7 6.7 214.5 0 197.5 0L64 0zm56 256c-13.3 0-24 10.7-24 24s10.7 24 24 24l144 0c13.3 0 24-10.7 24-24s-10.7-24-24-24l-144 0zm0 96c-13.3 0-24 10.7-24 24s10.7 24 24 24l144 0c13.3 0 24-10.7 24-24s-10.7-24-24-24l-144 0z"></path></svg>Theses</h1><div class="ui segment" style="margin-top:50px"><div class="thesis ui vertical segment stackable grid" data-id="phd-2025-cabral-mota"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Mota PhD 2025</span></p><p class="color" style="font-size:1.3em"><b>Modeling, Designing, and Evaluating Lens Visualizations for 3D and Immersive Analytics</b></p><p><a href="/pr-preview/pr-139/people/roberta-cabral-mota/"><img alt="Roberta Cabral Mota picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/roberta-cabral-mota.jpg 1x" src="/pr-preview/pr-139/static/images/people/roberta-cabral-mota.jpg"/><span class="author-link">Roberta Cabral Mota</span></a><span class="role"> (author)</span>, <span>Usman Alim</span><span class="role"> (advisor)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span>, <span>Mario Costa Sousa</span><span class="role"> (committee)</span>, <span>Nivan Ferreira</span><span class="role"> (committee)</span>, <a href="/pr-preview/pr-139/people/fateme-rajabiyazdi/"><img alt="Fateme Rajabiyazdi picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/fateme-rajabiyazdi.jpg 1x" src="/pr-preview/pr-139/static/images/people/fateme-rajabiyazdi.jpg"/><span class="author-link">Fateme Rajabiyazdi</span></a><span class="role"> (committee)</span>, <span>Parmit K. Chilana</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Focus Context Visualization</span><span class="ui brown basic label">Lens Visualization</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Immersive Analytics</span><span class="ui brown basic label">3 D Data</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2024-mckendrick"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">McKendrick PhD 2024</span></p><p class="color" style="font-size:1.3em"><b>The Virtual Rehearsal Suite: Drama and Performance Approaches for Virtual Reality and Human-Computer Interaction</b></p><p><a href="/pr-preview/pr-139/people/zachary-mckendrick/"><img alt="Zachary E. R. McKendrick picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/zachary-mckendrick.jpg 1x" src="/pr-preview/pr-139/static/images/people/zachary-mckendrick.jpg"/><span class="author-link">Zachary E. R. McKendrick</span></a><span class="role"> (author)</span>, <span>Patrick Finn</span><span class="role"> (advisor)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span>, <span>Cosmin Munteanu</span><span class="role"> (committee)</span>, <a href="/pr-preview/pr-139/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-139/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"> (committee)</span>, <span>April Viczko</span><span class="role"> (committee)</span>, <span>Michael Ullyot</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Drama</span><span class="ui brown basic label">Performance</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Extended Reality</span><span class="ui brown basic label">Human Comouter Interaction</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2024-friedel"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Friedel MSc 2024</span></p><p class="color" style="font-size:1.3em"><b>Large-surface Passive Haptic Interactions using Pantograph Mechanisms</b></p><p><a href="/pr-preview/pr-139/people/marcus-friedel/"><img alt="Marcus Kenneth Ernst Friedel picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/marcus-friedel.jpg 1x" src="/pr-preview/pr-139/static/images/people/marcus-friedel.jpg"/><span class="author-link">Marcus Kenneth Ernst Friedel</span></a><span class="role"> (author)</span>, <a href="/pr-preview/pr-139/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-139/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"> (advisor)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span>, <span>Aditya Nittala</span><span class="role"> (committee)</span>, <span>Richard Zhao</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">HCI</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Passive Haptics</span><span class="ui brown basic label">Pantographs</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2023-smith"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Smith MSc 2023</span></p><p class="color" style="font-size:1.3em"><b>Expanding the User Interactions and Design Process of Haptic Experiences in Virtual Reality</b></p><p><a href="/pr-preview/pr-139/people/christopher-smith/"><img alt="Christopher Geoffrey Smith picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/christopher-smith.jpg 1x" src="/pr-preview/pr-139/static/images/people/christopher-smith.jpg"/><span class="author-link">Christopher Geoffrey Smith</span></a><span class="role"> (author)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span>, <a href="/pr-preview/pr-139/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-139/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"> (advisor)</span>, <a href="/pr-preview/pr-139/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-139/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"> (advisor)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span>, <a href="/pr-preview/pr-139/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-139/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"> (advisor)</span>, <a href="/pr-preview/pr-139/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-139/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"> (advisor)</span>, <span>Richard Zhao</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Design Tool</span><span class="ui brown basic label">Tactile Feedback</span><span class="ui brown basic label">Design Process</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2023-wei"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Wei MSc 2023</span></p><p class="color" style="font-size:1.3em"><b>Design of Anthropomorphic Interfaces for Autonomous Vehicle-Pedestrian Interaction</b></p><p><a href="/pr-preview/pr-139/people/wei-wei/"><img alt="Wei Wei picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/wei-wei.jpg 1x" src="/pr-preview/pr-139/static/images/people/wei-wei.jpg"/><span class="author-link">Wei Wei</span></a><span class="role"> (author)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span>, <span>Zhangxing Chen</span><span class="role"> (committee)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span>, <a href="/pr-preview/pr-139/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-139/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"> (committee)</span>, <a href="/pr-preview/pr-139/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-139/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Anthropomorphism</span><span class="ui brown basic label">AV Pedestrian Interaction</span><span class="ui brown basic label">Immersive Analytics</span><span class="ui brown basic label">VR</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2021-asha"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Asha MSc 2021</span></p><p class="color" style="font-size:1.3em"><b>Designing Interaction with Autonomous Vehicles: External Displays and Interfaces for Vulnerable Road Users</b></p><p><a href="/pr-preview/pr-139/people/ashratuz-zavin-asha/"><img alt="Ashratuz Zavin Asha picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ashratuz-zavin-asha.jpg 1x" src="/pr-preview/pr-139/static/images/people/ashratuz-zavin-asha.jpg"/><span class="author-link">Ashratuz Zavin Asha</span></a><span class="role"> (author)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span>, <span>Michael John Jacobson Jr.</span><span class="role"> (committee)</span>, <span>Barry Wylant</span><span class="role"> (committee)</span>, <span>Patrick Finn</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Autonomous Vehicles</span><span class="ui brown basic label">External Automotive Displays</span><span class="ui brown basic label">Pedestrians With Hearing Aids</span><span class="ui brown basic label">Pedestrians In Wheelchairs</span><span class="ui brown basic label">Co Design</span><span class="ui brown basic label">VR Simulation</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2019-mahadevan"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Mahadevan MSc 2019</span></p><p class="color" style="font-size:1.3em"><b>Exploring the Design of Autonomous Vehicle-Pedestrian Interaction</b></p><p><a href="/pr-preview/pr-139/people/karthik-mahadevan/"><img alt="Karthik Mahadevan picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/karthik-mahadevan.jpg 1x" src="/pr-preview/pr-139/static/images/people/karthik-mahadevan.jpg"/><span class="author-link">Karthik Mahadevan</span></a><span class="role"> (author)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span>, <a href="/pr-preview/pr-139/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-139/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Autonomous Vehicle Pedestrian Interaction</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Interaction Design</span><span class="ui brown basic label">Virtual Reality</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2019-li"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Li PhD 2019</span></p><p class="color" style="font-size:1.3em"><b>Applications of Interactive Topographic Maps: Tangibility with Improved Spatial Awareness and Readability</b></p><p><span>Hao Li</span><span class="role"> (author)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span>, <span>Mario Costa Sousa</span><span class="role"> (advisor)</span>, <span>Kazuki Takashima</span><span class="role"> (committee)</span>, <span>Zhangxing Chen</span><span class="role"> (committee)</span>, <span>Pablo Figueroa</span><span class="role"> (committee)</span>, <a href="/pr-preview/pr-139/people/wesley-willett/"><img alt="Wesley J. Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-139/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley J. Willett</span></a><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Tangible User Interface</span><span class="ui brown basic label">Topographic Map</span><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Physicalization</span><span class="ui brown basic label">Physical Visualization</span><span class="ui brown basic label">Spatial Awareness</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2018-cartwright"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Cartwright MSc 2018</span></p><p class="color" style="font-size:1.3em"><b>Secure Collaboration Across the Reality-Virtuality Continuum Using Reservoir Data</b></p><p><span>Stephen Cartwright</span><span class="role"> (author)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span>, <span>Mario Costa Sousa</span><span class="role"> (advisor)</span>, <span>Zhangxing Chen</span><span class="role"> (committee)</span>, <span>Naser El-Sheimy</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Extended Reality</span><span class="ui brown basic label">Collaboration</span><span class="ui brown basic label">Information Security</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2018-ta"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Ta MSc 2018</span></p><p class="color" style="font-size:1.3em"><b>Exploring Prototyping Tools for Interactive Fashion Design</b></p><p><span>Kevin Ta</span><span class="role"> (author)</span>, <span>Lora A. Oehlberg</span><span class="role"> (advisor)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span>, <span>Anthony Tony</span><span class="role"> (committee)</span>, <span>Joshua M. Taron</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Prototyping Tools</span><span class="ui brown basic label">Electronic Fashion</span><span class="ui brown basic label">Augmented Reality</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2018-mostafa"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Mostafa PhD 2018</span></p><p class="color" style="font-size:1.3em"><b>Mediating Experiential Learning in Interactive Immersive Environments</b></p><p><span>Ahmed Mostafa</span><span class="role"> (author)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span>, <span>Mário Costa Sousa</span><span class="role"> (advisor)</span>, <span>Sonny Chan</span><span class="role"> (committee)</span>, <span>Kazuki Takashima</span><span class="role"> (committee)</span>, <span>Pierre Boulanger</span><span class="role"> (committee)</span>, <span>Naser El-Sheimy</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Virtual Environments</span><span class="ui brown basic label">Interactive</span><span class="ui brown basic label">Education</span><span class="ui brown basic label">Learning</span><span class="ui brown basic label">Simulation</span><span class="ui brown basic label">Immersion</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2017-somanath"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Somanath PhD 2017</span></p><p class="color" style="font-size:1.3em"><b>&#x27;Making&#x27; within Material, Cultural, and Emotional Constraints</b></p><p><a href="/pr-preview/pr-139/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-139/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"> (author)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span>, <span>Mário Costa Sousa</span><span class="role"> (advisor)</span>, <a href="/pr-preview/pr-139/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-139/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"> (committee)</span>, <span>Janette Hughes</span><span class="role"> (committee)</span>, <span>Vera Parlac</span><span class="role"> (committee)</span>, <span>Oscar Meruvia Pastor</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Computer Science</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2015-li"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Li MSc 2015</span></p><p class="color" style="font-size:1.3em"><b>Two-Sided Transparent Display as a Collaborative Medium</b></p><p><a href="/pr-preview/pr-139/people/jiannan-li/"><img alt="Jiannan Li picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/jiannan-li.jpg 1x" src="/pr-preview/pr-139/static/images/people/jiannan-li.jpg"/><span class="author-link">Jiannan Li</span></a><span class="role"> (author)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span>, <a href="/pr-preview/pr-139/people/saul-greenberg/"><img alt="Saul Greenberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/saul-greenberg.jpg 1x" src="/pr-preview/pr-139/static/images/people/saul-greenberg.jpg"/><span class="author-link">Saul Greenberg</span></a><span class="role"> (advisor)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Computer Science</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="mmus-2012-pon"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Pon MMus 2012</span></p><p class="color" style="font-size:1.3em"><b>Vuzik: Exploring a Medium for Painting Music</b></p><p><span>Aura Pon</span><span class="role"> (author)</span>, <span>David Eagle</span><span class="role"> (advisor)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Music</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2012-lapides"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Lapides MSc 2012</span></p><p class="color" style="font-size:1.3em"><b>Designing video games with social, physical, and authorship gameplay</b></p><p><a href="/pr-preview/pr-139/people/paul-lapides/"><img alt="Paul Lapides picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/paul-lapides.jpg 1x" src="/pr-preview/pr-139/static/images/people/paul-lapides.jpg"/><span class="author-link">Paul Lapides</span></a><span class="role"> (author)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span>, <span>Mário Costa Sousa</span><span class="role"> (advisor)</span></p><div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2011-harris"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Harris MSc 2011</span></p><p class="color" style="font-size:1.3em"><b>Exploring the affect of emotive motion in social human robot interaction</b></p><p><span>John J. R. Harris</span><span class="role"> (author)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span></p><div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2011-sultanum"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Sultanum MSc 2011</span></p><p class="color" style="font-size:1.3em"><b>Exploring novel interfaces for 3d visualization of reservoir simulation post-processing data</b></p><p><span>Nicole Barbosa Sultanum</span><span class="role"> (author)</span>, <span>Mário Costa Sousa</span><span class="role"> (advisor)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span></p><div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2010-young"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Young PhD 2010</span></p><p class="color" style="font-size:1.3em"><b>Exploring social interaction between robots and people</b></p><p><span>James E. Young</span><span class="role"> (author)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span></p><div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2008-guo"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Guo MSc 2008</span></p><p class="color" style="font-size:1.3em"><b>New paradigms for human-robot interaction using tangible user interfaces</b></p><p><span>Cheng Guo</span><span class="role"> (author)</span>, <a href="/pr-preview/pr-139/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (advisor)</span></p><div></div></div></div></div><div id="theses-modal"><div id="phd-2025-cabral-mota" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-139/theses/phd-2025-cabral-mota/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2025-cabral-mota</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-139/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Mota PhD 2025</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2025-cabral-mota" target="_blank">Modeling, Designing, and Evaluating Lens Visualizations for 3D and Immersive Analytics</a></h1><p class="meta"><a href="/people/roberta-cabral-mota"><img alt="roberta-cabral-mota photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/roberta-cabral-mota.jpg 1x" src="/pr-preview/pr-139/static/images/people/roberta-cabral-mota.jpg"/><strong>Roberta Cabral Mota</strong></a><span class="role"> (author)</span>, <span>Usman Alim<!-- --> <span class="role"> (advisor)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span>, <span>Mario Costa Sousa<!-- --> <span class="role"> (committee)</span></span>, <span>Nivan Ferreira<!-- --> <span class="role"> (committee)</span></span>, <a href="/people/fateme-rajabiyazdi"><img alt="fateme-rajabiyazdi photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/fateme-rajabiyazdi.jpg 1x" src="/pr-preview/pr-139/static/images/people/fateme-rajabiyazdi.jpg"/><strong>Fateme Rajabiyazdi</strong></a><span class="role"> (committee)</span>, <span>Parmit K. Chilana<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Lens visualization has been a prominent research area in the visualization community, fueled by the continuous need to mitigate visual clutter and occlusion resulting from the increasingly large datasets. Interactive lenses for 3D data, particularly, challenge visualization designers to conceive design strategies that facilitates the analysis of dense, multifaceted data with spatial referents. Given their relevance, the overarching research goal of this dissertation is to investigate how visualization lenses may support 3D data exploration and analysis—across both conventional and immersive environments. To this end, we begin with (1) modeling lenses by conducting a systematic review of existing lenses operating within spatial contexts. From this survey, we derive a design space that captures core design dimensions and choices involved in constructing spatially-embedded visualization lenses. Building upon this theoretical foundation, we proceed with (2) designing lenses, through the design, development, and evaluation of four immersive lenses tailored to support distinct forms of 3D data analysis: i) a deformable quadric lens for heterogeneous feature analysis, ii) a dual-imagery lens for multivariate, multi-geometry analysis, iii) a view-dependent lens for reservoir uncertainty analysis, and iv) a shape-conformal lens for spatiotemporal urban data analysis. Afterwards, we probe (3) evaluating lenses by complementing our prior theoretical and practical endeavors with empirical evidence from a controlled study comparing the efficacy of four lens-based designs in supporting time-varying urban data analysis. Throughout the course of our research agenda, we document domain-specific lessons learned and translate them into generalizable design guidelines intended to inform the broader visualization community. Finally, this dissertation concludes by suggesting future research directions for advancing visualization lenses in 3D data exploration and analysis—across both traditional and immersive environments. In the long term, we hope that this dissertation serve valuable reference points for visualization researchers and practitioners operating at either a theoretical, design, or empirical level.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Focus Context Visualization</span><span class="ui brown basic label">Lens Visualization</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Immersive Analytics</span><span class="ui brown basic label">3 D Data</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Roberta Cabral Mota<!-- -->. <b>Modeling, Designing, and Evaluating Lens Visualizations for 3D and Immersive Analytics</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2025-09-22<!-- -->. <!-- -->DOI: <a href="https://dx.doi.org/10.11575/PRISM/50555" target="_blank">https://dx.doi.org/10.11575/PRISM/50555</a>URL: <a href="https://hdl.handle.net/1880/122961" target="_blank">https://hdl.handle.net/1880/122961</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2024-mckendrick" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-139/theses/phd-2024-mckendrick/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2024-mckendrick</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-139/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">McKendrick PhD 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2024-mckendrick" target="_blank">The Virtual Rehearsal Suite: Drama and Performance Approaches for Virtual Reality and Human-Computer Interaction</a></h1><p class="meta"><a href="/people/zachary-mckendrick"><img alt="zachary-mckendrick photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/zachary-mckendrick.jpg 1x" src="/pr-preview/pr-139/static/images/people/zachary-mckendrick.jpg"/><strong>Zachary E. R. McKendrick</strong></a><span class="role"> (author)</span>, <span>Patrick Finn<!-- --> <span class="role"> (advisor)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span>, <span>Cosmin Munteanu<!-- --> <span class="role"> (committee)</span></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-139/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"> (committee)</span>, <span>April Viczko<!-- --> <span class="role"> (committee)</span></span>, <span>Michael Ullyot<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>This dissertation explores the intersection of Drama, virtual reality (VR), and Human-Computer Interaction (HCI), examining their parallels and collective impact on shaping immersive digital experiences. Throughout our research, we ask: how can VR support traditional performance practices? And how can we leverage existing performance practices to support wholistic user engagement? From our interdisciplinary position we identify gaps in existing research and offer novel solutions and innovative frameworks for navigating and structuring experiences within VR. In pursuit of answers to our research questions, we contribute C1) Performance-Based Multimodal Methodological Approaches for HCI, and C2) A Demonstration of Interdisciplinary Possibilities to the landscape of HCI VR research. We propose a symbiotic relationship between the user&#x27;s cognitive engagement and the digital environment&#x27;s architectural and interaction design can be enhanced through the application of practices and principles from Drama and performance. The practical application of these theoretical constructs is showcased in C3) Thresholding Protocols for Digital State Change, and C4) The Virtual Rehearsal Suite (VRS), an immersive VR environment that supports solo performance training, demonstrating how Drama and performance methodologies can enhance the user&#x27;s experience, offering tested perspectives and techniques that promote interaction, presence, and embodiment. Starting with our related works, we identify our interdisciplinary position with a foundation that draws from both the academic and artistic communities interested in VR as a domain, research, and performance tool. We then cast a wide net to understand the dimensions of virtual technologies and their impact on user experience with subsequent chapters investigating the layers of reality, immersion, embodiment, performance rituals, and thresholding concepts. Each chapter contributes to the identification of gaps and parallels across research domains and the discussion of how Drama and performance can elevate the understanding and advancement of VR and HCI systems. The dissertation concludes that the confluence of Drama and performance practice with Interaction Design holds the potential to shape the future aesthetics and experiential facets of virtual environments. The embrace of VR as both a tool and a medium for creative expression is positioned as a transformative leap forward in both HCI and Drama, heralding a new era of digital interaction that needs to embrace the full spectrum of human experience for success and longevity. Our work positions actors as interaction specialists, capable of existing in multiple realities at once and providing insightful reflections on their experiences in iterative processes. The VRS study demonstrates this ability, while emphasizing that virtual environments are not merely technological constructs but complex experiential spaces where the physical and digital converge, challenging traditional perceptions of reality. It highlights the importance of centralizing the user as key to creating compelling virtual experiences that con only be achieved through meticulously designed interactions that resonate with the user&#x27;s sensory and cognitive faculties. Our study underscores the efficacy of VR in supporting actor training with minimal digital interventions, facilitating a seamless transition into and out of VR, enhancing focus during VR engagement, and addressing issues such as VR sickness. It highlights the centralization of the human element as the pivotal factor in VR creation, emphasizing that VR environments should cater to the nuanced spectrum of human emotions, behaviours, and social interactions.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Drama</span><span class="ui brown basic label">Performance</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Extended Reality</span><span class="ui brown basic label">Human Comouter Interaction</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Zachary E. R. McKendrick<!-- -->. <b>The Virtual Rehearsal Suite: Drama and Performance Approaches for Virtual Reality and Human-Computer Interaction</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2024-05-14<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.11575/PRISM/46371" target="_blank">https://doi.org/10.11575/PRISM/46371</a>URL: <a href="https://hdl.handle.net/1880/118774" target="_blank">https://hdl.handle.net/1880/118774</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2024-friedel" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-139/theses/msc-2024-friedel/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2024-friedel</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-139/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Friedel MSc 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2024-friedel" target="_blank">Large-surface Passive Haptic Interactions using Pantograph Mechanisms</a></h1><p class="meta"><a href="/people/marcus-friedel"><img alt="marcus-friedel photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/marcus-friedel.jpg 1x" src="/pr-preview/pr-139/static/images/people/marcus-friedel.jpg"/><strong>Marcus Kenneth Ernst Friedel</strong></a><span class="role"> (author)</span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-139/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"> (advisor)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span>, <span>Aditya Nittala<!-- --> <span class="role"> (committee)</span></span>, <span>Richard Zhao<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Dexterous and natural haptic interaction with the environment in Virtual Reality promises a new era of embodied and intuitive computing. But among the remaining challenges stands the difficulty of natural wall interactions. Personal haptic devices for natural wall interaction in virtual reality should be portable and should provide passive, body-scale interactions. However, existing techniques fall short: Room-scale proxies lack portability, wearable robotic arms are energy-intensive and induce friction, and existing hand-scale passive interaction techniques are unsuitable for continuous large-scale renders. In this thesis, we introduce PantographHaptics, a technique which uses the scaling properties of a pantograph to passively render body-scale surfaces. A pantograph is a classical linkage mechanism which can enlarge or shrink designs by coordinating nodes to move in scaled, geometrically similar paths. To our knowledge, no prior work has applied the pantograph mechanism to large-scale immersive haptics. PantographHaptics is a novel method for passively achieving body-scale haptics which uses a pantograph to scale up a small positional constraint into an encounterable midair render. We present the conceptual foundation underpinning of PantographHaptics by describing the operation of the pantograph mechanism and detailing how we apply it for haptics. Then we verify the PantographHaptics technique through two prototypes: HapticLever, a grounded system, and Feedbackpack, a wearable device. We detail the designs, implementations, and technical evaluations of both prototypes, and we highlight the challenges and solutions involved in their development. We evaluate these prototypes with user evaluations, which contribute assessments of their interaction fidelity, investigations of their usability, comparisons of their performance against other haptic modalities, and recorded participant experiences of using the devices. By introducing and verifying PantographHaptics, we show that this novel technique is a viable and promising approach for interactions with large surfaces. By documenting the development of our prototype artifacts and reporting user experiences with the devices, we contribute a foundation for future research.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Haptics</span><span class="ui brown basic label">HCI</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Passive Haptics</span><span class="ui brown basic label">Pantographs</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Marcus Kenneth Ernst Friedel<!-- -->. <b>Large-surface Passive Haptic Interactions using Pantograph Mechanisms</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2024-01-17<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.11575/PRISM/42844" target="_blank">https://doi.org/10.11575/PRISM/42844</a>URL: <a href="https://hdl.handle.net/1880/118000" target="_blank">https://hdl.handle.net/1880/118000</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2023-smith" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-139/theses/msc-2023-smith/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2023-smith</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-139/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Smith MSc 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2023-smith" target="_blank">Expanding the User Interactions and Design Process of Haptic Experiences in Virtual Reality</a></h1><p class="meta"><a href="/people/christopher-smith"><img alt="christopher-smith photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/christopher-smith.jpg 1x" src="/pr-preview/pr-139/static/images/people/christopher-smith.jpg"/><strong>Christopher Geoffrey Smith</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span>, <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-139/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"> (advisor)</span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-139/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"> (advisor)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span>, <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-139/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"> (advisor)</span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-139/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"> (advisor)</span>, <span>Richard Zhao<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Virtual reality can be a highly immersive experience due to its realistic visual presentation. This immersive state is useful for applications including education, training, and entertainment. To enhance the state of immersion provided by virtual reality further, devices capable of simulating touch and force have been researched to allow not only a visual and audio experience but a haptic experience as well. Such research has investigated many approaches to generating haptics for virtual reality but often does not explore how to create an immersive haptic experience using them. In this thesis, we present a discussion on four proposed areas of the virtual reality haptic experience design process using a demonstration methodology. To investigate the application of haptic devices, we designed a modular ungrounded haptic system which was used to create a general-purpose device capable of force-based feedback and used it in the three topics of exploration. The first area explored is the application of existing haptic theory for aircraft control to the field of virtual reality drone control. The second area explored is the presence of the size-weight sensory illusion within virtual reality when using a simulated haptic force.  The third area explored is how authoring within a virtual reality medium can be used by a designer to create VR haptic experiences. From these explorations, we begin a higher-level discussion of the broader process of creating a virtual reality haptic experience. Using the results of each project as a representation of our proposed design steps, we discuss not only the broader concepts the steps contribute to the process and their importance, but also draw connections between them. By doing this, we present a more holistic approach to the large-scale design of virtual reality haptic experiences and the benefits we believe it provides.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Design Tool</span><span class="ui brown basic label">Tactile Feedback</span><span class="ui brown basic label">Design Process</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Christopher Geoffrey Smith<!-- -->. <b>Expanding the User Interactions and Design Process of Haptic Experiences in Virtual Reality</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2023-08<!-- -->. <!-- -->DOI: <a href="https://dx.doi.org/10.11575/PRISM/41738" target="_blank">https://dx.doi.org/10.11575/PRISM/41738</a>URL: <a href="https://hdl.handle.net/1880/116896" target="_blank">https://hdl.handle.net/1880/116896</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2023-wei" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-139/theses/msc-2023-wei/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2023-wei</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-139/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Wei MSc 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2023-wei" target="_blank">Design of Anthropomorphic Interfaces for Autonomous Vehicle-Pedestrian Interaction</a></h1><p class="meta"><a href="/people/wei-wei"><img alt="wei-wei photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/wei-wei.jpg 1x" src="/pr-preview/pr-139/static/images/people/wei-wei.jpg"/><strong>Wei Wei</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span>, <span>Zhangxing Chen<!-- --> <span class="role"> (committee)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-139/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"> (committee)</span>, <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-139/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"> (committee)</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Autonomous Vehicle (AV) technology promises to revolutionize human life. The promise of AVs includes reduced highway congestion, more efficient energy usage, and cheaper goods and services. However, without careful design, removing human drivers from vehicles will eliminate the natural communication channels which enable pedestrians to navigate safely. This thesis aims to design, present, and study anthropomorphic interfaces for autonomous vehicles, with the objective of enabling AVs to communicate with pedestrians through non-verbal cues. Non-verbal human communication is vital in human relationships. People use non-verbal communication when speech is impractical, such as when interacting with vehicles. When looking into ways in which AVs can use non-verbal communication to interact with pedestrians, we were inspired by the prospect of using anthropomorphic interfaces. This concept is well explored in Human-Robot Interaction (HRI) but has not been investigated in the context of AVs. For this thesis, we explored the design of anthropomorphic interfaces for autonomous vehicles. First, we proposed three types of anthropomorphic interfaces for AVs: facial expressions, hand gestures, and humanoid torsos. We developed a design space for each category using sketches and a low-fi prototype. Then, to research the benefits and limitations of anthropomorphic AVs, we implemented our AV interfaces in a Virtual Reality (VR) environment and developed two testbeds to evaluate their feasibility and scalability. Finally, we conducted two studies using the two testbeds. We investigated the study results using immersive analytics alongside traditional methods and revealed that anthropomorphic AVs could be helpful in AV-pedestrian interaction when designed by specific guidelines. Since we studied anthropomorphic AVs in VR, we were interested in the possibilities of analyzing the data of our study in an immersive environment. We designed a VR prototype specifically to analyze the data collected from the anthropomorphic AV study. The prototype provided basic immersive analytics features for the AV study data. We conducted an expert session with two domain experts to evaluate our immersive analytics prototype. The study contributed insights into the opportunities and challenges of utilizing immersive analytics to analyze AV studies.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Anthropomorphism</span><span class="ui brown basic label">AV Pedestrian Interaction</span><span class="ui brown basic label">Immersive Analytics</span><span class="ui brown basic label">VR</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Wei Wei<!-- -->. <b>Design of Anthropomorphic Interfaces for Autonomous Vehicle-Pedestrian Interaction</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2023-01<!-- -->. <!-- -->DOI: <a href="https://dx.doi.org/10.11575/PRISM/40689" target="_blank">https://dx.doi.org/10.11575/PRISM/40689</a>URL: <a href="http://hdl.handle.net/1880/115776" target="_blank">http://hdl.handle.net/1880/115776</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2021-asha" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-139/theses/msc-2021-asha/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2021-asha</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-139/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Asha MSc 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2021-asha" target="_blank">Designing Interaction with Autonomous Vehicles: External Displays and Interfaces for Vulnerable Road Users</a></h1><p class="meta"><a href="/people/ashratuz-zavin-asha"><img alt="ashratuz-zavin-asha photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ashratuz-zavin-asha.jpg 1x" src="/pr-preview/pr-139/static/images/people/ashratuz-zavin-asha.jpg"/><strong>Ashratuz Zavin Asha</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span>, <span>Michael John Jacobson Jr.<!-- --> <span class="role"> (committee)</span></span>, <span>Barry Wylant<!-- --> <span class="role"> (committee)</span></span>, <span>Patrick Finn<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>In the near future, mixed traffic consisting of manual and autonomous vehicles (AVs) will be common. Autonomous vehicles with advanced technology offer opportunities for innovative designs and introduce communication challenges for vulnerable road users such as pedestrians and cyclists. Our goal is to explore the emerging new domain of interaction between different road users and autonomous vehicles in a future AV transportation ecosystem. This led us to conduct the thesis following these two themes: 1) understanding design opportunities for external automotive displays (EADs) of AVs; 2) exploring the design of interactions between vulnerable road users (VRUs) and AVs. In theme 1, our work extends contemporary research into visualizations and related applications for autonomous vehicles. Focusing on external car bodies as a design space we introduce a set of EADs. EADs show visualizations to share context and user-specific information and offer opportunities for interaction between users and AVs. We conducted a design study to explore design concepts for EADs to provide services to different road users: pedestrians, passengers, and drivers of other vehicles. Based on the design study, we prototyped four EADs in virtual reality (VR) to demonstrate the potential of our approach. This exploration contributes to our vision for EADs, a design critique of the prototypes, and a discussion of the possible impact and future usage of external automotive displays. In theme 2, we are interested in the ways pedestrians will interact with autonomous vehicles in the absence of non-verbal cues from the driver (such as eye movements, hand gestures, etc.). Crossing streets in these new situations could be more dangerous for VRUs without a proper communication medium. We examined a subset of this challenge with two groups of pedestrians: interaction between AVs and pedestrians with hearing aids (PHAs), and pedestrians in wheelchairs (PWs). First, we worked with hearing aid users as a preliminary exploration of this research. We conduct a co-design study with a co-designer with hearing impairment who has lived experience of wearing hearing aid enhancements. This study contributes several insights and design recommendations on how potential audio cues can be designed to enhance direct communications between PHAs and AVs. For the second part of our research, we designed interactions between pedestrians in wheelchairs and AVs. From an early exploration of potential interface designs through a design study with interaction designers, we prototyped different interfaces in VR. Then, we evaluated the implemented simulations during a co-design study with a powered wheelchair user following inclusive design practices. We identify and reflect on interface design ideas that can help PWs make safe crossing decisions at intersections and discuss design insights for implementing different inclusive interfaces.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Autonomous Vehicles</span><span class="ui brown basic label">External Automotive Displays</span><span class="ui brown basic label">Pedestrians With Hearing Aids</span><span class="ui brown basic label">Pedestrians In Wheelchairs</span><span class="ui brown basic label">Co Design</span><span class="ui brown basic label">VR Simulation</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ashratuz Zavin Asha<!-- -->. <b>Designing Interaction with Autonomous Vehicles: External Displays and Interfaces for Vulnerable Road Users</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2021-09-02<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/113831" target="_blank">http://hdl.handle.net/1880/113831</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2019-mahadevan" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-139/theses/msc-2019-mahadevan/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2019-mahadevan</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-139/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Mahadevan MSc 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2019-mahadevan" target="_blank">Exploring the Design of Autonomous Vehicle-Pedestrian Interaction</a></h1><p class="meta"><a href="/people/karthik-mahadevan"><img alt="karthik-mahadevan photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/karthik-mahadevan.jpg 1x" src="/pr-preview/pr-139/static/images/people/karthik-mahadevan.jpg"/><strong>Karthik Mahadevan</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span>, <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-139/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"> (committee)</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Autonomous vehicle research today places an emphasis on developing better sensors and algorithms to enable the vehicle to localize itself in the environment, plan routes, and control its movement. Surveying the general public reveals optimism about the technology but also some skepticism about its ability to communicate with vulnerable road users such as pedestrians and cyclists. In today&#x27;s interaction with vehicles at crosswalks, pedestrians rely on cues originating from the vehicle and the driver. Vehicle cues relate to its kinematics such as speed and stopping distance while driver cues are concerned with communication such as eye gaze and contact, head and body movement, and hand gestures. In autonomous vehicles, however, a driver is not expected to be on-board to provide cues to pedestrians. We attempted to tackle the problem of designing novel ways to facilitate autonomous vehicle-pedestrian interaction at crosswalks. We propose interfaces which communicate an autonomous vehicle&#x27;s awareness and intent as a means of helping pedestrians make safe crossing decisions. Through our exploration, we make several contributions. First, we propose a design space for building interfaces using different cue modalities and cue locations. From an early exploration of this design space, we prototype interfaces designed to facilitate autonomous vehicle-pedestrian interaction. The interaction between vehicles and pedestrians will become more challenging during the transition period until all vehicles on the road are fully autonomous. During this period which we term mixed traffic, vehicles of varying levels of autonomy will occupy roads, some of which will have drivers, others such as semi-autonomous which may have distracted drivers, and fully autonomous vehicles which may or may not have drivers. To study this problem, we contribute a virtual reality-based pedestrian simulator. Our final contribution relates to the evaluation of interfaces in the real and virtual world where we found their inclusion helped pedestrians make safe crossing decisions.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Autonomous Vehicle Pedestrian Interaction</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Interaction Design</span><span class="ui brown basic label">Virtual Reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Karthik Mahadevan<!-- -->. <b>Exploring the Design of Autonomous Vehicle-Pedestrian Interaction</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2019-09-12<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/110924" target="_blank">http://hdl.handle.net/1880/110924</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2019-li" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-139/theses/phd-2019-li/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2019-li</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-139/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Li PhD 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2019-li" target="_blank">Applications of Interactive Topographic Maps: Tangibility with Improved Spatial Awareness and Readability</a></h1><p class="meta"><span>Hao Li<!-- --> <span class="role"> (author)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span>, <span>Mario Costa Sousa<!-- --> <span class="role"> (advisor)</span></span>, <span>Kazuki Takashima<!-- --> <span class="role"> (committee)</span></span>, <span>Zhangxing Chen<!-- --> <span class="role"> (committee)</span></span>, <span>Pablo Figueroa<!-- --> <span class="role"> (committee)</span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-139/static/images/people/wesley-willett.jpg"/><strong>Wesley J. Willett</strong></a><span class="role"> (committee)</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Traditional flat topographic maps are difficult to understand due to the distortion and compromise of the 3-dimensional (3D) spatial representation when it is folded into lower-dimension media (e.g. 2D). During the process, the x-y coordinate of a location can be captured but its physical elevation must be transformed using some visualization techniques, resulting in noticeable cognitive effort in comprehending the original geometric and geographic properties of the original terrain. In this manuscript-based dissertation, I present a collection of my past publications that aim to increase the readability of topographic maps by restoring the original spatiality of the terrain - including the elevations - with a physical map representation and then superimpose additional data visualization on top of it. In this way, the entire terrain topology is kept in a scaled physical representation, allowing users to view it with natural human perceptions. Additionally, user gestures can be tracked in real-time as a sketch-based input to allow novel dynamic interaction of the map interface and data manipulation of the spatial information. Through the chapters, I present the aforementioned concept, named interactive topographic interface, along with a few applications of it in different academic and industrial environments. I also report the design and results of a user study that compares the interface with traditional flat topographic maps. In the long-term, I hope that research mentioned in this dissertation inspires future interactive physical cartography to not only improve map comprehension but also facilitate better spatial and situational awareness over the map interface, resulting in an evolved map usefulness.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Tangible User Interface</span><span class="ui brown basic label">Topographic Map</span><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Physicalization</span><span class="ui brown basic label">Physical Visualization</span><span class="ui brown basic label">Spatial Awareness</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Hao Li<!-- -->. <b>Applications of Interactive Topographic Maps: Tangibility with Improved Spatial Awareness and Readability</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2019-07-02<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/110577" target="_blank">http://hdl.handle.net/1880/110577</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2018-cartwright" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-139/theses/msc-2018-cartwright/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2018-cartwright</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-139/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Cartwright MSc 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2018-cartwright" target="_blank">Secure Collaboration Across the Reality-Virtuality Continuum Using Reservoir Data</a></h1><p class="meta"><span>Stephen Cartwright<!-- --> <span class="role"> (author)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span>, <span>Mario Costa Sousa<!-- --> <span class="role"> (advisor)</span></span>, <span>Zhangxing Chen<!-- --> <span class="role"> (committee)</span></span>, <span>Naser El-Sheimy<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Acquiring, storing, processing, synthesizing, visualizing, and interpreting data are core to scientific knowledge discovery. This data life cycle is common to many diverse fields such as medicine and petroleum engineering. A wide range of techniques may be used for interacting with and visualizing data. Immersive technologies such as augmented reality and virtual reality show great potential to enhance important workplace activities such as collaboration. To this end an immersive, collaborative tool for visualizing reservoir data is discussed. Some collaborative scenarios using these technologies are then described. It is important to carefully consider how these technologies will be incorporated into a professional setting to ensure tools based on these technologies will provide a high quality user experience while meeting the security needs of industry. In order to further this goal, some of the architectural considerations of a collaboration tool that uses a variety of technologies from the reality-virtuality continuum are explored. A prototype tool is then presented that has been developed for collaborating over petroleum reservoir scenarios involving sensitive data. This tool incorporates visual protection mechanisms to facilitate collaboration while providing enhanced control over information disclosure. A user feedback session was performed with reservoir engineering subject matter experts, and the results from this exploratory evaluation are reported.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Extended Reality</span><span class="ui brown basic label">Collaboration</span><span class="ui brown basic label">Information Security</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Stephen Cartwright<!-- -->. <b>Secure Collaboration Across the Reality-Virtuality Continuum Using Reservoir Data</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2018-12-20<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/109396" target="_blank">http://hdl.handle.net/1880/109396</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2018-ta" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-139/theses/msc-2018-ta/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2018-ta</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-139/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Ta MSc 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2018-ta" target="_blank">Exploring Prototyping Tools for Interactive Fashion Design</a></h1><p class="meta"><span>Kevin Ta<!-- --> <span class="role"> (author)</span></span>, <span>Lora A. Oehlberg<!-- --> <span class="role"> (advisor)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span>, <span>Anthony Tony<!-- --> <span class="role"> (committee)</span></span>, <span>Joshua M. Taron<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Interactive garments enable new forms of communication between our bodies and with other people. In electronic fashion (eFashion) design, interactive garments on high fashion runways envision how people might use interactive technologies to enhance our clothing with new sensing and output capabilities. Researchers and fashion designers have since explored new interactive textiles that enable aesthetics-driven, interactive, and new material properties to explore on clothing. While there exist physical tools to implement interactive garments and software tools to create the visual aesthetic of a garment, these tools cannot yet enable designers to use new eFashion technologies in their garments because they require engineering expertise and specialized laboratory equipment. In this thesis, I explore the use of computer-aided prototyping tools to develop interactive eFashion garments. I present case studies with makers and two experienced eFashion designers about their design practices and formulate design guidelines for prototyping tools. I then present two prototyping tools for implementation and exploration of interactive garments. Finally, I discuss future work for physical and virtual prototyping tools in eFashion.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Prototyping Tools</span><span class="ui brown basic label">Electronic Fashion</span><span class="ui brown basic label">Augmented Reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kevin Ta<!-- -->. <b>Exploring Prototyping Tools for Interactive Fashion Design</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2018-09-21<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/108718" target="_blank">http://hdl.handle.net/1880/108718</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2018-mostafa" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-139/theses/phd-2018-mostafa/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2018-mostafa</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-139/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Mostafa PhD 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2018-mostafa" target="_blank">Mediating Experiential Learning in Interactive Immersive Environments</a></h1><p class="meta"><span>Ahmed Mostafa<!-- --> <span class="role"> (author)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span>, <span>Mário Costa Sousa<!-- --> <span class="role"> (advisor)</span></span>, <span>Sonny Chan<!-- --> <span class="role"> (committee)</span></span>, <span>Kazuki Takashima<!-- --> <span class="role"> (committee)</span></span>, <span>Pierre Boulanger<!-- --> <span class="role"> (committee)</span></span>, <span>Naser El-Sheimy<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Simulation and immersive environments are gaining popularity in various contexts. Arguably, such interactive systems have the potential to benefit many users in a variety of education and training scenarios. However, some of these systems especially with the lack of skilled instructors are still faced by challenges of operational complexity, the incorporation of different technologies and features, and the limited availability of performance measures and feedback. Therefore, the design of these systems would benefit from integrating experiential aspects and essential educational aids. For example, users of such learning systems, especially the novice ones, can be better supported by a smoother learning curve, detailed guidance features, the availability of feedback and performance reporting, and the integration of engaging &amp; reflective capabilities. In essence, we recognize a need to re-explore learning aids and how they impact design, usage, and overall learning experience in interactive immersive environments.<br/>The goal of this dissertation is to mediate experiential learning in interactive immersive environments. This includes exploring existing and novel learning aids that would facilitate learning with improved engagement and immersion, enrich learners with insightful reflections, better support novice users’ learning and training needs, and ultimately enhance the overall experience.<br/>To achieve this goal, we utilized existing learning models and simulation-based training approaches and proposed a framework of learning aids to mediate learning in interactive immersive environments. Working closely with domain expert collaborators, we designed, implemented and evaluated four new interactive immersive prototypes in an attempt to validate the practicality of our aids. The first prototype, NeuroSimVR, is a stereoscopic visualization augmented with educational aids to support how medical users learn about a common back surgery procedure. The second prototype, ReflectiveSpineVR, is an immersive virtual reality surgical simulation with innovative<br/>interaction history capabilities that aim to empower users’ memories and enable deliberate repetitive practice as needed. The third prototype, JackVR, is an interactive immersive training system, utilizing novel gamification elements, and aims to support oil-and-gas experts in the process of landing oil rigs. Our fourth prototype, RoboTeacher, involves a humanoid robot instructor for teaching people industrial assembly tasks. In our prototypes, we presented novel learning aids, visualization, and interaction techniques that are new to many of the current immersive learning tools. We conclude this dissertation with lessons learned and guidelines for designing with learning aids in future research directions that target interactive experiential environments.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Virtual Environments</span><span class="ui brown basic label">Interactive</span><span class="ui brown basic label">Education</span><span class="ui brown basic label">Learning</span><span class="ui brown basic label">Simulation</span><span class="ui brown basic label">Immersion</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ahmed Mostafa<!-- -->. <b>Mediating Experiential Learning in Interactive Immersive Environments</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2018-01-22<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/106342" target="_blank">http://hdl.handle.net/1880/106342</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2017-somanath" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-139/theses/phd-2017-somanath/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2017-somanath</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-139/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Somanath PhD 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2017-somanath" target="_blank">&#x27;Making&#x27; within Material, Cultural, and Emotional Constraints</a></h1><p class="meta"><a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-139/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span>, <span>Mário Costa Sousa<!-- --> <span class="role"> (advisor)</span></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-139/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"> (committee)</span>, <span>Janette Hughes<!-- --> <span class="role"> (committee)</span></span>, <span>Vera Parlac<!-- --> <span class="role"> (committee)</span></span>, <span>Oscar Meruvia Pastor<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>The Maker Movement aims to democratize technological practices and promises many benefits for people including improved technical literacy, a means for self-expression and agency, and an opportunity to become more than consumers of technology. As part of the Maker Movement, people build hobbyist and utilitarian projects by themselves using programmable electronics (e.g., microcontroller, sensors, actuators) and software tools. While the Maker Movement is gaining momentum globally, some people are left out. Constraints such as material limitations, educational culture restrictions, and emotional or behavioral difficulties can often limit people from taking part in the Maker Movement. We refer to the systematic investigation of how diverse people respond to making-centered activities within constraints as an exploration of making within constraints.<br/>In this dissertation, we (1) study how people respond to creating physical objects by themselves within constraints and, (2) investigate how to design technology that can help makers within constraints.  We conducted an observational study in an impoverished school in India and identified the students&#x27; challenges and their strategies for making within material and educational culture constraints. We conducted a second study with at-promise youth in Canada and identified a set of lessons learned to engage youth within emotional and behavioral constraints in making-centered activities. Leveraging our observations, we proposed Augmented Reality (AR)-mediated prototyping as a way to address material constraints. AR-mediated prototyping can help makers to build, program, interact with and iterate on physical computing projects that combine both real-world and stand-in virtual electronic components. We designed, implemented, and evaluated a technology probe, Polymorphic Cube (PMC), as an instance of our vision. Our results show that PMC helped participants prototype despite missing I/O electronic components, and highlighted how AR-mediated prototyping extends to exploring project ideas, tinkering with implementation, and making with others.<br/>Informed by our empirical and design explorations, we suggest a set of characteristics of constraints and implications for designing future technologies for makers within constraints. In the long-term, we hope that this research will inspire interaction designers to develop new tools that can help resolve constraints for making.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Computer Science</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sowmya Somanath<!-- -->. <b>&#x27;Making&#x27; within Material, Cultural, and Emotional Constraints</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2017<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/11023/4237" target="_blank">http://hdl.handle.net/11023/4237</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2015-li" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-139/theses/msc-2015-li/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2015-li</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-139/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Li MSc 2015</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2015-li" target="_blank">Two-Sided Transparent Display as a Collaborative Medium</a></h1><p class="meta"><a href="/people/jiannan-li"><img alt="jiannan-li photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/jiannan-li.jpg 1x" src="/pr-preview/pr-139/static/images/people/jiannan-li.jpg"/><strong>Jiannan Li</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span>, <a href="/people/saul-greenberg"><img alt="saul-greenberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/saul-greenberg.jpg 1x" src="/pr-preview/pr-139/static/images/people/saul-greenberg.jpg"/><strong>Saul Greenberg</strong></a><span class="role"> (advisor)</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Transparent displays are ‘see-through’ screens: a person can simultaneously view both the graphics on the screen and real-world content visible through the screen. Interactive transparent displays can serve as an important medium supporting face-to-face collaboration, where people interact with both sides of the display and work together. Such displays enhance workspace awareness, which smooths collaboration: when a person is working on one side of a transparent display, the person on the other side can see the other&#x27;s hand gestures, gaze, and what s/he is currently manipulating on the shared screen. Even so, we argue that in order to provide effective support for collaboration, designing such transparent displays must go beyond current offerings. We propose using two-sided transparent displays, which can present different content on both sides. The displays should also accept interactive input on both sides and visually augment users’ actions when display transparency is reduced. We operationalized these design requirements with our two-sided transparent display prototype, FACINGBOARD-II, and devised a palette of supportive interaction techniques. Through empirical studies, we found that the workspace awareness provided by transparent displays is compromised with degrading display transparency, and that visually enhancing user actions can compensate for this awareness loss.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Computer Science</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jiannan Li<!-- -->. <b>Two-Sided Transparent Display as a Collaborative Medium</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2015-01-28<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/11023/2033" target="_blank">http://hdl.handle.net/11023/2033</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="mmus-2012-pon" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-139/theses/mmus-2012-pon/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>mmus-2012-pon</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-139/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Pon MMus 2012</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/mmus-2012-pon" target="_blank">Vuzik: Exploring a Medium for Painting Music</a></h1><p class="meta"><span>Aura Pon<!-- --> <span class="role"> (author)</span></span>, <span>David Eagle<!-- --> <span class="role"> (advisor)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>What if one could paint music? Music is often shaped by the medium we use to interact with it, and thus the development of different ways to compose and experience music can open up new creative possibilities to people. This somewhat whimsical proposition of painting music, and a quest to find a new medium to create, experience, and interact with music, gave impetus and shape to the development of a musical interface known as Vuzik. Vuzik is an interface for creating and visualizing music through painting gestures on a large interactive surface. This thesis presents the vision, design and implementation of Vuzik. It then describes explorations of it as a tool for music education, and for the performance and creation of electronic music. Finally, it presents evaluation efforts of Vuzik in its performance of each of these explorations, and reflects on their implications regarding the nature and potential of Vuzik.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Music</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Aura Pon<!-- -->. <b>Vuzik: Exploring a Medium for Painting Music</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Music (MMus)<!-- -->. <!-- -->2012-10-04<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/11023/302" target="_blank">http://hdl.handle.net/11023/302</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2012-lapides" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-139/theses/msc-2012-lapides/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2012-lapides</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-139/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Lapides MSc 2012</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2012-lapides" target="_blank">Designing video games with social, physical, and authorship gameplay</a></h1><p class="meta"><a href="/people/paul-lapides"><img alt="paul-lapides photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/paul-lapides.jpg 1x" src="/pr-preview/pr-139/static/images/people/paul-lapides.jpg"/><strong>Paul Lapides</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span>, <span>Mário Costa Sousa<!-- --> <span class="role"> (advisor)</span></span></p></div></div></div><div class="block"></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Paul Lapides<!-- -->. <b>Designing video games with social, physical, and authorship gameplay</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2012<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/105630" target="_blank">http://hdl.handle.net/1880/105630</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2011-harris" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-139/theses/msc-2011-harris/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2011-harris</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-139/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Harris MSc 2011</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2011-harris" target="_blank">Exploring the affect of emotive motion in social human robot interaction</a></h1><p class="meta"><span>John J. R. Harris<!-- --> <span class="role"> (author)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span></p></div></div></div><div class="block"></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">John J. R. Harris<!-- -->. <b>Exploring the affect of emotive motion in social human robot interaction</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2011<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/105508" target="_blank">http://hdl.handle.net/1880/105508</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2011-sultanum" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-139/theses/msc-2011-sultanum/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2011-sultanum</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-139/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Sultanum MSc 2011</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2011-sultanum" target="_blank">Exploring novel interfaces for 3d visualization of reservoir simulation post-processing data</a></h1><p class="meta"><span>Nicole Barbosa Sultanum<!-- --> <span class="role"> (author)</span></span>, <span>Mário Costa Sousa<!-- --> <span class="role"> (advisor)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span></p></div></div></div><div class="block"></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nicole Barbosa Sultanum<!-- -->. <b>Exploring novel interfaces for 3d visualization of reservoir simulation post-processing data</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2011<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/105516" target="_blank">http://hdl.handle.net/1880/105516</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2010-young" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-139/theses/phd-2010-young/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2010-young</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-139/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Young PhD 2010</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2010-young" target="_blank">Exploring social interaction between robots and people</a></h1><p class="meta"><span>James E. Young<!-- --> <span class="role"> (author)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span></p></div></div></div><div class="block"></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">James E. Young<!-- -->. <b>Exploring social interaction between robots and people</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2010<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/104850" target="_blank">http://hdl.handle.net/1880/104850</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2008-guo" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-139/theses/msc-2008-guo/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2008-guo</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-139/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Guo MSc 2008</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2008-guo" target="_blank">New paradigms for human-robot interaction using tangible user interfaces</a></h1><p class="meta"><span>Cheng Guo<!-- --> <span class="role"> (author)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-139/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (advisor)</span></p></div></div></div><div class="block"></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Cheng Guo<!-- -->. <b>New paradigms for human-robot interaction using tangible user interfaces</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2008<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/103564" target="_blank">http://hdl.handle.net/1880/103564</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><div class="content"><a href="https://ucalgary.ca"><img alt="University of Calgary logo" loading="lazy" width="200" height="0" decoding="async" data-nimg="1" style="color:transparent;max-width:200px;margin:0px auto;height:auto" srcSet="/pr-preview/pr-139/static/images/logo-4.png 1x, /pr-preview/pr-139/static/images/logo-4.png 2x" src="/pr-preview/pr-139/static/images/logo-4.png"/></a><div class="sub header"><a class="item" href="https://cpsc.ucalgary.ca">Department of Computer Science</a></div></div></div></footer></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{},"__N_SSG":true},"page":"/theses","query":{},"buildId":"eiqJHKxDgc2G2ZNhdY9n-","assetPrefix":"/pr-preview/pr-139","runtimeConfig":{"basePath":"/pr-preview/pr-139"},"isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>