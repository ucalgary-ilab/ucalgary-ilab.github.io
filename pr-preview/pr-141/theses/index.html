<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link rel="preload" href="/pr-preview/pr-141/_next/static/media/dc84b505c4b06e35-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/pr-preview/pr-141/_next/static/css/a27038bcf0372bf1.css" as="style"/><link rel="preload" href="/pr-preview/pr-141/_next/static/css/a1a0497113412518.css" as="style"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              // $('.ui.sidebar')
              //   .sidebar('attach events', '.sidebar.icon')

              $('.sidebar.icon').on('click', function(event) {
                $('.ui.sidebar')
                  .sidebar('toggle')
              })

              $('.project').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })

              $('.thesis').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><link rel="stylesheet" href="/pr-preview/pr-141/_next/static/css/a27038bcf0372bf1.css" data-n-g=""/><link rel="stylesheet" href="/pr-preview/pr-141/_next/static/css/a1a0497113412518.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/pr-preview/pr-141/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/pr-preview/pr-141/_next/static/chunks/webpack-b210f545b1308757.js" defer=""></script><script src="/pr-preview/pr-141/_next/static/chunks/340-47d7ad906b805f71.js" defer=""></script><script src="/pr-preview/pr-141/_next/static/chunks/main-237fefe93cf728ed.js" defer=""></script><script src="/pr-preview/pr-141/_next/static/chunks/vendor-styles-4a09968927195584.js" defer=""></script><script src="/pr-preview/pr-141/_next/static/chunks/505-7200babb68eb2ce0.js" defer=""></script><script src="/pr-preview/pr-141/_next/static/chunks/pages/_app-ef1f9bbf0259e57f.js" defer=""></script><script src="/pr-preview/pr-141/_next/static/chunks/347-b06884df1d9b1140.js" defer=""></script><script src="/pr-preview/pr-141/_next/static/chunks/590-d0cdcecb498c4cb2.js" defer=""></script><script src="/pr-preview/pr-141/_next/static/chunks/pages/theses-66d0faefca9d3b21.js" defer=""></script><script src="/pr-preview/pr-141/_next/static/2yw-8fd7LrJK4HtCbqyi1/_buildManifest.js" defer=""></script><script src="/pr-preview/pr-141/_next/static/2yw-8fd7LrJK4HtCbqyi1/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_5b4856"><div class="ui center aligned container"><div class="ui secondary huge compact menu"><a class="item" href="/pr-preview/pr-141/"><img loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui tiny image" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/ilab-logo-3d.gif 1x" src="/pr-preview/pr-141/static/images/ilab-logo-3d.gif"/></a><a class="item" href="/pr-preview/pr-141/people/">People</a><a class="item" href="/pr-preview/pr-141/publications/">Research</a></div></div><div id="theses" class="category ui container"><h1 class="ui horizontal divider header"><svg data-prefix="far" data-icon="file-lines" class="svg-inline--fa fa-file-lines" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M64 48l112 0 0 88c0 39.8 32.2 72 72 72l88 0 0 240c0 8.8-7.2 16-16 16L64 464c-8.8 0-16-7.2-16-16L48 64c0-8.8 7.2-16 16-16zM224 67.9l92.1 92.1-68.1 0c-13.3 0-24-10.7-24-24l0-68.1zM64 0C28.7 0 0 28.7 0 64L0 448c0 35.3 28.7 64 64 64l256 0c35.3 0 64-28.7 64-64l0-261.5c0-17-6.7-33.3-18.7-45.3L242.7 18.7C230.7 6.7 214.5 0 197.5 0L64 0zm56 256c-13.3 0-24 10.7-24 24s10.7 24 24 24l144 0c13.3 0 24-10.7 24-24s-10.7-24-24-24l-144 0zm0 96c-13.3 0-24 10.7-24 24s10.7 24 24 24l144 0c13.3 0 24-10.7 24-24s-10.7-24-24-24l-144 0z"></path></svg>Theses</h1><div class="ui segment" style="margin-top:50px"><div class="thesis ui vertical segment stackable grid" data-id="msc-2025-chulpongsatorn"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2025</span></p><p class="color" style="font-size:1.3em"><b>Physicality and Cross-Device Interaction in Augmented Reality</b></p><p><a href="/pr-preview/pr-141/people/neil-chulpongsatorn/"><img alt="Thobthai Chulpongsatorn picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-141/static/images/people/neil-chulpongsatorn.jpg"/><span class="author-link">Thobthai Chulpongsatorn</span></a><span class="role"> (author)</span>, <a href="/pr-preview/pr-141/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-141/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"> (supervisor)</span>, <a href="/pr-preview/pr-141/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-141/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"> (supervisor)</span>, <a href="/pr-preview/pr-141/people/christian-frisson/"><img alt="Christian Frisson picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-141/static/images/people/christian-frisson.jpg"/><span class="author-link">Christian Frisson</span></a><span class="role"> (committee)</span>, <span>Frank Maurer</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Human Computer Interactions</span><span class="ui brown basic label">Physicality</span><span class="ui brown basic label">Mobile Devices</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Augmented Interface</span><span class="ui brown basic label">Embedded Data Visualization</span><span class="ui brown basic label">Tangible Interaction</span><span class="ui brown basic label">Cross Device Interaction</span><span class="ui brown basic label">Holographic Cross Device Interaction</span><span class="ui brown basic label">Holographic Interface</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2023-jadon"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2023</span></p><p class="color" style="font-size:1.3em"><b>Mixed Reality Interfaces for Augmented Text and Speech</b></p><p><a href="/pr-preview/pr-141/people/shivesh-jadon/"><img alt="Shivesh Singh Jadon picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/shivesh-jadon.jpg 1x" src="/pr-preview/pr-141/static/images/people/shivesh-jadon.jpg"/><span class="author-link">Shivesh Singh Jadon</span></a><span class="role"> (author)</span>, <a href="/pr-preview/pr-141/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-141/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"> (supervisor)</span>, <a href="/pr-preview/pr-141/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-141/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"> (supervisor)</span>, <span>Frank Maurer</span><span class="role"> (committee)</span>, <span>Ruofei Du</span><span class="role"> (committee)</span>, <span>Kangsoo Kim</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Natural Language Processing</span><span class="ui brown basic label">Speech Recognition</span><span class="ui brown basic label">Keyword Extraction</span><span class="ui brown basic label">Mixed Reality</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2023-dhawka"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2023</span></p><p class="color" style="font-size:1.3em"><b>Demographically Diverse Anthropographics: Exploring Equitable Visual Representations of Diversity</b></p><p><a href="/pr-preview/pr-141/people/priya-dhawka/"><img alt="Priyadarshinee Dhawka picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/priya-dhawka.jpg 1x" src="/pr-preview/pr-141/static/images/people/priya-dhawka.jpg"/><span class="author-link">Priyadarshinee Dhawka</span></a><span class="role"> (author)</span>, <a href="/pr-preview/pr-141/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-141/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"> (supervisor)</span>, <span>Leanne Wu</span><span class="role"> (committee)</span>, <span>Geoffrey Messier</span><span class="role"> (committee)</span>, <span>Ryan Henry</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Diverse Anthropographics</span><span class="ui brown basic label">Equity</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Anthropographics</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2022-hull"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">PhD 2022</span></p><p class="color" style="font-size:1.3em"><b>Building with Data: Bridging Architectural Design Practices and Information Visualization</b></p><p><a href="/pr-preview/pr-141/people/carmen-hull/"><img alt="Carmen Hull picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/carmen-hull.jpg 1x" src="/pr-preview/pr-141/static/images/people/carmen-hull.jpg"/><span class="author-link">Carmen Hull</span></a><span class="role"> (author)</span>, <a href="/pr-preview/pr-141/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-141/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"> (supervisor)</span>, <span>Gerald Hushlak</span><span class="role"> (supervisor)</span>, <a href="/pr-preview/pr-141/people/sheelagh-carpendale/"><img alt="Sheelagh Carpendale picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-141/static/images/people/sheelagh-carpendale.jpg"/><span class="author-link">Sheelagh Carpendale</span></a><span class="role"> (committee)</span>, <span>Barrett Ens</span><span class="role"> (committee)</span>, <span>Laleh Bejat</span><span class="role"> (committee)</span>, <span>Daniel Keefe</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Architectural Methods</span><span class="ui brown basic label">Data Physicalization</span><span class="ui brown basic label">Generative Design</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2021-wannamaker"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2021</span></p><p class="color" style="font-size:1.3em"><b>Situated Self-Tracking: Ideating, Designing, and Deploying Dedicated User-driven Personal Informatics Systems</b></p><p><a href="/pr-preview/pr-141/people/kendra-wannamaker/"><img alt="Kendra Wannamaker picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/kendra-wannamaker.jpg 1x" src="/pr-preview/pr-141/static/images/people/kendra-wannamaker.jpg"/><span class="author-link">Kendra Wannamaker</span></a><span class="role"> (author)</span>, <a href="/pr-preview/pr-141/people/wesley-willett/"><img alt="Wesley J. Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-141/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley J. Willett</span></a><span class="role"> (supervisor)</span>, <a href="/pr-preview/pr-141/people/anthony-tang/"><img alt="Tony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-141/static/images/people/anthony-tang.jpg"/><span class="author-link">Tony Tang</span></a><span class="role"> (committee)</span>, <a href="/pr-preview/pr-141/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-141/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"> (committee)</span>, <a href="/pr-preview/pr-141/people/wesley-willett/"><img alt="Wesley J. Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-141/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley J. Willett</span></a><span class="role"> (supervisor)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Situated Visualization</span><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Design Workshops</span><span class="ui brown basic label">Small Displays</span><span class="ui brown basic label">Ideation</span><span class="ui brown basic label">Sketching</span><span class="ui brown basic label">Personal Data Tracking</span><span class="ui brown basic label">Ambient Devices Internet Of Things</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2019-danyluk"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2019</span></p><p class="color" style="font-size:1.3em"><b>Designing Camera Controls for Map Environments</b></p><p><a href="/pr-preview/pr-141/people/kurtis-danyluk/"><img alt="Kurtis Danyluk picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-141/static/images/people/kurtis-danyluk.jpg"/><span class="author-link">Kurtis Danyluk</span></a><span class="role"> (author)</span>, <a href="/pr-preview/pr-141/people/wesley-willett/"><img alt="Wesley J. Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-141/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley J. Willett</span></a><span class="role"> (supervisor)</span>, <a href="/pr-preview/pr-141/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-141/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (committee)</span>, <span>Faramarz Samavati</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Human Computer Interactions</span><span class="ui brown basic label">Interactive Camera Control</span><span class="ui brown basic label">User Interfaces</span><span class="ui brown basic label">Input Devices And Stratagies</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2017-hu"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2017</span></p><p class="color" style="font-size:1.3em"><b>Designing and Evaluating a Lightweight Video Player for Language Learning</b></p><p><span>Sathaporn Hu</span><span class="role"> (author)</span>, <a href="/pr-preview/pr-141/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-141/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"> (supervisor)</span>, <span>Usman Alim</span><span class="role"> (committee)</span>, <span>Parmit Chilana</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Computer Science</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2017-payne"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2017</span></p><p class="color" style="font-size:1.3em"><b>Examining the Utility of Constructing Physical Representations of Data</b></p><p><span>Jennifer Payne</span><span class="role"> (author)</span>, <a href="/pr-preview/pr-141/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-141/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"> (supervisor)</span>, <span>Jason Johnson</span><span class="role"> (supervisor)</span>, <a href="/pr-preview/pr-141/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-141/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (committee)</span>, <span>Barry Wylant</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Computer Science</span></div></div></div></div></div><div id="theses-modal"><div id="msc-2025-chulpongsatorn" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-141/theses/msc-2025-chulpongsatorn/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2025-chulpongsatorn</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-141/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2025</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2025-chulpongsatorn" target="_blank">Physicality and Cross-Device Interaction in Augmented Reality</a></h1><p class="meta"><a href="/people/neil-chulpongsatorn"><img alt="neil-chulpongsatorn photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-141/static/images/people/neil-chulpongsatorn.jpg"/><strong>Thobthai Chulpongsatorn</strong></a><span class="role"> (author)</span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-141/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"> (supervisor)</span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-141/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"> (supervisor)</span>, <a href="/people/christian-frisson"><img alt="christian-frisson photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-141/static/images/people/christian-frisson.jpg"/><strong>Christian Frisson</strong></a><span class="role"> (committee)</span>, <span>Frank Maurer<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>This thesis explores novel uses of physicality and cross-device interaction with augmented reality through three different research prototypes. (1) Augmented Math leverages the physical world as a basis for generating augmentations through an AI-assisted method that allows educators to create interactive math content from static diagrams. (2) HoloTouch enables users to physically interact with holograms by repurposing personal devices like smartphones as tangible input tools. (3) HoloDevice explores how AR can transform remote collaboration by simulating co-located interactions through holographic representations of users and their devices. These systems were developed through iterative prototyping, collaborative design discussions, user studies, and interviews. Together, they demonstrate how novel interaction techniques grounded in physical and cross-device affordances can enhance usability and engagement in AR environments.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Human Computer Interactions</span><span class="ui brown basic label">Physicality</span><span class="ui brown basic label">Mobile Devices</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Augmented Interface</span><span class="ui brown basic label">Embedded Data Visualization</span><span class="ui brown basic label">Tangible Interaction</span><span class="ui brown basic label">Cross Device Interaction</span><span class="ui brown basic label">Holographic Cross Device Interaction</span><span class="ui brown basic label">Holographic Interface</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Thobthai Chulpongsatorn<!-- -->. <b>Physicality and Cross-Device Interaction in Augmented Reality</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2025-09-18<!-- -->. <!-- -->DOI: <a href="https://dx.doi.org/10.11575/PRISM/50597" target="_blank">https://dx.doi.org/10.11575/PRISM/50597</a>URL: <a href="https://hdl.handle.net/1880/123003" target="_blank">https://hdl.handle.net/1880/123003</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2023-jadon" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-141/theses/msc-2023-jadon/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2023-jadon</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-141/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2023-jadon" target="_blank">Mixed Reality Interfaces for Augmented Text and Speech</a></h1><p class="meta"><a href="/people/shivesh-jadon"><img alt="shivesh-jadon photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/shivesh-jadon.jpg 1x" src="/pr-preview/pr-141/static/images/people/shivesh-jadon.jpg"/><strong>Shivesh Singh Jadon</strong></a><span class="role"> (author)</span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-141/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"> (supervisor)</span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-141/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"> (supervisor)</span>, <span>Frank Maurer<!-- --> <span class="role"> (committee)</span></span>, <span>Ruofei Du<!-- --> <span class="role"> (committee)</span></span>, <span>Kangsoo Kim<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>While technology plays a vital role in human communication, there still remain many significant challenges when using them in everyday life. Modern computing technologies, such as smartphones, offer convenient and swift access to information, facilitating tasks like reading documents or communicating with friends. However, these tools frequently lack adaptability, become distracting, consume excessive time, and impede interactions with people and contextual information. Furthermore, they often require numerous steps and significant time investment to gather pertinent information. We want to explore an efficient process of contextual information gathering for mixed reality (MR) interfaces that provide information directly in the user’s view. This approach allows for a seamless and flexible transition between language and subsequent contextual references, without disrupting the flow of communication. ’Augmented Language’ can be defined as the integration of language and communication with mixed reality to enhance, transform, or manipulate language-related aspects and various forms of linguistic augmentations (such as annotation/referencing, aiding social interactions, translation, localization, etc.). In this thesis, our broad objective is to explore mixed reality interfaces and their potential to enhance augmented language, particularly in the domains of speech and text. Our aim is to create interfaces that offer a more natural, generalizable, on-demand, and real-time experience of accessing contextually relevant information and providing adaptive interactions. To better address this broader objective, we systematically break it down to focus on two instances of augmented language. First, enhancing augmented conversation to support on-the-fly, co-located in-person conversations using embedded references. And second, enhancing digital and physical documents using MR to provide on-demand reading support in the form of different summarization techniques. To examine the effectiveness of these speech and text interfaces, we conducted two studies in which we asked the participants to evaluate our system prototype in different use cases. The exploratory usability study for the first exploration confirms that our system decreases distraction and friction in conversation compared to smartphone search while providing highly useful and relevant information. For the second project, we conducted an exploratory design workshop to identify categories of document enhancements. We later conducted a user study with a mixed-reality prototype to highlight five board themes to discuss the benefits of MR document enhancement.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Natural Language Processing</span><span class="ui brown basic label">Speech Recognition</span><span class="ui brown basic label">Keyword Extraction</span><span class="ui brown basic label">Mixed Reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Shivesh Singh Jadon<!-- -->. <b>Mixed Reality Interfaces for Augmented Text and Speech</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2023-08<!-- -->. <!-- -->DOI: <a href="https://dx.doi.org/10.11575/PRISM/41696" target="_blank">https://dx.doi.org/10.11575/PRISM/41696</a>URL: <a href="https://hdl.handle.net/1880/116854" target="_blank">https://hdl.handle.net/1880/116854</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2023-dhawka" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-141/theses/msc-2023-dhawka/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2023-dhawka</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-141/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2023-dhawka" target="_blank">Demographically Diverse Anthropographics: Exploring Equitable Visual Representations of Diversity</a></h1><p class="meta"><a href="/people/priya-dhawka"><img alt="priya-dhawka photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/priya-dhawka.jpg 1x" src="/pr-preview/pr-141/static/images/people/priya-dhawka.jpg"/><strong>Priyadarshinee Dhawka</strong></a><span class="role"> (author)</span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-141/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"> (supervisor)</span>, <span>Leanne Wu<!-- --> <span class="role"> (committee)</span></span>, <span>Geoffrey Messier<!-- --> <span class="role"> (committee)</span></span>, <span>Ryan Henry<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this thesis, we explore the design of demographically diverse anthropographics from demographic data. Anthropographics are visualizations that often use generic human-shaped symbols as abstract representations of humans. These visualizations are frequently used to convey the human importance of data related to people’s experiences, usually focusing on demographic data such as age, gender, race among others. However, most current anthropographics employ generic human shapes to represent data about distinct demographic groups, which can hide important demographic and physical differences between these groups. The use of generic human shapes in current anthropographics highlights the lack of inclusive approaches for representing human physical diversity in data visualizations. In response,we explore the creation of demographically diverse anthropographics that communicate the visible physical diversity of demographically-distinct populations. Our contributions stem from a set of critical design explorations for visualizing demographic data with a focus on representing human physical diversity and a study exploring how viewers perceive visual representations of diversity in anthropographics. We make three contributions in this work. First, we describe critical design explorations from two prototypes for representing racial demographic data as physical characteristics of diversity (such as skin tones) in diverse anthropographics. Second, we explore how viewers may perceive visual representations of demographic diversity in anthropographics through an interview study on contemporary examples of homogeneous anthropographics from popular news media and our own set of diverse anthropographics. Finally, we identify a set of social and technical challenges in the creation of anthropographics and contribute a collection of forward-looking opportunities for advancing this line of research on equitable visual representations of diversity through demographically diverse anthropographics.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Diverse Anthropographics</span><span class="ui brown basic label">Equity</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Anthropographics</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Priyadarshinee Dhawka<!-- -->. <b>Demographically Diverse Anthropographics: Exploring Equitable Visual Representations of Diversity</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2023-07<!-- -->. <!-- -->DOI: <a href="https://dx.doi.org/10.11575/PRISM/41666" target="_blank">https://dx.doi.org/10.11575/PRISM/41666</a>URL: <a href="https://hdl.handle.net/1880/116824" target="_blank">https://hdl.handle.net/1880/116824</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2022-hull" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-141/theses/phd-2022-hull/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2022-hull</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-141/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">PhD 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2022-hull" target="_blank">Building with Data: Bridging Architectural Design Practices and Information Visualization</a></h1><p class="meta"><a href="/people/carmen-hull"><img alt="carmen-hull photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/carmen-hull.jpg 1x" src="/pr-preview/pr-141/static/images/people/carmen-hull.jpg"/><strong>Carmen Hull</strong></a><span class="role"> (author)</span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-141/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"> (supervisor)</span>, <span>Gerald Hushlak<!-- --> <span class="role"> (supervisor)</span></span>, <a href="/people/sheelagh-carpendale"><img alt="sheelagh-carpendale photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-141/static/images/people/sheelagh-carpendale.jpg"/><strong>Sheelagh Carpendale</strong></a><span class="role"> (committee)</span>, <span>Barrett Ens<!-- --> <span class="role"> (committee)</span></span>, <span>Laleh Bejat<!-- --> <span class="role"> (committee)</span></span>, <span>Daniel Keefe<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Our work seeks to augment new information visualization research with strategies and workflows from the fields of design and architecture. To this end, this research explores how to adopt tools and methods that can integrate the best of physical and digital modalities to multiple contexts and scales in HCI and data visualization. Designing information visualization systems creates a need for a design approach that addresses and ties together two main threads – 1) how we as humans interact with and make sense of our environment and 2) how we as designers create meaning through geometry, form, and material encodings. While the research community within data visualization has primarily focused on screen-based data visualizations, there is now an opportunity to study how we can create insight with hybrid physical and digital representations of data through the lens of architectural practice. My colleagues and I have conducted this research at the intersection of model building, diagrams, and generative design, applying this knowledge to the design of multifaceted digital environments, from micro to macro scale, in two- and three- dimensional worlds. To develop this research, we first observe and characterize the architectural methods of model making and their potential to facilitate the design process of interactive systems. Next, we describe how physical hand-crafted and digitally fabricated models of different types assist in various stages of the design process. To illustrate how model building could support fluid exploration of multiple data sets, we built a 3D interactive campus model visualizing multiple layers of building-specific data. The system uses physical models as tangible tokens on an interactive touch surface, visualizing energy use and weather data daily over a two-year period. As an extension of our design, we developed a conceptual framework from this project to highlight the potential of physical models for supporting embodied exploration of spatial and non-spatial visualizations through fluid interaction. We then examine the use of diagrams in architecture and develop a conceptual framework based on the concept of data tectonics to organize and structure the design process of physical and immersive data systems. To further study the use of diagrams and generative design for data visualization, I collaborated with researchers at Tableau Software to develop a patented Tableau extension that self-generates and evolves up to thirty different design permutations at a time. The system randomly assigns a pre-specified palette of mark types to a chosen dataset giving designers the option of adding or deleting options that they deem promising. As a final project for this research, we brought the three principles of model making, diagramming, and generative design together to create a large-scale physical and immersive data visualization. In collaboration with the Department of Social Work at the University of Calgary, the project uses diagrams and generative design to prototype a series of three-dimensional encodings visualizing Global Gender Gap statistics from the World Economic Forum. The tent-like forms evoke sheltering structures that can be registered, experienced, and measured with the whole body. For this project, we applied the diagrammatic approach used in parametric design to traditional information visualization design principles and identified workflows that support rapid exploration and fabrication of multiple data design alternatives. There is no doubt that data and digital technologies, including machine learning and AI, will be part of our human fabric in the future, but what that looks like and how it is structured is still up to us. We need artists, and more diversity in general, in order to do this to the best of our potential as humans. In determining which practices encourage the creation of rich data-driven environments, this research underscores the fundamental need of humans to make sense of the world, inspiring designers to develop new spatial constructs that integrate both the art and science of the built environment.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Architectural Methods</span><span class="ui brown basic label">Data Physicalization</span><span class="ui brown basic label">Generative Design</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Carmen Hull<!-- -->. <b>Building with Data: Bridging Architectural Design Practices and Information Visualization</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2022-01-28<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/114360" target="_blank">http://hdl.handle.net/1880/114360</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2021-wannamaker" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-141/theses/msc-2021-wannamaker/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2021-wannamaker</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-141/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2021-wannamaker" target="_blank">Situated Self-Tracking: Ideating, Designing, and Deploying Dedicated User-driven Personal Informatics Systems</a></h1><p class="meta"><a href="/people/kendra-wannamaker"><img alt="kendra-wannamaker photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/kendra-wannamaker.jpg 1x" src="/pr-preview/pr-141/static/images/people/kendra-wannamaker.jpg"/><strong>Kendra Wannamaker</strong></a><span class="role"> (author)</span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-141/static/images/people/wesley-willett.jpg"/><strong>Wesley J. Willett</strong></a><span class="role"> (supervisor)</span>, <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-141/static/images/people/anthony-tang.jpg"/><strong>Tony Tang</strong></a><span class="role"> (committee)</span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-141/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"> (committee)</span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-141/static/images/people/wesley-willett.jpg"/><strong>Wesley J. Willett</strong></a><span class="role"> (supervisor)</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this thesis, we examine the intersection between personal informatics and situated visualization. Personal Informatics systems aim to help people collect and utilize their own data. Situated visualizations aim to decentralize data consumption and support people in making data-driven decisions in-situ. We present I/O Bits, a prototype personal informatics system that explores the potential for situated self-tracking. With simple tactile inputs and small e-paper visualizations, I/O Bits are dedicated physical devices that allow individuals to track and visualize different kinds of personal activities in-situ. This is in contrast to most self-tracking systems, which automate data collection, centralize information displays, or integrate into multi-purpose devices like smartwatches or mobile phones. Our contributions stem from a set of situated ideation workshops, an e-paper visualization workshop, the development of I/O Bits, and a prototype deployment where participants constructed their own I/O Bits and used them to track a range of personal data. We make three contributions with this work. First, we report on methodologies from seven design workshops that used ideation and sketching activities to prototype new situated visualizations. Based on our diverse set of workshops, we identify challenges and opportunities for sketching and ideating situated visualizations and highlight promising methods for both designers and researchers. Second, we use our design workshop results to design our novel situated self-tracking system, I/O Bits. We discuss the tensions experienced during our iterative design and development process and explore the design space of small situated visualizations on e-paper displays. Finally, we examine our findings from the situated ideation workshops, e-paper visualization workshop, development process, and prototype deployment. Using sketches, photos, hardware, audio recordings, and transcripts, we distill a set of insights and opportunities for future research on situated self-tracking.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Situated Visualization</span><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Design Workshops</span><span class="ui brown basic label">Small Displays</span><span class="ui brown basic label">Ideation</span><span class="ui brown basic label">Sketching</span><span class="ui brown basic label">Personal Data Tracking</span><span class="ui brown basic label">Ambient Devices Internet Of Things</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kendra Wannamaker<!-- -->. <b>Situated Self-Tracking: Ideating, Designing, and Deploying Dedicated User-driven Personal Informatics Systems</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2021-01-20<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/113022" target="_blank">http://hdl.handle.net/1880/113022</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2019-danyluk" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-141/theses/msc-2019-danyluk/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2019-danyluk</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-141/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2019-danyluk" target="_blank">Designing Camera Controls for Map Environments</a></h1><p class="meta"><a href="/people/kurtis-danyluk"><img alt="kurtis-danyluk photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-141/static/images/people/kurtis-danyluk.jpg"/><strong>Kurtis Danyluk</strong></a><span class="role"> (author)</span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-141/static/images/people/wesley-willett.jpg"/><strong>Wesley J. Willett</strong></a><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-141/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (committee)</span>, <span>Faramarz Samavati<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present an exploration of two classes of navigation techniques designed for representations of real-world terrain. The first introduces look-from camera controls, a new style of camera control for touch devices designed with representations of real world-terrain in mind and provides an evaluation of three different implementations of this style of control. The second looks to virtual reality and compares the effectiveness of four existing and common camera control techniques within the context of a representations of real world-terrain. Effective camera controls greatly increase a user’s ability to engage with a virtual environment, and virtual map environments are no different. However, current camera controls are difficult to use within map-like environments, requiring burdensome sequences of interactions or performing poorly within ragged terrain. To examine the effectiveness of different camera controls in this space we conducted two studies in which we asked participants to perform map reading and interaction tasks. In both studies the camera control technique greatly influenced participant engagement and enjoyment within a scene. The first study highlights the effectiveness of look-from camera controls as light-weight additions to direct manipulation controls and provides design guidelines for the construction of look-from camera controls. The second study highlights which existing common navigation techniques are most appropriate within a map-like environment presented in immersive virtual reality and how combinations of these controls can combine the strengths of the controls to cover for the weaknesses of others.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Human Computer Interactions</span><span class="ui brown basic label">Interactive Camera Control</span><span class="ui brown basic label">User Interfaces</span><span class="ui brown basic label">Input Devices And Stratagies</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kurtis Danyluk<!-- -->. <b>Designing Camera Controls for Map Environments</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2019-01-16<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/109480" target="_blank">http://hdl.handle.net/1880/109480</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2017-hu" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-141/theses/msc-2017-hu/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2017-hu</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-141/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2017-hu" target="_blank">Designing and Evaluating a Lightweight Video Player for Language Learning</a></h1><p class="meta"><span>Sathaporn Hu<!-- --> <span class="role"> (author)</span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-141/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"> (supervisor)</span>, <span>Usman Alim<!-- --> <span class="role"> (committee)</span></span>, <span>Parmit Chilana<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Watching foreign language videos is a popular and convenient strategy used by many people for learning a new language. However, traditional video players, such as the YouTube player, are not designed to support language learning. We created two video players to explore and to address the issues of using traditional players as a language learning tool. Our players specifically target casual language learners. After evaluating the first player, we found that a traditional player makes it difficult for learners to (1) adjust the level of difficulty, (2) recover missed information, and (3) assess learning progress. We then created the second player to address these issues. The results of the evaluation of the second player demonstrate that people found the player to be helpful for language learning. We also found common usage patterns in the results and opportunities for future improvement.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Computer Science</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sathaporn Hu<!-- -->. <b>Designing and Evaluating a Lightweight Video Player for Language Learning</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2017<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/11023/4265" target="_blank">http://hdl.handle.net/11023/4265</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2017-payne" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-141/theses/msc-2017-payne/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2017-payne</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-141/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2017-payne" target="_blank">Examining the Utility of Constructing Physical Representations of Data</a></h1><p class="meta"><span>Jennifer Payne<!-- --> <span class="role"> (author)</span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-141/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"> (supervisor)</span>, <span>Jason Johnson<!-- --> <span class="role"> (supervisor)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-141/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-141/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (committee)</span>, <span>Barry Wylant<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>For millennia, people have constructed physicalizations---physical representations of information---by hand. Recent studies have shown that physicalizations can be more efficient for transmitting information than on-screen visualizations. In addition, innovations like shape-changing interfaces and digital fabrication now make it possible to create physicalizations with little manual effort. Yet many physicalizations are still constructed by hand. In this thesis, we explore how manual construction of physicalizations influences the way people approach and comprehend data, through two studies.<br/>One study compares bar chart authoring through physical construction to authoring using template-based chart creation software. A second study compares participant behaviour when constructing physicalizations to that when exploring previously-built physicalizations. Through comparison of these processes, we derive implications for the design of visualization authoring tools, and for the exploration of data.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Computer Science</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jennifer Payne<!-- -->. <b>Examining the Utility of Constructing Physical Representations of Data</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2017<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/11023/3613" target="_blank">http://hdl.handle.net/11023/3613</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><div class="content"><a href="https://ucalgary.ca"><img alt="University of Calgary logo" loading="lazy" width="200" height="0" decoding="async" data-nimg="1" style="color:transparent;max-width:200px;margin:0px auto;height:auto" srcSet="/pr-preview/pr-141/static/images/logo-4.png 1x, /pr-preview/pr-141/static/images/logo-4.png 2x" src="/pr-preview/pr-141/static/images/logo-4.png"/></a><div class="sub header"><a class="item" href="https://cpsc.ucalgary.ca">Department of Computer Science</a></div></div></div></footer></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{},"__N_SSG":true},"page":"/theses","query":{},"buildId":"2yw-8fd7LrJK4HtCbqyi1","assetPrefix":"/pr-preview/pr-141","runtimeConfig":{"basePath":"/pr-preview/pr-141"},"isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>