<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link rel="preload" href="/pr-preview/pr-121/_next/static/media/dc84b505c4b06e35-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/pr-preview/pr-121/_next/static/css/71a767ff54d87990.css" as="style"/><link rel="preload" href="/pr-preview/pr-121/_next/static/css/d9e3927f9c043bc5.css" as="style"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              // $('.ui.sidebar')
              //   .sidebar('attach events', '.sidebar.icon')

              $('.sidebar.icon').on('click', function(event) {
                $('.ui.sidebar')
                  .sidebar('toggle')
              })

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><link rel="stylesheet" href="/pr-preview/pr-121/_next/static/css/71a767ff54d87990.css" data-n-g=""/><link rel="stylesheet" href="/pr-preview/pr-121/_next/static/css/d9e3927f9c043bc5.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/pr-preview/pr-121/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/pr-preview/pr-121/_next/static/chunks/webpack-24745baa8ac08618.js" defer=""></script><script src="/pr-preview/pr-121/_next/static/chunks/340-3dd10c03f6f7bcf4.js" defer=""></script><script src="/pr-preview/pr-121/_next/static/chunks/main-237fefe93cf728ed.js" defer=""></script><script src="/pr-preview/pr-121/_next/static/chunks/vendor-styles-f4f891a233b3019a.js" defer=""></script><script src="/pr-preview/pr-121/_next/static/chunks/505-4969504d97a1353a.js" defer=""></script><script src="/pr-preview/pr-121/_next/static/chunks/pages/_app-b2c4abf66b60dfa7.js" defer=""></script><script src="/pr-preview/pr-121/_next/static/chunks/851-9ae2497a89059d76.js" defer=""></script><script src="/pr-preview/pr-121/_next/static/chunks/649-1fa53f4ef9e94906.js" defer=""></script><script src="/pr-preview/pr-121/_next/static/chunks/696-56f902de439e2ede.js" defer=""></script><script src="/pr-preview/pr-121/_next/static/chunks/pages/publications-9f07b2218ae902c4.js" defer=""></script><script src="/pr-preview/pr-121/_next/static/gasA4LnMQu4cOXKl1aJ2_/_buildManifest.js" defer=""></script><script src="/pr-preview/pr-121/_next/static/gasA4LnMQu4cOXKl1aJ2_/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_214227"><div class="ui center aligned container"><div class="ui secondary huge compact menu"><a class="item" href="/pr-preview/pr-121/"><img class="ui tiny image" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/ilab-logo-3d.gif 1x" src="/pr-preview/pr-121/static/images/ilab-logo-3d.gif"/></a><a class="item" href="/pr-preview/pr-121/people/">People</a><a class="item" href="/pr-preview/pr-121/publications/">Research</a></div></div><div id="publications" class="category"><h1 class="ui horizontal divider header"><svg data-prefix="far" data-icon="file-lines" class="svg-inline--fa fa-file-lines" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M64 48l112 0 0 88c0 39.8 32.2 72 72 72l88 0 0 240c0 8.8-7.2 16-16 16L64 464c-8.8 0-16-7.2-16-16L48 64c0-8.8 7.2-16 16-16zM224 67.9l92.1 92.1-68.1 0c-13.3 0-24-10.7-24-24l0-68.1zM64 0C28.7 0 0 28.7 0 64L0 448c0 35.3 28.7 64 64 64l256 0c35.3 0 64-28.7 64-64l0-261.5c0-17-6.7-33.3-18.7-45.3L242.7 18.7C230.7 6.7 214.5 0 197.5 0L64 0zm56 256c-13.3 0-24 10.7-24 24s10.7 24 24 24l144 0c13.3 0 24-10.7 24-24s-10.7-24-24-24l-144 0zm0 96c-13.3 0-24 10.7-24 24s10.7 24 24 24l144 0c13.3 0 24-10.7 24-24s-10.7-24-24-24l-144 0z"></path></svg>Publications</h1><div class="ui segment" style="margin-top:50px"><div class="publication ui vertical segment stackable grid" data-id="capstone-2025-haptic-floor-proxy"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">ENGG 501-502 Capstone 2025</span></p><p class="color" style="font-size:1.3em"><b>A low-cost open-source miniature proxy to experience, design for, and showcase haptic floors</b></p><p><a href="/pr-preview/pr-121/people/aidan-gaede-janke/"><img alt="Aidan Gaede-Janke picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aidan-gaede-janke.jpg 1x" src="/pr-preview/pr-121/static/images/people/aidan-gaede-janke.jpg"/><span class="author-link">Aidan Gaede-Janke</span></a> , <a href="/pr-preview/pr-121/people/isabella-huang/"><img alt="Isabella Huang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/isabella-huang.jpg 1x" src="/pr-preview/pr-121/static/images/people/isabella-huang.jpg"/><span class="author-link">Isabella Huang</span></a> , <a href="/pr-preview/pr-121/people/ebube-anachebe/"><img alt="Ebube Anachebe picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ebube-anachebe.jpg 1x" src="/pr-preview/pr-121/static/images/people/ebube-anachebe.jpg"/><span class="author-link">Ebube Anachebe</span></a> , <a href="/pr-preview/pr-121/people/nadeem-moosa/"><img alt="Nadeem Moosa picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/nadeem-moosa.jpg 1x" src="/pr-preview/pr-121/static/images/people/nadeem-moosa.jpg"/><span class="author-link">Nadeem Moosa</span></a> , <a href="/pr-preview/pr-121/people/sebastian-gil/"><img alt="Sebastian Gil picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sebastian-gil.jpg 1x" src="/pr-preview/pr-121/static/images/people/sebastian-gil.jpg"/><span class="author-link">Sebastian Gil</span></a> , <a href="/pr-preview/pr-121/people/yaseen-rashid/"><img alt="Yaseen Rashid picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/yaseen-rashid.jpg 1x" src="/pr-preview/pr-121/static/images/people/yaseen-rashid.jpg"/><span class="author-link">Yaseen Rashid</span></a> , <a href="/pr-preview/pr-121/people/isaac-ng/"><img alt="Isaac Ng picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/isaac-ng.jpg 1x" src="/pr-preview/pr-121/static/images/people/isaac-ng.jpg"/><span class="author-link">Isaac Ng</span></a> , <a href="/pr-preview/pr-121/people/christian-frisson/"><img alt="Christian Frisson picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/people/christian-frisson.jpg"/><span class="author-link">Christian Frisson</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Force Feedback</span><span class="ui brown basic label">Proxy</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2025-ghaneezabadi"><div class="three wide column" style="margin:auto"><img alt="chi-2025-ghaneezabadi cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2025-ghaneezabadi.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2025-ghaneezabadi.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2025</span></p><p class="color" style="font-size:1.3em"><b>IntelliLining: Activity Sensing through Textile Interlining Sensors Using TENGs</b></p><p><span>Mahdie GhaneEzabadi</span> , <a href="/pr-preview/pr-121/people/aditya-shekhar-nittala/"><img alt="Aditya Shekhar Nittala picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg"/><span class="author-link">Aditya Shekhar Nittala</span></a> , <a href="/pr-preview/pr-121/people/xing-dong-yang/"><img alt="Xing-Dong Yang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/xing-dong-yang.jpg 1x" src="/pr-preview/pr-121/static/images/people/xing-dong-yang.jpg"/><span class="author-link">Xing-Dong Yang</span></a> , <span>Te-yen Wu</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Interactive Textile</span><span class="ui brown basic label">TEN Gs</span><span class="ui brown basic label">Machine Learning</span><span class="ui brown basic label">Vibration Sensing</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2025-madill"><div class="three wide column" style="margin:auto"><img alt="chi-2025-madill cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2025-madill.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2025-madill.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2025</span></p><p class="color" style="font-size:1.3em"><b>Playing with Robots: Performing Arts Techniques for Designing and Understanding Robot Group Movement</b></p><p><span>Philippa Madill</span> , <span>Matthew Newton</span> , <span>Huanjun Zhao</span> , <span>Yichen Lian</span> , <a href="/pr-preview/pr-121/people/zachary-mckendrick/"><img alt="Zachary McKendrick picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/zachary-mckendrick.jpg 1x" src="/pr-preview/pr-121/static/images/people/zachary-mckendrick.jpg"/><span class="author-link">Zachary McKendrick</span></a> , <span>Patrick Finn</span> , <a href="/pr-preview/pr-121/people/aditya-shekhar-nittala/"><img alt="Aditya Shekhar Nittala picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg"/><span class="author-link">Aditya Shekhar Nittala</span></a> , <a href="/pr-preview/pr-121/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Humanities</span><span class="ui brown basic label">Art</span><span class="ui brown basic label">Robots</span><span class="ui brown basic label">Method</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2025-shiokawa"><div class="three wide column" style="margin:auto"><img alt="chi-2025-shiokawa cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2025-shiokawa.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2025-shiokawa.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2025</span></p><p class="color" style="font-size:1.3em"><b>Beyond Vacuuming: How Can We Exploit Domestic Robots&#x27; Idle Time?</b></p><p><span>Yoshiaki Shiokawa</span> , <span>Winnie Chen</span> , <a href="/pr-preview/pr-121/people/aditya-shekhar-nittala/"><img alt="Aditya Shekhar Nittala picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg"/><span class="author-link">Aditya Shekhar Nittala</span></a> , <span>Jason Alexander</span> , <span>Adwait Sharma</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Domestic Robots</span><span class="ui brown basic label">Ubiquitous</span><span class="ui brown basic label">Interaction</span><span class="ui brown basic label">Design Space</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="httf-2024-blair"><div class="three wide column" style="margin:auto"><img alt="httf-2024-blair cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/httf-2024-blair.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/httf-2024-blair.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">HTTF 2024</span></p><p class="color" style="font-size:1.3em"><b>Weaving Perspectives into Practice: A Manifesto for Combining Epistemological and Dissemination Strategies</b></p><p><a href="/pr-preview/pr-121/people/kathryn-blair/"><img alt="Kathryn Blair picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kathryn-blair.jpg 1x" src="/pr-preview/pr-121/static/images/people/kathryn-blair.jpg"/><span class="author-link">Kathryn Blair</span></a> , <span>Pil Hansen</span> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Fine Arts</span><span class="ui brown basic label">Methodology</span><span class="ui brown basic label">Emergent Ontology</span><span class="ui brown basic label">Epistemology</span><span class="ui brown basic label">Practice Based Research</span><span class="ui brown basic label">Qualitative Research</span><span class="ui brown basic label">Dissemination</span><span class="ui brown basic label">Knowledge Generation</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="mdpi-actuators-2024-piao"><div class="three wide column" style="margin:auto"><img alt="mdpi-actuators-2024-piao cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/mdpi-actuators-2024-piao.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/mdpi-actuators-2024-piao.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MDPI Actuators 2024 (Special Issue &quot;Actuators for Haptic and Tactile Stimulation Applications&quot;)</span></p><p class="color" style="font-size:1.3em"><b>Assessing the Impact of Force Feedback in Musical Knobs on Performance and User Experience</b></p><p><span>Ziyue Piao</span> , <a href="/pr-preview/pr-121/people/christian-frisson/"><img alt="Christian Frisson picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/people/christian-frisson.jpg"/><span class="author-link">Christian Frisson</span></a> , <span>Bavo Van Kerrebroeck</span> , <span>Marcelo M. Wanderley</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Rotary Force Feedback</span><span class="ui brown basic label">Digital Musical Instruments</span><span class="ui brown basic label">Knobs</span><span class="ui brown basic label">Torque Tuner</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2024-gunturu"><div class="three wide column" style="margin:auto"><img alt="uist-2024-gunturu cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2024-gunturu.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2024-gunturu.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2024</span><span class="ui big basic pink label"><b><svg data-prefix="fas" data-icon="trophy" class="svg-inline--fa fa-trophy" role="img" viewBox="0 0 512 512" aria-hidden="true"><path fill="currentColor" d="M144.3 0l224 0c26.5 0 48.1 21.8 47.1 48.2-.2 5.3-.4 10.6-.7 15.8l49.6 0c26.1 0 49.1 21.6 47.1 49.8-7.5 103.7-60.5 160.7-118 190.5-15.8 8.2-31.9 14.3-47.2 18.8-20.2 28.6-41.2 43.7-57.9 51.8l0 73.1 64 0c17.7 0 32 14.3 32 32s-14.3 32-32 32l-192 0c-17.7 0-32-14.3-32-32s14.3-32 32-32l64 0 0-73.1c-16-7.7-35.9-22-55.3-48.3-18.4-4.8-38.4-12.1-57.9-23.1-54.1-30.3-102.9-87.4-109.9-189.9-1.9-28.1 21-49.7 47.1-49.7l49.6 0c-.3-5.2-.5-10.4-.7-15.8-1-26.5 20.6-48.2 47.1-48.2zM101.5 112l-52.4 0c6.2 84.7 45.1 127.1 85.2 149.6-14.4-37.3-26.3-86-32.8-149.6zM380 256.8c40.5-23.8 77.1-66.1 83.3-144.8L411 112c-6.2 60.9-17.4 108.2-31 144.8z"></path></svg> Best Paper</b></span></p><p class="color" style="font-size:1.3em"><b>Augmented Physics: Creating Interactive and Embedded Physics Simulations from Static Textbook Diagrams</b></p><p><a href="/pr-preview/pr-121/people/aditya-gunturu/"><img alt="Aditya Gunturu picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-gunturu.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-gunturu.jpg"/><span class="author-link">Aditya Gunturu</span></a> , <span>Yi Wen</span> , <a href="/pr-preview/pr-121/people/nandi-zhang/"><img alt="Nandi Zhang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/nandi-zhang.jpg 1x" src="/pr-preview/pr-121/static/images/people/nandi-zhang.jpg"/><span class="author-link">Nandi Zhang</span></a> , <a href="/pr-preview/pr-121/people/jarin-thundathil/"><img alt="Jarin Thundathil picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/jarin-thundathil.jpg 1x" src="/pr-preview/pr-121/static/images/people/jarin-thundathil.jpg"/><span class="author-link">Jarin Thundathil</span></a> , <span>Rubaiat Habib Kazi</span> , <a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Physics Education</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Authoring Interfaces</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2024-roy"><div class="three wide column" style="margin:auto"><img alt="uist-2024-roy cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2024-roy.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2024-roy.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2024</span></p><p class="color" style="font-size:1.3em"><b>HoloChemie - Sustainable Fabrication of Soft Biochemical Holographic Devices for Ubiquitous Sensing</b></p><p><a href="/pr-preview/pr-121/people/sutirtha-roy/"><img alt="Sutirtha Roy picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sutirtha-roy.jpg 1x" src="/pr-preview/pr-121/static/images/people/sutirtha-roy.jpg"/><span class="author-link">Sutirtha Roy</span></a> , <span>Moshfiq-Us-Saleheen Chowdhury</span> , <span>Jurjaan Onayza Noim</span> , <span>Richa Pandey</span> , <a href="/pr-preview/pr-121/people/aditya-shekhar-nittala/"><img alt="Aditya Shekhar Nittala picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg"/><span class="author-link">Aditya Shekhar Nittala</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Physiological Sensing</span><span class="ui brown basic label">Biochemical Devices Sensing</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2024-danyluk"><div class="three wide column" style="margin:auto"><img alt="dis-2024-danyluk cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2024-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2024-danyluk.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2024</span></p><p class="color" style="font-size:1.3em"><b>Understanding Gesture and Microgesture Inputs for Augmented Reality Maps</b></p><p><a href="/pr-preview/pr-121/people/kurtis-danyluk/"><img alt="Kurtis Danyluk picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg"/><span class="author-link">Kurtis Danyluk</span></a> , <span>Simon Klueber</span> , <a href="/pr-preview/pr-121/people/aditya-shekhar-nittala/"><img alt="Aditya Shekhar Nittala picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg"/><span class="author-link">Aditya Shekhar Nittala</span></a> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Gestural Input</span><span class="ui brown basic label">Microgestures</span><span class="ui brown basic label">AR</span><span class="ui brown basic label">Maps</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2024-bressa"><div class="three wide column" style="margin:auto"><img alt="chi-2024-bressa cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2024-bressa.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2024-bressa.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2024</span></p><p class="color" style="font-size:1.3em"><b>Input Visualization: Collecting and Modifying Data with Visual Representations</b></p><p><a href="/pr-preview/pr-121/people/nathalie-bressa/"><img alt="Nathalie Bressa picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/nathalie-bressa.jpg 1x" src="/pr-preview/pr-121/static/images/people/nathalie-bressa.jpg"/><span class="author-link">Nathalie Bressa</span></a> , <span>Jordan Louis</span> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a> , <span>Samuel Huron</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Input Visualization</span><span class="ui brown basic label">Data Physicalization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2024-dhawka"><div class="three wide column" style="margin:auto"><img alt="chi-2024-dhawka cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2024-dhawka.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2024-dhawka.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2024</span></p><p class="color" style="font-size:1.3em"><b>Better Little People Pictures: Generative Creation of Demographically Diverse Anthropographics</b></p><p><a href="/pr-preview/pr-121/people/priya-dhawka/"><img alt="Priya Dhawka picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/priya-dhawka.jpg 1x" src="/pr-preview/pr-121/static/images/people/priya-dhawka.jpg"/><span class="author-link">Priya Dhawka</span></a> , <span>Lauren Perera</span> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Anthropographics</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Marginalized Populations</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2024-panigrahy"><div class="three wide column" style="margin:auto"><img alt="chi-2024-panigrahy cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2024-panigrahy.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2024-panigrahy.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2024</span></p><p class="color" style="font-size:1.3em"><b>ecSkin: Low-Cost Fabrication of Epidermal Electrochemical Sensors for Detecting Biomarkers in Sweat</b></p><p><span>Sai Nandan Panigrahy*</span> , <span>Chang Hyeon Lee*</span> , <span>Vrahant Nagoria</span> , <span>Mohammad Janghorban</span> , <span>Richa Pandey</span> , <a href="/pr-preview/pr-121/people/aditya-shekhar-nittala/"><img alt="Aditya Shekhar Nittala picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg"/><span class="author-link">Aditya Shekhar Nittala</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Physiological Sensing</span><span class="ui brown basic label">Electrochemical Devices</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2023-chulpongsatorn"><div class="three wide column" style="margin:auto"><img alt="uist-2023-chulpongsatorn cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2023-chulpongsatorn.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2023</span></p><p class="color" style="font-size:1.3em"><b>Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks</b></p><p><a href="/pr-preview/pr-121/people/neil-chulpongsatorn/"><img alt="Neil Chulpongsatorn picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-121/static/images/people/neil-chulpongsatorn.jpg"/><span class="author-link">Neil Chulpongsatorn</span></a> , <a href="/pr-preview/pr-121/people/mille-skovhus-lunding/"><img alt="Mille Skovhus Lunding picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/mille-skovhus-lunding.jpg 1x" src="/pr-preview/pr-121/static/images/people/mille-skovhus-lunding.jpg"/><span class="author-link">Mille Skovhus Lunding</span></a> , <a href="/pr-preview/pr-121/people/nishan-soni/"><img alt="Nishan Soni picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/no-profile-2.jpg 1x" src="/pr-preview/pr-121/static/images/people/no-profile-2.jpg"/><span class="author-link">Nishan Soni</span></a> , <a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Authoring Interfaces</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2023-ihara"><div class="three wide column" style="margin:auto"><img alt="uist-2023-ihara cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2023-ihara.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2023-ihara.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2023</span></p><p class="color" style="font-size:1.3em"><b>HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality</b></p><p><a href="/pr-preview/pr-121/people/keiichi-ihara/"><img alt="Keiichi Ihara picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/keiichi-ihara.jpg 1x" src="/pr-preview/pr-121/static/images/people/keiichi-ihara.jpg"/><span class="author-link">Keiichi Ihara</span></a> , <a href="/pr-preview/pr-121/people/mehrad-faridan/"><img alt="Mehrad Faridan picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/mehrad-faridan.jpg 1x" src="/pr-preview/pr-121/static/images/people/mehrad-faridan.jpg"/><span class="author-link">Mehrad Faridan</span></a> , <span>Ayumi Ichikawa</span> , <span>Ikkaku Kawaguchi</span> , <a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Physical Telepresence</span><span class="ui brown basic label">Mobile Robots</span><span class="ui brown basic label">Actuated Tangible Ui</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2023-xia"><div class="three wide column" style="margin:auto"><img alt="uist-2023-xia cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2023-xia.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2023-xia.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2023</span></p><p class="color" style="font-size:1.3em"><b>RealityCanvas: Augmented Reality Sketching for Embedded and Responsive Scribble Animation Effects</b></p><p><a href="/pr-preview/pr-121/people/zhijie-xia/"><img alt="Zhijie Xia picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/zhijie-xia.jpg 1x" src="/pr-preview/pr-121/static/images/people/zhijie-xia.jpg"/><span class="author-link">Zhijie Xia</span></a> , <a href="/pr-preview/pr-121/people/kyzyl-monteiro/"><img alt="Kyzyl Monteiro picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kyzyl-monteiro.jpg 1x" src="/pr-preview/pr-121/static/images/people/kyzyl-monteiro.jpg"/><span class="author-link">Kyzyl Monteiro</span></a> , <a href="/pr-preview/pr-121/people/kevin-van/"><img alt="Kevin Van picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kevin-van.jpg 1x" src="/pr-preview/pr-121/static/images/people/kevin-van.jpg"/><span class="author-link">Kevin Van</span></a> , <a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Sketching Interfaces</span><span class="ui brown basic label">Scribble Animation</span><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Real Time Authoring</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="assets-2023-mok"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">ASSETS 2023</span></p><p class="color" style="font-size:1.3em"><b>Experiences of Autistic Twitch Livestreamers: “I have made easily the most meaningful and impactful relationships”</b></p><p><a href="/pr-preview/pr-121/people/terrance-mok/"><img alt="Terrance Mok picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/terrance-mok.jpg 1x" src="/pr-preview/pr-121/static/images/people/terrance-mok.jpg"/><span class="author-link">Terrance Mok</span></a> , <a href="/pr-preview/pr-121/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a> , <span>Adam McCrimmon</span> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Autism</span><span class="ui brown basic label">Live Streaming</span><span class="ui brown basic label">Autistic</span><span class="ui brown basic label">Twitch</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2023-mukashev"><div class="three wide column" style="margin:auto"><img alt="uist-2023-mukashev cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2023-mukashev.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2023-mukashev.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2023</span></p><p class="color" style="font-size:1.3em"><b>TactTongue: Prototyping ElectroTactile Stimulations on the Tongue</b></p><p><a href="/pr-preview/pr-121/people/dinmukhammed-mukashev/"><img alt="Dinmukhammed Mukashev picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/dinmukhammed-mukashev.jpg 1x" src="/pr-preview/pr-121/static/images/people/dinmukhammed-mukashev.jpg"/><span class="author-link">Dinmukhammed Mukashev</span></a> , <span>Nimesha Ranasinghe</span> , <a href="/pr-preview/pr-121/people/aditya-shekhar-nittala/"><img alt="Aditya Shekhar Nittala picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg"/><span class="author-link">Aditya Shekhar Nittala</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Electrotactile Actuation</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2023-xia2"><div class="three wide column" style="margin:auto"><img alt="uist-2023-xia2 cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2023-xia2.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2023-xia2.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2023</span></p><p class="color" style="font-size:1.3em"><b>CrossTalk: Intelligent Substrates for Language-Oriented Interaction in Video-Based Communication and Collaboration</b></p><p><span>Haijun Xia</span> , <span>Tony Wang</span> , <a href="/pr-preview/pr-121/people/aditya-gunturu/"><img alt="Aditya Gunturu picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-gunturu.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-gunturu.jpg"/><span class="author-link">Aditya Gunturu</span></a> , <span>Peiling Jiang</span> , <span>William Duan</span> , <span>Xiaoshuo Yao</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Videoconferencing</span><span class="ui brown basic label">Natural Language Interface</span><span class="ui brown basic label">Language Oriented Interaction</span><span class="ui brown basic label">Context Aware Computing</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="mdpi-arts-2023-frisson"><div class="three wide column" style="margin:auto"><img alt="mdpi-arts-2023-frisson cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/mdpi-arts-2023-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/mdpi-arts-2023-frisson.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MDPI Arts 2023 (Special Issue Feeling the Future—Haptic Audio)</span></p><p class="color" style="font-size:1.3em"><b>Challenges and Opportunities of Force Feedback in Music</b></p><p><a href="/pr-preview/pr-121/people/christian-frisson/"><img alt="Christian Frisson picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/people/christian-frisson.jpg"/><span class="author-link">Christian Frisson</span></a> , <span>Marcelo M. Wanderley</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Force Feedback</span><span class="ui brown basic label">Digital Musical Instruments</span><span class="ui brown basic label">Torque Tuner</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="siggraph-labs-2023-seta"><div class="three wide column" style="margin:auto"><img alt="siggraph-labs-2023-seta cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/siggraph-labs-2023-seta.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/siggraph-labs-2023-seta.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">SIGGRAPH 2023 Labs</span></p><p class="color" style="font-size:1.3em"><b>Sketching Pipelines for Ephemeral Immersive Spaces</b></p><p><span>Michał Seta</span> , <span>Eduardo A. L. Meneses</span> , <span>Emmanuel Durand</span> , <a href="/pr-preview/pr-121/people/christian-frisson/"><img alt="Christian Frisson picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/people/christian-frisson.jpg"/><span class="author-link">Christian Frisson</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Art</span><span class="ui brown basic label">Interaction</span><span class="ui brown basic label">Pipeline</span><span class="ui brown basic label">Production</span><span class="ui brown basic label">Ui Tools</span><span class="ui brown basic label">VR AR MR</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2023-li"><div class="three wide column" style="margin:auto"><img alt="dis-2023-li cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2023-li.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2023-li.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2023</span></p><p class="color" style="font-size:1.3em"><b>Physica: Interactive Tangible Physics Simulation based on Tabletop Mobile Robots Towards Explorable Physics Education</b></p><p><span>Jiatong Li</span> , <a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a> , <span>Ken Nakagaki</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Physics Simulation</span><span class="ui brown basic label">Actuated Tangible Ui</span><span class="ui brown basic label">Swarm Ui</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2023-dhawka"><div class="three wide column" style="margin:auto"><img alt="chi-2023-dhawka cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2023-dhawka.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2023-dhawka.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2023</span></p><p class="color" style="font-size:1.3em"><b>We are the Data: Challenges and Opportunities for Creating Demographically Diverse Anthropographics</b></p><p><a href="/pr-preview/pr-121/people/priya-dhawka/"><img alt="Priya Dhawka picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/priya-dhawka.jpg 1x" src="/pr-preview/pr-121/static/images/people/priya-dhawka.jpg"/><span class="author-link">Priya Dhawka</span></a> , <a href="/pr-preview/pr-121/people/helen-ai-he/"><img alt="Helen Ai He picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/helen-ai-he.jpg 1x" src="/pr-preview/pr-121/static/images/people/helen-ai-he.jpg"/><span class="author-link">Helen Ai He</span></a> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Anthropographics</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Marginalized Populations</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2023-faridan"><div class="three wide column" style="margin:auto"><img alt="chi-2023-faridan cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2023-faridan.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2023-faridan.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2023</span></p><p class="color" style="font-size:1.3em"><b>ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms</b></p><p><a href="/pr-preview/pr-121/people/mehrad-faridan/"><img alt="Mehrad Faridan picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/mehrad-faridan.jpg 1x" src="/pr-preview/pr-121/static/images/people/mehrad-faridan.jpg"/><span class="author-link">Mehrad Faridan</span></a> , <a href="/pr-preview/pr-121/people/bheesha-kumari/"><img alt="Bheesha Kumari picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/bheesha-kumari.jpg 1x" src="/pr-preview/pr-121/static/images/people/bheesha-kumari.jpg"/><span class="author-link">Bheesha Kumari</span></a> , <a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Visual Cue</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Telepresence</span><span class="ui brown basic label">Remote Guidance</span><span class="ui brown basic label">Human Surrogates</span><span class="ui brown basic label">Hands On Training</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2023-monteiro"><div class="three wide column" style="margin:auto"><img alt="chi-2023-monteiro cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2023-monteiro.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2023-monteiro.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2023</span></p><p class="color" style="font-size:1.3em"><b>Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching</b></p><p><a href="/pr-preview/pr-121/people/kyzyl-monteiro/"><img alt="Kyzyl Monteiro picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kyzyl-monteiro.jpg 1x" src="/pr-preview/pr-121/static/images/people/kyzyl-monteiro.jpg"/><span class="author-link">Kyzyl Monteiro</span></a> , <a href="/pr-preview/pr-121/people/ritik-vatsal/"><img alt="Ritik Vatsal picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ritik-vatsal.jpg 1x" src="/pr-preview/pr-121/static/images/people/ritik-vatsal.jpg"/><span class="author-link">Ritik Vatsal</span></a> , <a href="/pr-preview/pr-121/people/neil-chulpongsatorn/"><img alt="Neil Chulpongsatorn picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-121/static/images/people/neil-chulpongsatorn.jpg"/><span class="author-link">Neil Chulpongsatorn</span></a> , <span>Aman Parnami</span> , <a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Prototyping Tools</span><span class="ui brown basic label">Tangible Interactions</span><span class="ui brown basic label">Everyday Objects</span><span class="ui brown basic label">Interactive Machine Teaching</span><span class="ui brown basic label">Human Centered Machine Learning</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-ea-2023-chulpongsatorn"><div class="three wide column" style="margin:auto"><img alt="chi-ea-2023-chulpongsatorn cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-ea-2023-chulpongsatorn.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-ea-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI EA 2023</span></p><p class="color" style="font-size:1.3em"><b>HoloTouch: Interacting with Mixed Reality Visualizations Through Smartphone Proxies</b></p><p><a href="/pr-preview/pr-121/people/neil-chulpongsatorn/"><img alt="Neil Chulpongsatorn picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-121/static/images/people/neil-chulpongsatorn.jpg"/><span class="author-link">Neil Chulpongsatorn</span></a> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a> , <a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Embedded Data Visualization</span><span class="ui brown basic label">Tangible Interaction</span><span class="ui brown basic label">Cross Device Interaction</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-ea-2023-fang"><div class="three wide column" style="margin:auto"><img alt="chi-ea-2023-fang cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-ea-2023-fang.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-ea-2023-fang.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI EA 2023</span></p><p class="color" style="font-size:1.3em"><b>VR Haptics at Home: Repurposing Everyday Objects and Environment for Casual and On-Demand VR Haptic Experiences</b></p><p><span>Cathy Mengying Fang</span> , <a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a> , <span>Daniel Leithinger</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Interaction Techniques</span><span class="ui brown basic label">Passive Haptics</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="vrst-2022-frisson"><div class="three wide column" style="margin:auto"><img alt="vrst-2022-frisson cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/vrst-2022-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/vrst-2022-frisson.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">VRST 2022 Poster</span></p><p class="color" style="font-size:1.3em"><b>LivePose: Democratizing Pose Detection for Multimedia Arts and Telepresence Applications on Open Edge Devices</b></p><p><a href="/pr-preview/pr-121/people/christian-frisson/"><img alt="Christian Frisson picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/people/christian-frisson.jpg"/><span class="author-link">Christian Frisson</span></a> , <span>Gabriel N. Downs</span> , <span>Marie-Ève Dumas</span> , <span>Farzaneh Askari</span> , <span>Emmanuel Durand</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Multimedia Arts</span><span class="ui brown basic label">Telepresence</span><span class="ui brown basic label">Edge Computing</span><span class="ui brown basic label">Pose Detection</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="tochi-2022-nittala"><div class="three wide column" style="margin:auto"><img alt="tochi-2022-nittala cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tochi-2022-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tochi-2022-nittala.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TOCHI 2022</span></p><p class="color" style="font-size:1.3em"><b>SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures</b></p><p><span>Adwait Sharma</span> , <span>Christina Salchow-Hömmen</span> , <span>Vimal Suresh Mollyn</span> , <a href="/pr-preview/pr-121/people/aditya-shekhar-nittala/"><img alt="Aditya Shekhar Nittala picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg"/><span class="author-link">Aditya Shekhar Nittala</span></a> , <span>Michael A. Hedderich</span> , <span>Marion Koelle</span> , <span>Thomas Seel</span> , <span>Jürgen Steimle</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Gesture Recognition</span><span class="ui brown basic label">Hand Gestures</span><span class="ui brown basic label">Sensor Placement</span><span class="ui brown basic label">IMU</span><span class="ui brown basic label">Objects</span><span class="ui brown basic label">Design Tool</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2022-kaimoto"><div class="three wide column" style="margin:auto"><img alt="uist-2022-kaimoto cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2022-kaimoto.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2022-kaimoto.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2022</span></p><p class="color" style="font-size:1.3em"><b>Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI</b></p><p><a href="/pr-preview/pr-121/people/hiroki-kaimoto/"><img alt="Hiroki Kaimoto picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/hiroki-kaimoto.jpg 1x" src="/pr-preview/pr-121/static/images/people/hiroki-kaimoto.jpg"/><span class="author-link">Hiroki Kaimoto</span></a> , <a href="/pr-preview/pr-121/people/kyzyl-monteiro/"><img alt="Kyzyl Monteiro picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kyzyl-monteiro.jpg 1x" src="/pr-preview/pr-121/static/images/people/kyzyl-monteiro.jpg"/><span class="author-link">Kyzyl Monteiro</span></a> , <a href="/pr-preview/pr-121/people/mehrad-faridan/"><img alt="Mehrad Faridan picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/mehrad-faridan.jpg 1x" src="/pr-preview/pr-121/static/images/people/mehrad-faridan.jpg"/><span class="author-link">Mehrad Faridan</span></a> , <span>Jiatong Li</span> , <a href="/pr-preview/pr-121/people/samin-farajian/"><img alt="Samin Farajian picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/samin-farajian.jpg 1x" src="/pr-preview/pr-121/static/images/people/samin-farajian.jpg"/><span class="author-link">Samin Farajian</span></a> , <span>Yasuaki Kakehi</span> , <span>Ken Nakagaki</span> , <a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Actuated Tangible Interfaces</span><span class="ui brown basic label">Swarm User Interfaces</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2022-liao"><div class="three wide column" style="margin:auto"><img alt="uist-2022-liao cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2022-liao.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2022-liao.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2022</span></p><p class="color" style="font-size:1.3em"><b>RealityTalk: Real-time Speech-driven Augmented Presentation for AR Live Storytelling</b></p><p><a href="/pr-preview/pr-121/people/jian-liao/"><img alt="Jian Liao picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/jian-liao.jpg 1x" src="/pr-preview/pr-121/static/images/people/jian-liao.jpg"/><span class="author-link">Jian Liao</span></a> , <a href="/pr-preview/pr-121/people/adnan-karim/"><img alt="Adnan Karim picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/adnan-karim.jpg 1x" src="/pr-preview/pr-121/static/images/people/adnan-karim.jpg"/><span class="author-link">Adnan Karim</span></a> , <a href="/pr-preview/pr-121/people/shivesh-jadon/"><img alt="Shivesh Jadon picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/shivesh-jadon.jpg 1x" src="/pr-preview/pr-121/static/images/people/shivesh-jadon.jpg"/><span class="author-link">Shivesh Jadon</span></a> , <span>Rubaiat Habib Kazi</span> , <a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Augmented Presentation</span><span class="ui brown basic label">Natural Language Processing</span><span class="ui brown basic label">Gestural And Speech Input</span><span class="ui brown basic label">Video</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2022-nisser"><div class="three wide column" style="margin:auto"><img alt="uist-2022-nisser cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2022-nisser.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2022-nisser.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2022</span></p><p class="color" style="font-size:1.3em"><b>Mixels: Fabricating Interfaces using Programmable Magnetic Pixels</b></p><p><span>Martin Nisser</span> , <span>Yashaswini Makaram</span> , <span>Lucian Covarrubias</span> , <span>Amadou Bah</span> , <span>Faraz Faruqi</span> , <a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a> , <span>Stefanie Mueller</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Programmable Materials</span><span class="ui brown basic label">Magnetic Interfaces</span><span class="ui brown basic label">Fabrication</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2022-nittala"><div class="three wide column" style="margin:auto"><img alt="uist-2022-nittala cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2022-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2022-nittala.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2022</span><span class="ui big basic pink label"><b><svg data-prefix="fas" data-icon="trophy" class="svg-inline--fa fa-trophy" role="img" viewBox="0 0 512 512" aria-hidden="true"><path fill="currentColor" d="M144.3 0l224 0c26.5 0 48.1 21.8 47.1 48.2-.2 5.3-.4 10.6-.7 15.8l49.6 0c26.1 0 49.1 21.6 47.1 49.8-7.5 103.7-60.5 160.7-118 190.5-15.8 8.2-31.9 14.3-47.2 18.8-20.2 28.6-41.2 43.7-57.9 51.8l0 73.1 64 0c17.7 0 32 14.3 32 32s-14.3 32-32 32l-192 0c-17.7 0-32-14.3-32-32s14.3-32 32-32l64 0 0-73.1c-16-7.7-35.9-22-55.3-48.3-18.4-4.8-38.4-12.1-57.9-23.1-54.1-30.3-102.9-87.4-109.9-189.9-1.9-28.1 21-49.7 47.1-49.7l49.6 0c-.3-5.2-.5-10.4-.7-15.8-1-26.5 20.6-48.2 47.1-48.2zM101.5 112l-52.4 0c6.2 84.7 45.1 127.1 85.2 149.6-14.4-37.3-26.3-86-32.8-149.6zM380 256.8c40.5-23.8 77.1-66.1 83.3-144.8L411 112c-6.2 60.9-17.4 108.2-31 144.8z"></path></svg> Best Paper</b></span></p><p class="color" style="font-size:1.3em"><b>Prototyping Soft Devices with Interactive Bioplastics</b></p><p><span>Marion Koelle</span> , <span>Madalina Nicolae</span> , <a href="/pr-preview/pr-121/people/aditya-shekhar-nittala/"><img alt="Aditya Shekhar Nittala picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg"/><span class="author-link">Aditya Shekhar Nittala</span></a> , <span>Marc Teyssier</span> , <span>Jürgen Steimle</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Bioplastics</span><span class="ui brown basic label">Biomaterials</span><span class="ui brown basic label">Do It Yourself</span><span class="ui brown basic label">DIY</span><span class="ui brown basic label">Sustainability</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-sic-2022-faridan"><div class="three wide column" style="margin:auto"><img alt="uist-sic-2022-faridan cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-sic-2022-faridan.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-sic-2022-faridan.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST SIC 2022</span><span class="ui big basic pink label"><b><svg data-prefix="fas" data-icon="award" class="svg-inline--fa fa-award" role="img" viewBox="0 0 448 512" aria-hidden="true"><path fill="currentColor" d="M245.9-25.9c-13.4-8.2-30.3-8.2-43.7 0-24.4 14.9-39.5 18.9-68.1 18.3-15.7-.4-30.3 8.1-37.9 21.9-13.7 25.1-24.8 36.2-49.9 49.9-13.8 7.5-22.2 22.2-21.9 37.9 .7 28.6-3.4 43.7-18.3 68.1-8.2 13.4-8.2 30.3 0 43.7 14.9 24.4 18.9 39.5 18.3 68.1-.4 15.7 8.1 30.3 21.9 37.9 22.1 12.1 33.3 22.1 45.1 41.5L42.7 458.5c-5.9 11.9-1.1 26.3 10.7 32.2l86 43c11.5 5.7 25.5 1.4 31.7-9.8l52.8-95.1 52.8 95.1c6.2 11.2 20.2 15.6 31.7 9.8l86-43c11.9-5.9 16.7-20.3 10.7-32.2l-48.6-97.2c11.7-19.4 23-29.4 45.1-41.5 13.8-7.5 22.2-22.2 21.9-37.9-.7-28.6 3.4-43.7 18.3-68.1 8.2-13.4 8.2-30.3 0-43.7-14.9-24.4-18.9-39.5-18.3-68.1 .4-15.7-8.1-30.3-21.9-37.9-25.1-13.7-36.2-24.8-49.9-49.9-7.5-13.8-22.2-22.2-37.9-21.9-28.6 .7-43.7-3.4-68.1-18.3zM224 96a96 96 0 1 1 0 192 96 96 0 1 1 0-192z"></path></svg> Honorable Mention</b></span></p><p class="color" style="font-size:1.3em"><b>UltraBots: Large-Area Mid-Air Haptics for VR with Robotically Actuated Ultrasound Transducers</b></p><p><a href="/pr-preview/pr-121/people/mehrad-faridan/"><img alt="Mehrad Faridan picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/mehrad-faridan.jpg 1x" src="/pr-preview/pr-121/static/images/people/mehrad-faridan.jpg"/><span class="author-link">Mehrad Faridan</span></a> , <a href="/pr-preview/pr-121/people/marcus-friedel/"><img alt="Marcus Friedel picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/marcus-friedel.jpg 1x" src="/pr-preview/pr-121/static/images/people/marcus-friedel.jpg"/><span class="author-link">Marcus Friedel</span></a> , <a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Ultrasound Transducers</span><span class="ui brown basic label">Robotics</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="iros-2022-suzuki"><div class="three wide column" style="margin:auto"><img alt="iros-2022-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/iros-2022-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/iros-2022-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IROS 2022</span></p><p class="color" style="font-size:1.3em"><b>Selective Self-Assembly using Re-Programmable Magnetic Pixels</b></p><p><span>Martin Nisser</span> , <span>Yashaswini Makaram</span> , <span>Faraz Faruqi</span> , <a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a> , <span>Stefanie Mueller</span></p><div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="gecco-2022-ivanov"><div class="three wide column" style="margin:auto"><img alt="gecco-2022-ivanov cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/gecco-2022-ivanov.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/gecco-2022-ivanov.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">GECCO 2022</span></p><p class="color" style="font-size:1.3em"><b>EvoIsland: Interactive Evolution via an Island-Inspired Spatial User Interface Framework</b></p><p><a href="/pr-preview/pr-121/people/sasha-ivanov/"><img alt="Sasha Ivanov picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sasha-ivanov.jpg 1x" src="/pr-preview/pr-121/static/images/people/sasha-ivanov.jpg"/><span class="author-link">Sasha Ivanov</span></a> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a> , <span>Christian Jacob</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Interactive Evolutionary Systems</span><span class="ui brown basic label">User Interfaces</span><span class="ui brown basic label">Visualization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="nime-2022-frisson"><div class="three wide column" style="margin:auto"><img alt="nime-2022-frisson cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/nime-2022-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/nime-2022-frisson.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">NIME 2022</span></p><p class="color" style="font-size:1.3em"><b>ForceHost: an open-source toolchain for generating firmware embedding the authoring and rendering of audio and force-feedback haptics</b></p><p><a href="/pr-preview/pr-121/people/christian-frisson/"><img alt="Christian Frisson picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/people/christian-frisson.jpg"/><span class="author-link">Christian Frisson</span></a> , <span>Mathias Kirkegaard</span> , <span>Thomas Pietrzak</span> , <span>Marcelo M. Wanderley</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Digital Musical Instrument</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Force Feedback</span><span class="ui brown basic label">Authoring</span><span class="ui brown basic label">Mapping</span><span class="ui brown basic label">Embedded Computing</span><span class="ui brown basic label">Torque Tuner</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="gi-2022-hull"><div class="three wide column" style="margin:auto"><img alt="gi-2022-hull cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/gi-2022-hull.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/gi-2022-hull.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">GI 2022</span></p><p class="color" style="font-size:1.3em"><b>Simultaneous Worlds: Supporting Fluid Exploration of Multiple Data Sets via Physical Models</b></p><p><a href="/pr-preview/pr-121/people/carmen-hull/"><img alt="Carmen Hull picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/carmen-hull.jpg 1x" src="/pr-preview/pr-121/static/images/people/carmen-hull.jpg"/><span class="author-link">Carmen Hull</span></a> , <a href="/pr-preview/pr-121/people/soren-knudsen/"><img alt="Søren Knudsen picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/soren-knudsen.jpg 1x" src="/pr-preview/pr-121/static/images/people/soren-knudsen.jpg"/><span class="author-link">Søren Knudsen</span></a> , <a href="/pr-preview/pr-121/people/sheelagh-carpendale/"><img alt="Sheelagh Carpendale picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg"/><span class="author-link">Sheelagh Carpendale</span></a> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Interactive Surfaces</span><span class="ui brown basic label">Data Physicalization</span><span class="ui brown basic label">Architectural Models</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2022-bressa"><div class="three wide column" style="margin:auto"><img alt="chi-2022-bressa cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2022-bressa.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2022-bressa.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2022</span></p><p class="color" style="font-size:1.3em"><b>Data Every Day: Designing and Living with Personal Situated Visualizations</b></p><p><a href="/pr-preview/pr-121/people/nathalie-bressa/"><img alt="Nathalie Bressa picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/nathalie-bressa.jpg 1x" src="/pr-preview/pr-121/static/images/people/nathalie-bressa.jpg"/><span class="author-link">Nathalie Bressa</span></a> , <span>Jo Vermeulen</span> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Self Tracking</span><span class="ui brown basic label">Situated Visualization</span><span class="ui brown basic label">Personal Data</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2022-ivanov"><div class="three wide column" style="margin:auto"><img alt="chi-2022-ivanov cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2022-ivanov.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2022-ivanov.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2022</span></p><p class="color" style="font-size:1.3em"><b>One Week in the Future: Previs Design Futuring for HCI Research</b></p><p><a href="/pr-preview/pr-121/people/sasha-ivanov/"><img alt="Sasha Ivanov picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sasha-ivanov.jpg 1x" src="/pr-preview/pr-121/static/images/people/sasha-ivanov.jpg"/><span class="author-link">Sasha Ivanov</span></a> , <a href="/pr-preview/pr-121/people/tim-au-yeung/"><img alt="Tim Au Yeung picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/tim-au-yeung.jpg 1x" src="/pr-preview/pr-121/static/images/people/tim-au-yeung.jpg"/><span class="author-link">Tim Au Yeung</span></a> , <a href="/pr-preview/pr-121/people/kathryn-blair/"><img alt="Kathryn Blair picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kathryn-blair.jpg 1x" src="/pr-preview/pr-121/static/images/people/kathryn-blair.jpg"/><span class="author-link">Kathryn Blair</span></a> , <a href="/pr-preview/pr-121/people/kurtis-danyluk/"><img alt="Kurtis Danyluk picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg"/><span class="author-link">Kurtis Danyluk</span></a> , <a href="/pr-preview/pr-121/people/georgina-freeman/"><img alt="Georgina Freeman picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/georgina-freeman.jpg 1x" src="/pr-preview/pr-121/static/images/people/georgina-freeman.jpg"/><span class="author-link">Georgina Freeman</span></a> , <a href="/pr-preview/pr-121/people/marcus-friedel/"><img alt="Marcus Friedel picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/marcus-friedel.jpg 1x" src="/pr-preview/pr-121/static/images/people/marcus-friedel.jpg"/><span class="author-link">Marcus Friedel</span></a> , <a href="/pr-preview/pr-121/people/carmen-hull/"><img alt="Carmen Hull picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/carmen-hull.jpg 1x" src="/pr-preview/pr-121/static/images/people/carmen-hull.jpg"/><span class="author-link">Carmen Hull</span></a> , <a href="/pr-preview/pr-121/people/michael-hung/"><img alt="Michael Hung picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/michael-hung.jpg 1x" src="/pr-preview/pr-121/static/images/people/michael-hung.jpg"/><span class="author-link">Michael Hung</span></a> , <a href="/pr-preview/pr-121/people/sydney-pratte/"><img alt="Sydney Pratte picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sydney-pratte.jpg 1x" src="/pr-preview/pr-121/static/images/people/sydney-pratte.jpg"/><span class="author-link">Sydney Pratte</span></a> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Design Futuring</span><span class="ui brown basic label">Prototyping</span><span class="ui brown basic label">Previsualization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2022-nittala"><div class="three wide column" style="margin:auto"><img alt="chi-2022-nittala cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2022-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2022-nittala.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2022</span></p><p class="color" style="font-size:1.3em"><b>Next Steps in Epidermal Computing: Opportunities and Challenges for Soft On-Skin Devices</b></p><p><a href="/pr-preview/pr-121/people/aditya-shekhar-nittala/"><img alt="Aditya Shekhar Nittala picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg"/><span class="author-link">Aditya Shekhar Nittala</span></a> , <span>Jürgen Steimle</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Wearable Devices</span><span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Survey</span><span class="ui brown basic label">Soft Wearables</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2022-suzuki"><div class="three wide column" style="margin:auto"><img alt="chi-2022-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2022-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2022-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2022</span></p><p class="color" style="font-size:1.3em"><b>Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces</b></p><p><a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a> , <a href="/pr-preview/pr-121/people/adnan-karim/"><img alt="Adnan Karim picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/adnan-karim.jpg 1x" src="/pr-preview/pr-121/static/images/people/adnan-karim.jpg"/><span class="author-link">Adnan Karim</span></a> , <a href="/pr-preview/pr-121/people/tian-xia/"><img alt="Tian Xia picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/tian-xia.jpg 1x" src="/pr-preview/pr-121/static/images/people/tian-xia.jpg"/><span class="author-link">Tian Xia</span></a> , <span>Hooman Hedayati</span> , <a href="/pr-preview/pr-121/people/nicolai-marquardt/"><img alt="Nicolai Marquardt picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/nicolai-marquardt.jpg 1x" src="/pr-preview/pr-121/static/images/people/nicolai-marquardt.jpg"/><span class="author-link">Nicolai Marquardt</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Robotics</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Actuated Tangible UI</span><span class="ui brown basic label">Shape Changing UI</span><span class="ui brown basic label">AR HRI</span><span class="ui brown basic label">VAM HRI</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-ea-2022-blair"><div class="three wide column" style="margin:auto"><img alt="chi-ea-2022-blair cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-ea-2022-blair.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-ea-2022-blair.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI EA 2022</span></p><p class="color" style="font-size:1.3em"><b>Art is Not Research. Research is not Art</b></p><p><a href="/pr-preview/pr-121/people/kathryn-blair/"><img alt="Kathryn Blair picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kathryn-blair.jpg 1x" src="/pr-preview/pr-121/static/images/people/kathryn-blair.jpg"/><span class="author-link">Kathryn Blair</span></a> , <span>Miriam Sturdee</span> , <span>Lindsay Macdonald Vermeulen</span> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Interdisciplinary Research</span><span class="ui brown basic label">Research Ethics</span><span class="ui brown basic label">Arts And Computing</span><span class="ui brown basic label">Research Methods</span><span class="ui brown basic label">Knowledge Creation</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="frobt-2022-suzuki"><div class="three wide column" style="margin:auto"><img alt="frobt-2022-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/frobt-2022-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/frobt-2022-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Frontiers 2022</span></p><p class="color" style="font-size:1.3em"><b>Designing Expandable-Structure Robots for Human-Robot Interaction</b></p><p><span>Hooman Hedayati</span> , <a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a> , <span>Wyatt Rees1</span> , <span>Daniel Leithinger</span> , <span>Daniel Szafir</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Deployable Robot</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Modular Robot</span><span class="ui brown basic label">Origami Robotics</span><span class="ui brown basic label">Deployable Structures</span><span class="ui brown basic label">Shape Changing Robots</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2021-suzuki"><div class="three wide column" style="margin:auto"><img alt="uist-2021-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2021-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2021-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2021</span></p><p class="color" style="font-size:1.3em"><b>HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots</b></p><p><a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a> , <span>Eyal Ofek</span> , <span>Mike Sinclair</span> , <span>Daniel Leithinger</span> , <span>Mar Gonzalez-Franco</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Encountered Type Haptics</span><span class="ui brown basic label">Tabletop Mobile Robots</span><span class="ui brown basic label">Swarm User Interfaces</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="ieee-2021-willett"><div class="three wide column" style="margin:auto"><img alt="ieee-2021-willett cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/ieee-2021-willett.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/ieee-2021-willett.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IEEE 2021</span><span class="ui big basic pink label"><b><svg data-prefix="fas" data-icon="trophy" class="svg-inline--fa fa-trophy" role="img" viewBox="0 0 512 512" aria-hidden="true"><path fill="currentColor" d="M144.3 0l224 0c26.5 0 48.1 21.8 47.1 48.2-.2 5.3-.4 10.6-.7 15.8l49.6 0c26.1 0 49.1 21.6 47.1 49.8-7.5 103.7-60.5 160.7-118 190.5-15.8 8.2-31.9 14.3-47.2 18.8-20.2 28.6-41.2 43.7-57.9 51.8l0 73.1 64 0c17.7 0 32 14.3 32 32s-14.3 32-32 32l-192 0c-17.7 0-32-14.3-32-32s14.3-32 32-32l64 0 0-73.1c-16-7.7-35.9-22-55.3-48.3-18.4-4.8-38.4-12.1-57.9-23.1-54.1-30.3-102.9-87.4-109.9-189.9-1.9-28.1 21-49.7 47.1-49.7l49.6 0c-.3-5.2-.5-10.4-.7-15.8-1-26.5 20.6-48.2 47.1-48.2zM101.5 112l-52.4 0c6.2 84.7 45.1 127.1 85.2 149.6-14.4-37.3-26.3-86-32.8-149.6zM380 256.8c40.5-23.8 77.1-66.1 83.3-144.8L411 112c-6.2 60.9-17.4 108.2-31 144.8z"></path></svg> Best Paper</b></span></p><p class="color" style="font-size:1.3em"><b>Perception! Immersion! Empowerment!: Superpowers as Inspiration for Visualization</b></p><p><a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a> , <a href="/pr-preview/pr-121/people/bon-adriel-aseniero/"><img alt="Bon Adriel Aseniero picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/bon-adriel-aseniero.jpg 1x" src="/pr-preview/pr-121/static/images/people/bon-adriel-aseniero.jpg"/><span class="author-link">Bon Adriel Aseniero</span></a> , <a href="/pr-preview/pr-121/people/sheelagh-carpendale/"><img alt="Sheelagh Carpendale picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg"/><span class="author-link">Sheelagh Carpendale</span></a> , <span>Pierre Dragicevic</span> , <span>Yvonne Jansen</span> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/pr-preview/pr-121/people/petra-isenberg/"><img alt="Petra Isenberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/no-profile-2.jpg 1x" src="/pr-preview/pr-121/static/images/people/no-profile-2.jpg"/><span class="author-link">Petra Isenberg</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Cognition</span><span class="ui brown basic label">Interactive Systems</span><span class="ui brown basic label">Tools</span><span class="ui brown basic label">Pragmatics</span><span class="ui brown basic label">Pattern Recognition</span><span class="ui brown basic label">Superpowers</span><span class="ui brown basic label">Empowerment</span><span class="ui brown basic label">Vision</span><span class="ui brown basic label">Perception</span><span class="ui brown basic label">Fiction</span><span class="ui brown basic label">Situated Visualization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2021-asha"><div class="three wide column" style="margin:auto"><img alt="dis-2021-asha cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2021-asha.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2021-asha.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2021</span></p><p class="color" style="font-size:1.3em"><b>Co-Designing Interactions between Pedestrians in Wheelchairs and Autonomous Vehicles</b></p><p><a href="/pr-preview/pr-121/people/ashratuz-zavin-asha/"><img alt="Ashratuz Zavin Asha picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ashratuz-zavin-asha.jpg 1x" src="/pr-preview/pr-121/static/images/people/ashratuz-zavin-asha.jpg"/><span class="author-link">Ashratuz Zavin Asha</span></a> , <a href="/pr-preview/pr-121/people/christopher-smith/"><img alt="Christopher Smith picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christopher-smith.jpg 1x" src="/pr-preview/pr-121/static/images/people/christopher-smith.jpg"/><span class="author-link">Christopher Smith</span></a> , <a href="/pr-preview/pr-121/people/georgina-freeman/"><img alt="Georgina Freeman picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/georgina-freeman.jpg 1x" src="/pr-preview/pr-121/static/images/people/georgina-freeman.jpg"/><span class="author-link">Georgina Freeman</span></a> , <span>Sean Crump</span> , <a href="/pr-preview/pr-121/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/pr-preview/pr-121/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Pedestrians In Wheelchairs</span><span class="ui brown basic label">Co Design</span><span class="ui brown basic label">Autonomous Vehicles</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2021-blair"><div class="three wide column" style="margin:auto"><img alt="dis-2021-blair cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2021-blair.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2021-blair.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2021</span></p><p class="color" style="font-size:1.3em"><b>Participatory Art for Public Exploration of Algorithmic Decision-Making</b></p><p><a href="/pr-preview/pr-121/people/kathryn-blair/"><img alt="Kathryn Blair picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kathryn-blair.jpg 1x" src="/pr-preview/pr-121/static/images/people/kathryn-blair.jpg"/><span class="author-link">Kathryn Blair</span></a> , <span>Pil Hansen</span> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Fine Arts</span><span class="ui brown basic label">Physical Artifact</span><span class="ui brown basic label">Education</span><span class="ui brown basic label">Social Impact Of Technology</span><span class="ui brown basic label">Participatory Art Installation</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2021-wannamaker"><div class="three wide column" style="margin:auto"><img alt="dis-2021-wannamaker cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2021-wannamaker.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2021-wannamaker.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2021</span></p><p class="color" style="font-size:1.3em"><b>I/O Bits: User-Driven, Situated, and Dedicated Self-Tracking</b></p><p><a href="/pr-preview/pr-121/people/kendra-wannamaker/"><img alt="Kendra Wannamaker picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kendra-wannamaker.jpg 1x" src="/pr-preview/pr-121/static/images/people/kendra-wannamaker.jpg"/><span class="author-link">Kendra Wannamaker</span></a> , <span>Sandeep Kollannur</span> , <a href="/pr-preview/pr-121/people/marian-doerk/"><img alt="Marian Dörk picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/no-profile-2.jpg 1x" src="/pr-preview/pr-121/static/images/people/no-profile-2.jpg"/><span class="author-link">Marian Dörk</span></a> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Personal Informatics</span><span class="ui brown basic label">Situated Visualization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2021-danyluk"><div class="three wide column" style="margin:auto"><img alt="chi-2021-danyluk cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2021-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2021-danyluk.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2021</span></p><p class="color" style="font-size:1.3em"><b>A Design Space Exploration of Worlds in Miniature</b></p><p><a href="/pr-preview/pr-121/people/kurtis-danyluk/"><img alt="Kurtis Danyluk picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg"/><span class="author-link">Kurtis Danyluk</span></a> , <span>Barrett Ens</span> , <span>Bernhard Jenny</span> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Virtual Augmented Reality</span><span class="ui brown basic label">Meta Analysis Literature Survey</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2021-ens"><div class="three wide column" style="margin:auto"><img alt="chi-2021-ens cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2021-ens.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2021-ens.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2021</span></p><p class="color" style="font-size:1.3em"><b>Grand Challenges in Immersive Analytics</b></p><p><span>Barrett Ens</span> , <span>Benjamin Bach</span> , <span>Maxime Cordeil</span> , <span>Ulrich Engelke</span> , <span>Marcos Serrano</span> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a> , <span>Arnaud Prouzeau</span> , <span>Christoph Anthes</span> , <span>Wolfgang Büschel</span> , <span>Cody Dunne</span> , <span>Tim Dwyer</span> , <span>Jens Grubert</span> , <span>Jason H. Haga</span> , <span>Nurit Kishenbaum</span> , <span>Dylan Kobayashi</span> , <span>Tica Lin</span> , <span>Monsurat Olaosebikan</span> , <span>Fabian Pointecker</span> , <span>David Saffo</span> , <span>Nazmus Saquib</span> , <span>Dieter Schmalsteig</span> , <span>Danielle Albers Szafir</span> , <span>Matthew Whitlock</span> , <span>Yalong Yang</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Immersive Analytics</span><span class="ui brown basic label">Grand Research Challenges</span><span class="ui brown basic label">Data Visualisation</span><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Virtual Reality</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2021-hammad"><div class="three wide column" style="margin:auto"><img alt="chi-2021-hammad cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2021-hammad.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2021-hammad.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2021</span></p><p class="color" style="font-size:1.3em"><b>Homecoming: Exploring Returns to Long-Term Single Player Games</b></p><p><span>Noor Hammad</span> , <span>Owen Brierley</span> , <a href="/pr-preview/pr-121/people/zachary-mckendrick/"><img alt="Zachary McKendrick picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/zachary-mckendrick.jpg 1x" src="/pr-preview/pr-121/static/images/people/zachary-mckendrick.jpg"/><span class="author-link">Zachary McKendrick</span></a> , <a href="/pr-preview/pr-121/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a> , <span>Patrick Finn</span> , <span>Jessica Hammer</span> , <a href="/pr-preview/pr-121/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Long Term Single Player Game</span><span class="ui brown basic label">Autobiographical Design</span><span class="ui brown basic label">Pivot Point</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="gi-2021-mactavish"><div class="three wide column" style="margin:auto"><img alt="gi-2021-mactavish cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/gi-2021-mactavish.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/gi-2021-mactavish.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">GI 2021</span></p><p class="color" style="font-size:1.3em"><b>Perspective Charts</b></p><p><span>Mia MacTavish</span> , <span>Katayoon Etemad</span> , <span>Faramarz Samavati</span> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Information Visualization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="ijac-2021-hosseini"><div class="three wide column" style="margin:auto"><img alt="ijac-2021-hosseini cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/ijac-2021-hosseini.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/ijac-2021-hosseini.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IJAC 2021</span></p><p class="color" style="font-size:1.3em"><b>Optically illusive architecture (OIA): Introduction and evaluation using virtual reality</b></p><p><span>Seyed Vahab Hosseini</span> , <span>Usman R Alim</span> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a> , <span>Joshua M Taron</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Architectural Representation</span><span class="ui brown basic label">Optical Illusion</span><span class="ui brown basic label">Design Evaluation</span><span class="ui brown basic label">Virtual Reality</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="cupum-2021-rout"><div class="three wide column" style="margin:auto"><img alt="cupum-2021-rout cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/cupum-2021-rout.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/cupum-2021-rout.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Urban Informatics and Future Cities</span></p><p class="color" style="font-size:1.3em"><b>(Big) Data in Urban Design Practice: Supporting High-Level Design Tasks Using a Visualization of Human Movement Data from Smartphones</b></p><p><span>Angela Rout</span> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Smartphone Data</span><span class="ui brown basic label">GPS</span><span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Architecture</span><span class="ui brown basic label">Urban Design</span><span class="ui brown basic label">Task Based Framework</span><span class="ui brown basic label">High Level Tasks</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="tei-2021-pratte"><div class="three wide column" style="margin:auto"><img alt="tei-2021-pratte cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tei-2021-pratte.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tei-2021-pratte.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TEI 2021</span></p><p class="color" style="font-size:1.3em"><b>Evoking Empathy: A Framework for Describing Empathy Tools</b></p><p><a href="/pr-preview/pr-121/people/sydney-pratte/"><img alt="Sydney Pratte picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sydney-pratte.jpg 1x" src="/pr-preview/pr-121/static/images/people/sydney-pratte.jpg"/><span class="author-link">Sydney Pratte</span></a> , <a href="/pr-preview/pr-121/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Empathy</span><span class="ui brown basic label">Empathy Tools</span><span class="ui brown basic label">Design Strategies</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2020-suzuki"><div class="three wide column" style="margin:auto"><img alt="uist-2020-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2020-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2020-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2020</span><span class="ui big basic pink label"><b><svg data-prefix="fas" data-icon="award" class="svg-inline--fa fa-award" role="img" viewBox="0 0 448 512" aria-hidden="true"><path fill="currentColor" d="M245.9-25.9c-13.4-8.2-30.3-8.2-43.7 0-24.4 14.9-39.5 18.9-68.1 18.3-15.7-.4-30.3 8.1-37.9 21.9-13.7 25.1-24.8 36.2-49.9 49.9-13.8 7.5-22.2 22.2-21.9 37.9 .7 28.6-3.4 43.7-18.3 68.1-8.2 13.4-8.2 30.3 0 43.7 14.9 24.4 18.9 39.5 18.3 68.1-.4 15.7 8.1 30.3 21.9 37.9 22.1 12.1 33.3 22.1 45.1 41.5L42.7 458.5c-5.9 11.9-1.1 26.3 10.7 32.2l86 43c11.5 5.7 25.5 1.4 31.7-9.8l52.8-95.1 52.8 95.1c6.2 11.2 20.2 15.6 31.7 9.8l86-43c11.9-5.9 16.7-20.3 10.7-32.2l-48.6-97.2c11.7-19.4 23-29.4 45.1-41.5 13.8-7.5 22.2-22.2 21.9-37.9-.7-28.6 3.4-43.7 18.3-68.1 8.2-13.4 8.2-30.3 0-43.7-14.9-24.4-18.9-39.5-18.3-68.1 .4-15.7-8.1-30.3-21.9-37.9-25.1-13.7-36.2-24.8-49.9-49.9-7.5-13.8-22.2-22.2-37.9-21.9-28.6 .7-43.7-3.4-68.1-18.3zM224 96a96 96 0 1 1 0 192 96 96 0 1 1 0-192z"></path></svg> Honorable Mention</b></span></p><p class="color" style="font-size:1.3em"><b>RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching</b></p><p><a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a> , <span>Rubaiat Habib Kazi</span> , <span>Li-Yi Wei</span> , <span>Stephen DiVerdi</span> , <span>Wilmot Li</span> , <span>Daniel Leithinger</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Embedded Data Visualization</span><span class="ui brown basic label">Real Time Authoring</span><span class="ui brown basic label">Sketching Interfaces</span><span class="ui brown basic label">Tangible Interaction</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2020-yixian"><div class="three wide column" style="margin:auto"><img alt="uist-2020-yixian cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2020-yixian.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2020-yixian.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2020</span></p><p class="color" style="font-size:1.3em"><b>ZoomWalls: Dynamic Walls that Simulate Haptic Infrastructure for Room-scale VR World</b></p><p><span>Yan Yixian</span> , <span>Kazuki Takashima</span> , <a href="/pr-preview/pr-121/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a> , <span>Takayuki Tanno</span> , <span>Kazuyuki Fujita</span> , <span>Yoshifumi Kitamura</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Encountered Type Haptic Devices</span><span class="ui brown basic label">Immersive Experience</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="iros-2020-hedayati"><div class="three wide column" style="margin:auto"><img alt="iros-2020-hedayati cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/iros-2020-hedayati.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/iros-2020-hedayati.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IROS 2020</span></p><p class="color" style="font-size:1.3em"><b>PufferBot: Actuated Expandable Structures for Aerial Robots</b></p><p><span>Hooman Hedayati</span> , <a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a> , <span>Daniel Leithinger</span> , <span>Daniel Szafir</span></p><div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="tvcg-2020-danyluk"><div class="three wide column" style="margin:auto"><img alt="tvcg-2020-danyluk cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tvcg-2020-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tvcg-2020-danyluk.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TVCG 2020</span></p><p class="color" style="font-size:1.3em"><b>Touch and Beyond: Comparing Physical and Virtual Reality Visualizations</b></p><p><a href="/pr-preview/pr-121/people/kurtis-danyluk/"><img alt="Kurtis Thorvald Danyluk picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg"/><span class="author-link">Kurtis Thorvald Danyluk</span></a> , <span>Teoman Tomo Ulusoy</span> , <a href="/pr-preview/pr-121/people/wei-wei/"><img alt="Wei Wei picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wei-wei.jpg 1x" src="/pr-preview/pr-121/static/images/people/wei-wei.jpg"/><span class="author-link">Wei Wei</span></a> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley J. Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley J. Willett</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Physicalization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="haid-2020-frisson"><div class="three wide column" style="margin:auto"><img alt="haid-2020-frisson cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/haid-2020-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/haid-2020-frisson.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">HAID 2022</span></p><p class="color" style="font-size:1.3em"><b>Printgets: an Open-Source Toolbox for Designing Vibrotactile Widgets with Industrial-Grade Printed Actuators and Sensors</b></p><p><a href="/pr-preview/pr-121/people/christian-frisson/"><img alt="Christian Frisson picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/people/christian-frisson.jpg"/><span class="author-link">Christian Frisson</span></a> , <span>Julien Decaudin</span> , <span>Mario Sanz-Lopez</span> , <span>Thomas Pietrzak</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Vibrotactile</span><span class="ui brown basic label">Fabrication</span><span class="ui brown basic label">Piezoelectric</span><span class="ui brown basic label">Capacitive Sensing</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="nime-2020-kirkegaard"><div class="three wide column" style="margin:auto"><img alt="nime-2020-kirkegaard cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/nime-2020-kirkegaard.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/nime-2020-kirkegaard.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">NIME 2020</span></p><p class="color" style="font-size:1.3em"><b>TorqueTuner: A self contained module for designing rotary haptic force feedback for digital musical instruments</b></p><p><span>Mathias Kirkegaard</span> , <span>Mathias Bredholt</span> , <a href="/pr-preview/pr-121/people/christian-frisson/"><img alt="Christian Frisson picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/people/christian-frisson.jpg"/><span class="author-link">Christian Frisson</span></a> , <span>Marcelo M. Wanderley</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Digital Musical Instruments</span><span class="ui brown basic label">Haptic Feedback</span><span class="ui brown basic label">Force Feedback Interaction</span><span class="ui brown basic label">Sound Synthesis</span><span class="ui brown basic label">Mapping</span><span class="ui brown basic label">Embedded Systems</span><span class="ui brown basic label">Torque Tuner</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="nime-2020-ko"><div class="three wide column" style="margin:auto"><img alt="nime-2020-ko cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/nime-2020-ko.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/nime-2020-ko.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">NIME 2020</span></p><p class="color" style="font-size:1.3em"><b>Touch Responsive Augmented Violin Interface System II: Integrating Sensors into a 3D Printed Fingerboard</b></p><p><span>Chantelle Ko</span> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Violin</span><span class="ui brown basic label">Touch Sensor</span><span class="ui brown basic label">FSR</span><span class="ui brown basic label">Fingerboard</span><span class="ui brown basic label">Augmented</span><span class="ui brown basic label">3 D Printing</span><span class="ui brown basic label">Conductive Filament</span><span class="ui brown basic label">Interactive</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="imwut-2020-wang"><div class="three wide column" style="margin:auto"><img alt="imwut-2020-wang cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/imwut-2020-wang.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/imwut-2020-wang.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IMWUT 2020</span></p><p class="color" style="font-size:1.3em"><b>AssessBlocks: Exploring Toy Block Play Features for Assessing Stress in Young Children after Natural Disasters</b></p><p><span>Xiyue Wang</span> , <span>Kazuki Takashima</span> , <span>Tomoaki Adachi</span> , <span>Patrick Finn</span> , <a href="/pr-preview/pr-121/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a> , <span>Yoshifumi Kitamura</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Well Being</span><span class="ui brown basic label">Toy Blocks</span><span class="ui brown basic label">PTSD</span><span class="ui brown basic label">Tangibles For Health</span><span class="ui brown basic label">Stress Assessment</span><span class="ui brown basic label">Play</span><span class="ui brown basic label">Children</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="imx-2020-mok"><div class="three wide column" style="margin:auto"><img alt="imx-2020-mok cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/imx-2020-mok.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/imx-2020-mok.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IMX 2020</span></p><p class="color" style="font-size:1.3em"><b>Talk Like Somebody is Watching: Understanding and Supporting Novice Live Streamers</b></p><p><a href="/pr-preview/pr-121/people/terrance-mok/"><img alt="Terrance Mok picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/terrance-mok.jpg 1x" src="/pr-preview/pr-121/static/images/people/terrance-mok.jpg"/><span class="author-link">Terrance Mok</span></a> , <a href="/pr-preview/pr-121/people/colin-auyeung/"><img alt="Colin Au Yeung picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/colin-auyeung.jpg 1x" src="/pr-preview/pr-121/static/images/people/colin-auyeung.jpg"/><span class="author-link">Colin Au Yeung</span></a> , <a href="/pr-preview/pr-121/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Live Streams</span><span class="ui brown basic label">Streamers</span><span class="ui brown basic label">Chatbot</span><span class="ui brown basic label">Audience</span><span class="ui brown basic label">Virtual Audience</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-suzuki"><div class="three wide column" style="margin:auto"><img alt="chi-2020-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2020-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2020-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020</span></p><p class="color" style="font-size:1.3em"><b>RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots</b></p><p><a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a> , <span>Hooman Hedayati</span> , <span>Clement Zheng</span> , <span>James Bohn</span> , <span>Daniel Szafir</span> , <span>Ellen Yi-Luen Do</span> , <span>Mark D Gross</span> , <span>Daniel Leithinger</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Room Scale Haptics</span><span class="ui brown basic label">Haptic Interfaces</span><span class="ui brown basic label">Swarm Robots</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="gi-2020-rajabiyazdi"><div class="three wide column" style="margin:auto"><img alt="gi-2020-rajabiyazdi cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/gi-2020-rajabiyazdi.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/gi-2020-rajabiyazdi.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">GI 2020</span></p><p class="color" style="font-size:1.3em"><b>Exploring the Design of Patient-Generated Data Visualizations</b></p><p><a href="/pr-preview/pr-121/people/fateme-rajabiyazdi/"><img alt="Fateme Rajabiyazdi picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/fateme-rajabiyazdi.jpg 1x" src="/pr-preview/pr-121/static/images/people/fateme-rajabiyazdi.jpg"/><span class="author-link">Fateme Rajabiyazdi</span></a> , <span>Charles Perin</span> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/pr-preview/pr-121/people/sheelagh-carpendale/"><img alt="Sheelagh Carpendale picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg"/><span class="author-link">Sheelagh Carpendale</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Information Visualization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-anjani"><div class="three wide column" style="margin:auto"><img alt="chi-2020-anjani cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2020-anjani.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2020-anjani.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020</span></p><p class="color" style="font-size:1.3em"><b>Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</b></p><p><span>Laurensia Anjani</span> , <a href="/pr-preview/pr-121/people/terrance-mok/"><img alt="Terrance Mok picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/terrance-mok.jpg 1x" src="/pr-preview/pr-121/static/images/people/terrance-mok.jpg"/><span class="author-link">Terrance Mok</span></a> , <a href="/pr-preview/pr-121/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a> , <span>Wooi Boon Goh</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Video Streams</span><span class="ui brown basic label">Mukbang</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-asha"><div class="three wide column" style="margin:auto"><img alt="chi-2020-asha cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2020-asha.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2020-asha.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020 LBW</span></p><p class="color" style="font-size:1.3em"><b>Views from the Wheelchair: Understanding Interaction between Autonomous Vehicle and Pedestrians with Reduced Mobility</b></p><p><a href="/pr-preview/pr-121/people/ashratuz-zavin-asha/"><img alt="Ashratuz Zavin Asha picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ashratuz-zavin-asha.jpg 1x" src="/pr-preview/pr-121/static/images/people/ashratuz-zavin-asha.jpg"/><span class="author-link">Ashratuz Zavin Asha</span></a> , <a href="/pr-preview/pr-121/people/christopher-smith/"><img alt="Christopher Smith picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christopher-smith.jpg 1x" src="/pr-preview/pr-121/static/images/people/christopher-smith.jpg"/><span class="author-link">Christopher Smith</span></a> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/pr-preview/pr-121/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a> , <a href="/pr-preview/pr-121/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Autonomous Vehicle</span><span class="ui brown basic label">Pedestrian With Reduced Mobility</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-goffin"><div class="three wide column" style="margin:auto"><img alt="chi-2020-goffin cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2020-goffin.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2020-goffin.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020</span></p><p class="color" style="font-size:1.3em"><b>Interaction Techniques for Visual Exploration Using Embedded Word-Scale Visualizations</b></p><p><span>Pascal Goffin</span> , <span>Tanja Blascheck</span> , <a href="/pr-preview/pr-121/people/petra-isenberg/"><img alt="Petra Isenberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/no-profile-2.jpg 1x" src="/pr-preview/pr-121/static/images/people/no-profile-2.jpg"/><span class="author-link">Petra Isenberg</span></a> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Glyphs</span><span class="ui brown basic label">Word Scale Visualization</span><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Interaction Techniques</span><span class="ui brown basic label">Text Visualization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-hou"><div class="three wide column" style="margin:auto"><img alt="chi-2020-hou cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2020-hou.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2020-hou.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020</span></p><p class="color" style="font-size:1.3em"><b>Autonomous Vehicle-Cyclist Interaction: Peril and Promise</b></p><p><span>Ming Hou</span> , <a href="/pr-preview/pr-121/people/karthik-mahadevan/"><img alt="Karthik Mahadevan picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/karthik-mahadevan.jpg 1x" src="/pr-preview/pr-121/static/images/people/karthik-mahadevan.jpg"/><span class="author-link">Karthik Mahadevan</span></a> , <a href="/pr-preview/pr-121/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a> , <a href="/pr-preview/pr-121/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Autonomous Vehicle Cyclist Interaction</span><span class="ui brown basic label">Interfaces For Communicating Intent And Awareness</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="tei-2020-suzuki"><div class="three wide column" style="margin:auto"><img alt="tei-2020-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tei-2020-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tei-2020-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TEI 2020</span></p><p class="color" style="font-size:1.3em"><b>LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces</b></p><p><a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a> , <span>Ryosuke Nakayama</span> , <span>Dan Liu</span> , <span>Yasuaki Kakehi</span> , <span>Mark D. Gross</span> , <span>Daniel Leithinger</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Shape Changing Interfaces</span><span class="ui brown basic label">Inflatables</span><span class="ui brown basic label">Large Scale Interactions</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="cmj-2020-ko"><div class="three wide column" style="margin:auto"><img alt="cmj-2020-ko cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/cmj-2020-ko.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/cmj-2020-ko.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CMJ 2020</span></p><p class="color" style="font-size:1.3em"><b>Construction and Performance Applications of an Augmented Violin: TRAVIS II</b></p><p><span>Chantelle Ko</span> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a></p><div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="mobilehci-2019-hung"><div class="three wide column" style="margin:auto"><img alt="mobilehci-2019-hung cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/mobilehci-2019-hung.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/mobilehci-2019-hung.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MobileHCI 2019</span></p><p class="color" style="font-size:1.3em"><b>WatchPen: Using Cross-Device Interaction Concepts to Augment Pen-Based Interaction</b></p><p><a href="/pr-preview/pr-121/people/michael-hung/"><img alt="Michael Hung picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/michael-hung.jpg 1x" src="/pr-preview/pr-121/static/images/people/michael-hung.jpg"/><span class="author-link">Michael Hung</span></a> , <a href="/pr-preview/pr-121/people/david-ledo/"><img alt="David Ledo picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/david-ledo.jpg 1x" src="/pr-preview/pr-121/static/images/people/david-ledo.jpg"/><span class="author-link">David Ledo</span></a> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Smartwatch</span><span class="ui brown basic label">Cross Device Interaction</span><span class="ui brown basic label">Pen Interaction</span><span class="ui brown basic label">Interaction Techniques</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2019-suzuki"><div class="three wide column" style="margin:auto"><img alt="uist-2019-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2019-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2019-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2019</span></p><p class="color" style="font-size:1.3em"><b>ShapeBots: Shape-changing Swarm Robots</b></p><p><a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a> , <span>Clement Zheng</span> , <span>Yasuaki Kakehi</span> , <span>Tom Yeh</span> , <span>Ellen Yi-Luen Do</span> , <span>Mark D. Gross</span> , <span>Daniel Leithinger</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Swarm User Interfaces</span><span class="ui brown basic label">Shape Changing User Interfaces</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="tvcg-2019-walny"><div class="three wide column" style="margin:auto"><img alt="tvcg-2019-walny cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tvcg-2019-walny.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tvcg-2019-walny.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TVCG 2019</span><span class="ui big basic pink label"><b><svg data-prefix="fas" data-icon="trophy" class="svg-inline--fa fa-trophy" role="img" viewBox="0 0 512 512" aria-hidden="true"><path fill="currentColor" d="M144.3 0l224 0c26.5 0 48.1 21.8 47.1 48.2-.2 5.3-.4 10.6-.7 15.8l49.6 0c26.1 0 49.1 21.6 47.1 49.8-7.5 103.7-60.5 160.7-118 190.5-15.8 8.2-31.9 14.3-47.2 18.8-20.2 28.6-41.2 43.7-57.9 51.8l0 73.1 64 0c17.7 0 32 14.3 32 32s-14.3 32-32 32l-192 0c-17.7 0-32-14.3-32-32s14.3-32 32-32l64 0 0-73.1c-16-7.7-35.9-22-55.3-48.3-18.4-4.8-38.4-12.1-57.9-23.1-54.1-30.3-102.9-87.4-109.9-189.9-1.9-28.1 21-49.7 47.1-49.7l49.6 0c-.3-5.2-.5-10.4-.7-15.8-1-26.5 20.6-48.2 47.1-48.2zM101.5 112l-52.4 0c6.2 84.7 45.1 127.1 85.2 149.6-14.4-37.3-26.3-86-32.8-149.6zM380 256.8c40.5-23.8 77.1-66.1 83.3-144.8L411 112c-6.2 60.9-17.4 108.2-31 144.8z"></path></svg> Best Paper</b></span></p><p class="color" style="font-size:1.3em"><b>Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff</b></p><p><a href="/pr-preview/pr-121/people/jagoda-walny/"><img alt="Jagoda Walny picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/no-profile-2.jpg 1x" src="/pr-preview/pr-121/static/images/people/no-profile-2.jpg"/><span class="author-link">Jagoda Walny</span></a> , <a href="/pr-preview/pr-121/people/christian-frisson/"><img alt="Christian Frisson picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/people/christian-frisson.jpg"/><span class="author-link">Christian Frisson</span></a> , <span>Mieka West</span> , <span>Doris Kosminsky</span> , <a href="/pr-preview/pr-121/people/soren-knudsen/"><img alt="Søren Knudsen picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/soren-knudsen.jpg 1x" src="/pr-preview/pr-121/static/images/people/soren-knudsen.jpg"/><span class="author-link">Søren Knudsen</span></a> , <a href="/pr-preview/pr-121/people/sheelagh-carpendale/"><img alt="Sheelagh Carpendale picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg"/><span class="author-link">Sheelagh Carpendale</span></a> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Design Handoff</span><span class="ui brown basic label">Data Mapping</span><span class="ui brown basic label">Design Process</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="cgi-2019-danyluk"><div class="three wide column" style="margin:auto"><img alt="cgi-2019-danyluk cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/cgi-2019-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/cgi-2019-danyluk.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CGI 2019</span></p><p class="color" style="font-size:1.3em"><b>Evaluating the Performance of Virtual Reality Navigation Techniques for Large Environments</b></p><p><a href="/pr-preview/pr-121/people/kurtis-danyluk/"><img alt="Kurtis Thorvald Danyluk picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg"/><span class="author-link">Kurtis Thorvald Danyluk</span></a> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley J. Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley J. Willett</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Human Computer Interaction HCI</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Digital Maps</span><span class="ui brown basic label">Navigation</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="cnc-2019-hammad"><div class="three wide column" style="margin:auto"><img alt="cnc-2019-hammad cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/cnc-2019-hammad.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/cnc-2019-hammad.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">C&amp;C 2019</span></p><p class="color" style="font-size:1.3em"><b>Mutation: Leveraging Performing Arts Practices in Cyborg Transitioning</b></p><p><a href="/pr-preview/pr-121/people/nour-hammad/"><img alt="Nour Hammad picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/nour-hammad.jpg 1x" src="/pr-preview/pr-121/static/images/people/nour-hammad.jpg"/><span class="author-link">Nour Hammad</span></a> , <span>Elaheh Sanoubari</span> , <span>Patrick Finn</span> , <a href="/pr-preview/pr-121/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a> , <span>James E. Young</span> , <a href="/pr-preview/pr-121/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Interaction Design</span><span class="ui brown basic label">Cyborgs</span><span class="ui brown basic label">User Experience</span><span class="ui brown basic label">Performing Art Techniques</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2019-blair"><div class="three wide column" style="margin:auto"><img alt="dis-2019-blair cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2019-blair.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2019-blair.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2019</span></p><p class="color" style="font-size:1.3em"><b>Exploring Public Engagement with the Social Impact of Algorithms</b></p><p><a href="/pr-preview/pr-121/people/kathryn-blair/"><img alt="Kathryn Blair picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kathryn-blair.jpg 1x" src="/pr-preview/pr-121/static/images/people/kathryn-blair.jpg"/><span class="author-link">Kathryn Blair</span></a> , <span>Jean-Rene Leblanc</span> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Fine Arts</span><span class="ui brown basic label">Physical Artifact</span><span class="ui brown basic label">Education</span><span class="ui brown basic label">Social Impact Of Technology</span><span class="ui brown basic label">Participatory Art Installation</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2019-bressa"><div class="three wide column" style="margin:auto"><img alt="dis-2019-bressa cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2019-bressa.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2019-bressa.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2019</span></p><p class="color" style="font-size:1.3em"><b>Sketching and Ideation Activities for Situated Visualization Design</b></p><p><a href="/pr-preview/pr-121/people/nathalie-bressa/"><img alt="Nathalie Bressa picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/nathalie-bressa.jpg 1x" src="/pr-preview/pr-121/static/images/people/nathalie-bressa.jpg"/><span class="author-link">Nathalie Bressa</span></a> , <a href="/pr-preview/pr-121/people/kendra-wannamaker/"><img alt="Kendra Wannamaker picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kendra-wannamaker.jpg 1x" src="/pr-preview/pr-121/static/images/people/kendra-wannamaker.jpg"/><span class="author-link">Kendra Wannamaker</span></a> , <span>Henrik Korsgaard</span> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a> , <span>Jo Vermeulen</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Ideation</span><span class="ui brown basic label">Design Workshops</span><span class="ui brown basic label">Situated Visualization</span><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Small Displays</span><span class="ui brown basic label">Sketching</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2019-ledo"><div class="three wide column" style="margin:auto"><img alt="dis-2019-ledo cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2019-ledo.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2019-ledo.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2019</span></p><p class="color" style="font-size:1.3em"><b>Astral: Prototyping Mobile and Smart Object Interactive Behaviours Using Familiar Applications</b></p><p><a href="/pr-preview/pr-121/people/david-ledo/"><img alt="David Ledo picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/david-ledo.jpg 1x" src="/pr-preview/pr-121/static/images/people/david-ledo.jpg"/><span class="author-link">David Ledo</span></a> , <span>Jo Vermeulen</span> , <a href="/pr-preview/pr-121/people/sheelagh-carpendale/"><img alt="Sheelagh Carpendale picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg"/><span class="author-link">Sheelagh Carpendale</span></a> , <a href="/pr-preview/pr-121/people/saul-greenberg/"><img alt="Saul Greenberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/saul-greenberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/saul-greenberg.jpg"/><span class="author-link">Saul Greenberg</span></a> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a> , <span>Sebastian Boring</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Smart Objects</span><span class="ui brown basic label">Mobile Interfaces</span><span class="ui brown basic label">Prototyping</span><span class="ui brown basic label">Design Tool</span><span class="ui brown basic label">Interactive Behaviour</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2019-mahadevan"><div class="three wide column" style="margin:auto"><img alt="dis-2019-mahadevan cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2019-mahadevan.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2019-mahadevan.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2019</span></p><p class="color" style="font-size:1.3em"><b>AV-Pedestrian Interaction Design Using a Pedestrian Mixed Traffic Simulator</b></p><p><a href="/pr-preview/pr-121/people/karthik-mahadevan/"><img alt="Karthik Mahadevan picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/karthik-mahadevan.jpg 1x" src="/pr-preview/pr-121/static/images/people/karthik-mahadevan.jpg"/><span class="author-link">Karthik Mahadevan</span></a> , <span>Elaheh Sanoubari</span> , <a href="/pr-preview/pr-121/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a> , <span>James E. Young</span> , <a href="/pr-preview/pr-121/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Mixed Traffic</span><span class="ui brown basic label">Pedestrian Simulator</span><span class="ui brown basic label">Autonomous Vehicle Pedestrian Interaction</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2019-nakayama"><div class="three wide column" style="margin:auto"><img alt="dis-2019-nakayama cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2019-nakayama.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2019-nakayama.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2019</span><span class="ui big basic pink label"><b><svg data-prefix="fas" data-icon="trophy" class="svg-inline--fa fa-trophy" role="img" viewBox="0 0 512 512" aria-hidden="true"><path fill="currentColor" d="M144.3 0l224 0c26.5 0 48.1 21.8 47.1 48.2-.2 5.3-.4 10.6-.7 15.8l49.6 0c26.1 0 49.1 21.6 47.1 49.8-7.5 103.7-60.5 160.7-118 190.5-15.8 8.2-31.9 14.3-47.2 18.8-20.2 28.6-41.2 43.7-57.9 51.8l0 73.1 64 0c17.7 0 32 14.3 32 32s-14.3 32-32 32l-192 0c-17.7 0-32-14.3-32-32s14.3-32 32-32l64 0 0-73.1c-16-7.7-35.9-22-55.3-48.3-18.4-4.8-38.4-12.1-57.9-23.1-54.1-30.3-102.9-87.4-109.9-189.9-1.9-28.1 21-49.7 47.1-49.7l49.6 0c-.3-5.2-.5-10.4-.7-15.8-1-26.5 20.6-48.2 47.1-48.2zM101.5 112l-52.4 0c6.2 84.7 45.1 127.1 85.2 149.6-14.4-37.3-26.3-86-32.8-149.6zM380 256.8c40.5-23.8 77.1-66.1 83.3-144.8L411 112c-6.2 60.9-17.4 108.2-31 144.8z"></path></svg> Best Paper</b></span></p><p class="color" style="font-size:1.3em"><b>MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction</b></p><p><span>Ryosuke Nakayama</span> , <a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a> , <span>Satoshi Nakamaru</span> , <span>Ryuma Niiyama</span> , <span>Yoshihiro Kawahara</span> , <span>Yasuaki Kakehi</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Shape Changing Interfaces</span><span class="ui brown basic label">Programming By Demonstration</span><span class="ui brown basic label">Soft Robots</span><span class="ui brown basic label">Pneumatic Actuation</span><span class="ui brown basic label">Tangible Interactions</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2019-seyed"><div class="three wide column" style="margin:auto"><img alt="dis-2019-seyed cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2019-seyed.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2019-seyed.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2019</span><span class="ui big basic pink label"><b><svg data-prefix="fas" data-icon="award" class="svg-inline--fa fa-award" role="img" viewBox="0 0 448 512" aria-hidden="true"><path fill="currentColor" d="M245.9-25.9c-13.4-8.2-30.3-8.2-43.7 0-24.4 14.9-39.5 18.9-68.1 18.3-15.7-.4-30.3 8.1-37.9 21.9-13.7 25.1-24.8 36.2-49.9 49.9-13.8 7.5-22.2 22.2-21.9 37.9 .7 28.6-3.4 43.7-18.3 68.1-8.2 13.4-8.2 30.3 0 43.7 14.9 24.4 18.9 39.5 18.3 68.1-.4 15.7 8.1 30.3 21.9 37.9 22.1 12.1 33.3 22.1 45.1 41.5L42.7 458.5c-5.9 11.9-1.1 26.3 10.7 32.2l86 43c11.5 5.7 25.5 1.4 31.7-9.8l52.8-95.1 52.8 95.1c6.2 11.2 20.2 15.6 31.7 9.8l86-43c11.9-5.9 16.7-20.3 10.7-32.2l-48.6-97.2c11.7-19.4 23-29.4 45.1-41.5 13.8-7.5 22.2-22.2 21.9-37.9-.7-28.6 3.4-43.7 18.3-68.1 8.2-13.4 8.2-30.3 0-43.7-14.9-24.4-18.9-39.5-18.3-68.1 .4-15.7-8.1-30.3-21.9-37.9-25.1-13.7-36.2-24.8-49.9-49.9-7.5-13.8-22.2-22.2-37.9-21.9-28.6 .7-43.7-3.4-68.1-18.3zM224 96a96 96 0 1 1 0 192 96 96 0 1 1 0-192z"></path></svg> Honorable Mention</b></span></p><p class="color" style="font-size:1.3em"><b>Mannequette: Understanding and Enabling Collaboration and Creativity on Avant-Garde Fashion-Tech Runways</b></p><p><a href="/pr-preview/pr-121/people/teddy-seyed/"><img alt="Teddy Seyed picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/teddy-seyed.jpg 1x" src="/pr-preview/pr-121/static/images/people/teddy-seyed.jpg"/><span class="author-link">Teddy Seyed</span></a> , <a href="/pr-preview/pr-121/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Fashion</span><span class="ui brown basic label">Haute Couture</span><span class="ui brown basic label">E Textiles</span><span class="ui brown basic label">Maker Culture</span><span class="ui brown basic label">Fashion Tech</span><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Avant Garde</span><span class="ui brown basic label">Haute Tech Couture</span><span class="ui brown basic label">Modular</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="tvcg-2019-blascheck"><div class="three wide column" style="margin:auto"><img alt="tvcg-2019-blascheck cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tvcg-2019-blascheck.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tvcg-2019-blascheck.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TVCG 2019</span></p><p class="color" style="font-size:1.3em"><b>Exploration Strategies for Discovery of Interactivity in Visualizations</b></p><p><span>Tanja Blascheck</span> , <span>Lindsay MacDonald Vermeulen</span> , <span>Jo Vermeulen</span> , <span>Charles Perin</span> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a> , <span>Thomas Ertl</span> , <a href="/pr-preview/pr-121/people/sheelagh-carpendale/"><img alt="Sheelagh Carpendale picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg"/><span class="author-link">Sheelagh Carpendale</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Discovery</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Open Data</span><span class="ui brown basic label">Evaluation</span><span class="ui brown basic label">Eye Tracking</span><span class="ui brown basic label">Interaction Logs</span><span class="ui brown basic label">Think Aloud</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2019-george"><div class="three wide column" style="margin:auto"><img alt="chi-2019-george cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2019-george.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2019-george.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2019 LBW</span></p><p class="color" style="font-size:1.3em"><b>Improving Texture Discrimination in Virtual Tasks by using Stochastic Resonance</b></p><p><span>Sandeep Zechariah George</span> , <span>Hooman Khosravi</span> , <span>Ryan Peters</span> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a> , <span>Sonny Chan</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Stochastic Resonance</span><span class="ui brown basic label">Virtual Tasks</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Texture Rendering</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2019-danyluk"><div class="three wide column" style="margin:auto"><img alt="chi-2019-danyluk cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2019-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2019-danyluk.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2019</span><span class="ui big basic pink label"><b><svg data-prefix="fas" data-icon="award" class="svg-inline--fa fa-award" role="img" viewBox="0 0 448 512" aria-hidden="true"><path fill="currentColor" d="M245.9-25.9c-13.4-8.2-30.3-8.2-43.7 0-24.4 14.9-39.5 18.9-68.1 18.3-15.7-.4-30.3 8.1-37.9 21.9-13.7 25.1-24.8 36.2-49.9 49.9-13.8 7.5-22.2 22.2-21.9 37.9 .7 28.6-3.4 43.7-18.3 68.1-8.2 13.4-8.2 30.3 0 43.7 14.9 24.4 18.9 39.5 18.3 68.1-.4 15.7 8.1 30.3 21.9 37.9 22.1 12.1 33.3 22.1 45.1 41.5L42.7 458.5c-5.9 11.9-1.1 26.3 10.7 32.2l86 43c11.5 5.7 25.5 1.4 31.7-9.8l52.8-95.1 52.8 95.1c6.2 11.2 20.2 15.6 31.7 9.8l86-43c11.9-5.9 16.7-20.3 10.7-32.2l-48.6-97.2c11.7-19.4 23-29.4 45.1-41.5 13.8-7.5 22.2-22.2 21.9-37.9-.7-28.6 3.4-43.7 18.3-68.1 8.2-13.4 8.2-30.3 0-43.7-14.9-24.4-18.9-39.5-18.3-68.1 .4-15.7-8.1-30.3-21.9-37.9-25.1-13.7-36.2-24.8-49.9-49.9-7.5-13.8-22.2-22.2-37.9-21.9-28.6 .7-43.7-3.4-68.1-18.3zM224 96a96 96 0 1 1 0 192 96 96 0 1 1 0-192z"></path></svg> Honorable Mention</b></span></p><p class="color" style="font-size:1.3em"><b>Look-From Camera Control for 3D Terrain Maps</b></p><p><a href="/pr-preview/pr-121/people/kurtis-danyluk/"><img alt="Kurtis Danyluk picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg"/><span class="author-link">Kurtis Danyluk</span></a> , <span>Bernhard Jenny</span> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Terrain</span><span class="ui brown basic label">Touch</span><span class="ui brown basic label">Map Interaction</span><span class="ui brown basic label">Look From Camera Control</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="tei-2019-mikalauskas"><div class="three wide column" style="margin:auto"><img alt="tei-2019-mikalauskas cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tei-2019-mikalauskas.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tei-2019-mikalauskas.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TEI 2019</span></p><p class="color" style="font-size:1.3em"><b>Beyond the Bare Stage: Exploring Props as Potential Improviser-Controlled Technology</b></p><p><span>Claire Mikalauskas</span> , <span>April Viczko</span> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Props</span><span class="ui brown basic label">Performer Controlled Technology</span><span class="ui brown basic label">Improvisational Theatre</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="tei-2019-tolley"><div class="three wide column" style="margin:auto"><img alt="tei-2019-tolley cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tei-2019-tolley.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tei-2019-tolley.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TEI 2019</span></p><p class="color" style="font-size:1.3em"><b>WindyWall: Exploring Creative Wind Simulations</b></p><p><span>David Tolley</span> , <span>Thi Ngoc Tram Nguyen</span> , <a href="/pr-preview/pr-121/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a> , <span>Nimesha Ranasinghe</span> , <span>Kensaku Kawauchi</span> , <span>Ching-Chiuan Yen</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Tactile Haptic Interaction</span><span class="ui brown basic label">Multimodal Interaction</span><span class="ui brown basic label">Novel Actuators Displays</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="tei-2019-wun"><div class="three wide column" style="margin:auto"><img alt="tei-2019-wun cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tei-2019-wun.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tei-2019-wun.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TEI 2019</span></p><p class="color" style="font-size:1.3em"><b>You say Potato, I say Po-Data: Physical Template Tools for Authoring Visualizations</b></p><p><span>Tiffany Wun</span> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a> , <span>Miriam Sturdee</span> , <a href="/pr-preview/pr-121/people/sheelagh-carpendale/"><img alt="Sheelagh Carpendale picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg"/><span class="author-link">Sheelagh Carpendale</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Potato</span><span class="ui brown basic label">Tangible Tools</span><span class="ui brown basic label">Authoring Visualizations</span><span class="ui brown basic label">Block Printing</span><span class="ui brown basic label">Physical Template Tools</span><span class="ui brown basic label">Information Visualization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="vr-2019-satriadi"><div class="three wide column" style="margin:auto"><img alt="vr-2019-satriadi cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/vr-2019-satriadi.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/vr-2019-satriadi.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IEEE VR 2019</span></p><p class="color" style="font-size:1.3em"><b>Augmented Reality Map Navigation with Freehand Gestures</b></p><p><span>Kadek Ananta Satriadi</span> , <span>Barrett Ens</span> , <span>Maxime Cordeil</span> , <span>Bernhard Jenny</span> , <span>Tobias Czauderna</span> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Gesture Recognition</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Interactive Devices</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="cga-2019-ivanov"><div class="three wide column" style="margin:auto"><img alt="cga-2019-ivanov cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/cga-2019-ivanov.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/cga-2019-ivanov.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IEEE CG&amp;A 2019</span></p><p class="color" style="font-size:1.3em"><b>A Walk Among the Data</b></p><p><a href="/pr-preview/pr-121/people/sasha-ivanov/"><img alt="Alexander Ivanov picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sasha-ivanov.jpg 1x" src="/pr-preview/pr-121/static/images/people/sasha-ivanov.jpg"/><span class="author-link">Alexander Ivanov</span></a> , <a href="/pr-preview/pr-121/people/kurtis-danyluk/"><img alt="Kurtis Danyluk picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg"/><span class="author-link">Kurtis Danyluk</span></a> , <span>Christian Jacob</span> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Art</span><span class="ui brown basic label">Tools</span><span class="ui brown basic label">Virtual Environments</span><span class="ui brown basic label">Two Dimensional Displays</span><span class="ui brown basic label">Anthropomorphism</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2018-suzuki"><div class="three wide column" style="margin:auto"><img alt="uist-2018-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2018-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2018-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2018</span></p><p class="color" style="font-size:1.3em"><b>Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation</b></p><p><a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a> , <span>Junichi Yamaoka</span> , <span>Daniel Leithinger</span> , <span>Tom Yeh</span> , <span>Mark D. Gross</span> , <span>Yoshihiro Kawahara</span> , <span>Yasuaki Kakehi</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Digital Materials</span><span class="ui brown basic label">Dynamic 3 D Printing</span><span class="ui brown basic label">Shape Displays</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2018-mikalauskas"><div class="three wide column" style="margin:auto"><img alt="dis-2018-mikalauskas cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2018-mikalauskas.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2018-mikalauskas.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2018</span></p><p class="color" style="font-size:1.3em"><b>Improvising with an Audience-Controlled Robot Performer</b></p><p><span>Claire Mikalauskas</span> , <span>Tiffany Wun</span> , <span>Kevin Ta</span> , <span>Joshua Horacsek</span> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Improvised Theatre</span><span class="ui brown basic label">Creativity Support Tools</span><span class="ui brown basic label">Crowdsourcing</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2018-pham"><div class="three wide column" style="margin:auto"><img alt="dis-2018-pham cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2018-pham.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2018-pham.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2018</span></p><p class="color" style="font-size:1.3em"><b>Scale Impacts Elicited Gestures for Manipulating Holograms: Implications for AR Gesture Design</b></p><p><span>Tran Pham</span> , <span>Jo Vermeulen</span> , <a href="/pr-preview/pr-121/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a> , <span>Lindsay MacDonald Vermeulen</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Gestures</span><span class="ui brown basic label">Gesture Elicitation</span><span class="ui brown basic label">Hololens</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2018-ta"><div class="three wide column" style="margin:auto"><img alt="dis-2018-ta cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2018-ta.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2018-ta.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2018</span></p><p class="color" style="font-size:1.3em"><b>Bod-IDE: An Augmented Reality Sandbox for eFashion Garments</b></p><p><span>Kevin Ta</span> , <a href="/pr-preview/pr-121/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Electronic Fashion</span><span class="ui brown basic label">Creativity Support Tool</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="tei-2016-somanath"><div class="three wide column" style="margin:auto"><img alt="tei-2016-somanath cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tei-2016-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tei-2016-somanath.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TEI 2016</span></p><p class="color" style="font-size:1.3em"><b>Engaging &#x27;At-Risk&#x27; Students through Maker Culture Activities</b></p><p><a href="/pr-preview/pr-121/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a> , <span>Laura Morrison</span> , <span>Janette Hughes</span> , <a href="/pr-preview/pr-121/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a> , <span>Mario Costa Sousa</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">DIY</span><span class="ui brown basic label">At Risk Students</span><span class="ui brown basic label">Maker Culture</span><span class="ui brown basic label">Education</span><span class="ui brown basic label">Young Learners</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-oh"><div class="three wide column" style="margin:auto"><img alt="chi-2018-oh cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2018-oh.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2018-oh.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-Based Electronic Devices</b></p><p><span>Hyunjoo Oh</span> , <span>Tung D. Ta</span> , <a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a> , <span>Mark D. Gross</span> , <span>Yoshihiro Kawahara</span> , <span>Lining Yao</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Paper Electronics</span><span class="ui brown basic label">3 D Sculpting</span><span class="ui brown basic label">Paper Craft</span><span class="ui brown basic label">Fabrication Techniques</span><span class="ui brown basic label">Prototyping</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-suzuki"><div class="three wide column" style="margin:auto"><img alt="chi-2018-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2018-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2018-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation</b></p><p><a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a> , <span>Jun Kato</span> , <span>Mark D. Gross</span> , <span>Tom Yeh</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Direct Manipulation</span><span class="ui brown basic label">Tangible Programming</span><span class="ui brown basic label">Swarm User Interfaces</span><span class="ui brown basic label">Programming By Demonstration</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-dillman"><div class="three wide column" style="margin:auto"><img alt="chi-2018-dillman cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2018-dillman.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2018-dillman.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality</b></p><p><span>Kody R. Dillman</span> , <a href="/pr-preview/pr-121/people/terrance-mok/"><img alt="Terrance Mok picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/terrance-mok.jpg 1x" src="/pr-preview/pr-121/static/images/people/terrance-mok.jpg"/><span class="author-link">Terrance Mok</span></a> , <a href="/pr-preview/pr-121/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a> , <span>Alex Mitchell</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Game Design</span><span class="ui brown basic label">Guidance</span><span class="ui brown basic label">Interaction Cues</span><span class="ui brown basic label">Augmented Reality</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-feick"><div class="three wide column" style="margin:auto"><img alt="chi-2018-feick cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2018-feick.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2018-feick.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span><span class="ui big basic pink label"><b><svg data-prefix="fas" data-icon="award" class="svg-inline--fa fa-award" role="img" viewBox="0 0 448 512" aria-hidden="true"><path fill="currentColor" d="M245.9-25.9c-13.4-8.2-30.3-8.2-43.7 0-24.4 14.9-39.5 18.9-68.1 18.3-15.7-.4-30.3 8.1-37.9 21.9-13.7 25.1-24.8 36.2-49.9 49.9-13.8 7.5-22.2 22.2-21.9 37.9 .7 28.6-3.4 43.7-18.3 68.1-8.2 13.4-8.2 30.3 0 43.7 14.9 24.4 18.9 39.5 18.3 68.1-.4 15.7 8.1 30.3 21.9 37.9 22.1 12.1 33.3 22.1 45.1 41.5L42.7 458.5c-5.9 11.9-1.1 26.3 10.7 32.2l86 43c11.5 5.7 25.5 1.4 31.7-9.8l52.8-95.1 52.8 95.1c6.2 11.2 20.2 15.6 31.7 9.8l86-43c11.9-5.9 16.7-20.3 10.7-32.2l-48.6-97.2c11.7-19.4 23-29.4 45.1-41.5 13.8-7.5 22.2-22.2 21.9-37.9-.7-28.6 3.4-43.7 18.3-68.1 8.2-13.4 8.2-30.3 0-43.7-14.9-24.4-18.9-39.5-18.3-68.1 .4-15.7-8.1-30.3-21.9-37.9-25.1-13.7-36.2-24.8-49.9-49.9-7.5-13.8-22.2-22.2-37.9-21.9-28.6 .7-43.7-3.4-68.1-18.3zM224 96a96 96 0 1 1 0 192 96 96 0 1 1 0-192z"></path></svg> Honorable Mention</b></span></p><p class="color" style="font-size:1.3em"><b>Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration</b></p><p><a href="/pr-preview/pr-121/people/martin-feick/"><img alt="Martin Feick picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/martin-feick.jpg 1x" src="/pr-preview/pr-121/static/images/people/martin-feick.jpg"/><span class="author-link">Martin Feick</span></a> , <span>Terrance Tin Hoi Mok</span> , <a href="/pr-preview/pr-121/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/pr-preview/pr-121/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Cscw</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Object Focused Collaboration</span><span class="ui brown basic label">Physical Telepresence</span><span class="ui brown basic label">Collaborative Physical Tasks</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-heshmat"><div class="three wide column" style="margin:auto"><img alt="chi-2018-heshmat cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2018-heshmat.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2018-heshmat.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>Geocaching with a Beam: Shared Outdoor Activities through a Telepresence Robot with 360 Degree Viewing</b></p><p><span>Yasamin Heshmat</span> , <a href="/pr-preview/pr-121/people/brennan-jones/"><img alt="Brennan Jones picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/brennan-jones.jpg 1x" src="/pr-preview/pr-121/static/images/people/brennan-jones.jpg"/><span class="author-link">Brennan Jones</span></a> , <span>Xiaoxuan Xiong</span> , <a href="/pr-preview/pr-121/people/carman-neustaedter/"><img alt="Carman Neustaedter picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/carman-neustaedter.jpg 1x" src="/pr-preview/pr-121/static/images/people/carman-neustaedter.jpg"/><span class="author-link">Carman Neustaedter</span></a> , <a href="/pr-preview/pr-121/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a> , <span>Bernhard E. Riecke</span> , <span>Lillian Yang</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Video Communication</span><span class="ui brown basic label">Telepresence Robots</span><span class="ui brown basic label">Leisure Activities</span><span class="ui brown basic label">Social Presence</span><span class="ui brown basic label">Geocaching</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-ledo"><div class="three wide column" style="margin:auto"><img alt="chi-2018-ledo cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2018-ledo.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2018-ledo.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>Evaluation Strategies for HCI Toolkit Research</b></p><p><a href="/pr-preview/pr-121/people/david-ledo/"><img alt="David Ledo picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/david-ledo.jpg 1x" src="/pr-preview/pr-121/static/images/people/david-ledo.jpg"/><span class="author-link">David Ledo</span></a> , <span>Steven Houben</span> , <span>Jo Vermeulen</span> , <a href="/pr-preview/pr-121/people/nicolai-marquardt/"><img alt="Nicolai Marquardt picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/nicolai-marquardt.jpg 1x" src="/pr-preview/pr-121/static/images/people/nicolai-marquardt.jpg"/><span class="author-link">Nicolai Marquardt</span></a> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/pr-preview/pr-121/people/saul-greenberg/"><img alt="Saul Greenberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/saul-greenberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/saul-greenberg.jpg"/><span class="author-link">Saul Greenberg</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">User Interfaces</span><span class="ui brown basic label">Design</span><span class="ui brown basic label">Evaluation</span><span class="ui brown basic label">Prototyping</span><span class="ui brown basic label">Toolkits</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-mahadevan"><div class="three wide column" style="margin:auto"><img alt="chi-2018-mahadevan cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2018-mahadevan.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2018-mahadevan.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>Communicating Awareness and Intent in Autonomous Vehicle-Pedestrian Interaction</b></p><p><a href="/pr-preview/pr-121/people/karthik-mahadevan/"><img alt="Karthik Mahadevan picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/karthik-mahadevan.jpg 1x" src="/pr-preview/pr-121/static/images/people/karthik-mahadevan.jpg"/><span class="author-link">Karthik Mahadevan</span></a> , <a href="/pr-preview/pr-121/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a> , <a href="/pr-preview/pr-121/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Autonomous Vehicle Pedestrian Interaction</span><span class="ui brown basic label">Perceived Awareness And Intent In Autonomous Vehicles</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-neustaedter"><div class="three wide column" style="margin:auto"><img alt="chi-2018-neustaedter cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2018-neustaedter.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2018-neustaedter.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span><span class="ui big basic pink label"><b><svg data-prefix="fas" data-icon="award" class="svg-inline--fa fa-award" role="img" viewBox="0 0 448 512" aria-hidden="true"><path fill="currentColor" d="M245.9-25.9c-13.4-8.2-30.3-8.2-43.7 0-24.4 14.9-39.5 18.9-68.1 18.3-15.7-.4-30.3 8.1-37.9 21.9-13.7 25.1-24.8 36.2-49.9 49.9-13.8 7.5-22.2 22.2-21.9 37.9 .7 28.6-3.4 43.7-18.3 68.1-8.2 13.4-8.2 30.3 0 43.7 14.9 24.4 18.9 39.5 18.3 68.1-.4 15.7 8.1 30.3 21.9 37.9 22.1 12.1 33.3 22.1 45.1 41.5L42.7 458.5c-5.9 11.9-1.1 26.3 10.7 32.2l86 43c11.5 5.7 25.5 1.4 31.7-9.8l52.8-95.1 52.8 95.1c6.2 11.2 20.2 15.6 31.7 9.8l86-43c11.9-5.9 16.7-20.3 10.7-32.2l-48.6-97.2c11.7-19.4 23-29.4 45.1-41.5 13.8-7.5 22.2-22.2 21.9-37.9-.7-28.6 3.4-43.7 18.3-68.1 8.2-13.4 8.2-30.3 0-43.7-14.9-24.4-18.9-39.5-18.3-68.1 .4-15.7-8.1-30.3-21.9-37.9-25.1-13.7-36.2-24.8-49.9-49.9-7.5-13.8-22.2-22.2-37.9-21.9-28.6 .7-43.7-3.4-68.1-18.3zM224 96a96 96 0 1 1 0 192 96 96 0 1 1 0-192z"></path></svg> Honorable Mention</b></span></p><p class="color" style="font-size:1.3em"><b>The Benefits and Challenges of Video Calling for Emergency Situations</b></p><p><a href="/pr-preview/pr-121/people/carman-neustaedter/"><img alt="Carman Neustaedter picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/carman-neustaedter.jpg 1x" src="/pr-preview/pr-121/static/images/people/carman-neustaedter.jpg"/><span class="author-link">Carman Neustaedter</span></a> , <a href="/pr-preview/pr-121/people/brennan-jones/"><img alt="Brennan Jones picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/brennan-jones.jpg 1x" src="/pr-preview/pr-121/static/images/people/brennan-jones.jpg"/><span class="author-link">Brennan Jones</span></a> , <span>Kenton O&#x27;Hara</span> , <span>Abigail Sellen</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Collaboration</span><span class="ui brown basic label">Situation Awareness</span><span class="ui brown basic label">Emergency Calling</span><span class="ui brown basic label">Call Takers</span><span class="ui brown basic label">Mobile Video Calling</span><span class="ui brown basic label">Dispatchers</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-wuertz"><div class="three wide column" style="margin:auto"><img alt="chi-2018-wuertz cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2018-wuertz.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2018-wuertz.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>A Design Framework for Awareness Cues in Distributed Multiplayer Games</b></p><p><span>Jason Wuertz</span> , <span>Sultan A. Alharthi</span> , <span>William A. Hamilton</span> , <span>Scott Bateman</span> , <a href="/pr-preview/pr-121/people/carl-gutwin/"><img alt="Carl Gutwin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/carl-gutwin.jpg 1x" src="/pr-preview/pr-121/static/images/people/carl-gutwin.jpg"/><span class="author-link">Carl Gutwin</span></a> , <a href="/pr-preview/pr-121/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a> , <span>Zachary O. Toups</span> , <span>Jessica Hammer</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Workspace Awareness</span><span class="ui brown basic label">Situation Awareness</span><span class="ui brown basic label">Game Design</span><span class="ui brown basic label">Distributed Multiplayer Games</span><span class="ui brown basic label">Awareness Cues</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="hri-2018-feick"><div class="three wide column" style="margin:auto"><img alt="hri-2018-feick cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/hri-2018-feick.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/hri-2018-feick.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">HRI 2018</span></p><p class="color" style="font-size:1.3em"><b>The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</b></p><p><a href="/pr-preview/pr-121/people/martin-feick/"><img alt="Martin Feick picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/martin-feick.jpg 1x" src="/pr-preview/pr-121/static/images/people/martin-feick.jpg"/><span class="author-link">Martin Feick</span></a> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/pr-preview/pr-121/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a> , <span>André Miede</span> , <a href="/pr-preview/pr-121/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Movement Trajectory Velocity</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Robot Surrogate</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="assets-2017-suzuki"><div class="three wide column" style="margin:auto"><img alt="assets-2017-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/assets-2017-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/assets-2017-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">ASSETS 2020</span></p><p class="color" style="font-size:1.3em"><b>FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers</b></p><p><a href="/pr-preview/pr-121/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a> , <span>Abigale Stangl</span> , <span>Mark D. Gross</span> , <span>Tom Yeh</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Visual Impairment</span><span class="ui brown basic label">Dynamic Tactile Markers</span><span class="ui brown basic label">Tangible Interfaces</span><span class="ui brown basic label">Interactive Tactile Graphics</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="sui-2017-li"><div class="three wide column" style="margin:auto"><img alt="sui-2017-li cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/sui-2017-li.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/sui-2017-li.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">SUI 2017</span></p><p class="color" style="font-size:1.3em"><b>Visibility Perception and Dynamic Viewsheds for Topographic Maps and Models</b></p><p><span>Nico Li</span> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a> , <a href="/pr-preview/pr-121/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a> , <span>Mario Costa Sousa</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Terrain Visualization</span><span class="ui brown basic label">Geospatial Visualization</span><span class="ui brown basic label">Dynamic Viewshed</span><span class="ui brown basic label">Topographic Maps</span><span class="ui brown basic label">Tangible User Interfaces</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2017-mok"><div class="three wide column" style="margin:auto"><img alt="dis-2017-mok cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2017-mok.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2017-mok.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2017</span></p><p class="color" style="font-size:1.3em"><b>Critiquing Physical Prototypes for a Remote Audience</b></p><p><a href="/pr-preview/pr-121/people/terrance-mok/"><img alt="Terrance Mok picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/terrance-mok.jpg 1x" src="/pr-preview/pr-121/static/images/people/terrance-mok.jpg"/><span class="author-link">Terrance Mok</span></a> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Design Review</span><span class="ui brown basic label">Prototype Critique</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Material Experience</span><span class="ui brown basic label">Open Hardware</span><span class="ui brown basic label">Video Conferencing</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2017-aoki"><div class="three wide column" style="margin:auto"><img alt="chi-2017-aoki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2017-aoki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2017-aoki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2017</span></p><p class="color" style="font-size:1.3em"><b>Environmental Protection and Agency: Motivations, Capacity, and Goals in Participatory Sensing</b></p><p><span>Paul Aoki</span> , <span>Allison Woodruff</span> , <span>Baladitya Yellapragada</span> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Citizen Science</span><span class="ui brown basic label">Environmental Sensing</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2017-hull"><div class="three wide column" style="margin:auto"><img alt="chi-2017-hull cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2017-hull.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2017-hull.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2017</span></p><p class="color" style="font-size:1.3em"><b>Building with Data: Architectural Models as Inspiration for Data Physicalization</b></p><p><a href="/pr-preview/pr-121/people/carmen-hull/"><img alt="Carmen Hull picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/carmen-hull.jpg 1x" src="/pr-preview/pr-121/static/images/people/carmen-hull.jpg"/><span class="author-link">Carmen Hull</span></a> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Design Process</span><span class="ui brown basic label">Architectural Models</span><span class="ui brown basic label">Data Physicalization</span><span class="ui brown basic label">Embodied Interaction</span><span class="ui brown basic label">Data Visualization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2017-ledo"><div class="three wide column" style="margin:auto"><img alt="chi-2017-ledo cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2017-ledo.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2017-ledo.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2017</span></p><p class="color" style="font-size:1.3em"><b>Pineal: Bringing Passive Objects to Life with Embedded Mobile Devices</b></p><p><a href="/pr-preview/pr-121/people/david-ledo/"><img alt="David Ledo picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/david-ledo.jpg 1x" src="/pr-preview/pr-121/static/images/people/david-ledo.jpg"/><span class="author-link">David Ledo</span></a> , <span>Fraser Anderson</span> , <span>Ryan Schmidt</span> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/pr-preview/pr-121/people/saul-greenberg/"><img alt="Saul Greenberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/saul-greenberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/saul-greenberg.jpg"/><span class="author-link">Saul Greenberg</span></a> , <span>Tovi Grossman</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Fabrication</span><span class="ui brown basic label">3 D Printing</span><span class="ui brown basic label">Smart Objects</span><span class="ui brown basic label">Rapid Prototyping</span><span class="ui brown basic label">Toolkits</span><span class="ui brown basic label">Prototyping Tool</span><span class="ui brown basic label">Interaction Design</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2017-somanath"><div class="three wide column" style="margin:auto"><img alt="chi-2017-somanath cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2017-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2017-somanath.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2017</span></p><p class="color" style="font-size:1.3em"><b>&#x27;Maker&#x27; within Constraints: Exploratory Study of Young Learners using Arduino at a High School in India</b></p><p><a href="/pr-preview/pr-121/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a> , <span>Janette Hughes</span> , <a href="/pr-preview/pr-121/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a> , <span>Mario Costa Sousa</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">India</span><span class="ui brown basic label">HCI 4 D</span><span class="ui brown basic label">Physical Computing</span><span class="ui brown basic label">DIY</span><span class="ui brown basic label">Young Learners</span><span class="ui brown basic label">Maker Culture</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="tvcg-2017-goffin"><div class="three wide column" style="margin:auto"><img alt="tvcg-2017-goffin cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tvcg-2017-goffin.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tvcg-2017-goffin.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TVCG 2017</span></p><p class="color" style="font-size:1.3em"><b>An Exploratory Study of Word-Scale Graphics in Data-Rich Text Documents</b></p><p><span>Pascal Goffin</span> , <span>Jeremy Boy</span> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a> , <a href="/pr-preview/pr-121/people/petra-isenberg/"><img alt="Petra Isenberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/no-profile-2.jpg 1x" src="/pr-preview/pr-121/static/images/people/no-profile-2.jpg"/><span class="author-link">Petra Isenberg</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Word Scale Visualization</span><span class="ui brown basic label">Word Scale Graphic</span><span class="ui brown basic label">Text Visualization</span><span class="ui brown basic label">Sparklines</span><span class="ui brown basic label">Authoring Tool</span><span class="ui brown basic label">Information Visualization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="tvcg-2017-willett"><div class="three wide column" style="margin:auto"><img alt="tvcg-2017-willett cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tvcg-2017-willett.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tvcg-2017-willett.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TVCG 2017</span></p><p class="color" style="font-size:1.3em"><b>Embedded Data Representations</b></p><p><a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a> , <span>Yvonne Jansen</span> , <span>Pierre Dragicevic</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Data Physicalization</span><span class="ui brown basic label">Ambient Displays</span><span class="ui brown basic label">Ubiquitous Computing</span><span class="ui brown basic label">Augmented Reality</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2016-jones"><div class="three wide column" style="margin:auto"><img alt="dis-2016-jones cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2016-jones.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2016-jones.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2016</span></p><p class="color" style="font-size:1.3em"><b>Elevating Communication, Collaboration, and Shared Experiences in Mobile Video through Drones</b></p><p><a href="/pr-preview/pr-121/people/brennan-jones/"><img alt="Brennan Jones picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/brennan-jones.jpg 1x" src="/pr-preview/pr-121/static/images/people/brennan-jones.jpg"/><span class="author-link">Brennan Jones</span></a> , <span>Kody Dillman</span> , <span>Richard Tang</span> , <a href="/pr-preview/pr-121/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a> , <a href="/pr-preview/pr-121/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/pr-preview/pr-121/people/carman-neustaedter/"><img alt="Carman Neustaedter picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/carman-neustaedter.jpg 1x" src="/pr-preview/pr-121/static/images/people/carman-neustaedter.jpg"/><span class="author-link">Carman Neustaedter</span></a> , <span>Scott Bateman</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Cscw</span><span class="ui brown basic label">Telepresence</span><span class="ui brown basic label">Video Communication</span><span class="ui brown basic label">Shared Experiences</span><span class="ui brown basic label">Teleoperation</span><span class="ui brown basic label">Drones</span><span class="ui brown basic label">Collaboration</span><span class="ui brown basic label">Hri</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="tvcg-2016-lopez"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TVCG 2016</span></p><p class="color" style="font-size:1.3em"><b>Towards An Understanding of Mobile Touch Navigation in a Stereoscopic Viewing Environment for 3D Data Exploration</b></p><p><span>David Lopez</span> , <a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a> , <span>Candemir Doger</span> , <a href="/pr-preview/pr-121/people/tobias-isenberg/"><img alt="Tobias Isenberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/no-profile-2.jpg 1x" src="/pr-preview/pr-121/static/images/people/no-profile-2.jpg"/><span class="author-link">Tobias Isenberg</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Visualization Of 3 D Data</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Expert Interaction</span><span class="ui brown basic label">Direct Touch Input</span><span class="ui brown basic label">Mobile Displays</span><span class="ui brown basic label">Stereoscopic Environments</span><span class="ui brown basic label">VR</span><span class="ui brown basic label">AR</span><span class="ui brown basic label">Conceptual Model Of Interaction</span><span class="ui brown basic label">Interaction Reference Frame Mapping</span><span class="ui brown basic label">Observational Study</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="mobilehci-2015-ledo"><div class="three wide column" style="margin:auto"><img alt="mobilehci-2015-ledo cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/mobilehci-2015-ledo.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/mobilehci-2015-ledo.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MobileHCI 2015</span></p><p class="color" style="font-size:1.3em"><b>Proxemic-Aware Controls: Designing Remote Controls for Ubiquitous Computing Ecologies</b></p><p><a href="/pr-preview/pr-121/people/david-ledo/"><img alt="David Ledo picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/david-ledo.jpg 1x" src="/pr-preview/pr-121/static/images/people/david-ledo.jpg"/><span class="author-link">David Ledo</span></a> , <a href="/pr-preview/pr-121/people/saul-greenberg/"><img alt="Saul Greenberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/saul-greenberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/saul-greenberg.jpg"/><span class="author-link">Saul Greenberg</span></a> , <a href="/pr-preview/pr-121/people/nicolai-marquardt/"><img alt="Nicolai Marquardt picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/nicolai-marquardt.jpg 1x" src="/pr-preview/pr-121/static/images/people/nicolai-marquardt.jpg"/><span class="author-link">Nicolai Marquardt</span></a> , <span>Sebastian Boring</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Ubiquitous Computing</span><span class="ui brown basic label">Proxemic Interaction</span><span class="ui brown basic label">Mobile Interaction</span><span class="ui brown basic label">Control Of Appliances</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2015-oehlberg"><div class="three wide column" style="margin:auto"><img alt="chi-2015-oehlberg cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2015-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2015-oehlberg.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2015</span></p><p class="color" style="font-size:1.3em"><b>Patterns of Physical Design Remixing in Online Maker Communities</b></p><p><a href="/pr-preview/pr-121/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a> , <span>Wendy E. Mackay</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Customization</span><span class="ui brown basic label">Maker Communities</span><span class="ui brown basic label">User Innovation</span><span class="ui brown basic label">Collaboration</span><span class="ui brown basic label">Hacking</span><span class="ui brown basic label">Remixing</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2015-aseniero"><div class="three wide column" style="margin:auto"><img alt="chi-2015-aseniero cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2015-aseniero.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2015-aseniero.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2015</span></p><p class="color" style="font-size:1.3em"><b>Stratos: Using Visualization to Support Decisions in Strategic Software Release Planning</b></p><p><a href="/pr-preview/pr-121/people/bon-adriel-aseniero/"><img alt="Bon Adriel Aseniero picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/bon-adriel-aseniero.jpg 1x" src="/pr-preview/pr-121/static/images/people/bon-adriel-aseniero.jpg"/><span class="author-link">Bon Adriel Aseniero</span></a> , <span>Tiffany Wun</span> , <a href="/pr-preview/pr-121/people/david-ledo/"><img alt="David Ledo picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/david-ledo.jpg 1x" src="/pr-preview/pr-121/static/images/people/david-ledo.jpg"/><span class="author-link">David Ledo</span></a> , <span>Guenther Ruhe</span> , <a href="/pr-preview/pr-121/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a> , <a href="/pr-preview/pr-121/people/sheelagh-carpendale/"><img alt="Sheelagh Carpendale picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg"/><span class="author-link">Sheelagh Carpendale</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Software Engineering</span><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Release Planning</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2015-jones"><div class="three wide column" style="margin:auto"><img alt="chi-2015-jones cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2015-jones.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2015-jones.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2015</span></p><p class="color" style="font-size:1.3em"><b>Mechanics of Camera Work in Mobile Video Collaboration</b></p><p><a href="/pr-preview/pr-121/people/brennan-jones/"><img alt="Brennan Jones picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/brennan-jones.jpg 1x" src="/pr-preview/pr-121/static/images/people/brennan-jones.jpg"/><span class="author-link">Brennan Jones</span></a> , <span>Anna Witcraft</span> , <span>Scott Bateman</span> , <a href="/pr-preview/pr-121/people/carman-neustaedter/"><img alt="Carman Neustaedter picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/carman-neustaedter.jpg 1x" src="/pr-preview/pr-121/static/images/people/carman-neustaedter.jpg"/><span class="author-link">Carman Neustaedter</span></a> , <a href="/pr-preview/pr-121/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a></p><div><div class="ui large basic labels"><span class="ui brown basic label">Video Communication</span><span class="ui brown basic label">Collaboration</span><span class="ui brown basic label">Mobile Computing</span><span class="ui brown basic label">Handheld Devices</span><span class="ui brown basic label">Cscw</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2015-willett"><div class="three wide column" style="margin:auto"><img alt="chi-2015-willett cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2015-willett.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2015-willett.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2015</span></p><p class="color" style="font-size:1.3em"><b>Lightweight Relief Shearing for Enhanced Terrain Perception on Interactive Maps</b></p><p><a href="/pr-preview/pr-121/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a> , <span>Bernhard Jenny</span> , <a href="/pr-preview/pr-121/people/tobias-isenberg/"><img alt="Tobias Isenberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/no-profile-2.jpg 1x" src="/pr-preview/pr-121/static/images/people/no-profile-2.jpg"/><span class="author-link">Tobias Isenberg</span></a> , <span>Pierre Dragicevic</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Plan Oblique Relief</span><span class="ui brown basic label">Interaction</span><span class="ui brown basic label">Depth Perception</span><span class="ui brown basic label">Terrain Maps</span><span class="ui brown basic label">Relief Shearing</span></div></div></div></div></div><div id="publications-modal"><div id="capstone-2025-haptic-floor-proxy" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/capstone-2025-haptic-floor-proxy/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>capstone-2025-haptic-floor-proxy</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">ENGG 501-502 Capstone 2025</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/publications/capstone-2025-haptic-floor-proxy" target="_blank">A low-cost open-source miniature proxy to experience, design for, and showcase haptic floors</a></h1><p class="meta"><a href="/people/aidan-gaede-janke"><img alt="aidan-gaede-janke photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aidan-gaede-janke.jpg 1x" src="/pr-preview/pr-121/static/images/people/aidan-gaede-janke.jpg"/><strong>Aidan Gaede-Janke</strong></a> , <a href="/people/isabella-huang"><img alt="isabella-huang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/isabella-huang.jpg 1x" src="/pr-preview/pr-121/static/images/people/isabella-huang.jpg"/><strong>Isabella Huang</strong></a> , <a href="/people/ebube-anachebe"><img alt="ebube-anachebe photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ebube-anachebe.jpg 1x" src="/pr-preview/pr-121/static/images/people/ebube-anachebe.jpg"/><strong>Ebube Anachebe</strong></a> , <a href="/people/nadeem-moosa"><img alt="nadeem-moosa photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/nadeem-moosa.jpg 1x" src="/pr-preview/pr-121/static/images/people/nadeem-moosa.jpg"/><strong>Nadeem Moosa</strong></a> , <a href="/people/sebastian-gil"><img alt="sebastian-gil photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sebastian-gil.jpg 1x" src="/pr-preview/pr-121/static/images/people/sebastian-gil.jpg"/><strong>Sebastian Gil</strong></a> , <a href="/people/yaseen-rashid"><img alt="yaseen-rashid photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/yaseen-rashid.jpg 1x" src="/pr-preview/pr-121/static/images/people/yaseen-rashid.jpg"/><strong>Yaseen Rashid</strong></a> , <a href="/people/isaac-ng"><img alt="isaac-ng photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/isaac-ng.jpg 1x" src="/pr-preview/pr-121/static/images/people/isaac-ng.jpg"/><strong>Isaac Ng</strong></a> , <a href="/people/christian-frisson"><img alt="christian-frisson photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/people/christian-frisson.jpg"/><strong>Christian Frisson</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p></p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Force Feedback</span><span class="ui brown basic label">Proxy</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Aidan Gaede-Janke<!-- -->, <!-- -->Isabella Huang<!-- -->, <!-- -->Ebube Anachebe<!-- -->, <!-- -->Nadeem Moosa<!-- -->, <!-- -->Sebastian Gil<!-- -->, <!-- -->Yaseen Rashid<!-- -->, <!-- -->Isaac Ng<!-- -->, <!-- -->Christian Frisson<!-- -->. <b>A low-cost open-source miniature proxy to experience, design for, and showcase haptic floors</b>. <i>(<!-- -->ENGG 501-502 Capstone 2025<!-- -->)</i>. <!-- --> </p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2025-ghaneezabadi" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2025-ghaneezabadi/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2025-ghaneezabadi</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2025</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2025-ghaneezabadi cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2025-ghaneezabadi.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2025-ghaneezabadi.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2025-ghaneezabadi" target="_blank">IntelliLining: Activity Sensing through Textile Interlining Sensors Using TENGs</a></h1><p class="meta"><span>Mahdie GhaneEzabadi</span> , <a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a> , <a href="/people/xing-dong-yang"><img alt="xing-dong-yang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/xing-dong-yang.jpg 1x" src="/pr-preview/pr-121/static/images/people/xing-dong-yang.jpg"/><strong>Xing-Dong Yang</strong></a> , <span>Te-yen Wu</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2025-ghaneezabadi.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2025-ghaneezabadi.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We introduce a novel component for smart garments: smart interlining, and validate its technical feasibility through a series of experiments. Our work involved the implementation of a prototype that employs a textile vibration sensor based on Triboelectric Nanogenerators (TENGs), commonly used for activity detection. We explore several unique features of smart interlining, including how sensor signals and patterns are influenced by factors such as the size and shape of the interlining sensor, the location of the vibration source within the sensor area, and various propagation media, such as airborne and surface vibrations. We present our study results and discuss how these findings support the feasibility of smart interlining. Additionally, we demonstrate that smart interlinings on a shirt can detect a variety of user activities involving the hand, mouth, and upper body, achieving an accuracy rate of 93.9% in the tested activities.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Interactive Textile</span><span class="ui brown basic label">TEN Gs</span><span class="ui brown basic label">Machine Learning</span><span class="ui brown basic label">Vibration Sensing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Mahdie GhaneEzabadi<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Xing-Dong Yang<!-- -->, <!-- -->Te-yen Wu<!-- -->. <b>IntelliLining: Activity Sensing through Textile Interlining Sensors Using TENGs</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2025<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->DOI: <a href="https://doi.org/10.1145/3706598.3713167" target="_blank">https://doi.org/10.1145/3706598.3713167</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2025-madill" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2025-madill/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2025-madill</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2025</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2025-madill cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2025-madill.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2025-madill.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2025-madill" target="_blank">Playing with Robots: Performing Arts Techniques for Designing and Understanding Robot Group Movement</a></h1><p class="meta"><span>Philippa Madill</span> , <span>Matthew Newton</span> , <span>Huanjun Zhao</span> , <span>Yichen Lian</span> , <a href="/people/zachary-mckendrick"><img alt="zachary-mckendrick photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/zachary-mckendrick.jpg 1x" src="/pr-preview/pr-121/static/images/people/zachary-mckendrick.jpg"/><strong>Zachary McKendrick</strong></a> , <span>Patrick Finn</span> , <a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a> , <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2025-madill.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2025-madill.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this work, we introduce a formal design approach derived from the performing arts to design robot group behaviour. In our first experiment, we worked with professional actors, directors, and non-specialists using a participatory design approach to identify common group behaviour patterns. In a follow-up studio work, we identified twelve common group movement patterns, transposed them into a performance script, built a scale model to support the performance process, and evaluated the patterns with a senior actor under studio conditions. We evaluated our refined models with 20 volunteers in a user study in the third experiment. Results from our affective circumplex modelling suggest that the patterns elicit positive emotional responses from the users. Also, participants performed better than chance in identifying the motion patterns without prior training. Based on our results, we propose design guidelines for social robots’ behaviour and movement design to improve their overall comprehensibility in interaction.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Humanities</span><span class="ui brown basic label">Art</span><span class="ui brown basic label">Robots</span><span class="ui brown basic label">Method</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Philippa Madill<!-- -->, <!-- -->Matthew Newton<!-- -->, <!-- -->Huanjun Zhao<!-- -->, <!-- -->Yichen Lian<!-- -->, <!-- -->Zachary McKendrick<!-- -->, <!-- -->Patrick Finn<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>Playing with Robots: Performing Arts Techniques for Designing and Understanding Robot Group Movement</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2025<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->DOI: <a href="https://dl.acm.org/doi/10.1145/3706598.3713996" target="_blank">https://dl.acm.org/doi/10.1145/3706598.3713996</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2025-shiokawa" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2025-shiokawa/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2025-shiokawa</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2025</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2025-shiokawa cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2025-shiokawa.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2025-shiokawa.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2025-shiokawa" target="_blank">Beyond Vacuuming: How Can We Exploit Domestic Robots&#x27; Idle Time?</a></h1><p class="meta"><span>Yoshiaki Shiokawa</span> , <span>Winnie Chen</span> , <a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a> , <span>Jason Alexander</span> , <span>Adwait Sharma</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2025-shiokawa.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2025-shiokawa.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/npYIDenYb2Y" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/npYIDenYb2Y?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/npYIDenYb2Y/maxresdefault.jpg src=https://img.youtube.com/vi/npYIDenYb2Y/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We are increasingly adopting domestic robots (e.g., Roomba) that provide relief from mundane household tasks. However, these robots usually only spend little time executing their specific task and remain idle for long periods. They typically possess advanced mobility and sensing capabilities, and therefore have significant potential applications beyond their designed use. Our work explores this untapped potential of domestic robots in ubiquitous computing, focusing on how they can improve and support modern lifestyles. We conducted two studies: an online survey (n=50) to understand current usage patterns of these robots within homes and an exploratory study (n=12) with HCI and HRI experts. Our thematic analysis revealed 12 key dimensions for developing interactions with domestic robots and outlined over 100 use cases, illustrating how these robots can offer proactive assistance and provide privacy. Finally, we implemented a proof-of-concept prototype to demonstrate the feasibility of reappropriating domestic robots for diverse ubiquitous computing applications.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Domestic Robots</span><span class="ui brown basic label">Ubiquitous</span><span class="ui brown basic label">Interaction</span><span class="ui brown basic label">Design Space</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Yoshiaki Shiokawa<!-- -->, <!-- -->Winnie Chen<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Jason Alexander<!-- -->, <!-- -->Adwait Sharma<!-- -->. <b>Beyond Vacuuming: How Can We Exploit Domestic Robots&#x27; Idle Time?</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2025<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->DOI: <a href="https://dl.acm.org/doi/10.1145/3706598.3714266" target="_blank">https://dl.acm.org/doi/10.1145/3706598.3714266</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="httf-2024-blair" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/httf-2024-blair/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>httf-2024-blair</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">HTTF 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="httf-2024-blair cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/httf-2024-blair.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/httf-2024-blair.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/httf-2024-blair" target="_blank">Weaving Perspectives into Practice: A Manifesto for Combining Epistemological and Dissemination Strategies</a></h1><p class="meta"><a href="/people/kathryn-blair"><img alt="kathryn-blair photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kathryn-blair.jpg 1x" src="/pr-preview/pr-121/static/images/people/kathryn-blair.jpg"/><strong>Kathryn Blair</strong></a> , <span>Pil Hansen</span> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/httf-2024-blair.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>httf-2024-blair.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>This paper is an invitation to HCI designers and artist-researchers to weave together practice-based ways of knowing with others’ experiences with their work, focused on understanding cognitive and aesthetic impressions garnered during exhibitions. It describes the first author&#x27;s process for weaving together the warp (practice-based ways of knowing) and weft (interview-based insights into others’ experiences) by leveraging the exhibition site as a place to generate knowledge with attendees experiencing their work. Understanding others’ experiences informs the experimentation that practice-based knowledge generation is founded on, deepening and enriching the resulting work.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Fine Arts</span><span class="ui brown basic label">Methodology</span><span class="ui brown basic label">Emergent Ontology</span><span class="ui brown basic label">Epistemology</span><span class="ui brown basic label">Practice Based Research</span><span class="ui brown basic label">Qualitative Research</span><span class="ui brown basic label">Dissemination</span><span class="ui brown basic label">Knowledge Generation</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kathryn Blair<!-- -->, <!-- -->Pil Hansen<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Weaving Perspectives into Practice: A Manifesto for Combining Epistemological and Dissemination Strategies</b>. <i>(<!-- -->HTTF 2024<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->4<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3686169.3686205" target="_blank">https://doi.org/10.1145/3686169.3686205</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="mdpi-actuators-2024-piao" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/mdpi-actuators-2024-piao/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>mdpi-actuators-2024-piao</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MDPI Actuators 2024 (Special Issue &quot;Actuators for Haptic and Tactile Stimulation Applications&quot;)</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="mdpi-actuators-2024-piao cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/mdpi-actuators-2024-piao.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/mdpi-actuators-2024-piao.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/mdpi-actuators-2024-piao" target="_blank">Assessing the Impact of Force Feedback in Musical Knobs on Performance and User Experience</a></h1><p class="meta"><span>Ziyue Piao</span> , <a href="/people/christian-frisson"><img alt="christian-frisson photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/people/christian-frisson.jpg"/><strong>Christian Frisson</strong></a> , <span>Bavo Van Kerrebroeck</span> , <span>Marcelo M. Wanderley</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/mdpi-actuators-2024-piao.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>mdpi-actuators-2024-piao.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>This paper examined how rotary force feedback in knobs can enhance control over musical techniques, focusing on both performance and user experience. To support our study, we developed the Bend-aid system, a web-based sequencer with pre-designed haptic modes for pitch modulation, integrated with TorqueTuner, a rotary haptic device that controls pitch through programmable haptic effects. Then, twenty musically trained participants evaluated three haptic modes (No-force feedback (No-FF), Spring, and Detent) by performing a vibrato mimicry task, rating their experience on a Likert scale, and providing qualitative feedback in post-experiment interviews. The study assessed objective performance metrics (Pitch Error and Pitch Deviation) and subjective user experience ratings (Comfort, Ease of Control, and Helpfulness) of each haptic mode. User experience results showed that participants found force feedback helpful. Performance results showed that the Detent mode significantly improved pitch accuracy and vibrato stability compared to No-FF, while the Spring mode did not show a similar improvement. Post-experiment interviews showed that preferences for Spring and Detent modes varied, and the applicants provided suggestions for future knob designs. These findings suggest that force feedback may enhance both control and the experience of control in rotary knobs, with potential applications for more nuanced control in DMIs.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Rotary Force Feedback</span><span class="ui brown basic label">Digital Musical Instruments</span><span class="ui brown basic label">Knobs</span><span class="ui brown basic label">Torque Tuner</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ziyue Piao<!-- -->, <!-- -->Christian Frisson<!-- -->, <!-- -->Bavo Van Kerrebroeck<!-- -->, <!-- -->Marcelo M. Wanderley<!-- -->. <b>Assessing the Impact of Force Feedback in Musical Knobs on Performance and User Experience</b>. <i>(<!-- -->MDPI Actuators 2024 (Special Issue &quot;Actuators for Haptic and Tactile Stimulation Applications&quot;)<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->15<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.3390/act13110462" target="_blank">https://doi.org/10.3390/act13110462</a></p></div></div><div class="block"><h1>Materials</h1><div class="ui horizontal small divided link list"><div class="item"><a href="https://github.com/piaoziyue/Bend-aid-Actuators-2024" target="_blank" style="font-size:1.2em"><svg data-prefix="fab" data-icon="github-alt" class="svg-inline--fa fa-github-alt" role="img" viewBox="0 0 512 512" aria-hidden="true"><path fill="currentColor" d="M202.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM496 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3l48.2 0c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"></path></svg>https://github.com/piaoziyue/Bend-aid-Actuators-2024</a></div></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2024-gunturu" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/uist-2024-gunturu/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2024-gunturu</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2024-gunturu cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2024-gunturu.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2024-gunturu.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2024-gunturu" target="_blank">Augmented Physics: Creating Interactive and Embedded Physics Simulations from Static Textbook Diagrams</a></h1><p class="meta"><a href="/people/aditya-gunturu"><img alt="aditya-gunturu photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-gunturu.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-gunturu.jpg"/><strong>Aditya Gunturu</strong></a> , <span>Yi Wen</span> , <a href="/people/nandi-zhang"><img alt="nandi-zhang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/nandi-zhang.jpg 1x" src="/pr-preview/pr-121/static/images/people/nandi-zhang.jpg"/><strong>Nandi Zhang</strong></a> , <a href="/people/jarin-thundathil"><img alt="jarin-thundathil photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/jarin-thundathil.jpg 1x" src="/pr-preview/pr-121/static/images/people/jarin-thundathil.jpg"/><strong>Jarin Thundathil</strong></a> , <span>Rubaiat Habib Kazi</span> , <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2024-gunturu.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2024-gunturu.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/MOdSeUp8YcE" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/MOdSeUp8YcE?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/MOdSeUp8YcE/maxresdefault.jpg src=https://img.youtube.com/vi/MOdSeUp8YcE/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce Augmented Physics, a machine learning-integrated authoring tool designed for creating embedded interactive physics simulations from static textbook diagrams. Leveraging recent advancements in computer vision, such as Segment Anything and Multi-modal LLMs, our web-based system enables users to semi-automatically extract diagrams from physics textbooks and generate interactive simulations based on the extracted content. These interactive diagrams are seamlessly integrated into scanned textbook pages, facilitating interactive and personalized learning experiences across various physics concepts, such as optics, circuits, and kinematics. Drawing from an elicitation study with seven physics instructors, we explore four key augmentation strategies: 1) augmented experiments, 2) animated diagrams, 3) bi-directional binding, and 4) parameter visualization. We evaluate our system through technical evaluation, a usability study (N=12), and expert interviews (N=12). Study findings suggest that our system can facilitate more engaging and personalized learning experiences in physics education.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Physics Education</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Authoring Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Aditya Gunturu<!-- -->, <!-- -->Yi Wen<!-- -->, <!-- -->Nandi Zhang<!-- -->, <!-- -->Jarin Thundathil<!-- -->, <!-- -->Rubaiat Habib Kazi<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>Augmented Physics: Creating Interactive and Embedded Physics Simulations from Static Textbook Diagrams</b>. <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- -->(<!-- -->UIST 2024<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->12<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3654777.3676392" target="_blank">https://doi.org/10.1145/3654777.3676392</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2024-roy" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/uist-2024-roy/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2024-roy</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2024-roy cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2024-roy.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2024-roy.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2024-roy" target="_blank">HoloChemie - Sustainable Fabrication of Soft Biochemical Holographic Devices for Ubiquitous Sensing</a></h1><p class="meta"><a href="/people/sutirtha-roy"><img alt="sutirtha-roy photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sutirtha-roy.jpg 1x" src="/pr-preview/pr-121/static/images/people/sutirtha-roy.jpg"/><strong>Sutirtha Roy</strong></a> , <span>Moshfiq-Us-Saleheen Chowdhury</span> , <span>Jurjaan Onayza Noim</span> , <span>Richa Pandey</span> , <a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2024-roy.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2024-roy.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/0FrYD1xInNs" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/0FrYD1xInNs?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/0FrYD1xInNs/maxresdefault.jpg src=https://img.youtube.com/vi/0FrYD1xInNs/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Sustainable fabrication approaches and biomaterials are increasingly being used in HCI to fabricate interactive devices. However, the majority of the work has focused on integrating electronics. This paper takes a sustainable approach to exploring the fabrication of biochemical sensing devices. Firstly, we contribute a set of biochemical formulations for biological and environmental sensing with bio-sourced and environment-friendly substrate materials. Our formulations are based on a combination of enzymes derived from bacteria and fungi, plant extracts and commercially available chemicals to sense both liquid and gaseous analytes: glucose, lactic acid, pH levels and carbon dioxide. Our novel holographic sensing scheme allows for detecting the presence of analytes and enables quantitative estimation of the analyte levels. We present a set of application scenarios that demonstrate the versatility of our approach and discuss the sustainability aspects, its limitations, and the implications for bio-chemical systems in HCI.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Physiological Sensing</span><span class="ui brown basic label">Biochemical Devices Sensing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sutirtha Roy<!-- -->, <!-- -->Moshfiq-Us-Saleheen Chowdhury<!-- -->, <!-- -->Jurjaan Onayza Noim<!-- -->, <!-- -->Richa Pandey<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->. <b>HoloChemie - Sustainable Fabrication of Soft Biochemical Holographic Devices for Ubiquitous Sensing</b>. <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- -->(<!-- -->UIST 2024<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->19<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3654777.3676448" target="_blank">https://doi.org/10.1145/3654777.3676448</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2024-danyluk" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/dis-2024-danyluk/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2024-danyluk</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2024-danyluk cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2024-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2024-danyluk.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2024-danyluk" target="_blank">Understanding Gesture and Microgesture Inputs for Augmented Reality Maps</a></h1><p class="meta"><a href="/people/kurtis-danyluk"><img alt="kurtis-danyluk photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg"/><strong>Kurtis Danyluk</strong></a> , <span>Simon Klueber</span> , <a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We explore the potential for subtle on-hand gesture and microgesture interactions for map navigation with augmented reality (AR) devices. We describe a design exercise and follow-up elicitation study in which we identified on-hand gestures for cartographic interaction primitives. Microgestures and on-hand interactions are a promising space for AR map navigation as they offers always-available, tactile, and memorable spaces for interaction. Our findings show a clear set of microgesture interaction patterns that are well suited for supporting map navigation and manipulation. In particular, we highlight how the properties of various microgestures align with particular cartographic interaction tasks. We also describe our experience creating an exploratory proof-of-concept AR map prototype which helped us identify new opportunities and practical challenges for microgesture control. Finally, we discuss how future AR map systems could benefit from on-hand and microgesture input schemes.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Gestural Input</span><span class="ui brown basic label">Microgestures</span><span class="ui brown basic label">AR</span><span class="ui brown basic label">Maps</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kurtis Danyluk<!-- -->, <!-- -->Simon Klueber<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Understanding Gesture and Microgesture Inputs for Augmented Reality Maps</b>. <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- -->(<!-- -->DIS 2024<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->DOI: <a href="https://doi.org/10.1145/3643834.3661630" target="_blank">https://doi.org/10.1145/3643834.3661630</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2024-bressa" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2024-bressa/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2024-bressa</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2024-bressa cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2024-bressa.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2024-bressa.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2024-bressa" target="_blank">Input Visualization: Collecting and Modifying Data with Visual Representations</a></h1><p class="meta"><a href="/people/nathalie-bressa"><img alt="nathalie-bressa photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/nathalie-bressa.jpg 1x" src="/pr-preview/pr-121/static/images/people/nathalie-bressa.jpg"/><strong>Nathalie Bressa</strong></a> , <span>Jordan Louis</span> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a> , <span>Samuel Huron</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/RAfv2quE6nA" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/RAfv2quE6nA?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/RAfv2quE6nA/maxresdefault.jpg src=https://img.youtube.com/vi/RAfv2quE6nA/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We examine input visualizations, visual representations that are designed to collect (and represent) new data rather than encode preexisting datasets. Information visualization is commonly used to reveal insights and stories within existing data. As a result, most contemporary visualization approaches assume existing datasets as the starting point for design, through which that data is mapped to visual encodings. Meanwhile, the implications of visualizations as inputs and as data sources have received little attention—despite the existence of visual and physical examples stretching back centuries. In this paper, we present a design space of 50 input visualizations analyzing their visual representation, data, artifact, context, and input. Based on this, we identify input modalities, purposes of input visualizations, and a set of design considerations. Finally, we discuss the relationship between input visualization and traditional visualization design and suggest opportunities for future research to better understand these visual representations and their potential.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Input Visualization</span><span class="ui brown basic label">Data Physicalization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nathalie Bressa<!-- -->, <!-- -->Jordan Louis<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Samuel Huron<!-- -->. <b>Input Visualization: Collecting and Modifying Data with Visual Representations</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2024<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->DOI: <a href="https://doi.org/10.1145/3613904.3642808" target="_blank">https://doi.org/10.1145/3613904.3642808</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2024-dhawka" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2024-dhawka/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2024-dhawka</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2024-dhawka cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2024-dhawka.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2024-dhawka.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2024-dhawka" target="_blank">Better Little People Pictures: Generative Creation of Demographically Diverse Anthropographics</a></h1><p class="meta"><a href="/people/priya-dhawka"><img alt="priya-dhawka photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/priya-dhawka.jpg 1x" src="/pr-preview/pr-121/static/images/people/priya-dhawka.jpg"/><strong>Priya Dhawka</strong></a> , <span>Lauren Perera</span> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/dCEFvx4AqIo" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/dCEFvx4AqIo?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/dCEFvx4AqIo/maxresdefault.jpg src=https://img.youtube.com/vi/dCEFvx4AqIo/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We explore the potential of generative AI text-to-image models to help designers efficiently craft unique, representative, and demographically diverse anthropographics that visualize data about people. Currently, creating data-driven iconic images to represent individuals in a dataset often requires considerable design effort. Generative text-to-image models can streamline the process of creating these images, but risk perpetuating designer biases in addition to stereotypes latent in the models. In response, we outline a conceptual workflow for crafting anthropographic assets for visualizations, highlighting possible sources of risk and bias as well as opportunities for reflection and refinement by a human designer. Using an implementation of this workflow with Stable Diffusion and Google Colab, we illustrate a variety of new anthropographic designs that showcase the visual expressiveness and scalability of these generative approaches. Based on our experiments, we also identify challenges and research opportunities for new AI-enabled anthropographic visualization tools.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Anthropographics</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Marginalized Populations</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Priya Dhawka<!-- -->, <!-- -->Lauren Perera<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Better Little People Pictures: Generative Creation of Demographically Diverse Anthropographics</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2024<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->DOI: <a href="https://doi.org/10.1145/3613904.3641957" target="_blank">https://doi.org/10.1145/3613904.3641957</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2024-panigrahy" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2024-panigrahy/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2024-panigrahy</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2024-panigrahy cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2024-panigrahy.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2024-panigrahy.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2024-panigrahy" target="_blank">ecSkin: Low-Cost Fabrication of Epidermal Electrochemical Sensors for Detecting Biomarkers in Sweat</a></h1><p class="meta"><span>Sai Nandan Panigrahy*</span> , <span>Chang Hyeon Lee*</span> , <span>Vrahant Nagoria</span> , <span>Mohammad Janghorban</span> , <span>Richa Pandey</span> , <a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2024-panigrahy.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2024-panigrahy.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/OqW3owQyMk8" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/OqW3owQyMk8?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/OqW3owQyMk8/maxresdefault.jpg src=https://img.youtube.com/vi/OqW3owQyMk8/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>The development of low-cost and non-invasive biosensors for monitoring electrochemical biomarkers in sweat holds great promise for personalized healthcare and early disease detection. In this work, we present ecSkin, a novel fabrication approach for realizing epidermal electrochemical sensors that can detect two vital biomarkers in sweat: glucose and cortisol. We contribute the synthesis of functional reusable inks, that can be formulated using simple household materials. Electrical characterization of inks indicates that they outperform commercially available carbon inks. Cyclic voltammetry experiments show that our inks are electrochemically active and detect glucose and cortisol at activation voltages of -0.36 V and -0.22 V, respectively. Chronoamperometry experiments show that the sensors can detect the full range of glucose and cortisol levels typically found in sweat. Results from a user evaluation show that ecSkin sensors successfully function on the skin. Finally, we demonstrate three applications to illustrate how ecSkin devices can be deployed for various interactive applications.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Physiological Sensing</span><span class="ui brown basic label">Electrochemical Devices</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sai Nandan Panigrahy*<!-- -->, <!-- -->Chang Hyeon Lee*<!-- -->, <!-- -->Vrahant Nagoria<!-- -->, <!-- -->Mohammad Janghorban<!-- -->, <!-- -->Richa Pandey<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->. <b>ecSkin: Low-Cost Fabrication of Epidermal Electrochemical Sensors for Detecting Biomarkers in Sweat</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2024<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->20<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3613904.3642232" target="_blank">https://doi.org/10.1145/3613904.3642232</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2023-chulpongsatorn" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/uist-2023-chulpongsatorn/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2023-chulpongsatorn</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2023-chulpongsatorn cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2023-chulpongsatorn.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2023-chulpongsatorn" target="_blank">Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks</a></h1><p class="meta"><a href="/people/neil-chulpongsatorn"><img alt="neil-chulpongsatorn photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-121/static/images/people/neil-chulpongsatorn.jpg"/><strong>Neil Chulpongsatorn</strong></a> , <a href="/people/mille-skovhus-lunding"><img alt="mille-skovhus-lunding photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/mille-skovhus-lunding.jpg 1x" src="/pr-preview/pr-121/static/images/people/mille-skovhus-lunding.jpg"/><strong>Mille Skovhus Lunding</strong></a> , <a href="/people/nishan-soni"><img alt="nishan-soni photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/nishan-soni.jpg 1x" src="/pr-preview/pr-121/static/images/people/nishan-soni.jpg"/><strong>Nishan Soni</strong></a> , <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2023-chulpongsatorn.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2023-chulpongsatorn.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Zv6JQ5T-qn0" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Zv6JQ5T-qn0?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/Zv6JQ5T-qn0/maxresdefault.jpg src=https://img.youtube.com/vi/Zv6JQ5T-qn0/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce Augmented Math, a machine learning-based approach to authoring AR explorable explanations by augmenting static math textbooks without programming. To augment a static document, our system first extracts mathematical formulas and figures from a given document using optical character recognition (OCR) and computer vision. By binding and manipulating these extracted contents, the user can see the interactive animation overlaid onto the document through mobile AR interfaces. This empowers non-technical users, such as teachers or students, to transform existing math textbooks and handouts into on-demand and personalized explorable explanations. To design our system, we first analyzed existing explorable math explanations to identify common design strategies. Based on the findings, we developed a set of augmentation techniques that can be automatically generated based on the extracted content, which are 1) dynamic values, 2) interactive figures, 3) relationship highlights, 4) concrete examples, and 5) step-by-step hints. To evaluate our system, we conduct two user studies: preliminary user testing and expert interviews. The study results confirm that our system allows more engaging experiences for learning math concepts.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Authoring Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Neil Chulpongsatorn<!-- -->, <!-- -->Mille Skovhus Lunding<!-- -->, <!-- -->Nishan Soni<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks</b>. <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- -->(<!-- -->UIST 2023<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->16<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3586183.3606827" target="_blank">https://doi.org/10.1145/3586183.3606827</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2023-ihara" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/uist-2023-ihara/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2023-ihara</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2023-ihara cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2023-ihara.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2023-ihara.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2023-ihara" target="_blank">HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality</a></h1><p class="meta"><a href="/people/keiichi-ihara"><img alt="keiichi-ihara photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/keiichi-ihara.jpg 1x" src="/pr-preview/pr-121/static/images/people/keiichi-ihara.jpg"/><strong>Keiichi Ihara</strong></a> , <a href="/people/mehrad-faridan"><img alt="mehrad-faridan photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/mehrad-faridan.jpg 1x" src="/pr-preview/pr-121/static/images/people/mehrad-faridan.jpg"/><strong>Mehrad Faridan</strong></a> , <span>Ayumi Ichikawa</span> , <span>Ikkaku Kawaguchi</span> , <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2023-ihara.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2023-ihara.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/KSBPtiXy8Hg" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/KSBPtiXy8Hg?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/KSBPtiXy8Hg/maxresdefault.jpg src=https://img.youtube.com/vi/KSBPtiXy8Hg/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces HoloBots, a mixed reality remote collaboration system that augments holographic telepresence with synchronized mobile robots. Beyond existing mixed reality telepresence, HoloBots lets remote users not only be visually and spatially present, but also physically engage with local users and their environment. HoloBots allows the users to touch, grasp, manipulate, and interact with the remote physical environment as if they were co-located in the same shared space. We achieve this by synchronizing holographic user motion (Hololens 2 and Azure Kinect) with tabletop mobile robots (Sony Toio). Beyond the existing physical telepresence, HoloBots contributes to an exploration of broader design space, such as object actuation, virtual hand physicalization, world-in-miniature exploration, shared tangible interfaces, embodied guidance, and haptic communication. We evaluate our system with twelve participants by comparing it with hologram-only and robot-only conditions. Both quantitative and qualitative results confirm that our system significantly enhances the level of co-presence and shared experience, compared to the other conditions.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Physical Telepresence</span><span class="ui brown basic label">Mobile Robots</span><span class="ui brown basic label">Actuated Tangible Ui</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Keiichi Ihara<!-- -->, <!-- -->Mehrad Faridan<!-- -->, <!-- -->Ayumi Ichikawa<!-- -->, <!-- -->Ikkaku Kawaguchi<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality</b>. <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- -->(<!-- -->UIST 2023<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->12<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3586183.3606727" target="_blank">https://doi.org/10.1145/3586183.3606727</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2023-xia" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/uist-2023-xia/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2023-xia</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2023-xia cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2023-xia.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2023-xia.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2023-xia" target="_blank">RealityCanvas: Augmented Reality Sketching for Embedded and Responsive Scribble Animation Effects</a></h1><p class="meta"><a href="/people/zhijie-xia"><img alt="zhijie-xia photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/zhijie-xia.jpg 1x" src="/pr-preview/pr-121/static/images/people/zhijie-xia.jpg"/><strong>Zhijie Xia</strong></a> , <a href="/people/kyzyl-monteiro"><img alt="kyzyl-monteiro photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kyzyl-monteiro.jpg 1x" src="/pr-preview/pr-121/static/images/people/kyzyl-monteiro.jpg"/><strong>Kyzyl Monteiro</strong></a> , <a href="/people/kevin-van"><img alt="kevin-van photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kevin-van.jpg 1x" src="/pr-preview/pr-121/static/images/people/kevin-van.jpg"/><strong>Kevin Van</strong></a> , <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2023-xia.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2023-xia.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/HVOgH1quDsc" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/HVOgH1quDsc?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/HVOgH1quDsc/maxresdefault.jpg src=https://img.youtube.com/vi/HVOgH1quDsc/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce RealityCanvas, a mobile AR sketching tool that can easily augment real-world physical motion with responsive hand-drawn animation. Recent research in AR sketching tools has enabled users to not only embed static drawings into the real world but also dynamically animate them with physical motion. However, existing tools often lack the flexibility and expressiveness of possible animations, as they primarily support simple line-based geometry. To address this limitation, we explore both expressive and improvisational AR sketched animation by introducing a set of responsive scribble animation techniques that can be directly embedded through sketching interactions: 1) object binding, 2) flip-book animation, 3) action trigger, 4) particle effects, 5) motion trajectory, and 6) contour highlight. These six animation effects were derived from the analysis of 172 existing video-edited scribble animations. We showcase these techniques through various applications, such as video creation, augmented education, storytelling, and AR prototyping. The results of our user study and expert interviews confirm that our tool can lower the barrier to creating AR-based sketched animation, while allowing creative, expressive, and improvisational AR sketching experiences.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Sketching Interfaces</span><span class="ui brown basic label">Scribble Animation</span><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Real Time Authoring</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Zhijie Xia<!-- -->, <!-- -->Kyzyl Monteiro<!-- -->, <!-- -->Kevin Van<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>RealityCanvas: Augmented Reality Sketching for Embedded and Responsive Scribble Animation Effects</b>. <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- -->(<!-- -->UIST 2023<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->14<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3586183.3606716" target="_blank">https://doi.org/10.1145/3586183.3606716</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="assets-2023-mok" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/assets-2023-mok/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>assets-2023-mok</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">ASSETS 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/publications/assets-2023-mok" target="_blank">Experiences of Autistic Twitch Livestreamers: “I have made easily the most meaningful and impactful relationships”</a></h1><p class="meta"><a href="/people/terrance-mok"><img alt="terrance-mok photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/terrance-mok.jpg 1x" src="/pr-preview/pr-121/static/images/people/terrance-mok.jpg"/><strong>Terrance Mok</strong></a> , <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a> , <span>Adam McCrimmon</span> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/assets-2023-mok.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>assets-2023-mok.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present perspectives from 10 autistic Twitch streamers regarding their experiences as livestreamers and how autism uniquely colors their experiences. Livestreaming offers a social online experience distinct from in-person, face-to-face communication, where autistic people tend to encounter challenges. Our reflexive thematic analysis of interviews with 10 participants showcases autistic livestreamers’ perspectives in their own words. Our findings center on the importance of having streamers establishing connections with other, sharing autistic identities, controlling a space for social interaction, personal growth, and accessibility challenges. In our discussion, we highlight the crucial value of having a medium for autistic representation, as well as design opportunities for streaming platforms to onboard autistic livestreamers and to facilitate livestreamers communication with their audience.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Autism</span><span class="ui brown basic label">Live Streaming</span><span class="ui brown basic label">Autistic</span><span class="ui brown basic label">Twitch</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Terrance Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Adam McCrimmon<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Experiences of Autistic Twitch Livestreamers: “I have made easily the most meaningful and impactful relationships”</b>. <i>(<!-- -->ASSETS 2023<!-- -->)</i>. <!-- --> <!-- -->DOI: <a href="https://doi.org/10.1145/3597638.3608416" target="_blank">https://doi.org/10.1145/3597638.3608416</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2023-mukashev" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/uist-2023-mukashev/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2023-mukashev</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2023-mukashev cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2023-mukashev.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2023-mukashev.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2023-mukashev" target="_blank">TactTongue: Prototyping ElectroTactile Stimulations on the Tongue</a></h1><p class="meta"><a href="/people/dinmukhammed-mukashev"><img alt="dinmukhammed-mukashev photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/dinmukhammed-mukashev.jpg 1x" src="/pr-preview/pr-121/static/images/people/dinmukhammed-mukashev.jpg"/><strong>Dinmukhammed Mukashev</strong></a> , <span>Nimesha Ranasinghe</span> , <a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2023-mukashev.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2023-mukashev.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/zCUdJNNRz5s" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/zCUdJNNRz5s?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/zCUdJNNRz5s/maxresdefault.jpg src=https://img.youtube.com/vi/zCUdJNNRz5s/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>The tongue is a remarkable human organ with a high concentration of taste receptors and an exceptional ability to sense touch. This work uses electro-tactile stimulation to explore the intricate interplay between tactile perception and taste rendering on the tongue. To facilitate this exploration, we utilized a fexible, high-resolution electro-tactile prototyping platform that can be administered in the mouth. We have created a design tool that abstracts users from the low-level stimulation parameters, enabling them to focus on higher-level design objectives. Through this platform, we present the results of three studies. Our frst study evaluates the design tool’s qualitative and formative aspects. In contrast, the second study measures the qualitative attributes of the sensations produced by our device, including tactile sensations and taste. In the third study, we demonstrate the ability of our device to sense touch input through the tongue when placed on the hard palate region in the mouth. Finally, we present a range of application demonstrators that span diverse domains, including accessibility, medical surgeries, and extended reality. These demonstrators showcase the versatility and potential of our platform, highlighting its ability to enable researchers and practitioners to explore new ways of leveraging the tongue’s unique capabilities. Overall, this work presents new opportunities to deploy tongue interfaces and has broad implications for designing interfaces that incorporate the tongue as a sensory organ.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Electrotactile Actuation</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Dinmukhammed Mukashev<!-- -->, <!-- -->Nimesha Ranasinghe<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->. <b>TactTongue: Prototyping ElectroTactile Stimulations on the Tongue</b>. <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- -->(<!-- -->UIST 2023<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->14<!-- -->. <!-- -->DOI: <a href="https://dl.acm.org/doi/10.1145/3586183.3606829" target="_blank">https://dl.acm.org/doi/10.1145/3586183.3606829</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2023-xia2" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/uist-2023-xia2/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2023-xia2</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2023-xia2 cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2023-xia2.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2023-xia2.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2023-xia2" target="_blank">CrossTalk: Intelligent Substrates for Language-Oriented Interaction in Video-Based Communication and Collaboration</a></h1><p class="meta"><span>Haijun Xia</span> , <span>Tony Wang</span> , <a href="/people/aditya-gunturu"><img alt="aditya-gunturu photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-gunturu.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-gunturu.jpg"/><strong>Aditya Gunturu</strong></a> , <span>Peiling Jiang</span> , <span>William Duan</span> , <span>Xiaoshuo Yao</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2023-xia2.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2023-xia2.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/8I1yXNRcm54" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/8I1yXNRcm54?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/8I1yXNRcm54/maxresdefault.jpg src=https://img.youtube.com/vi/8I1yXNRcm54/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Despite the advances and ubiquity of digital communication media such as videoconferencing and virtual reality, they remain oblivious to the rich intentions expressed by users. Beyond transmitting audio, videos, and messages, we envision digital communication media as proactive facilitators that can provide unobtrusive assistance to enhance communication and collaboration. Informed by the results of a formative study, we propose three key design concepts to explore the systematic integration of intelligence into communication and collaboration, including the panel substrate, language-based intent recognition, and lightweight interaction techniques. We developed CrossTalk, a videoconferencing system that instantiates these concepts, which was found to enable a more fluid and flexible communication and collaboration experience.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Videoconferencing</span><span class="ui brown basic label">Natural Language Interface</span><span class="ui brown basic label">Language Oriented Interaction</span><span class="ui brown basic label">Context Aware Computing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Haijun Xia<!-- -->, <!-- -->Tony Wang<!-- -->, <!-- -->Aditya Gunturu<!-- -->, <!-- -->Peiling Jiang<!-- -->, <!-- -->William Duan<!-- -->, <!-- -->Xiaoshuo Yao<!-- -->. <b>CrossTalk: Intelligent Substrates for Language-Oriented Interaction in Video-Based Communication and Collaboration</b>. <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- -->(<!-- -->UIST 2023<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->16<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3586183.3606773" target="_blank">https://doi.org/10.1145/3586183.3606773</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="mdpi-arts-2023-frisson" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/mdpi-arts-2023-frisson/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>mdpi-arts-2023-frisson</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MDPI Arts 2023 (Special Issue Feeling the Future—Haptic Audio)</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="mdpi-arts-2023-frisson cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/mdpi-arts-2023-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/mdpi-arts-2023-frisson.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/mdpi-arts-2023-frisson" target="_blank">Challenges and Opportunities of Force Feedback in Music</a></h1><p class="meta"><a href="/people/christian-frisson"><img alt="christian-frisson photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/people/christian-frisson.jpg"/><strong>Christian Frisson</strong></a> , <span>Marcelo M. Wanderley</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/mdpi-arts-2023-frisson.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>mdpi-arts-2023-frisson.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>A growing body of work on musical haptics focuses on vibrotactile feedback, while musical applications of force feedback, though more than four decades old, are sparser. This paper reviews related work combining music and haptics, focusing on force feedback. We then discuss the limitations of these works and elicit the main challenges in current applications of force feedback and music (FF&amp;M), which are as follows: modularity; replicability; affordability; and usability. We call for the following opportunities in future research works on FF&amp;M: embedding audio and haptic software into hardware modules, networking multiple modules with distributed control, and authoring with audio-inspired and audio-coupled tools. We illustrate our review with recent efforts to develop an affordable, open-source and self-contained 1-Degree-of-Freedom (DoF) rotary force-feedback device for musical applications, i.e., the TorqueTuner, and to embed audio and haptic processing and authoring in module firmware, with ForceHost, and examine their advantages and drawbacks in light of the opportunities presented in the text.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Force Feedback</span><span class="ui brown basic label">Digital Musical Instruments</span><span class="ui brown basic label">Torque Tuner</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Christian Frisson<!-- -->, <!-- -->Marcelo M. Wanderley<!-- -->. <b>Challenges and Opportunities of Force Feedback in Music</b>. <i>(<!-- -->MDPI Arts 2023 (Special Issue Feeling the Future—Haptic Audio)<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.3390/arts12040147" target="_blank">https://doi.org/10.3390/arts12040147</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="siggraph-labs-2023-seta" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/siggraph-labs-2023-seta/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>siggraph-labs-2023-seta</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">SIGGRAPH 2023 Labs</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="siggraph-labs-2023-seta cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/siggraph-labs-2023-seta.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/siggraph-labs-2023-seta.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/siggraph-labs-2023-seta" target="_blank">Sketching Pipelines for Ephemeral Immersive Spaces</a></h1><p class="meta"><span>Michał Seta</span> , <span>Eduardo A. L. Meneses</span> , <span>Emmanuel Durand</span> , <a href="/people/christian-frisson"><img alt="christian-frisson photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/people/christian-frisson.jpg"/><strong>Christian Frisson</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/siggraph-labs-2023-seta.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>siggraph-labs-2023-seta.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>This hands-on class will allow artists to use open-source tools to create interactive and immersive experiences. These tools have been created and incubated at the Society for Arts and Technology (SAT), a unique non-profit organization in Canada whose mission is to democratize technologies to enable people to experience and author multisensory immersions. During the class we invite participants to use their favorite software on platforms they are already familiar with, to interface with our tools. The toolset will include transmission protocols, video mapping tools, sound spatialization software, and gestural control using pose detection. The class will be organized in two parts: a presentation of the tools and context involving the development and applications, and a hands-on session with an ephemeral immersive space. This event is designed for art researchers, artists, designers, content creators, and other creatives interested in creating immersive spaces using research-developed tools. Participants will learn how to employ open-source tools for different artistic tasks so that they will be able to deploy their own immersive spaces after the class.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Art</span><span class="ui brown basic label">Interaction</span><span class="ui brown basic label">Pipeline</span><span class="ui brown basic label">Production</span><span class="ui brown basic label">Ui Tools</span><span class="ui brown basic label">VR AR MR</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Michał Seta<!-- -->, <!-- -->Eduardo A. L. Meneses<!-- -->, <!-- -->Emmanuel Durand<!-- -->, <!-- -->Christian Frisson<!-- -->. <b>Sketching Pipelines for Ephemeral Immersive Spaces</b>. <i>(<!-- -->SIGGRAPH 2023 Labs<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->2<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3588029.3599740" target="_blank">https://doi.org/10.1145/3588029.3599740</a></p></div></div><div class="block"><h1>Materials</h1><div class="ui horizontal small divided link list"><div class="item"><a href="https://sat-mtl.gitlab.io/tools/" target="_blank" style="font-size:1.2em"><svg data-prefix="fab" data-icon="gitlab" class="svg-inline--fa fa-gitlab" role="img" viewBox="0 0 512 512" aria-hidden="true"><path fill="currentColor" d="M504 204.6l-.7-1.8-69.7-181.8c-1.4-3.6-3.9-6.6-7.2-8.6-2.4-1.6-5.1-2.5-8-2.8s-5.7 .1-8.4 1.1-5.1 2.7-7.1 4.8c-1.9 2.1-3.3 4.7-4.1 7.4l-47 144-190.5 0-47.1-144c-.8-2.8-2.2-5.3-4.1-7.4-2-2.1-4.4-3.7-7.1-4.8-2.6-1-5.5-1.4-8.4-1.1s-5.6 1.2-8 2.8c-3.2 2-5.8 5.1-7.2 8.6L9.8 202.8 9 204.6c-10 26.2-11.3 55-3.5 82 7.7 26.9 24 50.7 46.4 67.6l.3 .2 .6 .4 106 79.5c38.5 29.1 66.7 50.3 84.6 63.9 3.7 1.9 8.3 4.3 13 4.3s9.3-2.4 13-4.3c17.9-13.5 46.1-34.9 84.6-63.9l106.7-79.9 .3-.3c22.4-16.9 38.7-40.6 45.6-67.5 8.6-27 7.4-55.8-2.6-82z"></path></svg>https://sat-mtl.gitlab.io/tools/</a></div></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2023-li" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/dis-2023-li/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2023-li</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2023-li cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2023-li.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2023-li.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2023-li" target="_blank">Physica: Interactive Tangible Physics Simulation based on Tabletop Mobile Robots Towards Explorable Physics Education</a></h1><p class="meta"><span>Jiatong Li</span> , <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a> , <span>Ken Nakagaki</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/dis-2023-li.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>dis-2023-li.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/7DKpq52282g" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/7DKpq52282g?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/7DKpq52282g/maxresdefault.jpg src=https://img.youtube.com/vi/7DKpq52282g/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>In this paper, we introduce Physica, a tangible physics simulation system and approach based on tabletop mobile robots. In Physica, each tabletop robot can physically represent distinct simulated objects that are controlled through an underlying physics simulation, such as gravitational force, molecular movement, and spring force. It aims to bring the benefits of tangible and haptic interaction into explorable physics learning, which was traditionally only available on screen-based interfaces. The system utilizes off-the-shelf mobile robots (Sony Toio) and an open-source physics simulation tool (Teilchen). Built on top of them, we implement the interaction software pipeline that consists of 1) an event detector to reflect tangible interaction by users, and 2) target speed control to minimize the gap between the robot motion and simulated moving objects. To present the potential for physics education, we demonstrate various application scenarios that illustrate different forms of learning using Physica. In our user study, we investigate the effect and the potential of our approach through a perception study and interviews with physics educators.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Physics Simulation</span><span class="ui brown basic label">Actuated Tangible Ui</span><span class="ui brown basic label">Swarm Ui</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jiatong Li<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Ken Nakagaki<!-- -->. <b>Physica: Interactive Tangible Physics Simulation based on Tabletop Mobile Robots Towards Explorable Physics Education</b>. <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- -->(<!-- -->DIS 2023<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->15<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3526113.3545626" target="_blank">https://doi.org/10.1145/3526113.3545626</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2023-dhawka" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2023-dhawka/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2023-dhawka</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2023-dhawka cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2023-dhawka.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2023-dhawka.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2023-dhawka" target="_blank">We are the Data: Challenges and Opportunities for Creating Demographically Diverse Anthropographics</a></h1><p class="meta"><a href="/people/priya-dhawka"><img alt="priya-dhawka photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/priya-dhawka.jpg 1x" src="/pr-preview/pr-121/static/images/people/priya-dhawka.jpg"/><strong>Priya Dhawka</strong></a> , <a href="/people/helen-ai-he"><img alt="helen-ai-he photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/helen-ai-he.jpg 1x" src="/pr-preview/pr-121/static/images/people/helen-ai-he.jpg"/><strong>Helen Ai He</strong></a> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2023-dhawka.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2023-dhawka.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/iBzv2jS3ECM" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/iBzv2jS3ECM?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/iBzv2jS3ECM/maxresdefault.jpg src=https://img.youtube.com/vi/iBzv2jS3ECM/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Anthropographics are human-shaped visualizations that aim to emphasize the human importance of datasets and the people behind them. However, current anthropographics tend to employ homogeneous human shapes to encode data about diverse demographic groups. Such anthropographics can obscure important differences between groups and contemporary designs exemplify the lack of inclusive approaches for representing human diversity in visualizations. In response, we explore the creation of demographically diverse anthropographics that communicate the visible diversity of demographically distinct populations. Building on previous anthropographics research, we explore strategies for visualizing datasets about people in ways that explicitly encode diversity—illustrating these approaches with examples in a variety of visual styles. We also critically reflect on strategies for creating diverse anthropographics, identifying social and technical challenges that can result in harmful representations. Finally, we highlight a set of forward-looking research opportunities for advancing the design and understanding of diverse anthropographics.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Anthropographics</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Marginalized Populations</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Priya Dhawka<!-- -->, <!-- -->Helen Ai He<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>We are the Data: Challenges and Opportunities for Creating Demographically Diverse Anthropographics</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2023<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->14<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3544548.3581086" target="_blank">https://doi.org/10.1145/3544548.3581086</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2023-faridan" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2023-faridan/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2023-faridan</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2023-faridan cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2023-faridan.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2023-faridan.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2023-faridan" target="_blank">ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms</a></h1><p class="meta"><a href="/people/mehrad-faridan"><img alt="mehrad-faridan photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/mehrad-faridan.jpg 1x" src="/pr-preview/pr-121/static/images/people/mehrad-faridan.jpg"/><strong>Mehrad Faridan</strong></a> , <a href="/people/bheesha-kumari"><img alt="bheesha-kumari photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/bheesha-kumari.jpg 1x" src="/pr-preview/pr-121/static/images/people/bheesha-kumari.jpg"/><strong>Bheesha Kumari</strong></a> , <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2023-faridan.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2023-faridan.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/VOe3fETd3sk" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/VOe3fETd3sk?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/VOe3fETd3sk/maxresdefault.jpg src=https://img.youtube.com/vi/VOe3fETd3sk/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present ChameleonControl, a real-human teleoperation system for scalable remote instruction in hands-on classrooms. In contrast to the existing video or AR/VR-based remote hands-on education, ChameleonControl uses a real human as a surrogate of a remote instructor. Building on existing human-based telepresence approaches (e.g. ChameleonMask), we contribute a novel method to teleoperate a human surrogate through synchronized mixed reality (MR) hand gestural navigation and verbal communication. By overlaying the remote instructor&#x27;s virtual hands in the local user&#x27;s MR view, the remote instructor can guide and control the local user as if they were physically present. This allows the local user/surrogate to synchronize their hand movements and gestures with the remote instructor, effectively ``teleoperating&#x27;&#x27; a real human. We evaluate our system through the in-the-wild deployment for physiotherapy classrooms, as well as lab-based experiments for other application domains such as mechanical assembly, sign language, and cooking lessons. The study results confirm that our approach can increase engagement and the sense of co-presence, showing potential for the future of remote hands-on classrooms.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Visual Cue</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Telepresence</span><span class="ui brown basic label">Remote Guidance</span><span class="ui brown basic label">Human Surrogates</span><span class="ui brown basic label">Hands On Training</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Mehrad Faridan<!-- -->, <!-- -->Bheesha Kumari<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2023<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3544548.3581381" target="_blank">https://doi.org/10.1145/3544548.3581381</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2023-monteiro" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2023-monteiro/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2023-monteiro</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2023-monteiro cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2023-monteiro.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2023-monteiro.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2023-monteiro" target="_blank">Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching</a></h1><p class="meta"><a href="/people/kyzyl-monteiro"><img alt="kyzyl-monteiro photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kyzyl-monteiro.jpg 1x" src="/pr-preview/pr-121/static/images/people/kyzyl-monteiro.jpg"/><strong>Kyzyl Monteiro</strong></a> , <a href="/people/ritik-vatsal"><img alt="ritik-vatsal photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ritik-vatsal.jpg 1x" src="/pr-preview/pr-121/static/images/people/ritik-vatsal.jpg"/><strong>Ritik Vatsal</strong></a> , <a href="/people/neil-chulpongsatorn"><img alt="neil-chulpongsatorn photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-121/static/images/people/neil-chulpongsatorn.jpg"/><strong>Neil Chulpongsatorn</strong></a> , <span>Aman Parnami</span> , <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2023-monteiro.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2023-monteiro.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/JssiyfrhIJw" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/JssiyfrhIJw?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/JssiyfrhIJw/maxresdefault.jpg src=https://img.youtube.com/vi/JssiyfrhIJw/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces Teachable Reality, an augmented reality (AR) prototyping tool for creating interactive tangible AR applications with arbitrary everyday objects. Teachable Reality leverages vision-based interactive machine teaching (e.g., Teachable Machine), which captures real-world interactions for AR prototyping. It identifies the user-defined tangible and gestural interactions using an on-demand computer vision model. Based on this, the user can easily create functional AR prototypes without programming, enabled by a trigger-action authoring interface. Therefore, our approach allows the flexibility, customizability, and generalizability of tangible AR applications that can address the limitation of current marker-based approaches. We explore the design space and demonstrate various AR prototypes, which include tangible and deformable interfaces, context-aware assistants, and body-driven AR applications. The results of our user study and expert interviews confirm that our approach can lower the barrier to creating functional AR prototypes while also allowing flexible and general-purpose prototyping experiences.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Prototyping Tools</span><span class="ui brown basic label">Tangible Interactions</span><span class="ui brown basic label">Everyday Objects</span><span class="ui brown basic label">Interactive Machine Teaching</span><span class="ui brown basic label">Human Centered Machine Learning</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kyzyl Monteiro<!-- -->, <!-- -->Ritik Vatsal<!-- -->, <!-- -->Neil Chulpongsatorn<!-- -->, <!-- -->Aman Parnami<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2023<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->15<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3544548.3581449" target="_blank">https://doi.org/10.1145/3544548.3581449</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-ea-2023-chulpongsatorn" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-ea-2023-chulpongsatorn/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-ea-2023-chulpongsatorn</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI EA 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-ea-2023-chulpongsatorn cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-ea-2023-chulpongsatorn.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-ea-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-ea-2023-chulpongsatorn" target="_blank">HoloTouch: Interacting with Mixed Reality Visualizations Through Smartphone Proxies</a></h1><p class="meta"><a href="/people/neil-chulpongsatorn"><img alt="neil-chulpongsatorn photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-121/static/images/people/neil-chulpongsatorn.jpg"/><strong>Neil Chulpongsatorn</strong></a> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a> , <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-ea-2023-chulpongsatorn.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-ea-2023-chulpongsatorn.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We contribute interaction techniques for augmenting mixed reality (MR) visualizations with smartphone proxies. By combining head-mounted displays (HMDs) with mobile touchscreens, we can augment low-resolution holographic 3D charts with precise touch input, haptics feedback, high-resolution 2D graphics, and physical manipulation. Our approach aims to complement both MR and physical visualizations. Most current MR visualizations suffer from unreliable tracking, low visual resolution, and imprecise input. Data physicalizations on the other hand, although allowing for natural physical manipulation, are limited in dynamic and interactive modification. We demonstrate how mobile devices such as smartphones or tablets can serve as physical proxies for MR data interactions, creating dynamic visualizations that support precise manipulation and rich input and output. We describe 6 interaction techniques that leverage the combined physicality, sensing, and output capabilities of HMDs and smartphones, and demonstrate those interactions via a prototype system. Based on an evaluation, we outline opportunities for combining the advantages of both MR and physical charts.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Embedded Data Visualization</span><span class="ui brown basic label">Tangible Interaction</span><span class="ui brown basic label">Cross Device Interaction</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Neil Chulpongsatorn<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>HoloTouch: Interacting with Mixed Reality Visualizations Through Smartphone Proxies</b>. <i>In <!-- -->Extended Abstracts of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI EA 2023<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->8<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3544549.3585738" target="_blank">https://doi.org/10.1145/3544549.3585738</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-ea-2023-fang" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-ea-2023-fang/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-ea-2023-fang</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI EA 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-ea-2023-fang cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-ea-2023-fang.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-ea-2023-fang.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-ea-2023-fang" target="_blank">VR Haptics at Home: Repurposing Everyday Objects and Environment for Casual and On-Demand VR Haptic Experiences</a></h1><p class="meta"><span>Cathy Mengying Fang</span> , <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a> , <span>Daniel Leithinger</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-ea-2023-fang.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-ea-2023-fang.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces VR Haptics at Home, a method of repurposing everyday objects in the home to provide casual and on-demand haptic experiences. Current VR haptic devices are often expensive, complex, and unreliable, which limits the opportunities for rich haptic experiences outside research labs. In contrast, we envision that, by repurposing everyday objects as passive haptics props, we can create engaging VR experiences for casual uses with minimal cost and setup. To explore and evaluate this idea, we conducted an in-the-wild study with eight participants, in which they used our proof-of-concept system to turn their surrounding objects such as chairs, tables, and pillows at their own homes into haptic props. The study results show that our method can be adapted to different homes and environments, enabling more engaging VR experiences without the need for complex setup process. Based on our findings, we propose a possible design space to showcase the potential for future investigation.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Interaction Techniques</span><span class="ui brown basic label">Passive Haptics</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Cathy Mengying Fang<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Daniel Leithinger<!-- -->. <b>VR Haptics at Home: Repurposing Everyday Objects and Environment for Casual and On-Demand VR Haptic Experiences</b>. <i>In <!-- -->Extended Abstracts of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI EA 2023<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->7<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3544549.3585871" target="_blank">https://doi.org/10.1145/3544549.3585871</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="vrst-2022-frisson" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/vrst-2022-frisson/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>vrst-2022-frisson</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">VRST 2022 Poster</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="vrst-2022-frisson cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/vrst-2022-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/vrst-2022-frisson.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/vrst-2022-frisson" target="_blank">LivePose: Democratizing Pose Detection for Multimedia Arts and Telepresence Applications on Open Edge Devices</a></h1><p class="meta"><a href="/people/christian-frisson"><img alt="christian-frisson photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/people/christian-frisson.jpg"/><strong>Christian Frisson</strong></a> , <span>Gabriel N. Downs</span> , <span>Marie-Ève Dumas</span> , <span>Farzaneh Askari</span> , <span>Emmanuel Durand</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/vrst-2022-frisson.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>vrst-2022-frisson.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/604196712" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/604196712?autoplay=1&gt;&lt;Image width={0} height={0} alt=undefined src=undefined&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present LivePose: an open-source (GPL license) tool that democratizes pose detection for multimedia arts and telepresence applications, optimized for and distributed on open edge devices. We designed the architecture of LivePose with a 5-stage pipeline (frame capture, pose estimation, dimension mapping, filtering, output) sharing streams of data flow, distributable on networked nodes. We distribute LivePose and dependencies packages and filesystem images optimized for edge devices (NVIDIA Jetson). We showcase multimedia arts and telepresence applications enabled by LivePose.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Multimedia Arts</span><span class="ui brown basic label">Telepresence</span><span class="ui brown basic label">Edge Computing</span><span class="ui brown basic label">Pose Detection</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Christian Frisson<!-- -->, <!-- -->Gabriel N. Downs<!-- -->, <!-- -->Marie-Ève Dumas<!-- -->, <!-- -->Farzaneh Askari<!-- -->, <!-- -->Emmanuel Durand<!-- -->. <b>LivePose: Democratizing Pose Detection for Multimedia Arts and Telepresence Applications on Open Edge Devices</b>. <i>(<!-- -->VRST 2022 Poster<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->2<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3562939.3565660" target="_blank">https://doi.org/10.1145/3562939.3565660</a></p></div></div><div class="block"><h1>Materials</h1><div class="ui horizontal small divided link list"><div class="item"><a href="https://gitlab.com/sat-mtl/tools/livepose" target="_blank" style="font-size:1.2em"><svg data-prefix="fab" data-icon="gitlab" class="svg-inline--fa fa-gitlab" role="img" viewBox="0 0 512 512" aria-hidden="true"><path fill="currentColor" d="M504 204.6l-.7-1.8-69.7-181.8c-1.4-3.6-3.9-6.6-7.2-8.6-2.4-1.6-5.1-2.5-8-2.8s-5.7 .1-8.4 1.1-5.1 2.7-7.1 4.8c-1.9 2.1-3.3 4.7-4.1 7.4l-47 144-190.5 0-47.1-144c-.8-2.8-2.2-5.3-4.1-7.4-2-2.1-4.4-3.7-7.1-4.8-2.6-1-5.5-1.4-8.4-1.1s-5.6 1.2-8 2.8c-3.2 2-5.8 5.1-7.2 8.6L9.8 202.8 9 204.6c-10 26.2-11.3 55-3.5 82 7.7 26.9 24 50.7 46.4 67.6l.3 .2 .6 .4 106 79.5c38.5 29.1 66.7 50.3 84.6 63.9 3.7 1.9 8.3 4.3 13 4.3s9.3-2.4 13-4.3c17.9-13.5 46.1-34.9 84.6-63.9l106.7-79.9 .3-.3c22.4-16.9 38.7-40.6 45.6-67.5 8.6-27 7.4-55.8-2.6-82z"></path></svg>https://gitlab.com/sat-mtl/tools/livepose</a></div></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tochi-2022-nittala" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/tochi-2022-nittala/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>tochi-2022-nittala</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TOCHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="tochi-2022-nittala cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tochi-2022-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tochi-2022-nittala.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tochi-2022-nittala" target="_blank">SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures</a></h1><p class="meta"><span>Adwait Sharma</span> , <span>Christina Salchow-Hömmen</span> , <span>Vimal Suresh Mollyn</span> , <a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a> , <span>Michael A. Hedderich</span> , <span>Marion Koelle</span> , <span>Thomas Seel</span> , <span>Jürgen Steimle</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/tochi-2022-nittala.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>tochi-2022-nittala.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Gestural interaction with freehands and while grasping an everyday object enables always-available input. To sense such gestures, minimal instrumentation of the user’s hand is desirable. However, the choice of an effective but minimal IMU layout remains challenging, due to the complexity of the multi-factorial space that comprises diverse finger gestures, objects and grasps. We present SparseIMU, a rapid method for selecting minimal inertial sensor-based layouts for effective gesture recognition. Furthermore, we contribute a computational tool to guide designers with optimal sensor placement. Our approach builds on an extensive microgestures dataset that we collected with a dense network of 17 inertial measurement units (IMUs). We performed a series of analyses, including an evaluation of the entire combinatorial space for freehand and grasping microgestures (393K layouts), and quantified the performance across different layout choices, revealing new gesture detection opportunities with IMUs. Finally, we demonstrate the versatility of our method with four scenarios.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Gesture Recognition</span><span class="ui brown basic label">Hand Gestures</span><span class="ui brown basic label">Sensor Placement</span><span class="ui brown basic label">IMU</span><span class="ui brown basic label">Objects</span><span class="ui brown basic label">Design Tool</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Adwait Sharma<!-- -->, <!-- -->Christina Salchow-Hömmen<!-- -->, <!-- -->Vimal Suresh Mollyn<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Michael A. Hedderich<!-- -->, <!-- -->Marion Koelle<!-- -->, <!-- -->Thomas Seel<!-- -->, <!-- -->Jürgen Steimle<!-- -->. <b>SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures</b>. <i>(<!-- -->TOCHI 2022<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->40<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3569894" target="_blank">https://doi.org/10.1145/3569894</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2022-kaimoto" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/uist-2022-kaimoto/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2022-kaimoto</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2022-kaimoto cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2022-kaimoto.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2022-kaimoto.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2022-kaimoto" target="_blank">Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI</a></h1><p class="meta"><a href="/people/hiroki-kaimoto"><img alt="hiroki-kaimoto photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/hiroki-kaimoto.jpg 1x" src="/pr-preview/pr-121/static/images/people/hiroki-kaimoto.jpg"/><strong>Hiroki Kaimoto</strong></a> , <a href="/people/kyzyl-monteiro"><img alt="kyzyl-monteiro photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kyzyl-monteiro.jpg 1x" src="/pr-preview/pr-121/static/images/people/kyzyl-monteiro.jpg"/><strong>Kyzyl Monteiro</strong></a> , <a href="/people/mehrad-faridan"><img alt="mehrad-faridan photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/mehrad-faridan.jpg 1x" src="/pr-preview/pr-121/static/images/people/mehrad-faridan.jpg"/><strong>Mehrad Faridan</strong></a> , <span>Jiatong Li</span> , <a href="/people/samin-farajian"><img alt="samin-farajian photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/samin-farajian.jpg 1x" src="/pr-preview/pr-121/static/images/people/samin-farajian.jpg"/><strong>Samin Farajian</strong></a> , <span>Yasuaki Kakehi</span> , <span>Ken Nakagaki</span> , <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2022-kaimoto.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2022-kaimoto.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/xy-IeVgoEpY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/xy-IeVgoEpY?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/xy-IeVgoEpY/maxresdefault.jpg src=https://img.youtube.com/vi/xy-IeVgoEpY/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces Sketched Reality, an approach that com- bines AR sketching and actuated tangible user interfaces (TUI) for bi-directional sketching interaction. Bi-directional sketching enables virtual sketches and physical objects to affect each other through physical actuation and digital computation. In the existing AR sketching, the relationship between virtual and physical worlds is only one-directional --- while physical interaction can affect virtual sketches, virtual sketches have no return effect on the physical objects or environment. In contrast, bi-directional sketching interaction allows the seamless coupling between sketches and actuated TUIs. In this paper, we employ tabletop-size small robots (Sony Toio) and an iPad-based AR sketching tool to demonstrate the concept. In our system, virtual sketches drawn and simulated on an iPad (e.g., lines, walls, pendulums, and springs) can move, actuate, collide, and constrain physical Toio robots, as if virtual sketches and the physical objects exist in the same space through seamless coupling between AR and robot motion. This paper contributes a set of novel interactions and a design space of bi-directional AR sketching. We demonstrate a series of potential applications, such as tangible physics education, explorable mechanism, tangible gaming for children, and in-situ robot programming via sketching.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Actuated Tangible Interfaces</span><span class="ui brown basic label">Swarm User Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Hiroki Kaimoto<!-- -->, <!-- -->Kyzyl Monteiro<!-- -->, <!-- -->Mehrad Faridan<!-- -->, <!-- -->Jiatong Li<!-- -->, <!-- -->Samin Farajian<!-- -->, <!-- -->Yasuaki Kakehi<!-- -->, <!-- -->Ken Nakagaki<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI</b>. <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- -->(<!-- -->UIST 2022<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->12<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3526113.3545626" target="_blank">https://doi.org/10.1145/3526113.3545626</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2022-liao" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/uist-2022-liao/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2022-liao</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2022-liao cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2022-liao.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2022-liao.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2022-liao" target="_blank">RealityTalk: Real-time Speech-driven Augmented Presentation for AR Live Storytelling</a></h1><p class="meta"><a href="/people/jian-liao"><img alt="jian-liao photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/jian-liao.jpg 1x" src="/pr-preview/pr-121/static/images/people/jian-liao.jpg"/><strong>Jian Liao</strong></a> , <a href="/people/adnan-karim"><img alt="adnan-karim photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/adnan-karim.jpg 1x" src="/pr-preview/pr-121/static/images/people/adnan-karim.jpg"/><strong>Adnan Karim</strong></a> , <a href="/people/shivesh-jadon"><img alt="shivesh-jadon photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/shivesh-jadon.jpg 1x" src="/pr-preview/pr-121/static/images/people/shivesh-jadon.jpg"/><strong>Shivesh Jadon</strong></a> , <span>Rubaiat Habib Kazi</span> , <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2022-liao.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2022-liao.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/vfIMeICV-7c" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/vfIMeICV-7c?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/vfIMeICV-7c/maxresdefault.jpg src=https://img.youtube.com/vi/vfIMeICV-7c/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present RealityTalk, a system that augments real-time live presentations with speech-driven interactive virtual elements. Augmented presentations leverage embedded visuals and animation for engaging and expressive storytelling. However, existing tools for live presentations often lack interactivity and improvisation, while creating such effects in video editing tools require significant time and expertise. RealityTalk enables users to create live augmented presentations with real-time speech-driven interactions. The user can interactively prompt, move, and manipulate graphical elements through real-time speech and supporting modalities. Based on our analysis of 177 existing video-edited augmented presentations, we propose a novel set of interaction techniques and then incorporated them into RealityTalk. We evaluate our tool from a presenter’s perspective to demonstrate the effectiveness of our system.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Augmented Presentation</span><span class="ui brown basic label">Natural Language Processing</span><span class="ui brown basic label">Gestural And Speech Input</span><span class="ui brown basic label">Video</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jian Liao<!-- -->, <!-- -->Adnan Karim<!-- -->, <!-- -->Shivesh Jadon<!-- -->, <!-- -->Rubaiat Habib Kazi<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>RealityTalk: Real-time Speech-driven Augmented Presentation for AR Live Storytelling</b>. <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- -->(<!-- -->UIST 2022<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->12<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3526113.3545702" target="_blank">https://doi.org/10.1145/3526113.3545702</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2022-nisser" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/uist-2022-nisser/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2022-nisser</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2022-nisser cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2022-nisser.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2022-nisser.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2022-nisser" target="_blank">Mixels: Fabricating Interfaces using Programmable Magnetic Pixels</a></h1><p class="meta"><span>Martin Nisser</span> , <span>Yashaswini Makaram</span> , <span>Lucian Covarrubias</span> , <span>Amadou Bah</span> , <span>Faraz Faruqi</span> , <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a> , <span>Stefanie Mueller</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2022-nisser.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2022-nisser.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/6SvFCQkVFtw" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/6SvFCQkVFtw?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/6SvFCQkVFtw/maxresdefault.jpg src=https://img.youtube.com/vi/6SvFCQkVFtw/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>In this paper, we present Mixels, programmable magnetic pixels that can be rapidly fabricated using an electromagnetic printhead mounted on an off-the-shelve 3-axis CNC machine. The ability to program magnetic material pixel-wise with varying magnetic force enables Mixels to create new tangible, tactile, and haptic interfaces. To facilitate the creation of interactive objects with Mixels, we provide a user interface that lets users specify the high-level magnetic behavior and that then computes the underlying magnetic pixel assignments and fabrication instructions to program the magnetic surface. Our custom hardware add-on based on an electromagnetic printhead and hall effect sensor clips onto a standard 3-axis CNC machine and can both write and read magnetic pixel values from magnetic material. Our evaluation shows that our system can reliably program and read magnetic pixels of various strengths, that we can predict the behavior of two interacting magnetic surfaces before programming them, that our electromagnet is strong enough to create pixels that utilize the maximum magnetic strength of the material being programmed, and that this material remains magnetized when removed from the magnetic plotter.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Programmable Materials</span><span class="ui brown basic label">Magnetic Interfaces</span><span class="ui brown basic label">Fabrication</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Martin Nisser<!-- -->, <!-- -->Yashaswini Makaram<!-- -->, <!-- -->Lucian Covarrubias<!-- -->, <!-- -->Amadou Bah<!-- -->, <!-- -->Faraz Faruqi<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Stefanie Mueller<!-- -->. <b>Mixels: Fabricating Interfaces using Programmable Magnetic Pixels</b>. <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- -->(<!-- -->UIST 2022<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->12<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3526113.3545698" target="_blank">https://doi.org/10.1145/3526113.3545698</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2022-nittala" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/uist-2022-nittala/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2022-nittala</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2022-nittala cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2022-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2022-nittala.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2022-nittala" target="_blank">Prototyping Soft Devices with Interactive Bioplastics</a></h1><p class="meta"><span>Marion Koelle</span> , <span>Madalina Nicolae</span> , <a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a> , <span>Marc Teyssier</span> , <span>Jürgen Steimle</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2022-nittala.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2022-nittala.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/8Paq3P3EsKQ" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/8Paq3P3EsKQ?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/8Paq3P3EsKQ/maxresdefault.jpg src=https://img.youtube.com/vi/8Paq3P3EsKQ/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Designers and makers are increasingly interested in leveraging bio-based and bio-degradable ‘do-it-yourself’ (DIY) materials for sustainable prototyping. Their self-produced bioplastics possess compelling properties such as self-adhesion but have so far not been functionalized to create soft interactive devices, due to a lack of DIY techniques for the fabrication of functional electronic circuits and sensors. In this paper, we contribute a DIY approach for creating Interactive Bioplastics that is accessible to a wide audience, making use of easy-to-obtain bio-based raw materials and familiar tools. We present three types of conductive bioplastic materials and their formulation: sheets, pastes and foams. Our materials enable additive and subtractive fabrication of soft circuits and sensors. Furthermore, we demonstrate how these materials can substitute conventional prototyping materials, be combined with off-the-shelf electronics, and be fed into a sustainable material ‘life-cycle’ including disassembly, re-use, and re-melting of materials. A formal characterization of our conductors highlights that they are even on-par with commercially available carbon-based conductive pastes.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Bioplastics</span><span class="ui brown basic label">Biomaterials</span><span class="ui brown basic label">Do It Yourself</span><span class="ui brown basic label">DIY</span><span class="ui brown basic label">Sustainability</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Marion Koelle<!-- -->, <!-- -->Madalina Nicolae<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Marc Teyssier<!-- -->, <!-- -->Jürgen Steimle<!-- -->. <b>Prototyping Soft Devices with Interactive Bioplastics</b>. <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- -->(<!-- -->UIST 2022<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->16<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3526113.3545623" target="_blank">https://doi.org/10.1145/3526113.3545623</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-sic-2022-faridan" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/uist-sic-2022-faridan/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-sic-2022-faridan</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST SIC 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-sic-2022-faridan cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-sic-2022-faridan.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-sic-2022-faridan.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-sic-2022-faridan" target="_blank">UltraBots: Large-Area Mid-Air Haptics for VR with Robotically Actuated Ultrasound Transducers</a></h1><p class="meta"><a href="/people/mehrad-faridan"><img alt="mehrad-faridan photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/mehrad-faridan.jpg 1x" src="/pr-preview/pr-121/static/images/people/mehrad-faridan.jpg"/><strong>Mehrad Faridan</strong></a> , <a href="/people/marcus-friedel"><img alt="marcus-friedel photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/marcus-friedel.jpg 1x" src="/pr-preview/pr-121/static/images/people/marcus-friedel.jpg"/><strong>Marcus Friedel</strong></a> , <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-sic-2022-faridan.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-sic-2022-faridan.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/jMYAQzzQ_PI" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/jMYAQzzQ_PI?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/jMYAQzzQ_PI/maxresdefault.jpg src=https://img.youtube.com/vi/jMYAQzzQ_PI/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce UltraBots, a system that combines ultrasound haptic feedback and robotic actuation for large-area mid-air haptics for VR. Ultrasound haptics can provide precise mid-air haptic feedback and versatile shape rendering, but the interaction area is often limited by the small size of the ultrasound devices, restricting the possible interactions for VR. To address this problem, this paper introduces a novel approach that combines robotic actuation with ultrasound haptics. More specifically, we will attach ultrasound transducer arrays to tabletop mobile robots or robotic arms for scalable, extendable, and translatable interaction areas. We plan to use Sony Toio robots for 2D translation and/or commercially available robotic arms for 3D translation. Using robotic actuation and hand tracking measured by a VR HMD (ex: Oculus Quest), our system can keep the ultrasound transducers underneath the user’s hands to provide on-demand haptics. We demonstrate applications with workspace environments, medical training, education and entertainment.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Ultrasound Transducers</span><span class="ui brown basic label">Robotics</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Mehrad Faridan<!-- -->, <!-- -->Marcus Friedel<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>UltraBots: Large-Area Mid-Air Haptics for VR with Robotically Actuated Ultrasound Transducers</b>. <i>(<!-- -->UIST SIC 2022<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->3<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3526114.3561350" target="_blank">https://doi.org/10.1145/3526114.3561350</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="iros-2022-suzuki" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/iros-2022-suzuki/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>iros-2022-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">IROS 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="iros-2022-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/iros-2022-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/iros-2022-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/iros-2022-suzuki" target="_blank">Selective Self-Assembly using Re-Programmable Magnetic Pixels</a></h1><p class="meta"><span>Martin Nisser</span> , <span>Yashaswini Makaram</span> , <span>Faraz Faruqi</span> , <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a> , <span>Stefanie Mueller</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/iros-2022-suzuki.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>iros-2022-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/HK9_ynH6A6w" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/HK9_ynH6A6w?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/HK9_ynH6A6w/maxresdefault.jpg src=https://img.youtube.com/vi/HK9_ynH6A6w/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces a method to generate highly selective encodings that can be magnetically programmed onto physical modules to enable them to self-assemble in chosen configurations. We generate these encodings based on Hadamard matrices, and show how to design the faces of modules to be maximally attractive to their intended mate, while remaining maximally agnostic to other faces. We derive guarantees on these bounds, and verify their attraction and agnosticism experimentally. Using cubic modules whose faces have been covered in soft magnetic material, we show how inexpensive, passive modules with planar faces can be used to selectively self-assemble into target shapes without geometric guides. We show that these modules can be easily re-programmed for new target shapes using a CNC-based magnetic plotter, and demonstrate self-assembly of 8 cubes in a water tank.</p></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Martin Nisser<!-- -->, <!-- -->Yashaswini Makaram<!-- -->, <!-- -->Faraz Faruqi<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Stefanie Mueller<!-- -->. <b>Selective Self-Assembly using Re-Programmable Magnetic Pixels</b>. <i>In <!-- -->Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems<!-- -->(<!-- -->IROS 2022<!-- -->)</i>. <!-- -->IEEE, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->8<!-- -->. </p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="gecco-2022-ivanov" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/gecco-2022-ivanov/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>gecco-2022-ivanov</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">GECCO 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="gecco-2022-ivanov cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/gecco-2022-ivanov.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/gecco-2022-ivanov.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/gecco-2022-ivanov" target="_blank">EvoIsland: Interactive Evolution via an Island-Inspired Spatial User Interface Framework</a></h1><p class="meta"><a href="/people/sasha-ivanov"><img alt="sasha-ivanov photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sasha-ivanov.jpg 1x" src="/pr-preview/pr-121/static/images/people/sasha-ivanov.jpg"/><strong>Sasha Ivanov</strong></a> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a> , <span>Christian Jacob</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/gecco-2022-ivanov.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>gecco-2022-ivanov.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present EvoIsland, a scalable interactive evolutionary user interface framework inspired by the spatially isolated land masses seen on Earth. Our generalizable interaction system encourages creators to spatially explore a wide range of design possibilities through the combination, separation, and rearrangement of hexagonal tiles on a grid. As these tiles are grouped into islandlike clusters, localized populations of designs form through an underlying evolutionary system. The interactions that take place within EvoIsland provide content creators with new ways to shape, display and assess populations in evolutionary systems that produce a wide range of solutions with visual phenotype outputs.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Interactive Evolutionary Systems</span><span class="ui brown basic label">User Interfaces</span><span class="ui brown basic label">Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sasha Ivanov<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Christian Jacob<!-- -->. <b>EvoIsland: Interactive Evolution via an Island-Inspired Spatial User Interface Framework</b>. <i>(<!-- -->GECCO 2022<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->8<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3512290.3528722" target="_blank">https://doi.org/10.1145/3512290.3528722</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="nime-2022-frisson" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/nime-2022-frisson/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>nime-2022-frisson</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">NIME 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="nime-2022-frisson cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/nime-2022-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/nime-2022-frisson.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/nime-2022-frisson" target="_blank">ForceHost: an open-source toolchain for generating firmware embedding the authoring and rendering of audio and force-feedback haptics</a></h1><p class="meta"><a href="/people/christian-frisson"><img alt="christian-frisson photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/people/christian-frisson.jpg"/><strong>Christian Frisson</strong></a> , <span>Mathias Kirkegaard</span> , <span>Thomas Pietrzak</span> , <span>Marcelo M. Wanderley</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/nime-2022-frisson.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>nime-2022-frisson.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/smFpkdw-J2w" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/smFpkdw-J2w?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/smFpkdw-J2w/maxresdefault.jpg src=https://img.youtube.com/vi/smFpkdw-J2w/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>ForceHost is an opensource toolchain for generating firmware that hosts authoring  and rendering of force-feedback and audio signals and that communicates through I2C  with guest motor and sensor boards. With ForceHost, the stability of audio and haptic  loops is no longer delegated to and dependent on operating systems and drivers, and  devices remain discoverable beyond planned obsolescence. We modified Faust, a highlevel language and compiler for real-time audio digital signal processing, to support  haptics. Our toolchain compiles audio-haptic firmware applications with Faust and  embeds web-based UIs exposing their parameters. We validate our toolchain by  example applications and modifications of integrated development environments:  script-based programming examples of haptic firmware applications with our haptic1D  Faust library, visual programming by mapping input and output signals between audio  and haptic devices in Webmapper, visual programming with physically-inspired massinteraction models in Synth-a-Modeler Designer. We distribute the documentation and  source code of ForceHost and all of its components and forks.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Digital Musical Instrument</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Force Feedback</span><span class="ui brown basic label">Authoring</span><span class="ui brown basic label">Mapping</span><span class="ui brown basic label">Embedded Computing</span><span class="ui brown basic label">Torque Tuner</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Christian Frisson<!-- -->, <!-- -->Mathias Kirkegaard<!-- -->, <!-- -->Thomas Pietrzak<!-- -->, <!-- -->Marcelo M. Wanderley<!-- -->. <b>ForceHost: an open-source toolchain for generating firmware embedding the authoring and rendering of audio and force-feedback haptics</b>. <i>(<!-- -->NIME 2022<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->34<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.21428/92fbeb44.76cfc96e" target="_blank">https://doi.org/10.21428/92fbeb44.76cfc96e</a></p></div></div><div class="block"><h1>Materials</h1><div class="ui horizontal small divided link list"><div class="item"><a href="https://gitlab.com/ForceHost" target="_blank" style="font-size:1.2em"><svg data-prefix="fab" data-icon="gitlab" class="svg-inline--fa fa-gitlab" role="img" viewBox="0 0 512 512" aria-hidden="true"><path fill="currentColor" d="M504 204.6l-.7-1.8-69.7-181.8c-1.4-3.6-3.9-6.6-7.2-8.6-2.4-1.6-5.1-2.5-8-2.8s-5.7 .1-8.4 1.1-5.1 2.7-7.1 4.8c-1.9 2.1-3.3 4.7-4.1 7.4l-47 144-190.5 0-47.1-144c-.8-2.8-2.2-5.3-4.1-7.4-2-2.1-4.4-3.7-7.1-4.8-2.6-1-5.5-1.4-8.4-1.1s-5.6 1.2-8 2.8c-3.2 2-5.8 5.1-7.2 8.6L9.8 202.8 9 204.6c-10 26.2-11.3 55-3.5 82 7.7 26.9 24 50.7 46.4 67.6l.3 .2 .6 .4 106 79.5c38.5 29.1 66.7 50.3 84.6 63.9 3.7 1.9 8.3 4.3 13 4.3s9.3-2.4 13-4.3c17.9-13.5 46.1-34.9 84.6-63.9l106.7-79.9 .3-.3c22.4-16.9 38.7-40.6 45.6-67.5 8.6-27 7.4-55.8-2.6-82z"></path></svg>https://gitlab.com/ForceHost</a></div></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="gi-2022-hull" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/gi-2022-hull/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>gi-2022-hull</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">GI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="gi-2022-hull cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/gi-2022-hull.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/gi-2022-hull.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/gi-2022-hull" target="_blank">Simultaneous Worlds: Supporting Fluid Exploration of Multiple Data Sets via Physical Models</a></h1><p class="meta"><a href="/people/carmen-hull"><img alt="carmen-hull photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/carmen-hull.jpg 1x" src="/pr-preview/pr-121/static/images/people/carmen-hull.jpg"/><strong>Carmen Hull</strong></a> , <a href="/people/soren-knudsen"><img alt="soren-knudsen photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/soren-knudsen.jpg 1x" src="/pr-preview/pr-121/static/images/people/soren-knudsen.jpg"/><strong>Søren Knudsen</strong></a> , <a href="/people/sheelagh-carpendale"><img alt="sheelagh-carpendale photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg"/><strong>Sheelagh Carpendale</strong></a> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/gi-2022-hull.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>gi-2022-hull.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We take the well-established use of physical scale models in architecture and identify new opportunities for using them to interactively visualize and examine multiple streams of geospatial data. Overlaying, comparing, or integrating visualizations of complementary data sets in the same physical space is often challenging given the constraints of various data types and the limited design space of possible visual encodings. Our vision of “simultaneous worlds” uses physical models as a substrate upon which visualizations of multiple data streams can be dynamically and concurrently integrated. To explore the potential of this concept, we created three design explorations that use an illuminated campus model to integrate visualizations about building energy use, climate, and movement paths on a university campus. We use a research through design approach, documenting how our interdisciplinary collaborations with domain experts, students, and architects informed our designs. Based on our observations, we characterize the benefits of models for 1) situating visualizations, 2) composing visualizations, and 3) manipulating and authoring visualizations. Our work highlights the potential of physical models to support embodied exploration of spatial and non-spatial visualizations through fluid interactions.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Interactive Surfaces</span><span class="ui brown basic label">Data Physicalization</span><span class="ui brown basic label">Architectural Models</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Carmen Hull<!-- -->, <!-- -->Søren Knudsen<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Simultaneous Worlds: Supporting Fluid Exploration of Multiple Data Sets via Physical Models</b>. <i>(<!-- -->GI 2022<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->10<!-- -->. <!-- -->DOI: <a href="http://hdl.handle.net/1880/114742" target="_blank">http://hdl.handle.net/1880/114742</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2022-bressa" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2022-bressa/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2022-bressa</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2022-bressa cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2022-bressa.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2022-bressa.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2022-bressa" target="_blank">Data Every Day: Designing and Living with Personal Situated Visualizations</a></h1><p class="meta"><a href="/people/nathalie-bressa"><img alt="nathalie-bressa photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/nathalie-bressa.jpg 1x" src="/pr-preview/pr-121/static/images/people/nathalie-bressa.jpg"/><strong>Nathalie Bressa</strong></a> , <span>Jo Vermeulen</span> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2022-bressa.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2022-bressa.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/B0bKMgDd1xY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/B0bKMgDd1xY?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/B0bKMgDd1xY/maxresdefault.jpg src=https://img.youtube.com/vi/B0bKMgDd1xY/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We explore the design and utility of situated manual self-tracking visualizations on dedicated displays that integrate data tracking into existing practices and physical environments. Situating self-tracking tools in relevant locations is a promising approach to enable reflection on and awareness of data without needing to rely on sensorized tracking or personal devices. In both a long-term autobiographical design process and a co-design study with six participants, we rapidly prototyped and deployed 30 situated self-tracking applications over a ten month period. Grounded in the experience of designing and living with these trackers, we contribute findings on logging and data entry, the use of situated displays, and the visual design and customization of trackers. Our results demonstrate the potential of customizable dedicated self-tracking visualizations that are situated in relevant physical spaces, and suggest future research opportunities and new potential applications for situated visualizations.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Self Tracking</span><span class="ui brown basic label">Situated Visualization</span><span class="ui brown basic label">Personal Data</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nathalie Bressa<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Data Every Day: Designing and Living with Personal Situated Visualizations</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2022<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->18<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3491102.3517737" target="_blank">https://doi.org/10.1145/3491102.3517737</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2022-ivanov" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2022-ivanov/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2022-ivanov</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2022-ivanov cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2022-ivanov.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2022-ivanov.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2022-ivanov" target="_blank">One Week in the Future: Previs Design Futuring for HCI Research</a></h1><p class="meta"><a href="/people/sasha-ivanov"><img alt="sasha-ivanov photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sasha-ivanov.jpg 1x" src="/pr-preview/pr-121/static/images/people/sasha-ivanov.jpg"/><strong>Sasha Ivanov</strong></a> , <a href="/people/tim-au-yeung"><img alt="tim-au-yeung photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/tim-au-yeung.jpg 1x" src="/pr-preview/pr-121/static/images/people/tim-au-yeung.jpg"/><strong>Tim Au Yeung</strong></a> , <a href="/people/kathryn-blair"><img alt="kathryn-blair photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kathryn-blair.jpg 1x" src="/pr-preview/pr-121/static/images/people/kathryn-blair.jpg"/><strong>Kathryn Blair</strong></a> , <a href="/people/kurtis-danyluk"><img alt="kurtis-danyluk photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg"/><strong>Kurtis Danyluk</strong></a> , <a href="/people/georgina-freeman"><img alt="georgina-freeman photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/georgina-freeman.jpg 1x" src="/pr-preview/pr-121/static/images/people/georgina-freeman.jpg"/><strong>Georgina Freeman</strong></a> , <a href="/people/marcus-friedel"><img alt="marcus-friedel photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/marcus-friedel.jpg 1x" src="/pr-preview/pr-121/static/images/people/marcus-friedel.jpg"/><strong>Marcus Friedel</strong></a> , <a href="/people/carmen-hull"><img alt="carmen-hull photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/carmen-hull.jpg 1x" src="/pr-preview/pr-121/static/images/people/carmen-hull.jpg"/><strong>Carmen Hull</strong></a> , <a href="/people/michael-hung"><img alt="michael-hung photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/michael-hung.jpg 1x" src="/pr-preview/pr-121/static/images/people/michael-hung.jpg"/><strong>Michael Hung</strong></a> , <a href="/people/sydney-pratte"><img alt="sydney-pratte photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sydney-pratte.jpg 1x" src="/pr-preview/pr-121/static/images/people/sydney-pratte.jpg"/><strong>Sydney Pratte</strong></a> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2022-ivanov.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2022-ivanov.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/qoIwYW83iSU" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/qoIwYW83iSU?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/qoIwYW83iSU/maxresdefault.jpg src=https://img.youtube.com/vi/qoIwYW83iSU/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We explore the use of cinematic “pre-visualization” (previs) techniques as a rapid ideation and design futuring method for human computer interaction (HCI) research. Previs approaches, which are widely used in animation and film production, use digital design tools to create medium-fidelity videos that capture richer interaction, motion, and context than sketches or static illustrations. When used as a design futuring method, previs can facilitate rapid, iterative discussions that reveal tensions, challenges, and opportunities for new research. We performed eight one-week design futuring sprints, in which individual HCI researchers collaborated with a lead designer to produce concept sketches, storyboards, and videos that examined future applications of their research. From these experiences, we identify recurring themes and challenges and present a One Week Futuring Workbook that other researchers can use to guide their own futuring sprints. We also highlight how variations of our approach could support other speculative design practices.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Design Futuring</span><span class="ui brown basic label">Prototyping</span><span class="ui brown basic label">Previsualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sasha Ivanov<!-- -->, <!-- -->Tim Au Yeung<!-- -->, <!-- -->Kathryn Blair<!-- -->, <!-- -->Kurtis Danyluk<!-- -->, <!-- -->Georgina Freeman<!-- -->, <!-- -->Marcus Friedel<!-- -->, <!-- -->Carmen Hull<!-- -->, <!-- -->Michael Hung<!-- -->, <!-- -->Sydney Pratte<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>One Week in the Future: Previs Design Futuring for HCI Research</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2022<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->15<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3491102.3517584" target="_blank">https://doi.org/10.1145/3491102.3517584</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2022-nittala" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2022-nittala/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2022-nittala</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2022-nittala cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2022-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2022-nittala.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2022-nittala" target="_blank">Next Steps in Epidermal Computing: Opportunities and Challenges for Soft On-Skin Devices</a></h1><p class="meta"><a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-121/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a> , <span>Jürgen Steimle</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2022-nittala.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2022-nittala.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Lj9Yk5IQsok" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Lj9Yk5IQsok?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/Lj9Yk5IQsok/maxresdefault.jpg src=https://img.youtube.com/vi/Lj9Yk5IQsok/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Skin is a promising interaction medium and has been widely explored for mobile, and expressive interaction. Recent research in HCI has seen the development of Epidermal Computing Devices: ultra-thin and non-invasive devices which reside on the user’s skin, offering intimate integration with the curved surfaces of the body, while having physical and mechanical properties that are akin to skin, expanding the horizon of on-body interaction. However, with rapid technological advancements in multiple disciplines, we see a need to synthesize the main open research questions and opportunities for the HCI community to advance future research in this area. By systematically analyzing Epidermal Devices contributed in the HCI community, physical sciences research and from our experiences in designing and building Epidermal Devices, we identify opportunities and challenges for advancing research across five themes. This multi-disciplinary synthesis enables multiple research communities to facilitate progression towards more coordinated endeavors for advancing Epidermal Computing.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Wearable Devices</span><span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Survey</span><span class="ui brown basic label">Soft Wearables</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Aditya Shekhar Nittala<!-- -->, <!-- -->Jürgen Steimle<!-- -->. <b>Next Steps in Epidermal Computing: Opportunities and Challenges for Soft On-Skin Devices</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2022<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->22<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3491102.3517668" target="_blank">https://doi.org/10.1145/3491102.3517668</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2022-suzuki" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2022-suzuki/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2022-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2022-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2022-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2022-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2022-suzuki" target="_blank">Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a> , <a href="/people/adnan-karim"><img alt="adnan-karim photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/adnan-karim.jpg 1x" src="/pr-preview/pr-121/static/images/people/adnan-karim.jpg"/><strong>Adnan Karim</strong></a> , <a href="/people/tian-xia"><img alt="tian-xia photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/tian-xia.jpg 1x" src="/pr-preview/pr-121/static/images/people/tian-xia.jpg"/><strong>Tian Xia</strong></a> , <span>Hooman Hedayati</span> , <a href="/people/nicolai-marquardt"><img alt="nicolai-marquardt photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/nicolai-marquardt.jpg 1x" src="/pr-preview/pr-121/static/images/people/nicolai-marquardt.jpg"/><strong>Nicolai Marquardt</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2022-suzuki.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2022-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/MvOWxQC_4uQ" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/MvOWxQC_4uQ?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/MvOWxQC_4uQ/maxresdefault.jpg src=https://img.youtube.com/vi/MvOWxQC_4uQ/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper contributes to a taxonomy of augmented reality and robotics based on a survey of 460 research papers. Augmented and mixed reality (AR/MR) have emerged as a new way to enhance human-robot interaction (HRI) and robotic interfaces (e.g., actuated and shape-changing interfaces). Recently, an increasing number of studies in HCI, HRI, and robotics have demonstrated how AR enables better interactions between people and robots. However, often research remains focused on individual explorations and key design strategies, and research questions are rarely analyzed systematically. In this paper, we synthesize and categorize this research field in the following dimensions: 1) approaches to augmenting reality; 2) characteristics of robots; 3) purposes and benefits; 4) classification of presented information; 5) design components and strategies for visual augmentation; 6) interaction techniques and modalities; 7) application domains; and 8) evaluation strategies. We formulate key challenges and opportunities to guide and inform future research in AR and robotics.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Robotics</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Actuated Tangible UI</span><span class="ui brown basic label">Shape Changing UI</span><span class="ui brown basic label">AR HRI</span><span class="ui brown basic label">VAM HRI</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Adnan Karim<!-- -->, <!-- -->Tian Xia<!-- -->, <!-- -->Hooman Hedayati<!-- -->, <!-- -->Nicolai Marquardt<!-- -->. <b>Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2022<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->33<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3491102.3517719" target="_blank">https://doi.org/10.1145/3491102.3517719</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-ea-2022-blair" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-ea-2022-blair/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-ea-2022-blair</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI EA 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-ea-2022-blair cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-ea-2022-blair.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-ea-2022-blair.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-ea-2022-blair" target="_blank">Art is Not Research. Research is not Art</a></h1><p class="meta"><a href="/people/kathryn-blair"><img alt="kathryn-blair photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kathryn-blair.jpg 1x" src="/pr-preview/pr-121/static/images/people/kathryn-blair.jpg"/><strong>Kathryn Blair</strong></a> , <span>Miriam Sturdee</span> , <span>Lindsay Macdonald Vermeulen</span> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-ea-2022-blair.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-ea-2022-blair.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/G0CSYn_uhIE" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/G0CSYn_uhIE?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/G0CSYn_uhIE/maxresdefault.jpg src=https://img.youtube.com/vi/G0CSYn_uhIE/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Art is not Research. Research is not Art. is a multimedia, multi-site participatory installation by a collective of artists and researchers from Calgary, Toronto, and Lancaster; it is informed by these contexts. It reflects the tensions between how “participants” are treated in participatory art and interaction research. It offers a framework through which we can explore how epistemologies might evolve in a blending between Art and Research. Visitors download the paper to read, critically reflect on the relationship between art and research, and experientially engage with the material through a series of creative prompts. A performance variation of the piece will be performed in-person and online through the ACM SIGCHI Conference on Human Factors in Computing Systems alt.chi track.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Interdisciplinary Research</span><span class="ui brown basic label">Research Ethics</span><span class="ui brown basic label">Arts And Computing</span><span class="ui brown basic label">Research Methods</span><span class="ui brown basic label">Knowledge Creation</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kathryn Blair<!-- -->, <!-- -->Miriam Sturdee<!-- -->, <!-- -->Lindsay Macdonald Vermeulen<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Art is Not Research. Research is not Art</b>. <i>In <!-- -->Extended Abstracts of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI EA 2022<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->9<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3491101.3516391" target="_blank">https://doi.org/10.1145/3491101.3516391</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="frobt-2022-suzuki" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/frobt-2022-suzuki/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>frobt-2022-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Frontiers 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="frobt-2022-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/frobt-2022-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/frobt-2022-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/frobt-2022-suzuki" target="_blank">Designing Expandable-Structure Robots for Human-Robot Interaction</a></h1><p class="meta"><span>Hooman Hedayati</span> , <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a> , <span>Wyatt Rees1</span> , <span>Daniel Leithinger</span> , <span>Daniel Szafir</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/frobt-2022-suzuki.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>frobt-2022-suzuki.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this paper, we survey the emerging design space of expandable structures in robotics, with a focus on how such structures may improve human-robot interactions. We detail various implementation considerations for researchers seeking to integrate such structures in their own work and describe how expandable structures may lead to novel forms of interaction for a variety of different robots and applications, including structures that enable robots to alter their form to augment or gain entirely new capabilities, such as enhancing manipulation or navigation, structures that improve robot safety, structures that enable new forms of communication, and structures for robot swarms that enable the swarm to change shape both individually and collectively. To illustrate how these considerations may be operationalized, we also present three case studies from our own research in expandable structure robots, sharing our design process and our findings regarding how such structures enable robots to produce novel behaviors that may capture human attention, convey information, mimic emotion, and provide new types of dynamic affordances.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Deployable Robot</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Modular Robot</span><span class="ui brown basic label">Origami Robotics</span><span class="ui brown basic label">Deployable Structures</span><span class="ui brown basic label">Shape Changing Robots</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Hooman Hedayati<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Wyatt Rees1<!-- -->, <!-- -->Daniel Leithinger<!-- -->, <!-- -->Daniel Szafir<!-- -->. <b>Designing Expandable-Structure Robots for Human-Robot Interaction</b>. <i>(<!-- -->Frontiers 2022<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->22<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.3389/frobt.2022.719639" target="_blank">https://doi.org/10.3389/frobt.2022.719639</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2021-suzuki" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/uist-2021-suzuki/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2021-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2021-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2021-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2021-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2021-suzuki" target="_blank">HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a> , <span>Eyal Ofek</span> , <span>Mike Sinclair</span> , <span>Daniel Leithinger</span> , <span>Mar Gonzalez-Franco</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2021-suzuki.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2021-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/HTiZgOESJyQ" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/HTiZgOESJyQ?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/HTiZgOESJyQ/maxresdefault.jpg src=https://img.youtube.com/vi/HTiZgOESJyQ/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic ap- proaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability---these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in- time touch-points on the user’s hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various ap- plications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Encountered Type Haptics</span><span class="ui brown basic label">Tabletop Mobile Robots</span><span class="ui brown basic label">Swarm User Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Eyal Ofek<!-- -->, <!-- -->Mike Sinclair<!-- -->, <!-- -->Daniel Leithinger<!-- -->, <!-- -->Mar Gonzalez-Franco<!-- -->. <b>HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots</b>. <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- -->(<!-- -->UIST 2021<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->16<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3472749.3474821" target="_blank">https://doi.org/10.1145/3472749.3474821</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="ieee-2021-willett" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/ieee-2021-willett/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>ieee-2021-willett</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">IEEE 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="ieee-2021-willett cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/ieee-2021-willett.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/ieee-2021-willett.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/ieee-2021-willett" target="_blank">Perception! Immersion! Empowerment!: Superpowers as Inspiration for Visualization</a></h1><p class="meta"><a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a> , <a href="/people/bon-adriel-aseniero"><img alt="bon-adriel-aseniero photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/bon-adriel-aseniero.jpg 1x" src="/pr-preview/pr-121/static/images/people/bon-adriel-aseniero.jpg"/><strong>Bon Adriel Aseniero</strong></a> , <a href="/people/sheelagh-carpendale"><img alt="sheelagh-carpendale photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg"/><strong>Sheelagh Carpendale</strong></a> , <span>Pierre Dragicevic</span> , <span>Yvonne Jansen</span> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a> , <a href="/people/petra-isenberg"><img alt="petra-isenberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/petra-isenberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/petra-isenberg.jpg"/><strong>Petra Isenberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We explore how the lens of fictional superpowers can help characterize how visualizations empower people and provide inspiration for new visualization systems. Researchers and practitioners often tout visualizations&#x27; ability to “make the invisible visible” and to “enhance cognitive abilities.” Meanwhile superhero comics and other modern fiction often depict characters with similarly fantastic abilities that allow them to see and interpret the world in ways that transcend traditional human perception. We investigate the intersection of these domains, and show how the language of superpowers can be used to characterize existing visualization systems and suggest opportunities for new and empowering ones. We introduce two frameworks: The first characterizes seven underlying mechanisms that form the basis for a variety of visual superpowers portrayed in fiction. The second identifies seven ways in which visualization tools and interfaces can instill a sense of empowerment in the people who use them. Building on these observations, we illustrate a diverse set of “visualization superpowers” and highlight opportunities for the visualization community to create new systems and interactions that empower new experiences with data. Material and illustrations are available under CC-BY 4.0 at osf.io/8yhfz.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Cognition</span><span class="ui brown basic label">Interactive Systems</span><span class="ui brown basic label">Tools</span><span class="ui brown basic label">Pragmatics</span><span class="ui brown basic label">Pattern Recognition</span><span class="ui brown basic label">Superpowers</span><span class="ui brown basic label">Empowerment</span><span class="ui brown basic label">Vision</span><span class="ui brown basic label">Perception</span><span class="ui brown basic label">Fiction</span><span class="ui brown basic label">Situated Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Wesley Willett<!-- -->, <!-- -->Bon Adriel Aseniero<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->, <!-- -->Pierre Dragicevic<!-- -->, <!-- -->Yvonne Jansen<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Petra Isenberg<!-- -->. <b>Perception! Immersion! Empowerment!: Superpowers as Inspiration for Visualization</b>. <i>(<!-- -->IEEE 2021<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->11<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1109/TVCG.2021.3114844" target="_blank">10.1109/TVCG.2021.3114844</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2021-asha" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/dis-2021-asha/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2021-asha</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2021-asha cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2021-asha.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2021-asha.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2021-asha" target="_blank">Co-Designing Interactions between Pedestrians in Wheelchairs and Autonomous Vehicles</a></h1><p class="meta"><a href="/people/ashratuz-zavin-asha"><img alt="ashratuz-zavin-asha photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ashratuz-zavin-asha.jpg 1x" src="/pr-preview/pr-121/static/images/people/ashratuz-zavin-asha.jpg"/><strong>Ashratuz Zavin Asha</strong></a> , <a href="/people/christopher-smith"><img alt="christopher-smith photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christopher-smith.jpg 1x" src="/pr-preview/pr-121/static/images/people/christopher-smith.jpg"/><strong>Christopher Smith</strong></a> , <a href="/people/georgina-freeman"><img alt="georgina-freeman photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/georgina-freeman.jpg 1x" src="/pr-preview/pr-121/static/images/people/georgina-freeman.jpg"/><strong>Georgina Freeman</strong></a> , <span>Sean Crump</span> , <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a> , <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In the near future, mixed traffic consisting of manual and autonomous vehicles (AVs) will be common. Questions surrounding how vulnerable road users such as pedestrians in wheelchairs (PWs) will make crossing decisions in these new situations are underexplored. We conducted a remote co-design study with one of the researchers of this work who has the lived experience as a powered wheelchair user and applied inclusive design practices. This allowed us to identify and reflect on interface design ideas that can help PWs make safe crossing decisions at intersections. Through an iterative five-week study, we implemented interfaces that can be placed on the vehicle, on the wheelchair, and on the street infrastructure and evaluated them during the co-design sessions using a VR simulator testbed. Informed by our findings, we discuss design insights for implementing inclusive interfaces to improve interactions between autonomous vehicles and vulnerable road users.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Pedestrians In Wheelchairs</span><span class="ui brown basic label">Co Design</span><span class="ui brown basic label">Autonomous Vehicles</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ashratuz Zavin Asha<!-- -->, <!-- -->Christopher Smith<!-- -->, <!-- -->Georgina Freeman<!-- -->, <!-- -->Sean Crump<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>Co-Designing Interactions between Pedestrians in Wheelchairs and Autonomous Vehicles</b>. <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- -->(<!-- -->DIS 2021<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3461778.3462068" target="_blank">10.1145/3461778.3462068</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2021-blair" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/dis-2021-blair/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2021-blair</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2021-blair cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2021-blair.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2021-blair.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2021-blair" target="_blank">Participatory Art for Public Exploration of Algorithmic Decision-Making</a></h1><p class="meta"><a href="/people/kathryn-blair"><img alt="kathryn-blair photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kathryn-blair.jpg 1x" src="/pr-preview/pr-121/static/images/people/kathryn-blair.jpg"/><strong>Kathryn Blair</strong></a> , <span>Pil Hansen</span> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/dis-2021-blair.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>dis-2021-blair.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Machine learning and predictive algorithms find patterns in large stores of data and make predictions which corporations and governments use to support decision-making. Yet, the system&#x27;s representation of reality can be more influential to outcomes than the complexities of daily life. They become problematic when they undermine the inclusivity of public decision making, and when their use perpetuates social or economic inequality. To address these challenges, the public must be able to participate in discourse about the implications of algorithmic systems. I propose a series of participatory installations exploring the impacts of algorithmic systems, providing contexts for active exploration of these concerns. I will conduct phenomenographic interviews to better understand how visitors experience art installations about technical topics, providing insight for subsequent installations. I will consolidate the results into a set of best practices about engaging the public on these topics.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Fine Arts</span><span class="ui brown basic label">Physical Artifact</span><span class="ui brown basic label">Education</span><span class="ui brown basic label">Social Impact Of Technology</span><span class="ui brown basic label">Participatory Art Installation</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kathryn Blair<!-- -->, <!-- -->Pil Hansen<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Participatory Art for Public Exploration of Algorithmic Decision-Making</b>. <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- -->(<!-- -->DIS 2021<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->4<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3461778.3462068" target="_blank">10.1145/3461778.3462068</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2021-wannamaker" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/dis-2021-wannamaker/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2021-wannamaker</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2021-wannamaker cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2021-wannamaker.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2021-wannamaker.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2021-wannamaker" target="_blank">I/O Bits: User-Driven, Situated, and Dedicated Self-Tracking</a></h1><p class="meta"><a href="/people/kendra-wannamaker"><img alt="kendra-wannamaker photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kendra-wannamaker.jpg 1x" src="/pr-preview/pr-121/static/images/people/kendra-wannamaker.jpg"/><strong>Kendra Wannamaker</strong></a> , <span>Sandeep Kollannur</span> , <a href="/people/marian-doerk"><img alt="marian-doerk photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/marian-doerk.jpg 1x" src="/pr-preview/pr-121/static/images/people/marian-doerk.jpg"/><strong>Marian Dörk</strong></a> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present I/O Bits, a prototype personal informatics system that explores the potential for user-driven and situated self-tracking. With simple tactile inputs and small e-paper visualizations, I/O Bits are dedicated physical devices that allow individuals to track and visualize different kinds of personal activities in-situ. This is in contrast to most self-tracking systems, which automate data collection, centralize information displays, or integrate into multi-purpose devices like smartwatches or mobile phones. We report findings from an e-paper visualization workshop and a prototype deployment where participants constructed their own I/O Bits and used them to track a range of personal data. Based on these experiences, we contribute insights and opportunities for situated and user-driven personal informatics.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Personal Informatics</span><span class="ui brown basic label">Situated Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kendra Wannamaker<!-- -->, <!-- -->Sandeep Kollannur<!-- -->, <!-- -->Marian Dörk<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>I/O Bits: User-Driven, Situated, and Dedicated Self-Tracking</b>. <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- -->(<!-- -->DIS 2021<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->10<!-- -->. <!-- -->DOI: <a href="http://hdl.handle.net/1880/113555" target="_blank">http://hdl.handle.net/1880/113555</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/yhMKURtgFZ0" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/yhMKURtgFZ0?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/yhMKURtgFZ0/maxresdefault.jpg src=https://img.youtube.com/vi/yhMKURtgFZ0/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2021-danyluk" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2021-danyluk/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2021-danyluk</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2021-danyluk cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2021-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2021-danyluk.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2021-danyluk" target="_blank">A Design Space Exploration of Worlds in Miniature</a></h1><p class="meta"><a href="/people/kurtis-danyluk"><img alt="kurtis-danyluk photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg"/><strong>Kurtis Danyluk</strong></a> , <span>Barrett Ens</span> , <span>Bernhard Jenny</span> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Worlds-in-Miniature (WiMs) are interactive worlds within a world and combine the advantages of an input space, a cartographicmap, and an overview+detail interface. They have been used across the extended virtuality spectrum for a variety of applications.Building on an analysis of examples of WiMs from the research literature we contribute a design space for WiMs based on sevendesign dimensions. Further, we expand upon existing definitions of WiMs to provide a definition that applies across the extendedreality spectrum. We identify the design dimensions of size-scope-scale, abstraction, geometry, reference frame, links, multiples, andvirtuality. Using our framework we describe existing Worlds-in-Miniature from the research literature and reveal unexplored researchareas. Finally, we generate new examples of WiMs using our framework to fill some of these gaps. With our findings, we identifyopportunities that can guide future research into WiMs.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Virtual Augmented Reality</span><span class="ui brown basic label">Meta Analysis Literature Survey</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kurtis Danyluk<!-- -->, <!-- -->Barrett Ens<!-- -->, <!-- -->Bernhard Jenny<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>A Design Space Exploration of Worlds in Miniature</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2021<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->20<!-- -->. </p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2021-ens" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2021-ens/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2021-ens</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2021-ens cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2021-ens.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2021-ens.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2021-ens" target="_blank">Grand Challenges in Immersive Analytics</a></h1><p class="meta"><span>Barrett Ens</span> , <span>Benjamin Bach</span> , <span>Maxime Cordeil</span> , <span>Ulrich Engelke</span> , <span>Marcos Serrano</span> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a> , <span>Arnaud Prouzeau</span> , <span>Christoph Anthes</span> , <span>Wolfgang Büschel</span> , <span>Cody Dunne</span> , <span>Tim Dwyer</span> , <span>Jens Grubert</span> , <span>Jason H. Haga</span> , <span>Nurit Kishenbaum</span> , <span>Dylan Kobayashi</span> , <span>Tica Lin</span> , <span>Monsurat Olaosebikan</span> , <span>Fabian Pointecker</span> , <span>David Saffo</span> , <span>Nazmus Saquib</span> , <span>Dieter Schmalsteig</span> , <span>Danielle Albers Szafir</span> , <span>Matthew Whitlock</span> , <span>Yalong Yang</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Immersive Analytics is a quickly evolving field that unites several areas such as visualisation, immersive environments, and human-computer interaction to support human data analysis with emerging technologies. This research has thrived over the past years with multiple workshops, seminars, and a growing body of publications, spanning several conferences. Given the rapid advancement of interaction technologies and novel application domains, this paper aims toward a broader research agenda to enable widespread adoption. We present 17 key research challenges developed over multiple sessions by a diverse group of 24 international experts, initiated from a virtual scientific workshop at ACM CHI 2020. These challenges aim to coordinate future work by providing a systematic roadmap of current directions and impending hurdles to facilitate productive and effective applications for Immersive Analytics.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Immersive Analytics</span><span class="ui brown basic label">Grand Research Challenges</span><span class="ui brown basic label">Data Visualisation</span><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Virtual Reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Barrett Ens<!-- -->, <!-- -->Benjamin Bach<!-- -->, <!-- -->Maxime Cordeil<!-- -->, <!-- -->Ulrich Engelke<!-- -->, <!-- -->Marcos Serrano<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Arnaud Prouzeau<!-- -->, <!-- -->Christoph Anthes<!-- -->, <!-- -->Wolfgang Büschel<!-- -->, <!-- -->Cody Dunne<!-- -->, <!-- -->Tim Dwyer<!-- -->, <!-- -->Jens Grubert<!-- -->, <!-- -->Jason H. Haga<!-- -->, <!-- -->Nurit Kishenbaum<!-- -->, <!-- -->Dylan Kobayashi<!-- -->, <!-- -->Tica Lin<!-- -->, <!-- -->Monsurat Olaosebikan<!-- -->, <!-- -->Fabian Pointecker<!-- -->, <!-- -->David Saffo<!-- -->, <!-- -->Nazmus Saquib<!-- -->, <!-- -->Dieter Schmalsteig<!-- -->, <!-- -->Danielle Albers Szafir<!-- -->, <!-- -->Matthew Whitlock<!-- -->, <!-- -->Yalong Yang<!-- -->. <b>Grand Challenges in Immersive Analytics</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2021<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->17<!-- -->. </p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2021-hammad" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2021-hammad/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2021-hammad</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2021-hammad cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2021-hammad.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2021-hammad.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2021-hammad" target="_blank">Homecoming: Exploring Returns to Long-Term Single Player Games</a></h1><p class="meta"><span>Noor Hammad</span> , <span>Owen Brierley</span> , <a href="/people/zachary-mckendrick"><img alt="zachary-mckendrick photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/zachary-mckendrick.jpg 1x" src="/pr-preview/pr-121/static/images/people/zachary-mckendrick.jpg"/><strong>Zachary McKendrick</strong></a> , <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a> , <span>Patrick Finn</span> , <span>Jessica Hammer</span> , <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present an autobiographical design journey exploring the experience of returning to long-term single player games. Continuing progress from a previously saved game, particularly when substantial time has passed, is an understudied area in games research. To begin our exploration in this domain, we investigated what the return experience is like first-hand. By returning to four long-term single player games played extensively in the past, we revealed a phenomenon we call The Pivot Point, a ‘eureka’ moment in return gameplay. The pivot point anchors our design explorations, where we created prototypes to leverage the pivot point in reconnecting with the experience. These return experiences and subsequent prototyping iterations inform our understanding of how to design better returns to gameplay, which can benefit both producers and consumers of long-term single player games.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Long Term Single Player Game</span><span class="ui brown basic label">Autobiographical Design</span><span class="ui brown basic label">Pivot Point</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Noor Hammad<!-- -->, <!-- -->Owen Brierley<!-- -->, <!-- -->Zachary McKendrick<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->Patrick Finn<!-- -->, <!-- -->Jessica Hammer<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>Homecoming: Exploring Returns to Long-Term Single Player Games</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2021<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3411764.3445357" target="_blank">https://doi.org/10.1145/3411764.3445357</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="gi-2021-mactavish" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/gi-2021-mactavish/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>gi-2021-mactavish</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">GI 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="gi-2021-mactavish cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/gi-2021-mactavish.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/gi-2021-mactavish.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/gi-2021-mactavish" target="_blank">Perspective Charts</a></h1><p class="meta"><span>Mia MacTavish</span> , <span>Katayoon Etemad</span> , <span>Faramarz Samavati</span> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Sp4Vt8mMhCs&amp;list=PLZQ0ePfDvRH4Ac7xfBdPlMQX0d0NYQ7Y-" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Sp4Vt8mMhCs&amp;list=PLZQ0ePfDvRH4Ac7xfBdPlMQX0d0NYQ7Y-?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/Sp4Vt8mMhCs&amp;list=PLZQ0ePfDvRH4Ac7xfBdPlMQX0d0NYQ7Y-/maxresdefault.jpg src=https://img.youtube.com/vi/Sp4Vt8mMhCs&amp;list=PLZQ0ePfDvRH4Ac7xfBdPlMQX0d0NYQ7Y-/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce three novel data visualizations, called perspective charts, based on the concept of size constancy in linear perspective projection. Bar charts are a popular and commonly used tool for the interpretation of datasets, however, representing datasets with multi-scale variation is challenging in a bar chart due to limitations in viewing space. Each of our designs focuses on the static representation of datasets with large ranges with respect to important variations in the data. Through a user study, we measure the effectiveness of our designs for representing these datasets in comparison to traditional methods, such as a standard bar chart or a broken-axis bar chart, and state-of-the-art methods, such as a scale-stack bar chart. The evaluation reveals that our designs allow pieces of data to be visually compared at a level of accuracy similar to traditional visualizations. Our designs demonstrate advantages when compared to state-of-the-art visualizations designed to represent datasets with large outliers.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Information Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Mia MacTavish<!-- -->, <!-- -->Katayoon Etemad<!-- -->, <!-- -->Faramarz Samavati<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Perspective Charts</b>. <i>(<!-- -->GI 2021<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->10<!-- -->. <!-- -->DOI: <a href="http://hdl.handle.net/1880/113671" target="_blank">http://hdl.handle.net/1880/113671</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="ijac-2021-hosseini" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/ijac-2021-hosseini/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>ijac-2021-hosseini</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">IJAC 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="ijac-2021-hosseini cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/ijac-2021-hosseini.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/ijac-2021-hosseini.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/ijac-2021-hosseini" target="_blank">Optically illusive architecture (OIA): Introduction and evaluation using virtual reality</a></h1><p class="meta"><span>Seyed Vahab Hosseini</span> , <span>Usman R Alim</span> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a> , <span>Joshua M Taron</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Architects and designers communicate their ideas within a range of representational methods. No single instance of these methods, either in the form of orthographic projections or perspectival representation, can address all questions regarding the design, but as a whole, they demonstrate a comprehensive range of information about the building or object they intend to represent. This explicates an inevitable degree of deficiency in representation, regardless of its type. In addition, perspective-based optical illusions manipulate our spatial perception by deliberately misrepresenting the reality. In this regard, they are not new concepts to architectural representation. As a consequence, Optically Illusive Architecture (OIA) is proposed, not as a solution to fill the gap between the representing and represented spaces, but as a design paradigm whose concept derives from and accounts for this gap. By OIA we aim to cast light to an undeniable role of viewpoints in designing architectural spaces. The idea is to establish a methodology in a way that the deficiency of current representational techniques—manifested as specific thread of optical illusions—flourishes into thoughtful results embodied as actual architectural spaces. Within our design paradigm, we define a framework to be able to effectively analyze its precedents, generate new space, and evaluate their efficiencies. Moreover, the framework raises a hierarchical set of questions to differentiate OIA from a visual gimmick. Furthermore, we study two OIA-driven environments, by conducting empirical studies using Virtual Reality (VR). These studies bear essential information, in terms of design performance, and the public’s ability to engage and interact with an OIA space, prior to the actual fabrication of the structures.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Architectural Representation</span><span class="ui brown basic label">Optical Illusion</span><span class="ui brown basic label">Design Evaluation</span><span class="ui brown basic label">Virtual Reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Seyed Vahab Hosseini<!-- -->, <!-- -->Usman R Alim<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Joshua M Taron<!-- -->. <b>Optically illusive architecture (OIA): Introduction and evaluation using virtual reality</b>. <i>(<!-- -->IJAC 2021<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->24<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1177/14780771211016600" target="_blank">10.1177/14780771211016600</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="cupum-2021-rout" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/cupum-2021-rout/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>cupum-2021-rout</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">Urban Informatics and Future Cities</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="cupum-2021-rout cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/cupum-2021-rout.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/cupum-2021-rout.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/cupum-2021-rout" target="_blank">(Big) Data in Urban Design Practice: Supporting High-Level Design Tasks Using a Visualization of Human Movement Data from Smartphones</a></h1><p class="meta"><span>Angela Rout</span> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Me8cU6RoCiA" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Me8cU6RoCiA?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/Me8cU6RoCiA/maxresdefault.jpg src=https://img.youtube.com/vi/Me8cU6RoCiA/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present the SmartCampus visualization tool, representing spatiotemporal data of over 200 student pathways and restpoints on a university campus. Based on our experiences with SmartCampus, we also propose a task-based framework that de-scribes how practicing urban designers (specifically, architects) can use human movement data visualizations in their work. Although extensive amounts of location data are produced daily by smartphones, existing geospatial tools are not customized to specifically support high-level urban design tasks. To help identify opportunities in urban design for visualizing human movement data from devices such as smartphones, we used our SmartCampus prototype to facilitate a series of 3 participatory design sessions (3 participants), a targeted online survey (14 participants), and semi-structured interviews (6 participants) with architectural experts. Our findings showcase the need for location analysis tools tailored to concrete urban design practices, and also highlight opportunities for Smart City researchers interested in developing domain specific, visualization tools.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Smartphone Data</span><span class="ui brown basic label">GPS</span><span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Architecture</span><span class="ui brown basic label">Urban Design</span><span class="ui brown basic label">Task Based Framework</span><span class="ui brown basic label">High Level Tasks</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Angela Rout<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>(Big) Data in Urban Design Practice: Supporting High-Level Design Tasks Using a Visualization of Human Movement Data from Smartphones</b>. <i>(<!-- -->Urban Informatics and Future Cities<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->301-318<!-- -->. <!-- -->DOI: <a href="http://hdl.handle.net/1880/113114" target="_blank">http://hdl.handle.net/1880/113114</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tei-2021-pratte" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/tei-2021-pratte/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>tei-2021-pratte</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TEI 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="tei-2021-pratte cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tei-2021-pratte.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tei-2021-pratte.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tei-2021-pratte" target="_blank">Evoking Empathy: A Framework for Describing Empathy Tools</a></h1><p class="meta"><a href="/people/sydney-pratte"><img alt="sydney-pratte photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sydney-pratte.jpg 1x" src="/pr-preview/pr-121/static/images/people/sydney-pratte.jpg"/><strong>Sydney Pratte</strong></a> , <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/tei-2021-pratte.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>tei-2021-pratte.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/JBCzPt5ILxo" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/JBCzPt5ILxo?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/JBCzPt5ILxo/maxresdefault.jpg src=https://img.youtube.com/vi/JBCzPt5ILxo/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Empathy tools are experiences designed to evoke empathetic responses by placing the user in another’s lived and felt experience. The problem is that designers do not have a common vocabulary to describe empathy tool experiences; consequently, it is difficult to compare/contrast empathy tool designs or to think about their efficacy. To address this problem, we analyzed 26 publications on empathy tools to develop a descriptive framework for designers of empathy tools. Based on our analysis, we found that empathy tools can be described along three dimensions: (i) the amount of agency the tool allows, (ii) the user’s perspective while using the tool, and (iii) the type of sensations that are experienced. We show that this framework can be used to describe a wide variety of empathy tools and provide recommendations for empathy tool designers, as well as techniques for measuring the efficacy of an empathy tool experience.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Empathy</span><span class="ui brown basic label">Empathy Tools</span><span class="ui brown basic label">Design Strategies</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sydney Pratte<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Evoking Empathy: A Framework for Describing Empathy Tools</b>. <i>In <!-- -->Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction<!-- -->(<!-- -->TEI 2021<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://dl.acm.org/doi/10.1145/3430524.3440644" target="_blank">https://dl.acm.org/doi/10.1145/3430524.3440644</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2020-suzuki" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/uist-2020-suzuki/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2020-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2020-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2020-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2020-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2020-suzuki" target="_blank">RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a> , <span>Rubaiat Habib Kazi</span> , <span>Li-Yi Wei</span> , <span>Stephen DiVerdi</span> , <span>Wilmot Li</span> , <span>Daniel Leithinger</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2020-suzuki.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2020-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/L0p-BNU9rXU" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/L0p-BNU9rXU?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/L0p-BNU9rXU/maxresdefault.jpg src=https://img.youtube.com/vi/L0p-BNU9rXU/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, floating in mid air without responding to the real world. This paper introduces a new way to embed dynamic and responsive graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-defined programs and configurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Embedded Data Visualization</span><span class="ui brown basic label">Real Time Authoring</span><span class="ui brown basic label">Sketching Interfaces</span><span class="ui brown basic label">Tangible Interaction</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Rubaiat Habib Kazi<!-- -->, <!-- -->Li-Yi Wei<!-- -->, <!-- -->Stephen DiVerdi<!-- -->, <!-- -->Wilmot Li<!-- -->, <!-- -->Daniel Leithinger<!-- -->. <b>RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching</b>. <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- -->(<!-- -->UIST 2020<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->16<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3379337.3415892" target="_blank">https://doi.org/10.1145/3379337.3415892</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2020-yixian" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/uist-2020-yixian/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2020-yixian</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2020-yixian cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2020-yixian.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2020-yixian.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2020-yixian" target="_blank">ZoomWalls: Dynamic Walls that Simulate Haptic Infrastructure for Room-scale VR World</a></h1><p class="meta"><span>Yan Yixian</span> , <span>Kazuki Takashima</span> , <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a> , <span>Takayuki Tanno</span> , <span>Kazuyuki Fujita</span> , <span>Yoshifumi Kitamura</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2020-yixian.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2020-yixian.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/j2iSNDkBxAY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/j2iSNDkBxAY?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/j2iSNDkBxAY/maxresdefault.jpg src=https://img.youtube.com/vi/j2iSNDkBxAY/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We focus on the problem of simulating the haptic infrastructure of a virtual environment (i.e. walls, doors). Our approach relies on multiple ZoomWalls---autonomous robotic encounter-type haptic wall-shaped props---that coordinate to provide haptic feedback for room-scale virtual reality. Based on a user&#x27;s movement through the physical space, ZoomWall props are coordinated through a predict-and-dispatch architecture to provide just-in-time haptic feedback for objects the user is about to touch. To refine our system, we conducted simulation studies of different prediction algorithms, which helped us to refine our algorithmic approach to realize the physical ZoomWall prototype. Finally, we evaluated our system through a user experience study, which showed that participants found that ZoomWalls increased their sense of presence in the VR environment. ZoomWalls represents an instance of autonomous mobile reusable props, which we view as an important design direction for haptics in VR.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Encountered Type Haptic Devices</span><span class="ui brown basic label">Immersive Experience</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Yan Yixian<!-- -->, <!-- -->Kazuki Takashima<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Takayuki Tanno<!-- -->, <!-- -->Kazuyuki Fujita<!-- -->, <!-- -->Yoshifumi Kitamura<!-- -->. <b>ZoomWalls: Dynamic Walls that Simulate Haptic Infrastructure for Room-scale VR World</b>. <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- -->(<!-- -->UIST 2020<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3379337.3415859" target="_blank">https://doi.org/10.1145/3379337.3415859</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="iros-2020-hedayati" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/iros-2020-hedayati/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>iros-2020-hedayati</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">IROS 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="iros-2020-hedayati cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/iros-2020-hedayati.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/iros-2020-hedayati.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/iros-2020-hedayati" target="_blank">PufferBot: Actuated Expandable Structures for Aerial Robots</a></h1><p class="meta"><span>Hooman Hedayati</span> , <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a> , <span>Daniel Leithinger</span> , <span>Daniel Szafir</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/iros-2020-hedayati.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>iros-2020-hedayati.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/XtPepCxWcBg" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/XtPepCxWcBg?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/XtPepCxWcBg/maxresdefault.jpg src=https://img.youtube.com/vi/XtPepCxWcBg/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present PufferBot, an aerial robot with an expandable structure that may expand to protect a drone’s propellers when the robot is close to obstacles or collocated humans. PufferBot is made of a custom 3D printed expandable scissor structure, which utilizes a one degree of freedom actuator with rack and pinion mechanism. We propose four designs for the expandable structure, each with unique charac- terizations which may be useful in different situations. Finally, we present three motivating scenarios in which PufferBot might be useful beyond existing static propeller guard structures.</p></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Hooman Hedayati<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Daniel Leithinger<!-- -->, <!-- -->Daniel Szafir<!-- -->. <b>PufferBot: Actuated Expandable Structures for Aerial Robots</b>. <i>In <!-- -->Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems<!-- -->(<!-- -->IROS 2020<!-- -->)</i>. <!-- -->IEEE, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->6<!-- -->. </p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tvcg-2020-danyluk" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/tvcg-2020-danyluk/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>tvcg-2020-danyluk</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TVCG 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="tvcg-2020-danyluk cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tvcg-2020-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tvcg-2020-danyluk.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tvcg-2020-danyluk" target="_blank">Touch and Beyond: Comparing Physical and Virtual Reality Visualizations</a></h1><p class="meta"><a href="/people/kurtis-danyluk"><img alt="kurtis-danyluk photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg"/><strong>Kurtis Thorvald Danyluk</strong></a> , <span>Teoman Tomo Ulusoy</span> , <a href="/people/wei-wei"><img alt="wei-wei photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wei-wei.jpg 1x" src="/pr-preview/pr-121/static/images/people/wei-wei.jpg"/><strong>Wei Wei</strong></a> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley J. Willett</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/tvcg-2020-danyluk.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>tvcg-2020-danyluk.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We compare physical and virtual reality (VR) versions of simple data visualizations and explore how the addition of virtual annotation and filtering tools affects how viewers solve basic data analysis tasks. We report on two studies, inspired by previous examinations of data physicalizations. The first study examines differences in how viewers interact with physical hand-scale, virtual hand-scale, and virtual table-scale visualizations and the impact that the different forms had on viewer’s problem solving behavior. A second study examines how interactive annotation and filtering tools might support new modes of use that transcend the limitations of physical representations. Our results highlight challenges associated with virtual reality representations and hint at the potential of interactive annotation and filtering tools in VR visualizations.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Physicalization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kurtis Thorvald Danyluk<!-- -->, <!-- -->Teoman Tomo Ulusoy<!-- -->, <!-- -->Wei Wei<!-- -->, <!-- -->Wesley J. Willett<!-- -->. <b>Touch and Beyond: Comparing Physical and Virtual Reality Visualizations</b>. <i>In <!-- -->IEEE Transactions on Visualization and Computer Graphics<!-- -->(<!-- -->TVCG 2020<!-- -->)</i>. <!-- -->IEEE, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->12<!-- -->. <!-- -->DOI: <a href="http://dx.doi.org/10.1109/TVCG.2020.3023336" target="_blank">http://dx.doi.org/10.1109/TVCG.2020.3023336</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="haid-2020-frisson" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/haid-2020-frisson/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>haid-2020-frisson</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">HAID 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="haid-2020-frisson cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/haid-2020-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/haid-2020-frisson.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/haid-2020-frisson" target="_blank">Printgets: an Open-Source Toolbox for Designing Vibrotactile Widgets with Industrial-Grade Printed Actuators and Sensors</a></h1><p class="meta"><a href="/people/christian-frisson"><img alt="christian-frisson photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/people/christian-frisson.jpg"/><strong>Christian Frisson</strong></a> , <span>Julien Decaudin</span> , <span>Mario Sanz-Lopez</span> , <span>Thomas Pietrzak</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/haid-2020-frisson.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>haid-2020-frisson.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>New technologies for printing sensors and actuators combine the flexibility of interface layouts of touchscreens with localized vibrotactile feedback, but their fabrication still requires industrial-grade facilities. Until these technologies become easily replicable, interaction designers need material for ideation.  We propose an open-source hardware and software toolbox providing maker-grade tools for iterative design of vibrotactile widgets with industrial-grade printed sensors and actuators. Our hardware toolbox provides a mechanical structure to clamp and stretch printed sheets, and electronic boards to drive sensors and actuators. Our software toolbox expands the design space of haptic interaction techniques by reusing the wide palette of available audio processing algorithms to generate real-time vibrotactile signals. We validate our toolbox with the implementation of three exemplar interface elements with tactile feedback: buttons, sliders, touchpads.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Vibrotactile</span><span class="ui brown basic label">Fabrication</span><span class="ui brown basic label">Piezoelectric</span><span class="ui brown basic label">Capacitive Sensing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Christian Frisson<!-- -->, <!-- -->Julien Decaudin<!-- -->, <!-- -->Mario Sanz-Lopez<!-- -->, <!-- -->Thomas Pietrzak<!-- -->. <b>Printgets: an Open-Source Toolbox for Designing Vibrotactile Widgets with Industrial-Grade Printed Actuators and Sensors</b>. <i>(<!-- -->HAID 2022<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->7<!-- -->. <!-- -->DOI: <a href="https://hal.archives-ouvertes.fr/hal-02901202" target="_blank">https://hal.archives-ouvertes.fr/hal-02901202</a></p></div></div><div class="block"><h1>Materials</h1><div class="ui horizontal small divided link list"><div class="item"><a href="https://gitlab.inria.fr/Loki/happiness/libhappiness" target="_blank" style="font-size:1.2em"><svg data-prefix="fab" data-icon="gitlab" class="svg-inline--fa fa-gitlab" role="img" viewBox="0 0 512 512" aria-hidden="true"><path fill="currentColor" d="M504 204.6l-.7-1.8-69.7-181.8c-1.4-3.6-3.9-6.6-7.2-8.6-2.4-1.6-5.1-2.5-8-2.8s-5.7 .1-8.4 1.1-5.1 2.7-7.1 4.8c-1.9 2.1-3.3 4.7-4.1 7.4l-47 144-190.5 0-47.1-144c-.8-2.8-2.2-5.3-4.1-7.4-2-2.1-4.4-3.7-7.1-4.8-2.6-1-5.5-1.4-8.4-1.1s-5.6 1.2-8 2.8c-3.2 2-5.8 5.1-7.2 8.6L9.8 202.8 9 204.6c-10 26.2-11.3 55-3.5 82 7.7 26.9 24 50.7 46.4 67.6l.3 .2 .6 .4 106 79.5c38.5 29.1 66.7 50.3 84.6 63.9 3.7 1.9 8.3 4.3 13 4.3s9.3-2.4 13-4.3c17.9-13.5 46.1-34.9 84.6-63.9l106.7-79.9 .3-.3c22.4-16.9 38.7-40.6 45.6-67.5 8.6-27 7.4-55.8-2.6-82z"></path></svg>https://gitlab.inria.fr/Loki/happiness/libhappiness</a></div></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/GYj5tYoQyqU" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/GYj5tYoQyqU?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/GYj5tYoQyqU/maxresdefault.jpg src=https://img.youtube.com/vi/GYj5tYoQyqU/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="nime-2020-kirkegaard" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/nime-2020-kirkegaard/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>nime-2020-kirkegaard</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">NIME 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="nime-2020-kirkegaard cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/nime-2020-kirkegaard.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/nime-2020-kirkegaard.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/nime-2020-kirkegaard" target="_blank">TorqueTuner: A self contained module for designing rotary haptic force feedback for digital musical instruments</a></h1><p class="meta"><span>Mathias Kirkegaard</span> , <span>Mathias Bredholt</span> , <a href="/people/christian-frisson"><img alt="christian-frisson photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/people/christian-frisson.jpg"/><strong>Christian Frisson</strong></a> , <span>Marcelo M. Wanderley</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/nime-2020-kirkegaard.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>nime-2020-kirkegaard.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/404592134" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/404592134?autoplay=1&gt;&lt;Image width={0} height={0} alt=undefined src=undefined&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>TorqueTuner is an embedded module that allows Digital Musical Instrument (DMI) designers to map sensors to parameters of haptic eﬀects and dynamically modify rotary force feedback in real-time. We embedded inside TorqueTuner a collection of haptic eﬀects (Wall, Magnet, Detents, Spring, Friction, Spin, Free) and a bi-directional interface through libmapper, a software library for making connections between data signals on a shared network. To increase aﬀordability and portability of force-feedback implementations in DMI design, we designed our platform to be wire-less, self-contained and built from commercially available components. To provide examples of modularity and portability, we integrated TorqueTuner into a standalone haptic knob and into an existing DMI, the T-Stick. We implemented 3 musical applications (Pitch wheel, Turntable and Exciter), by mapping sensors to sound synthesis in audio programming environment SuperCollider. While the original goal was to simulate the haptic feedback associated with turning a knob, we found that the platform allows for further expanding interaction possibilities in application scenarios where rotary control is familiar.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Digital Musical Instruments</span><span class="ui brown basic label">Haptic Feedback</span><span class="ui brown basic label">Force Feedback Interaction</span><span class="ui brown basic label">Sound Synthesis</span><span class="ui brown basic label">Mapping</span><span class="ui brown basic label">Embedded Systems</span><span class="ui brown basic label">Torque Tuner</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Mathias Kirkegaard<!-- -->, <!-- -->Mathias Bredholt<!-- -->, <!-- -->Christian Frisson<!-- -->, <!-- -->Marcelo M. Wanderley<!-- -->. <b>TorqueTuner: A self contained module for designing rotary haptic force feedback for digital musical instruments</b>. <i>(<!-- -->NIME 2020<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->6<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.5281/zenodo.4813359" target="_blank">https://doi.org/10.5281/zenodo.4813359</a></p></div></div><div class="block"><h1>Materials</h1><div class="ui horizontal small divided link list"><div class="item"><a href="https://github.com/idmil/torquetuner" target="_blank" style="font-size:1.2em"><svg data-prefix="fab" data-icon="github-alt" class="svg-inline--fa fa-github-alt" role="img" viewBox="0 0 512 512" aria-hidden="true"><path fill="currentColor" d="M202.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM496 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3l48.2 0c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"></path></svg>https://github.com/idmil/torquetuner</a></div></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/423615656" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/423615656?autoplay=1&gt;&lt;Image width={0} height={0} alt=undefined src=undefined&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="nime-2020-ko" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/nime-2020-ko/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>nime-2020-ko</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">NIME 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="nime-2020-ko cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/nime-2020-ko.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/nime-2020-ko.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/nime-2020-ko" target="_blank">Touch Responsive Augmented Violin Interface System II: Integrating Sensors into a 3D Printed Fingerboard</a></h1><p class="meta"><span>Chantelle Ko</span> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/nime-2020-ko.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>nime-2020-ko.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present TRAVIS II, an augmented acoustic violin with touch sensors integrated into its 3D printed fingerboard that track left-hand finger gestures in real time. The fingerboard has four strips of conductive PLA filament which produce an electric signal when fingers press down on each string. While these sensors are physically robust, they are mechanically assembled and thus easy to replace if damaged. The performer can also trigger presets via four FSRs attached to the body of the violin. The instrument is completely wireless, giving the performer the freedom to move throughout the performance space. While the sensing fingerboard is installed in place of the traditional fingerboard, all other electronics can be removed from the augmented instrument, maintaining the aesthetics of a traditional violin. Our design allows violinists to naturally create music for interactive performance and improvisation without requiring new instrumental techniques. In this paper, we describe the design of the instrument, experiments leading to the sensing fingerboard, and performative applications of the instrument.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Violin</span><span class="ui brown basic label">Touch Sensor</span><span class="ui brown basic label">FSR</span><span class="ui brown basic label">Fingerboard</span><span class="ui brown basic label">Augmented</span><span class="ui brown basic label">3 D Printing</span><span class="ui brown basic label">Conductive Filament</span><span class="ui brown basic label">Interactive</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Chantelle Ko<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Touch Responsive Augmented Violin Interface System II: Integrating Sensors into a 3D Printed Fingerboard</b>. <i>(<!-- -->NIME 2020<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. </p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/INmDzkcIO14" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/INmDzkcIO14?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/INmDzkcIO14/maxresdefault.jpg src=https://img.youtube.com/vi/INmDzkcIO14/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="imwut-2020-wang" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/imwut-2020-wang/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>imwut-2020-wang</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">IMWUT 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="imwut-2020-wang cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/imwut-2020-wang.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/imwut-2020-wang.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/imwut-2020-wang" target="_blank">AssessBlocks: Exploring Toy Block Play Features for Assessing Stress in Young Children after Natural Disasters</a></h1><p class="meta"><span>Xiyue Wang</span> , <span>Kazuki Takashima</span> , <span>Tomoaki Adachi</span> , <span>Patrick Finn</span> , <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a> , <span>Yoshifumi Kitamura</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/imwut-2020-wang.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>imwut-2020-wang.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/fxxvZBY80ug" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/fxxvZBY80ug?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/fxxvZBY80ug/maxresdefault.jpg src=https://img.youtube.com/vi/fxxvZBY80ug/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Natural disasters cause long-lasting mental health problems such as PTSD in children. Following the 2011 Earthquake and Tsunami in Japan, we witnessed a shift of toy block play behavior in young children who suffered from stress after the disaster. The behavior reflected their emotional responses to the traumatic event. In this paper, we explore the feasibility of using data captured from block-play to assess children&#x27;s stress after a major natural disaster. We prototyped sets of sensor-embedded toy blocks, AssessBlocks, that automate quantitative play data acquisition. During a three-year period, the blocks were dispatched to fifty-two post-disaster children. Within a free play session, we captured block features, a child&#x27;s playing behavior, and stress evaluated by several methods. The result from our analysis reveal correlations between block play features and stress measurements and show initial promise of using the effectiveness of using AssessBlocks to assess children&#x27;s stress after a disaster. We provide detailed insights into the potential as well as the challenges of our approach and unique conditions. From these insights we summarize guidelines for future research in automated play assessment systems that support children&#x27;s mental health.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Well Being</span><span class="ui brown basic label">Toy Blocks</span><span class="ui brown basic label">PTSD</span><span class="ui brown basic label">Tangibles For Health</span><span class="ui brown basic label">Stress Assessment</span><span class="ui brown basic label">Play</span><span class="ui brown basic label">Children</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Xiyue Wang<!-- -->, <!-- -->Kazuki Takashima<!-- -->, <!-- -->Tomoaki Adachi<!-- -->, <!-- -->Patrick Finn<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Yoshifumi Kitamura<!-- -->. <b>AssessBlocks: Exploring Toy Block Play Features for Assessing Stress in Young Children after Natural Disasters</b>. <i>In <!-- -->Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies<!-- -->(<!-- -->IMWUT 2020<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->29<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3381016" target="_blank">https://doi.org/10.1145/3381016</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="imx-2020-mok" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/imx-2020-mok/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>imx-2020-mok</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">IMX 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="imx-2020-mok cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/imx-2020-mok.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/imx-2020-mok.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/imx-2020-mok" target="_blank">Talk Like Somebody is Watching: Understanding and Supporting Novice Live Streamers</a></h1><p class="meta"><a href="/people/terrance-mok"><img alt="terrance-mok photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/terrance-mok.jpg 1x" src="/pr-preview/pr-121/static/images/people/terrance-mok.jpg"/><strong>Terrance Mok</strong></a> , <a href="/people/colin-auyeung"><img alt="colin-auyeung photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/colin-auyeung.jpg 1x" src="/pr-preview/pr-121/static/images/people/colin-auyeung.jpg"/><strong>Colin Au Yeung</strong></a> , <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We built a chatbot system–Audience Bot–that simulates an audience for novice live streamers to engage with while streaming. New live streamers on platforms like Twitch are expected to perform and talk to themselves, even while no one is watching. We ran an observational lab study on how Audience Bot assists novice live streamers as they acclimate to multitasking–simultaneously playing a video game while performing for a (simulated) audience.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Live Streams</span><span class="ui brown basic label">Streamers</span><span class="ui brown basic label">Chatbot</span><span class="ui brown basic label">Audience</span><span class="ui brown basic label">Virtual Audience</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Terrance Mok<!-- -->, <!-- -->Colin Au Yeung<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Talk Like Somebody is Watching: Understanding and Supporting Novice Live Streamers</b>. <i>(<!-- -->IMX 2020<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->6<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3391614.3399392" target="_blank">10.1145/3391614.3399392</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-suzuki" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2020-suzuki/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2020-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2020-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2020-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2020-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-suzuki" target="_blank">RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a> , <span>Hooman Hedayati</span> , <span>Clement Zheng</span> , <span>James Bohn</span> , <span>Daniel Szafir</span> , <span>Ellen Yi-Luen Do</span> , <span>Mark D Gross</span> , <span>Daniel Leithinger</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2020-suzuki.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2020-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/4OWU60gTOFE" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/4OWU60gTOFE?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/4OWU60gTOFE/maxresdefault.jpg src=https://img.youtube.com/vi/4OWU60gTOFE/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>RoomShift is a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Room Scale Haptics</span><span class="ui brown basic label">Haptic Interfaces</span><span class="ui brown basic label">Swarm Robots</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Hooman Hedayati<!-- -->, <!-- -->Clement Zheng<!-- -->, <!-- -->James Bohn<!-- -->, <!-- -->Daniel Szafir<!-- -->, <!-- -->Ellen Yi-Luen Do<!-- -->, <!-- -->Mark D Gross<!-- -->, <!-- -->Daniel Leithinger<!-- -->. <b>RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2020<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->11<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3313831.3376523" target="_blank">https://doi.org/10.1145/3313831.3376523</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="gi-2020-rajabiyazdi" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/gi-2020-rajabiyazdi/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>gi-2020-rajabiyazdi</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">GI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="gi-2020-rajabiyazdi cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/gi-2020-rajabiyazdi.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/gi-2020-rajabiyazdi.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/gi-2020-rajabiyazdi" target="_blank">Exploring the Design of Patient-Generated Data Visualizations</a></h1><p class="meta"><a href="/people/fateme-rajabiyazdi"><img alt="fateme-rajabiyazdi photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/fateme-rajabiyazdi.jpg 1x" src="/pr-preview/pr-121/static/images/people/fateme-rajabiyazdi.jpg"/><strong>Fateme Rajabiyazdi</strong></a> , <span>Charles Perin</span> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a> , <a href="/people/sheelagh-carpendale"><img alt="sheelagh-carpendale photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg"/><strong>Sheelagh Carpendale</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We were approached by a group of healthcare providers who are involved in the care of chronic patients looking for potential technologies to facilitate the process of reviewing patient-generated data during clinical visits. Aiming at understanding the healthcare providers’ attitudes towards reviewing patient-generated data, we (1) conducted a focus group with a mixed group of healthcare providers. Next, to gain the patients’ perspectives, we (2) interviewed eight chronic patients, collected a sample of their data and designed a series of visualizations representing patient data we collected. Last, we (3) sought feedback on the visualization designs from healthcare providers who requested this exploration. We found four factors shaping patient-generated data: data &amp; context, patient’s motivation, patient’s time commitment, and patient’s support circle. Informed by the results of our studies, we discussed the importance of designing patient-generated visualizations for individuals by considering both patient and healthcare provider rather than designing with the purpose of generalization and provided guidelines for designing future patient-generated data visualizations.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Information Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Fateme Rajabiyazdi<!-- -->, <!-- -->Charles Perin<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->. <b>Exploring the Design of Patient-Generated Data Visualizations</b>. <i>(<!-- -->GI 2020<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->12<!-- -->. <!-- -->DOI: <a href="https://hal.archives-ouvertes.fr/hal-02861239" target="_blank">https://hal.archives-ouvertes.fr/hal-02861239</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-anjani" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2020-anjani/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2020-anjani</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2020-anjani cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2020-anjani.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2020-anjani.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-anjani" target="_blank">Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</a></h1><p class="meta"><span>Laurensia Anjani</span> , <a href="/people/terrance-mok"><img alt="terrance-mok photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/terrance-mok.jpg 1x" src="/pr-preview/pr-121/static/images/people/terrance-mok.jpg"/><strong>Terrance Mok</strong></a> , <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a> , <span>Wooi Boon Goh</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2020-anjani.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2020-anjani.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present a mixed-methods study of viewers on their practices and motivations around watching mukbang — video streams of people eating large quantities of food. Viewers&#x27; experiences provide insight on future technologies for multisensorial video streams and technology-supported commensality (eating with others). We surveyed 104 viewers and interviewed 15 of them about their attitudes and reflections on their mukbang viewing habits, their physiological aspects of watching someone eat, and their perceived social relationship with mukbangers. Based on our findings, we propose design implications for remote commensality, and for synchronized multisensorial video streaming content.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Video Streams</span><span class="ui brown basic label">Mukbang</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Laurensia Anjani<!-- -->, <!-- -->Terrance Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Wooi Boon Goh<!-- -->. <b>Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2020<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3313831.3376567" target="_blank">https://doi.org/10.1145/3313831.3376567</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Dkp8A_em90M" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Dkp8A_em90M?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/Dkp8A_em90M/maxresdefault.jpg src=https://img.youtube.com/vi/Dkp8A_em90M/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-asha" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2020-asha/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2020-asha</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2020 LBW</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2020-asha cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2020-asha.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2020-asha.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-asha" target="_blank">Views from the Wheelchair: Understanding Interaction between Autonomous Vehicle and Pedestrians with Reduced Mobility</a></h1><p class="meta"><a href="/people/ashratuz-zavin-asha"><img alt="ashratuz-zavin-asha photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ashratuz-zavin-asha.jpg 1x" src="/pr-preview/pr-121/static/images/people/ashratuz-zavin-asha.jpg"/><strong>Ashratuz Zavin Asha</strong></a> , <a href="/people/christopher-smith"><img alt="christopher-smith photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christopher-smith.jpg 1x" src="/pr-preview/pr-121/static/images/people/christopher-smith.jpg"/><strong>Christopher Smith</strong></a> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a> , <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a> , <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/JNc49desa44" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/JNc49desa44?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/JNc49desa44/maxresdefault.jpg src=https://img.youtube.com/vi/JNc49desa44/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We are interested in the ways pedestrians will interact with autonomous vehicle (AV) in a future AV transportation ecosystem, when nonverbal cues from the driver such as eye movements, hand gestures, etc. are no longer provided. In this work, we examine a subset of this challenge: interaction between pedestrian with reduced mobility (PRM) and AV. This study explores interface designs between AVs and people in a wheelchair to help them interact with AVs by conducting a preliminary design study. We have assessed the data collected from the study using qualitative analysis and presented different findings on AV-PRM interactions. Our findings reflect on the importance of visual interfaces, changes to the wheelchair and the creative use of the street infrastructure.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Autonomous Vehicle</span><span class="ui brown basic label">Pedestrian With Reduced Mobility</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ashratuz Zavin Asha<!-- -->, <!-- -->Christopher Smith<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>Views from the Wheelchair: Understanding Interaction between Autonomous Vehicle and Pedestrians with Reduced Mobility</b>. <i>(<!-- -->CHI 2020 LBW<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->8<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3334480.3383041" target="_blank">10.1145/3334480.3383041</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-goffin" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2020-goffin/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2020-goffin</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2020-goffin cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2020-goffin.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2020-goffin.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-goffin" target="_blank">Interaction Techniques for Visual Exploration Using Embedded Word-Scale Visualizations</a></h1><p class="meta"><span>Pascal Goffin</span> , <span>Tanja Blascheck</span> , <a href="/people/petra-isenberg"><img alt="petra-isenberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/petra-isenberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/petra-isenberg.jpg"/><strong>Petra Isenberg</strong></a> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2020-goffin.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2020-goffin.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/wPaVdSWM8hU" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/wPaVdSWM8hU?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/wPaVdSWM8hU/maxresdefault.jpg src=https://img.youtube.com/vi/wPaVdSWM8hU/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We describe a design space of view manipulation interactions for small data-driven contextual visualizations (word-scale visualizations). These interaction techniques support an active reading experience and engage readers through exploration of embedded visualizations whose placement and content connect them to specific terms in a document. A reader could, for example, use our proposed interaction techniques to explore word-scale visualizations of stock market trends for companies listed in a market overview article. When readers wish to engage more deeply with the data, they can collect, arrange, compare, and navigate the document using the embedded word-scale visualizations, permitting more visualization-centric analyses. We support our design space with a concrete implementation, illustrate it with examples from three application domains, and report results from two experiments. The experiments show how view manipulation interactions helped readers examine embedded visualizations more quickly and with less scrolling and yielded qualitative feedback on usability and future opportunities.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Glyphs</span><span class="ui brown basic label">Word Scale Visualization</span><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Interaction Techniques</span><span class="ui brown basic label">Text Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Pascal Goffin<!-- -->, <!-- -->Tanja Blascheck<!-- -->, <!-- -->Petra Isenberg<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Interaction Techniques for Visual Exploration Using Embedded Word-Scale Visualizations</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2020<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3313831.3376842" target="_blank">https://doi.org/10.1145/3313831.3376842</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-hou" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2020-hou/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2020-hou</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2020-hou cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2020-hou.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2020-hou.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-hou" target="_blank">Autonomous Vehicle-Cyclist Interaction: Peril and Promise</a></h1><p class="meta"><span>Ming Hou</span> , <a href="/people/karthik-mahadevan"><img alt="karthik-mahadevan photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/karthik-mahadevan.jpg 1x" src="/pr-preview/pr-121/static/images/people/karthik-mahadevan.jpg"/><strong>Karthik Mahadevan</strong></a> , <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a> , <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2020-hou.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2020-hou.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/fsgbUeAaFfI" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/fsgbUeAaFfI?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/fsgbUeAaFfI/maxresdefault.jpg src=https://img.youtube.com/vi/fsgbUeAaFfI/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Autonomous vehicles (AVs) will redefine interactions between road users. Presently, cyclists and drivers communicate through implicit cues (vehicle motion) and explicit but imprecise signals (hand gestures, horns). Future AVs could consistently communicate awareness and intent and other feedback to cyclists based on their sensor data. We present an exploration of AV-cyclist interaction, starting with preliminary design studies which informed the implementation of an immersive VR AV-cyclist simulator, and the design and evaluation of a number of AV-cyclist interfaces. Our findings suggest that AV-cyclist interfaces can improve rider confidence in lane merging scenarios. We contribute an AV-cyclist immersive simulator, insights on trade-offs of various aspects of AV-cyclist interaction design including modalities, location, and complexity, and positive results suggesting improved rider confidence due to AV-cyclist interaction. While we are encouraged by the potential positive impact AV-cyclist interfaces can have on cyclist culture, we also emphasize the risks over-reliance can pose to cyclists.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Autonomous Vehicle Cyclist Interaction</span><span class="ui brown basic label">Interfaces For Communicating Intent And Awareness</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ming Hou<!-- -->, <!-- -->Karthik Mahadevan<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Autonomous Vehicle-Cyclist Interaction: Peril and Promise</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2020<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->12<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3313831.3376884" target="_blank">https://doi.org/10.1145/3313831.3376884</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/DtxkWAW9B1s" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/DtxkWAW9B1s?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/DtxkWAW9B1s/maxresdefault.jpg src=https://img.youtube.com/vi/DtxkWAW9B1s/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tei-2020-suzuki" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/tei-2020-suzuki/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>tei-2020-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TEI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="tei-2020-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tei-2020-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tei-2020-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tei-2020-suzuki" target="_blank">LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a> , <span>Ryosuke Nakayama</span> , <span>Dan Liu</span> , <span>Yasuaki Kakehi</span> , <span>Mark D. Gross</span> , <span>Daniel Leithinger</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/tei-2020-suzuki.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>tei-2020-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/0LHeTkOMR84" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/0LHeTkOMR84?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/0LHeTkOMR84/maxresdefault.jpg src=https://img.youtube.com/vi/0LHeTkOMR84/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Large-scale shape-changing interfaces have great potential, but creating such systems requires substantial time, cost, space, and efforts, which hinders the research community to explore interactions beyond the scale of human hands. We introduce modular inflatable actuators as building blocks for prototyping room-scale shape-changing interfaces. Each actuator can change its height from 15cm to 150cm, actuated and controlled by air pressure. Each unit is low-cost (8 USD), lightweight (10 kg), compact (15 cm), and robust, making it well-suited for prototyping room-scale shape transformations. Moreover, our modular and reconfigurable design allows researchers and designers to quickly construct different geometries and to explore various applications. This paper contributes to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Shape Changing Interfaces</span><span class="ui brown basic label">Inflatables</span><span class="ui brown basic label">Large Scale Interactions</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Ryosuke Nakayama<!-- -->, <!-- -->Dan Liu<!-- -->, <!-- -->Yasuaki Kakehi<!-- -->, <!-- -->Mark D. Gross<!-- -->, <!-- -->Daniel Leithinger<!-- -->. <b>LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces</b>. <i>In <!-- -->Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction<!-- -->(<!-- -->TEI 2020<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->9<!-- -->. <!-- -->DOI: <a href="https://dl.acm.org/doi/10.1145/3374920.3374941" target="_blank">https://dl.acm.org/doi/10.1145/3374920.3374941</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="cmj-2020-ko" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/cmj-2020-ko/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>cmj-2020-ko</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CMJ 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="cmj-2020-ko cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/cmj-2020-ko.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/cmj-2020-ko.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/cmj-2020-ko" target="_blank">Construction and Performance Applications of an Augmented Violin: TRAVIS II</a></h1><p class="meta"><span>Chantelle Ko</span> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present the second iteration of a Touch-Responsive Augmented Violin Interface System, called TRAVIS II, and two compositions that demonstrate its expressivity. TRAVIS II is an augmented acoustic violin with touch sensors integrated into its 3-D printed fingerboard that track left-hand finger gestures in real time. The fingerboard has four strips of conductive PLA filament that produce an electric signal when fingers press down on each string. Although these sensors are physically robust, they are mechanically assembled and thus easy to replace if damaged. The performer can also trigger presets via four sensors attached to the body of the violin. The instrument is completely wireless, giving the performer the freedom to move throughout the performance space. Although the sensing fingerboard is installed in place of the traditional fingerboard, all other electronics can be removed from the augmented instrument, maintaining the aesthetics of a traditional violin. Our design allows violinists to naturally create music for interactive performance and improvisation without requiring new instrumental techniques. The first author composed two compositions to highlight TRAVIS II: “Dream State” and “Kindred Dichotomy.” Both of these compositions involve improvisation in their creative process and include interactive visuals. In this article we describe the design of the instrument, experiments leading to the sensing fingerboard, performative applications of the instrument, and compositional considerations for the resultant pieces.</p></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Chantelle Ko<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Construction and Performance Applications of an Augmented Violin: TRAVIS II</b>. <i>(<!-- -->CMJ 2020<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->14<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1162/comj_a_00563" target="_blank">https://doi.org/10.1162/comj_a_00563</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="mobilehci-2019-hung" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/mobilehci-2019-hung/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>mobilehci-2019-hung</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MobileHCI 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="mobilehci-2019-hung cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/mobilehci-2019-hung.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/mobilehci-2019-hung.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/mobilehci-2019-hung" target="_blank">WatchPen: Using Cross-Device Interaction Concepts to Augment Pen-Based Interaction</a></h1><p class="meta"><a href="/people/michael-hung"><img alt="michael-hung photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/michael-hung.jpg 1x" src="/pr-preview/pr-121/static/images/people/michael-hung.jpg"/><strong>Michael Hung</strong></a> , <a href="/people/david-ledo"><img alt="david-ledo photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/david-ledo.jpg 1x" src="/pr-preview/pr-121/static/images/people/david-ledo.jpg"/><strong>David Ledo</strong></a> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/mobilehci-2019-hung.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>mobilehci-2019-hung.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/ilyJBzTzQAA" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/ilyJBzTzQAA?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/ilyJBzTzQAA/maxresdefault.jpg src=https://img.youtube.com/vi/ilyJBzTzQAA/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Pen-based input is often treated as auxiliary to mobile devices. We posit that cross-device interactions can inspire and extend the design space of pen-based interactions into new, expressive directions. We realize this through WatchPen, a smartwatch mounted on a passive, capacitive stylus that: (1) senses the usage context and leverages it for expression (e.g., changing colour), (2) contains tools and parameters within the display, and (3) acts as an on-demand output. As a result, it provides users with a dynamic relationship between inputs and outputs, awareness of current tool selection and parameters, and increased expressive match (e.g., added ability to mimic physical tools, showing clipboard contents). We discuss and reflect upon a series of interaction techniques that demonstrate WatchPen within a drawing application. We highlight the expressive power of leveraging multiple sensing and output capabilities across both the watch-augmented stylus and the tablet surface.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Smartwatch</span><span class="ui brown basic label">Cross Device Interaction</span><span class="ui brown basic label">Pen Interaction</span><span class="ui brown basic label">Interaction Techniques</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Michael Hung<!-- -->, <!-- -->David Ledo<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>WatchPen: Using Cross-Device Interaction Concepts to Augment Pen-Based Interaction</b>. <i>In <!-- -->Proceedings of the International Conference on Human-Computer Interaction with Mobile Devices and Services<!-- -->(<!-- -->MobileHCI 2019<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->8<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3338286.3340122" target="_blank">https://doi.org/10.1145/3338286.3340122</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2019-suzuki" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/uist-2019-suzuki/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2019-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2019-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2019-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2019-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2019-suzuki" target="_blank">ShapeBots: Shape-changing Swarm Robots</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a> , <span>Clement Zheng</span> , <span>Yasuaki Kakehi</span> , <span>Tom Yeh</span> , <span>Ellen Yi-Luen Do</span> , <span>Mark D. Gross</span> , <span>Daniel Leithinger</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2019-suzuki.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2019-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/cwPaof0kKdM" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/cwPaof0kKdM?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/cwPaof0kKdM/maxresdefault.jpg src=https://img.youtube.com/vi/cwPaof0kKdM/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce shape-changing swarm robots. A swarm of self-transformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions. The modular design of each actuator enables various shapes and geometries of self-transformation. We illustrate potential application scenarios and discuss how this type of interface opens up possibilities for the future of ubiquitous and distributed shape-changing interfaces.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Swarm User Interfaces</span><span class="ui brown basic label">Shape Changing User Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Clement Zheng<!-- -->, <!-- -->Yasuaki Kakehi<!-- -->, <!-- -->Tom Yeh<!-- -->, <!-- -->Ellen Yi-Luen Do<!-- -->, <!-- -->Mark D. Gross<!-- -->, <!-- -->Daniel Leithinger<!-- -->. <b>ShapeBots: Shape-changing Swarm Robots</b>. <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- -->(<!-- -->UIST 2019<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3332165.3347911" target="_blank">https://doi.org/10.1145/3332165.3347911</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tvcg-2019-walny" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/tvcg-2019-walny/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>tvcg-2019-walny</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TVCG 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="tvcg-2019-walny cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tvcg-2019-walny.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tvcg-2019-walny.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tvcg-2019-walny" target="_blank">Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff</a></h1><p class="meta"><a href="/people/jagoda-walny"><img alt="jagoda-walny photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/jagoda-walny.jpg 1x" src="/pr-preview/pr-121/static/images/people/jagoda-walny.jpg"/><strong>Jagoda Walny</strong></a> , <a href="/people/christian-frisson"><img alt="christian-frisson photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-121/static/images/people/christian-frisson.jpg"/><strong>Christian Frisson</strong></a> , <span>Mieka West</span> , <span>Doris Kosminsky</span> , <a href="/people/soren-knudsen"><img alt="soren-knudsen photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/soren-knudsen.jpg 1x" src="/pr-preview/pr-121/static/images/people/soren-knudsen.jpg"/><strong>Søren Knudsen</strong></a> , <a href="/people/sheelagh-carpendale"><img alt="sheelagh-carpendale photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg"/><strong>Sheelagh Carpendale</strong></a> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/tvcg-2019-walny.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>tvcg-2019-walny.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/360483702" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/360483702?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://i.vimeocdn.com/video/814665539_640.webp src=https://i.vimeocdn.com/video/814665539_640.webp&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Complex data visualization design projects often entail collaboration between people with different visualization-related skills. For example, many teams include both designers who create new visualization designs and developers who implement the resulting visualization software. We identify gaps between data characterization tools, visualization design tools, and development platforms that pose challenges for designer-developer teams working to create new data visualizations. While it is common for commercial interaction design tools to support collaboration between designers and developers, creating data visualizations poses several unique challenges that are not supported by current tools. In particular, visualization designers must characterize and build an understanding of the underlying data, then specify layouts, data encodings, and other data-driven parameters that will be robust across many different data values. In larger teams, designers must also clearly communicate these mappings and their dependencies to developers, clients, and other collaborators. We report observations and reflections from five large multidisciplinary visualization design projects and highlight six data-specific visualization challenges for design specification and handoff. These challenges include adapting to changing data, anticipating edge cases in data, understanding technical challenges, articulating data-dependent interactions, communicating data mappings, and preserving the integrity of data mappings across iterations. Based on these observations, we identify opportunities for future tools for prototyping, testing, and communicating data-driven designs, which might contribute to more successful and collaborative data visualization design.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Design Handoff</span><span class="ui brown basic label">Data Mapping</span><span class="ui brown basic label">Design Process</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jagoda Walny<!-- -->, <!-- -->Christian Frisson<!-- -->, <!-- -->Mieka West<!-- -->, <!-- -->Doris Kosminsky<!-- -->, <!-- -->Søren Knudsen<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff</b>. <i>In <!-- -->IEEE Transactions on Visualization and Computer Graphics<!-- -->(<!-- -->TVCG 2019<!-- -->)</i>. <!-- -->IEEE, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->10<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1109/TVCG.2019.2934538" target="_blank">https://doi.org/10.1109/TVCG.2019.2934538</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/368703151" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/368703151?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://i.vimeocdn.com/video/825448765_640.webp src=https://i.vimeocdn.com/video/825448765_640.webp&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="cgi-2019-danyluk" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/cgi-2019-danyluk/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>cgi-2019-danyluk</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CGI 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="cgi-2019-danyluk cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/cgi-2019-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/cgi-2019-danyluk.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/cgi-2019-danyluk" target="_blank">Evaluating the Performance of Virtual Reality Navigation Techniques for Large Environments</a></h1><p class="meta"><a href="/people/kurtis-danyluk"><img alt="kurtis-danyluk photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg"/><strong>Kurtis Thorvald Danyluk</strong></a> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley J. Willett</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present results from two studies comparing the performance of four different navigation techniques (flight, teleportation, world-in-miniature, and 3D cone-drag) and their combinations in large virtual reality map environments. While prior work has individually examined each of these techniques in other settings, our study presents the first direct comparison between them in large open environments, as well as one of the first comparisons in the context of current-generation virtual reality hardware. Our first study compared common techniques (flight, teleportation, and world-in-miniature) for search and navigation tasks. A follow-up study compared these techniques against 3D cone drag, a direct-manipulation navigation technique used in contemporary tools like Google Earth VR. Our results show the strength of flight as a stand-alone navigation technique, but also highlight five specific ways in which viewers can combine teleportation, world-in-miniature, and 3D cone drag with flight, drawing on the relative strengths of each technique to compensate for the weaknesses of others.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Human Computer Interaction HCI</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Digital Maps</span><span class="ui brown basic label">Navigation</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kurtis Thorvald Danyluk<!-- -->, <!-- -->Wesley J. Willett<!-- -->. <b>Evaluating the Performance of Virtual Reality Navigation Techniques for Large Environments</b>. <i>(<!-- -->CGI 2019<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="http://dx.doi.org/10.1007/978-3-030-22514-8_17" target="_blank">http://dx.doi.org/10.1007/978-3-030-22514-8_17</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="cnc-2019-hammad" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/cnc-2019-hammad/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>cnc-2019-hammad</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">C&amp;C 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="cnc-2019-hammad cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/cnc-2019-hammad.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/cnc-2019-hammad.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/cnc-2019-hammad" target="_blank">Mutation: Leveraging Performing Arts Practices in Cyborg Transitioning</a></h1><p class="meta"><a href="/people/nour-hammad"><img alt="nour-hammad photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/nour-hammad.jpg 1x" src="/pr-preview/pr-121/static/images/people/nour-hammad.jpg"/><strong>Nour Hammad</strong></a> , <span>Elaheh Sanoubari</span> , <span>Patrick Finn</span> , <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a> , <span>James E. Young</span> , <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/cnc-2019-hammad.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>cnc-2019-hammad.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/HFH59__Fkok" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/HFH59__Fkok?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/HFH59__Fkok/maxresdefault.jpg src=https://img.youtube.com/vi/HFH59__Fkok/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present Mutation: performing arts based approach that can help decrease the cognitive load associated with cyborg transitioning. Cyborgs are human-machine hybrids with organic and mechatronic body parts that can be implanted or worn. The transition into and out of experiencing additional body parts is not fully understood. Our goal is to draw from performing arts techniques in order to help decrease the cognitive load associated with becoming and unbecoming a cyborg. Actors constantly shift between states, whether from one character to another, or from pre- to post- performance. We contribute a straightforward adaptation of classic performing art practices to cyborg transitioning, and a study where actors used these protocols in order to enter a cyborg state, perform as a cyborg, and then exit the cyborg state. Our work on Mutation suggests that classic performing art practices can be useful in cyborg transitioning, as well as in other technology augmented experiences.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Interaction Design</span><span class="ui brown basic label">Cyborgs</span><span class="ui brown basic label">User Experience</span><span class="ui brown basic label">Performing Art Techniques</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nour Hammad<!-- -->, <!-- -->Elaheh Sanoubari<!-- -->, <!-- -->Patrick Finn<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->James E. Young<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>Mutation: Leveraging Performing Arts Practices in Cyborg Transitioning</b>. <i>In <!-- -->Proceedings of the ACM on Creativity and Cognition<!-- -->(<!-- -->C&amp;C 2019<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->DOI: <a href="https://doi.org/10.1145/3325480.3325508" target="_blank">https://doi.org/10.1145/3325480.3325508</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2019-blair" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/dis-2019-blair/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2019-blair</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2019-blair cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2019-blair.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2019-blair.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2019-blair" target="_blank">Exploring Public Engagement with the Social Impact of Algorithms</a></h1><p class="meta"><a href="/people/kathryn-blair"><img alt="kathryn-blair photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kathryn-blair.jpg 1x" src="/pr-preview/pr-121/static/images/people/kathryn-blair.jpg"/><strong>Kathryn Blair</strong></a> , <span>Jean-Rene Leblanc</span> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/dis-2019-blair.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>dis-2019-blair.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We discuss Logical Conclusion, an analog interactive installation which presents issues surrounding the social impacts of algorithms used by corporations and governments via logic puzzles with physical elements that visitors manipulate to solve. We present the combination of physicality and participation as promising tools to engage the public with the ways that complex technologies interact with society. We also pose questions regarding how such strategies might be extended by the addition of responsive tangible computing elements.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Fine Arts</span><span class="ui brown basic label">Physical Artifact</span><span class="ui brown basic label">Education</span><span class="ui brown basic label">Social Impact Of Technology</span><span class="ui brown basic label">Participatory Art Installation</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kathryn Blair<!-- -->, <!-- -->Jean-Rene Leblanc<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Exploring Public Engagement with the Social Impact of Algorithms</b>. <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- -->(<!-- -->DIS 2019<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->5<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3301019.3323897" target="_blank">10.1145/3301019.3323897</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2019-bressa" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/dis-2019-bressa/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2019-bressa</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2019-bressa cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2019-bressa.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2019-bressa.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2019-bressa" target="_blank">Sketching and Ideation Activities for Situated Visualization Design</a></h1><p class="meta"><a href="/people/nathalie-bressa"><img alt="nathalie-bressa photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/nathalie-bressa.jpg 1x" src="/pr-preview/pr-121/static/images/people/nathalie-bressa.jpg"/><strong>Nathalie Bressa</strong></a> , <a href="/people/kendra-wannamaker"><img alt="kendra-wannamaker photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kendra-wannamaker.jpg 1x" src="/pr-preview/pr-121/static/images/people/kendra-wannamaker.jpg"/><strong>Kendra Wannamaker</strong></a> , <span>Henrik Korsgaard</span> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a> , <span>Jo Vermeulen</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/dis-2019-bressa.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>dis-2019-bressa.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We report on findings from seven design workshops that used ideation and sketching activities to prototype new situated visualizations - representations of data that are displayed in proximity to the physical referents (such as people, objects, and locations) to which the data is related. Designing situated visualizations requires a fine-grained understanding of the context in which the visualizations are placed, as well as an exploration of different options for placement and form factors, which existing methods for visualization design do not account for. Focusing on small displays as a target platform, we reflect on our experiences of using a diverse range of sketching activities, materials, and prompts. Based on these observations, we identify challenges and opportunities for sketching and ideating situated visualizations. We also outline the space of design activities for situated visualization and highlight promising methods for both designers and researchers.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Ideation</span><span class="ui brown basic label">Design Workshops</span><span class="ui brown basic label">Situated Visualization</span><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Small Displays</span><span class="ui brown basic label">Sketching</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nathalie Bressa<!-- -->, <!-- -->Kendra Wannamaker<!-- -->, <!-- -->Henrik Korsgaard<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Jo Vermeulen<!-- -->. <b>Sketching and Ideation Activities for Situated Visualization Design</b>. <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- -->(<!-- -->DIS 2019<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->DOI: <a href="https://doi.org/10.1145/3322276.3322326" target="_blank">https://doi.org/10.1145/3322276.3322326</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2019-ledo" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/dis-2019-ledo/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2019-ledo</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2019-ledo cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2019-ledo.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2019-ledo.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2019-ledo" target="_blank">Astral: Prototyping Mobile and Smart Object Interactive Behaviours Using Familiar Applications</a></h1><p class="meta"><a href="/people/david-ledo"><img alt="david-ledo photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/david-ledo.jpg 1x" src="/pr-preview/pr-121/static/images/people/david-ledo.jpg"/><strong>David Ledo</strong></a> , <span>Jo Vermeulen</span> , <a href="/people/sheelagh-carpendale"><img alt="sheelagh-carpendale photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg"/><strong>Sheelagh Carpendale</strong></a> , <a href="/people/saul-greenberg"><img alt="saul-greenberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/saul-greenberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/saul-greenberg.jpg"/><strong>Saul Greenberg</strong></a> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a> , <span>Sebastian Boring</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/dis-2019-ledo.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>dis-2019-ledo.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Astral is a prototyping tool for authoring mobile and smart object interactive behaviours. It mirrors selected display contents of desktop applications onto mobile devices (smartphones and smartwatches), and streams/remaps mobile sensor data to desktop input events (mouse or keyboard) to manipulate selected desktop contents. This allows designers to use familiar desktop applications (e.g. PowerPoint, AfterEffects) to prototype rich interactive behaviours. Astral combines and integrates display mirroring, sensor streaming and input remapping, where designers can exploit familiar desktop applications to prototype, explore and fine-tune dynamic interactive behaviours. With Astral, designers can visually author rules to test real-time behaviours while interactions take place, as well as after the interaction has occurred. We demonstrate Astral&#x27;s applicability, workflow and expressiveness within the interaction design process through both new examples and replication of prior approaches that illustrate how various familiar desktop applications are leveraged and repurposed.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Smart Objects</span><span class="ui brown basic label">Mobile Interfaces</span><span class="ui brown basic label">Prototyping</span><span class="ui brown basic label">Design Tool</span><span class="ui brown basic label">Interactive Behaviour</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Ledo<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->, <!-- -->Saul Greenberg<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Sebastian Boring<!-- -->. <b>Astral: Prototyping Mobile and Smart Object Interactive Behaviours Using Familiar Applications</b>. <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- -->(<!-- -->DIS 2019<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->14<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3322276.3322329" target="_blank">https://doi.org/10.1145/3322276.3322329</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2019-mahadevan" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/dis-2019-mahadevan/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2019-mahadevan</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2019-mahadevan cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2019-mahadevan.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2019-mahadevan.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2019-mahadevan" target="_blank">AV-Pedestrian Interaction Design Using a Pedestrian Mixed Traffic Simulator</a></h1><p class="meta"><a href="/people/karthik-mahadevan"><img alt="karthik-mahadevan photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/karthik-mahadevan.jpg 1x" src="/pr-preview/pr-121/static/images/people/karthik-mahadevan.jpg"/><strong>Karthik Mahadevan</strong></a> , <span>Elaheh Sanoubari</span> , <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a> , <span>James E. Young</span> , <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/dis-2019-mahadevan.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>dis-2019-mahadevan.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>AV-pedestrian interaction will impact pedestrian safety, etiquette, and overall acceptance of AV technology. Evaluating AV-pedestrian interaction is challenging given limited availability of AVs and safety concerns. These challenges are compounded by &quot;mixed traffic&quot; conditions: studying AV-pedestrian interaction will be difficult in traffic consisting of vehicles varying in autonomy level. We propose immersive pedestrian simulators as design tools to study AV-pedestrian interaction, allowing rapid prototyping and evaluation of future AV-pedestrian interfaces. We present OnFoot: a VR-based simulator that immerses participants in mixed traffic conditions and allows examination of their behavior while controlling vehicles&#x27; autonomy-level, traffic and street characteristics, behavior of other virtual pedestrians, and integration of novel AV-pedestrian interfaces. We validated OnFoot against prior simulators and Wizard-of-Oz studies, and conducted a user study, manipulating vehicles&#x27; autonomy level, interfaces, and pedestrian group behavior. Our findings highlight the potential to use VR simulators as powerful tools for AV-pedestrian interaction design in mixed traffic.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Mixed Traffic</span><span class="ui brown basic label">Pedestrian Simulator</span><span class="ui brown basic label">Autonomous Vehicle Pedestrian Interaction</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Karthik Mahadevan<!-- -->, <!-- -->Elaheh Sanoubari<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->James E. Young<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>AV-Pedestrian Interaction Design Using a Pedestrian Mixed Traffic Simulator</b>. <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- -->(<!-- -->DIS 2019<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->12<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3322276.3322328" target="_blank">https://doi.org/10.1145/3322276.3322328</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2019-nakayama" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/dis-2019-nakayama/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2019-nakayama</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2019-nakayama cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2019-nakayama.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2019-nakayama.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2019-nakayama" target="_blank">MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction</a></h1><p class="meta"><span>Ryosuke Nakayama</span> , <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a> , <span>Satoshi Nakamaru</span> , <span>Ryuma Niiyama</span> , <span>Yoshihiro Kawahara</span> , <span>Yasuaki Kakehi</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/dis-2019-nakayama.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>dis-2019-nakayama.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/ZkCcazfFD-M" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/ZkCcazfFD-M?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/ZkCcazfFD-M/maxresdefault.jpg src=https://img.youtube.com/vi/ZkCcazfFD-M/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce MorphIO, entirely soft sensing and actuation modules for programming by demonstration of soft robots and shape-changing interfaces.MorphIO&#x27;s hardware consists of a soft pneumatic actuator containing a conductive sponge sensor.This allows both input and output of three-dimensional deformation of a soft material.Leveraging this capability, MorphIO enables a user to record and later playback physical motion of programmable shape-changing materials.In addition, the modular design of MorphIO&#x27;s unit allows the user to construct various shapes and topologies through magnetic connection.We demonstrate several application scenarios, including tangible character animation, locomotion experiment of a soft robot, and prototyping tools for animated soft objects.Our user study with six participants confirms the benefits of MorphIO, as compared to the existing programming paradigm.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Shape Changing Interfaces</span><span class="ui brown basic label">Programming By Demonstration</span><span class="ui brown basic label">Soft Robots</span><span class="ui brown basic label">Pneumatic Actuation</span><span class="ui brown basic label">Tangible Interactions</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryosuke Nakayama<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Satoshi Nakamaru<!-- -->, <!-- -->Ryuma Niiyama<!-- -->, <!-- -->Yoshihiro Kawahara<!-- -->, <!-- -->Yasuaki Kakehi<!-- -->. <b>MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction</b>. <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- -->(<!-- -->DIS 2019<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->11<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3322276.3322337" target="_blank">https://doi.org/10.1145/3322276.3322337</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2019-seyed" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/dis-2019-seyed/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2019-seyed</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2019-seyed cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2019-seyed.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2019-seyed.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2019-seyed" target="_blank">Mannequette: Understanding and Enabling Collaboration and Creativity on Avant-Garde Fashion-Tech Runways</a></h1><p class="meta"><a href="/people/teddy-seyed"><img alt="teddy-seyed photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/teddy-seyed.jpg 1x" src="/pr-preview/pr-121/static/images/people/teddy-seyed.jpg"/><strong>Teddy Seyed</strong></a> , <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/dis-2019-seyed.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>dis-2019-seyed.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Drawing upon multiple disciplines, avant-garde fashion-tech teams push the boundaries between fashion and technology. Many are well trained in envisioning aesthetic qualities of garments, but few have formal training on designing and fabricating technologies themselves. We introduce Mannequette, a prototyping tool for fashion-tech garments that enables teams to experiment with interactive technologies at early stages of their design processes. Mannequette provides an abstraction of light-based outputs and sensor-based inputs for garments through a DJ mixer-like interface that allows for dynamic changes and recording/playback of visual effects. The base of Mannequette can also be incorporated into the final garment, where it is then connected to the final components. We conducted an 8-week deployment study with eight design teams who created new garments for a runway show. Our results revealed Mannequette allowed teams to repeatedly consider new design and technical options early in their creative processes, and to communicate more effectively across disciplinary backgrounds.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Fashion</span><span class="ui brown basic label">Haute Couture</span><span class="ui brown basic label">E Textiles</span><span class="ui brown basic label">Maker Culture</span><span class="ui brown basic label">Fashion Tech</span><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Avant Garde</span><span class="ui brown basic label">Haute Tech Couture</span><span class="ui brown basic label">Modular</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Teddy Seyed<!-- -->, <!-- -->Anthony Tang<!-- -->. <b>Mannequette: Understanding and Enabling Collaboration and Creativity on Avant-Garde Fashion-Tech Runways</b>. <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- -->(<!-- -->DIS 2019<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3322276.3322305" target="_blank">https://doi.org/10.1145/3322276.3322305</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tvcg-2019-blascheck" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/tvcg-2019-blascheck/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>tvcg-2019-blascheck</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TVCG 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="tvcg-2019-blascheck cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tvcg-2019-blascheck.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tvcg-2019-blascheck.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tvcg-2019-blascheck" target="_blank">Exploration Strategies for Discovery of Interactivity in Visualizations</a></h1><p class="meta"><span>Tanja Blascheck</span> , <span>Lindsay MacDonald Vermeulen</span> , <span>Jo Vermeulen</span> , <span>Charles Perin</span> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a> , <span>Thomas Ertl</span> , <a href="/people/sheelagh-carpendale"><img alt="sheelagh-carpendale photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg"/><strong>Sheelagh Carpendale</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/tvcg-2019-blascheck.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>tvcg-2019-blascheck.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/289789025" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/289789025?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://i.vimeocdn.com/video/725516359_640.webp src=https://i.vimeocdn.com/video/725516359_640.webp&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We investigate how people discover the functionality of an interactive visualization that was designed for the general public. While interactive visualizations are increasingly available for public use, we still know little about how the general public discovers what they can do with these visualizations and what interactions are available. Developing a better understanding of this discovery process can help inform the design of visualizations for the general public, which in turn can help make data more accessible. To unpack this problem, we conducted a lab study in which participants were free to use their own methods to discover the functionality of a connected set of interactive visualizations of public energy data. We collected eye movement data and interaction logs as well as video and audio recordings. By analyzing this combined data, we extract exploration strategies that the participants employed to discover the functionality in these interactive visualizations. These exploration strategies illuminate possible design directions for improving the discoverability of a visualization&#x27;s functionality.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Discovery</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Open Data</span><span class="ui brown basic label">Evaluation</span><span class="ui brown basic label">Eye Tracking</span><span class="ui brown basic label">Interaction Logs</span><span class="ui brown basic label">Think Aloud</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Tanja Blascheck<!-- -->, <!-- -->Lindsay MacDonald Vermeulen<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Charles Perin<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Thomas Ertl<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->. <b>Exploration Strategies for Discovery of Interactivity in Visualizations</b>. <i>In <!-- -->IEEE Transactions on Visualization and Computer Graphics<!-- -->(<!-- -->TVCG 2019<!-- -->)</i>. <!-- -->IEEE, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1109/TVCG.2018.2802520" target="_blank">https://doi.org/10.1109/TVCG.2018.2802520</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2019-george" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2019-george/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2019-george</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2019 LBW</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2019-george cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2019-george.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2019-george.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2019-george" target="_blank">Improving Texture Discrimination in Virtual Tasks by using Stochastic Resonance</a></h1><p class="meta"><span>Sandeep Zechariah George</span> , <span>Hooman Khosravi</span> , <span>Ryan Peters</span> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a> , <span>Sonny Chan</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>We investigate enhancing virtual haptic experiences by applying Stochastic Resonance or SR noise to the user&#x27;s hands. Specifically, we focus on improving users&#x27; ability to discriminate between virtual textures modelled from nine grits of real sandpaper in a virtual texture discrimination task. We applied mechanical SR noise to the participant&#x27;s skin by attaching five flat actuators to different points on their hand. By fastening a linear voice-coil actuator and a 6-DOF haptic device to participants&#x27; index finger, we enabled them to interact and feel virtual sandpapers while inducing different levels of SR noise. We hypothesize that SR will improve their discrimination performance.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Stochastic Resonance</span><span class="ui brown basic label">Virtual Tasks</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Texture Rendering</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sandeep Zechariah George<!-- -->, <!-- -->Hooman Khosravi<!-- -->, <!-- -->Ryan Peters<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Sonny Chan<!-- -->. <b>Improving Texture Discrimination in Virtual Tasks by using Stochastic Resonance</b>. <i>(<!-- -->CHI 2019 LBW<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->6<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3290607.3312839" target="_blank">10.1145/3290607.3312839</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2019-danyluk" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2019-danyluk/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2019-danyluk</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2019-danyluk cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2019-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2019-danyluk.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2019-danyluk" target="_blank">Look-From Camera Control for 3D Terrain Maps</a></h1><p class="meta"><a href="/people/kurtis-danyluk"><img alt="kurtis-danyluk photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg"/><strong>Kurtis Danyluk</strong></a> , <span>Bernhard Jenny</span> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2019-danyluk.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2019-danyluk.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We introduce three lightweight interactive camera control techniques for 3D terrain maps on touch devices based on a look-from metaphor (Discrete Look-From-At, Continuous Look-From-Forwards, and Continuous Look-From-Towards). These techniques complement traditional touch screen pan, zoom, rotate, and pitch controls allowing viewers to quickly transition between top-down, oblique, and ground-level views. We present the results of a study in which we asked participants to perform elevation comparison and line-of-sight determination tasks using each technique. Our results highlight how look-from techniques can be integrated on top of current direct manipulation navigation approaches by combining several direct manipulation operations into a single look-from operation. Additionally, they show how look-from techniques help viewers complete a variety of common and challenging map-based tasks.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Terrain</span><span class="ui brown basic label">Touch</span><span class="ui brown basic label">Map Interaction</span><span class="ui brown basic label">Look From Camera Control</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kurtis Danyluk<!-- -->, <!-- -->Bernhard Jenny<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Look-From Camera Control for 3D Terrain Maps</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2019<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->12<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3290605.3300594" target="_blank">https://doi.org/10.1145/3290605.3300594</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tei-2019-mikalauskas" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/tei-2019-mikalauskas/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>tei-2019-mikalauskas</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TEI 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="tei-2019-mikalauskas cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tei-2019-mikalauskas.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tei-2019-mikalauskas.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tei-2019-mikalauskas" target="_blank">Beyond the Bare Stage: Exploring Props as Potential Improviser-Controlled Technology</a></h1><p class="meta"><span>Claire Mikalauskas</span> , <span>April Viczko</span> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/tei-2019-mikalauskas.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>tei-2019-mikalauskas.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>While improvised theatre (improv) is often performed on a bare stage, improvisers sometimes incorporate physical props to inspire new directions for a scene and to enrich their performance. A tech booth can improvise light and sound technical elements, but coordinating with improvisers&#x27; actions on-stage is challenging. Our goal is to inform the design of an augmented prop that lets improvisers tangibly control light and sound technical elements while performing. We interviewed five professional improvisers about their use of physical props in improv, and their expectations of a possible augmented prop that controls technical theatre elements. We propose a set of guidelines for the design of an augmented prop that fits with the existing world of unpredictable improvised performance.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Props</span><span class="ui brown basic label">Performer Controlled Technology</span><span class="ui brown basic label">Improvisational Theatre</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Claire Mikalauskas<!-- -->, <!-- -->April Viczko<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Beyond the Bare Stage: Exploring Props as Potential Improviser-Controlled Technology</b>. <i>In <!-- -->Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction<!-- -->(<!-- -->TEI 2019<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->9<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3294109.3295631" target="_blank">https://doi.org/10.1145/3294109.3295631</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tei-2019-tolley" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/tei-2019-tolley/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>tei-2019-tolley</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TEI 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="tei-2019-tolley cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tei-2019-tolley.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tei-2019-tolley.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tei-2019-tolley" target="_blank">WindyWall: Exploring Creative Wind Simulations</a></h1><p class="meta"><span>David Tolley</span> , <span>Thi Ngoc Tram Nguyen</span> , <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a> , <span>Nimesha Ranasinghe</span> , <span>Kensaku Kawauchi</span> , <span>Ching-Chiuan Yen</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/tei-2019-tolley.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>tei-2019-tolley.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Tn11UmsOsTE" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Tn11UmsOsTE?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/Tn11UmsOsTE/maxresdefault.jpg src=https://img.youtube.com/vi/Tn11UmsOsTE/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Wind simulations are typically one-off implementations for specific applications. We introduce WindyWall, a platform for creative design and exploration of wind simulations. WindyWall is a three-panel 90-fan array that encapsulates users with 270? of wind coverage. We describe the design and implementation of the array panels, discussing how the panels can be re-arranged, where various wind simulations can be realized as simple effects. To understand how people perceive &quot;wind&quot; generated from WindyWall, we conducted a pilot study of wind magnitude perception using different wind activation patterns from WindyWall. Our findings suggest that: horizontal wind activations are perceived more readily than vertical ones, and that people&#x27;s perceptions of wind are highly variable-most individuals will rate airflow differently in subsequent exposures. Based on our findings, we discuss the importance of developing a method for characterizing wind simulations, and provide design directions for others using fan arrays to simulate wind.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Tactile Haptic Interaction</span><span class="ui brown basic label">Multimodal Interaction</span><span class="ui brown basic label">Novel Actuators Displays</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Tolley<!-- -->, <!-- -->Thi Ngoc Tram Nguyen<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Nimesha Ranasinghe<!-- -->, <!-- -->Kensaku Kawauchi<!-- -->, <!-- -->Ching-Chiuan Yen<!-- -->. <b>WindyWall: Exploring Creative Wind Simulations</b>. <i>In <!-- -->Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction<!-- -->(<!-- -->TEI 2019<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->10<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3294109.3295624" target="_blank">https://doi.org/10.1145/3294109.3295624</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tei-2019-wun" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/tei-2019-wun/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>tei-2019-wun</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TEI 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="tei-2019-wun cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tei-2019-wun.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tei-2019-wun.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tei-2019-wun" target="_blank">You say Potato, I say Po-Data: Physical Template Tools for Authoring Visualizations</a></h1><p class="meta"><span>Tiffany Wun</span> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a> , <span>Miriam Sturdee</span> , <a href="/people/sheelagh-carpendale"><img alt="sheelagh-carpendale photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg"/><strong>Sheelagh Carpendale</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/tei-2019-wun.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>tei-2019-wun.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Providing data visualization authoring tools for the general public remains an ongoing challenge. Inspired by block-printing, we explore how visualization stamps as a physical visualization authoring tool could leverage both visual freedom and ease of repetition. We conducted a workshop with two groups---visualization experts and non-experts---where participants authored visualizations on paper using hand-carved stamps made from potatoes and sponges. The low-fidelity medium freed participants to test new stamp patterns and accept mistakes. From the created visualizations, we observed several unique traits and uses of block-printing tools for visualization authoring, including: modularity of patterns, annotation guides, creation of multiple patterns from one stamp, and various techniques to apply data onto paper. We discuss the issues around expressivity and effectiveness of block-printed stamps in visualization authoring, and identify implications for the design and assembly of primitives in potential visualization stamp kits, as well as applications for future use in non-digital environments.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Potato</span><span class="ui brown basic label">Tangible Tools</span><span class="ui brown basic label">Authoring Visualizations</span><span class="ui brown basic label">Block Printing</span><span class="ui brown basic label">Physical Template Tools</span><span class="ui brown basic label">Information Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Tiffany Wun<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Miriam Sturdee<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->. <b>You say Potato, I say Po-Data: Physical Template Tools for Authoring Visualizations</b>. <i>In <!-- -->Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction<!-- -->(<!-- -->TEI 2019<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->10<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3294109.3295627" target="_blank">https://doi.org/10.1145/3294109.3295627</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="vr-2019-satriadi" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/vr-2019-satriadi/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>vr-2019-satriadi</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">IEEE VR 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="vr-2019-satriadi cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/vr-2019-satriadi.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/vr-2019-satriadi.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/vr-2019-satriadi" target="_blank">Augmented Reality Map Navigation with Freehand Gestures</a></h1><p class="meta"><span>Kadek Ananta Satriadi</span> , <span>Barrett Ens</span> , <span>Maxime Cordeil</span> , <span>Bernhard Jenny</span> , <span>Tobias Czauderna</span> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/vr-2019-satriadi.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>vr-2019-satriadi.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/TE6AJEu8zdY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/TE6AJEu8zdY?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/TE6AJEu8zdY/maxresdefault.jpg src=https://img.youtube.com/vi/TE6AJEu8zdY/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Freehand gesture interaction has long been proposed as a `natural&#x27; input method for Augmented Reality (AR) applications, yet has been little explored for intensive applications like multiscale navigation. In multiscale navigation, such as digital map navigation, pan and zoom are the predominant interactions. A position-based input mapping (e.g. grabbing metaphor) is intuitive for such interactions, but is prone to arm fatigue. This work focuses on improving digital map navigation in AR with mid-air hand gestures, using a horizontal intangible map display. First, we conducted a user study to explore the effects of handedness (unimanual and bimanual) and input mapping (position-based and rate-based). From these findings we designed DiveZoom and TerraceZoom, two novel hybrid techniques that smoothly transition between position- and rate-based mappings. A second user study evaluated these designs. Our results indicate that the introduced input-mapping transitions can reduce perceived arm fatigue with limited impact on performance.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Gesture Recognition</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Interactive Devices</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kadek Ananta Satriadi<!-- -->, <!-- -->Barrett Ens<!-- -->, <!-- -->Maxime Cordeil<!-- -->, <!-- -->Bernhard Jenny<!-- -->, <!-- -->Tobias Czauderna<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Augmented Reality Map Navigation with Freehand Gestures</b>. <i>(<!-- -->IEEE VR 2019<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->11<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1109/VR.2019.8798340" target="_blank">https://doi.org/10.1109/VR.2019.8798340</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/jNeEbB3sTn0" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/jNeEbB3sTn0?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/jNeEbB3sTn0/maxresdefault.jpg src=https://img.youtube.com/vi/jNeEbB3sTn0/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="cga-2019-ivanov" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/cga-2019-ivanov/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>cga-2019-ivanov</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">IEEE CG&amp;A 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="cga-2019-ivanov cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/cga-2019-ivanov.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/cga-2019-ivanov.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/cga-2019-ivanov" target="_blank">A Walk Among the Data</a></h1><p class="meta"><a href="/people/sasha-ivanov"><img alt="sasha-ivanov photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sasha-ivanov.jpg 1x" src="/pr-preview/pr-121/static/images/people/sasha-ivanov.jpg"/><strong>Alexander Ivanov</strong></a> , <a href="/people/kurtis-danyluk"><img alt="kurtis-danyluk photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-121/static/images/people/kurtis-danyluk.jpg"/><strong>Kurtis Danyluk</strong></a> , <span>Christian Jacob</span> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/cga-2019-ivanov.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>cga-2019-ivanov.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We examine the potential for immersive unit visualizations—interactive virtual environments populated with objects representing individual items in a dataset. Our virtual reality prototype highlights how immersive unit visualizations can allow viewers to examine data at multiple scales, support immersive exploration, and create affective personal experiences with data.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Art</span><span class="ui brown basic label">Tools</span><span class="ui brown basic label">Virtual Environments</span><span class="ui brown basic label">Two Dimensional Displays</span><span class="ui brown basic label">Anthropomorphism</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Alexander Ivanov<!-- -->, <!-- -->Kurtis Danyluk<!-- -->, <!-- -->Christian Jacob<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>A Walk Among the Data</b>. <i>(<!-- -->IEEE CG&amp;A 2019<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->9<!-- -->. <!-- -->DOI: <a href="http://dx.doi.org/10.1109/MCG.2019.2898941" target="_blank">http://dx.doi.org/10.1109/MCG.2019.2898941</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2018-suzuki" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/uist-2018-suzuki/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2018-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2018-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/uist-2018-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/uist-2018-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2018-suzuki" target="_blank">Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a> , <span>Junichi Yamaoka</span> , <span>Daniel Leithinger</span> , <span>Tom Yeh</span> , <span>Mark D. Gross</span> , <span>Yoshihiro Kawahara</span> , <span>Yasuaki Kakehi</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2018-suzuki.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2018-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/92eGI-gYYc4" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/92eGI-gYYc4?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/92eGI-gYYc4/maxresdefault.jpg src=https://img.youtube.com/vi/92eGI-gYYc4/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces Dynamic 3D Printing, a fast and reconstructable shape formation system. Dynamic 3D Printing can assemble an arbitrary three-dimensional shape from a large number of small physical elements. Also, it can disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbitrary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and implementation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long-term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper, we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Digital Materials</span><span class="ui brown basic label">Dynamic 3 D Printing</span><span class="ui brown basic label">Shape Displays</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Junichi Yamaoka<!-- -->, <!-- -->Daniel Leithinger<!-- -->, <!-- -->Tom Yeh<!-- -->, <!-- -->Mark D. Gross<!-- -->, <!-- -->Yoshihiro Kawahara<!-- -->, <!-- -->Yasuaki Kakehi<!-- -->. <b>Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation</b>. <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- -->(<!-- -->UIST 2018<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->16<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3242587.3242659" target="_blank">https://doi.org/10.1145/3242587.3242659</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/R3FRUtOIiCQ" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/R3FRUtOIiCQ?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/R3FRUtOIiCQ/maxresdefault.jpg src=https://img.youtube.com/vi/R3FRUtOIiCQ/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2018-mikalauskas" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/dis-2018-mikalauskas/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2018-mikalauskas</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2018-mikalauskas cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2018-mikalauskas.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2018-mikalauskas.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2018-mikalauskas" target="_blank">Improvising with an Audience-Controlled Robot Performer</a></h1><p class="meta"><span>Claire Mikalauskas</span> , <span>Tiffany Wun</span> , <span>Kevin Ta</span> , <span>Joshua Horacsek</span> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/dis-2018-mikalauskas.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>dis-2018-mikalauskas.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In improvisational theatre (improv), actors perform unscripted scenes together, collectively creating a narrative. Audience suggestions introduce randomness and build audience engagement, but can be challenging to mediate at scale. We present Robot Improv Puppet Theatre (RIPT), which includes a performance robot (Pokey) who performs gestures and dialogue in short-form improv scenes based on audience input from a mobile interface. We evaluated RIPT in several initial informal performances, and in a rehearsal with seven professional improvisers. The improvisers noted how audience prompts can have a big impact on the scene - highlighting the delicate balance between ambiguity and constraints in improv. The open structure of RIPT performances allows for multiple interpretations of how to perform with Pokey, including one-on-one conversations or multi-performer scenes. While Pokey lacks key qualities of a good improviser, improvisers found his serendipitous dialogue and gestures particularly rewarding.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Improvised Theatre</span><span class="ui brown basic label">Creativity Support Tools</span><span class="ui brown basic label">Crowdsourcing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Claire Mikalauskas<!-- -->, <!-- -->Tiffany Wun<!-- -->, <!-- -->Kevin Ta<!-- -->, <!-- -->Joshua Horacsek<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Improvising with an Audience-Controlled Robot Performer</b>. <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- -->(<!-- -->DIS 2018<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->10<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3196709.3196757" target="_blank">https://doi.org/10.1145/3196709.3196757</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2018-pham" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/dis-2018-pham/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2018-pham</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2018-pham cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2018-pham.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2018-pham.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2018-pham" target="_blank">Scale Impacts Elicited Gestures for Manipulating Holograms: Implications for AR Gesture Design</a></h1><p class="meta"><span>Tran Pham</span> , <span>Jo Vermeulen</span> , <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a> , <span>Lindsay MacDonald Vermeulen</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/dis-2018-pham.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>dis-2018-pham.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Because gesture design for augmented reality (AR) remains idiosyncratic, people cannot necessarily use gestures learned in one AR application in another. To design discoverable gestures, we need to understand what gestures people expect to use. We explore how the scale of AR affects the gestures people expect to use to interact with 3D holograms. Using an elicitation study, we asked participants to generate gestures in response to holographic task referents, where we varied the scale of holograms from desktop-scale to room-scale objects. We found that the scale of objects and scenes in the AR experience moderates the generated gestures. Most gestures were informed by physical interaction, and when people interacted from a distance, they sought a good perspective on the target object before and during the interaction. These results suggest that gesture designers need to account for scale, and should not simply reuse gestures across different hologram sizes.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Gestures</span><span class="ui brown basic label">Gesture Elicitation</span><span class="ui brown basic label">Hololens</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Tran Pham<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lindsay MacDonald Vermeulen<!-- -->. <b>Scale Impacts Elicited Gestures for Manipulating Holograms: Implications for AR Gesture Design</b>. <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- -->(<!-- -->DIS 2018<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->14<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3196709.3196719" target="_blank">https://doi.org/10.1145/3196709.3196719</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2018-ta" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/dis-2018-ta/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2018-ta</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2018-ta cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2018-ta.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2018-ta.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2018-ta" target="_blank">Bod-IDE: An Augmented Reality Sandbox for eFashion Garments</a></h1><p class="meta"><span>Kevin Ta</span> , <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/dis-2018-ta.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>dis-2018-ta.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Electronic fashion (eFashion) garments use technology to augment the human body with wearable interaction. In developing ideas, eFashion designers need to prototype the role and behavior of the interactive garment in context; however, current wearable prototyping toolkits require semi-permanent construction with physical materials that cannot easily be altered. We present Bod-IDE, an augmented reality &#x27;mirror&#x27; that allows eFashion designers to create virtual interactive garment prototypes. Designers can quickly build, refine, and test on-the-body interactions without the need to connect or program electronics. By envisioning interaction with the body in mind, eFashion designers can focus more on reimagining the relationship between bodies, clothing, and technology.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Electronic Fashion</span><span class="ui brown basic label">Creativity Support Tool</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kevin Ta<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Bod-IDE: An Augmented Reality Sandbox for eFashion Garments</b>. <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- -->(<!-- -->DIS 2018<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->5<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3197391.3205408" target="_blank">https://doi.org/10.1145/3197391.3205408</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tei-2016-somanath" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/tei-2016-somanath/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>tei-2016-somanath</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TEI 2016</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="tei-2016-somanath cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tei-2016-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tei-2016-somanath.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tei-2016-somanath" target="_blank">Engaging &#x27;At-Risk&#x27; Students through Maker Culture Activities</a></h1><p class="meta"><a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a> , <span>Laura Morrison</span> , <span>Janette Hughes</span> , <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a> , <span>Mario Costa Sousa</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/tei-2016-somanath.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>tei-2016-somanath.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>This paper presents a set of lessons learnt from introducing maker culture and DIY paradigms to &#x27;at-risk&#x27; students (age 12-14). Our goal is to engage &#x27;at-risk&#x27; students through maker culture activities. While improved technology literacy is one of the outcomes we also wanted the learners to use technology to realize concepts and ideas, and to gain freedom of thinking similar to creators, artists and designers. We present our study and a set of high level suggestions to enable thinking about how maker culture activities can facilitate engagement and creative use of technology by 1) thinking about creativity in task, 2) facilitating different entry points, 3) the importance of personal relevance, and 4) relevance to education.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">DIY</span><span class="ui brown basic label">At Risk Students</span><span class="ui brown basic label">Maker Culture</span><span class="ui brown basic label">Education</span><span class="ui brown basic label">Young Learners</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sowmya Somanath<!-- -->, <!-- -->Laura Morrison<!-- -->, <!-- -->Janette Hughes<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Mario Costa Sousa<!-- -->. <b>Engaging &#x27;At-Risk&#x27; Students through Maker Culture Activities</b>. <i>In <!-- -->Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction<!-- -->(<!-- -->TEI 2016<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->9<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/2839462.2839482" target="_blank">https://doi.org/10.1145/2839462.2839482</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-oh" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2018-oh/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2018-oh</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2018-oh cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2018-oh.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2018-oh.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-oh" target="_blank">PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-Based Electronic Devices</a></h1><p class="meta"><span>Hyunjoo Oh</span> , <span>Tung D. Ta</span> , <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a> , <span>Mark D. Gross</span> , <span>Yoshihiro Kawahara</span> , <span>Lining Yao</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2018-oh.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2018-oh.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/DTd863suDN0" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/DTd863suDN0?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/DTd863suDN0/maxresdefault.jpg src=https://img.youtube.com/vi/DTd863suDN0/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present PEP (Printed Electronic Papercrafts), a set of design and fabrication techniques to integrate electronic based interactivities into printed papercrafts via 3D sculpting. We explore the design space of PEP, integrating four functions into 3D paper products: actuation, sensing, display, and communication, leveraging the expressive and technical opportunities enabled by paper-like functional layers with a stack of paper. We outline a seven-step workflow, introduce a design tool we developed as an add-on to an existing CAD environment, and demonstrate example applications that combine the electronic enabled functionality, the capability of 3D sculpting, and the unique creative affordances by the materiality of paper.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Paper Electronics</span><span class="ui brown basic label">3 D Sculpting</span><span class="ui brown basic label">Paper Craft</span><span class="ui brown basic label">Fabrication Techniques</span><span class="ui brown basic label">Prototyping</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Hyunjoo Oh<!-- -->, <!-- -->Tung D. Ta<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Mark D. Gross<!-- -->, <!-- -->Yoshihiro Kawahara<!-- -->, <!-- -->Lining Yao<!-- -->. <b>PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-Based Electronic Devices</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2018<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->12<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3173574.3174015" target="_blank">https://doi.org/10.1145/3173574.3174015</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-suzuki" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2018-suzuki/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2018-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2018-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2018-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2018-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-suzuki" target="_blank">Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a> , <span>Jun Kato</span> , <span>Mark D. Gross</span> , <span>Tom Yeh</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2018-suzuki.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2018-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Gb7brajKCVE" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Gb7brajKCVE?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/Gb7brajKCVE/maxresdefault.jpg src=https://img.youtube.com/vi/Gb7brajKCVE/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We explore a new approach to programming swarm user interfaces (Swarm UI) by leveraging direct physical manipulation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high-level interface design. Inspired by current UI programming practices, we introduce a four-step workflow-create elements, abstract attributes, specify behaviors, and propagate changes-for Swarm UI programming. We propose a set of direct physical manipulation techniques to support each step in this workflow. To demonstrate these concepts, we developed Reactile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studies-an in-class survey with 148 students and a lab interview with eight participants-confirm that our approach is intuitive and understandable for programming Swarm UIs.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Direct Manipulation</span><span class="ui brown basic label">Tangible Programming</span><span class="ui brown basic label">Swarm User Interfaces</span><span class="ui brown basic label">Programming By Demonstration</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Jun Kato<!-- -->, <!-- -->Mark D. Gross<!-- -->, <!-- -->Tom Yeh<!-- -->. <b>Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2018<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3173574.3173773" target="_blank">https://doi.org/10.1145/3173574.3173773</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-dillman" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2018-dillman/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2018-dillman</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2018-dillman cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2018-dillman.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2018-dillman.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-dillman" target="_blank">A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality</a></h1><p class="meta"><span>Kody R. Dillman</span> , <a href="/people/terrance-mok"><img alt="terrance-mok photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/terrance-mok.jpg 1x" src="/pr-preview/pr-121/static/images/people/terrance-mok.jpg"/><strong>Terrance Mok</strong></a> , <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a> , <span>Alex Mitchell</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2018-dillman.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2018-dillman.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Based on an analysis of 49 popular contemporary video games, we develop a descriptive framework of visual interaction cues in video games. These cues are used to inform players what can be interacted with, where to look, and where to go within the game world. These cues vary along three dimensions: the purpose of the cue, the visual design of the cue, and the circumstances under which the cue is shown. We demonstrate that this framework can also be used to describe interaction cues for augmented reality applications. Beyond this, we show how the framework can be used to generatively derive new design ideas for visual interaction cues in augmented reality experiences.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Game Design</span><span class="ui brown basic label">Guidance</span><span class="ui brown basic label">Interaction Cues</span><span class="ui brown basic label">Augmented Reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kody R. Dillman<!-- -->, <!-- -->Terrance Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Alex Mitchell<!-- -->. <b>A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2018<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->12<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3173574.3173714" target="_blank">https://doi.org/10.1145/3173574.3173714</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/3FoZStToALQ" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/3FoZStToALQ?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/3FoZStToALQ/maxresdefault.jpg src=https://img.youtube.com/vi/3FoZStToALQ/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-feick" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2018-feick/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2018-feick</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2018-feick cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2018-feick.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2018-feick.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-feick" target="_blank">Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration</a></h1><p class="meta"><a href="/people/martin-feick"><img alt="martin-feick photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/martin-feick.jpg 1x" src="/pr-preview/pr-121/static/images/people/martin-feick.jpg"/><strong>Martin Feick</strong></a> , <span>Terrance Tin Hoi Mok</span> , <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a> , <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2018-feick.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2018-feick.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/sfxTHsPJWHY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/sfxTHsPJWHY?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/sfxTHsPJWHY/maxresdefault.jpg src=https://img.youtube.com/vi/sfxTHsPJWHY/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Remote collaborators working together on physical objects have difficulty building a shared understanding of what each person is talking about. Conventional video chat systems are insufficient for many situations because they present a single view of the object in a flattened image. To understand how this limited perspective affects collaboration, we designed the Remote Manipulator (ReMa), which can reproduce orientation manipulations on a proxy object at a remote site. We conducted two studies with ReMa, with two main findings. First, a shared perspective is more effective and preferred compared to the opposing perspective offered by conventional video chat systems. Second, the physical proxy and video chat complement one another in a combined system: people used the physical proxy to understand objects, and used video chat to perform gestures and confirm remote actions.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Cscw</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Object Focused Collaboration</span><span class="ui brown basic label">Physical Telepresence</span><span class="ui brown basic label">Collaborative Physical Tasks</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Martin Feick<!-- -->, <!-- -->Terrance Tin Hoi Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2018<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3173574.3173855" target="_blank">https://doi.org/10.1145/3173574.3173855</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-heshmat" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2018-heshmat/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2018-heshmat</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2018-heshmat cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2018-heshmat.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2018-heshmat.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-heshmat" target="_blank">Geocaching with a Beam: Shared Outdoor Activities through a Telepresence Robot with 360 Degree Viewing</a></h1><p class="meta"><span>Yasamin Heshmat</span> , <a href="/people/brennan-jones"><img alt="brennan-jones photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/brennan-jones.jpg 1x" src="/pr-preview/pr-121/static/images/people/brennan-jones.jpg"/><strong>Brennan Jones</strong></a> , <span>Xiaoxuan Xiong</span> , <a href="/people/carman-neustaedter"><img alt="carman-neustaedter photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/carman-neustaedter.jpg 1x" src="/pr-preview/pr-121/static/images/people/carman-neustaedter.jpg"/><strong>Carman Neustaedter</strong></a> , <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a> , <span>Bernhard E. Riecke</span> , <span>Lillian Yang</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2018-heshmat.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2018-heshmat.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>People often enjoy sharing outdoor activities together such as walking and hiking. However, when family and friends are separated by distance it can be difficult if not impossible to share such activities. We explore this design space by investigating the benefits and challenges of using a telepresence robot to support outdoor leisure activities. In our study, participants participated in the outdoor activity of geocaching where one person geocached with the help of a remote partner via a telepresence robot. We compared a wide field of view (WFOV) camera to a 360° camera. Results show the benefits of having a physical embodiment and a sense of immersion with the 360° view. Yet challenges related to a lack of environmental awareness, safety issues, and privacy concerns resulting from bystander interactions. These findings illustrate the need to design telepresence robots with the environment and public in mind to provide an enhanced sensory experience while balancing safety and privacy issues resulting from being amongst the general public.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Video Communication</span><span class="ui brown basic label">Telepresence Robots</span><span class="ui brown basic label">Leisure Activities</span><span class="ui brown basic label">Social Presence</span><span class="ui brown basic label">Geocaching</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Yasamin Heshmat<!-- -->, <!-- -->Brennan Jones<!-- -->, <!-- -->Xiaoxuan Xiong<!-- -->, <!-- -->Carman Neustaedter<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Bernhard E. Riecke<!-- -->, <!-- -->Lillian Yang<!-- -->. <b>Geocaching with a Beam: Shared Outdoor Activities through a Telepresence Robot with 360 Degree Viewing</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2018<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3173574.3173933" target="_blank">https://doi.org/10.1145/3173574.3173933</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-ledo" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2018-ledo/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2018-ledo</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2018-ledo cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2018-ledo.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2018-ledo.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-ledo" target="_blank">Evaluation Strategies for HCI Toolkit Research</a></h1><p class="meta"><a href="/people/david-ledo"><img alt="david-ledo photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/david-ledo.jpg 1x" src="/pr-preview/pr-121/static/images/people/david-ledo.jpg"/><strong>David Ledo</strong></a> , <span>Steven Houben</span> , <span>Jo Vermeulen</span> , <a href="/people/nicolai-marquardt"><img alt="nicolai-marquardt photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/nicolai-marquardt.jpg 1x" src="/pr-preview/pr-121/static/images/people/nicolai-marquardt.jpg"/><strong>Nicolai Marquardt</strong></a> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a> , <a href="/people/saul-greenberg"><img alt="saul-greenberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/saul-greenberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/saul-greenberg.jpg"/><strong>Saul Greenberg</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2018-ledo.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2018-ledo.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/3lAwhCk60C4" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/3lAwhCk60C4?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/3lAwhCk60C4/maxresdefault.jpg src=https://img.youtube.com/vi/3lAwhCk60C4/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Toolkit research plays an important role in the field of HCI, as it can heavily influence both the design and implementation of interactive systems. For publication, the HCI community typically expects toolkit research to include an evaluation component. The problem is that toolkit evaluation is challenging, as it is often unclear what &#x27;evaluating&#x27; a toolkit means and what methods are appropriate. To address this problem, we analyzed 68 published toolkit papers. From our analysis, we provide an overview of, reflection on, and discussion of evaluation methods for toolkit contributions. We identify and discuss the value of four toolkit evaluation strategies, including the associated techniques that each employs. We offer a categorization of evaluation strategies for toolkit researchers, along with a discussion of the value, potential limitations, and trade-offs associated with each strategy.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">User Interfaces</span><span class="ui brown basic label">Design</span><span class="ui brown basic label">Evaluation</span><span class="ui brown basic label">Prototyping</span><span class="ui brown basic label">Toolkits</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Ledo<!-- -->, <!-- -->Steven Houben<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Nicolai Marquardt<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Saul Greenberg<!-- -->. <b>Evaluation Strategies for HCI Toolkit Research</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2018<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->17<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3173574.3173610" target="_blank">https://doi.org/10.1145/3173574.3173610</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/NOhsvN_Kv-I" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/NOhsvN_Kv-I?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/NOhsvN_Kv-I/maxresdefault.jpg src=https://img.youtube.com/vi/NOhsvN_Kv-I/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-mahadevan" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2018-mahadevan/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2018-mahadevan</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2018-mahadevan cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2018-mahadevan.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2018-mahadevan.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-mahadevan" target="_blank">Communicating Awareness and Intent in Autonomous Vehicle-Pedestrian Interaction</a></h1><p class="meta"><a href="/people/karthik-mahadevan"><img alt="karthik-mahadevan photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/karthik-mahadevan.jpg 1x" src="/pr-preview/pr-121/static/images/people/karthik-mahadevan.jpg"/><strong>Karthik Mahadevan</strong></a> , <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a> , <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2018-mahadevan.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2018-mahadevan.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/D_hhcGVREGA" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/D_hhcGVREGA?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/D_hhcGVREGA/maxresdefault.jpg src=https://img.youtube.com/vi/D_hhcGVREGA/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Drivers use nonverbal cues such as vehicle speed, eye gaze, and hand gestures to communicate awareness and intent to pedestrians. Conversely, in autonomous vehicles, drivers can be distracted or absent, leaving pedestrians to infer awareness and intent from the vehicle alone. In this paper, we investigate the usefulness of interfaces (beyond vehicle movement) that explicitly communicate awareness and intent of autonomous vehicles to pedestrians, focusing on crosswalk scenarios. We conducted a preliminary study to gain insight on designing interfaces that communicate autonomous vehicle awareness and intent to pedestrians. Based on study outcomes, we developed four prototype interfaces and deployed them in studies involving a Segway and a car. We found interfaces communicating vehicle awareness and intent: (1) can help pedestrians attempting to cross; (2) are not limited to the vehicle and can exist in the environment; and (3) should use a combination of modalities such as visual, auditory, and physical.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Autonomous Vehicle Pedestrian Interaction</span><span class="ui brown basic label">Perceived Awareness And Intent In Autonomous Vehicles</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Karthik Mahadevan<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>Communicating Awareness and Intent in Autonomous Vehicle-Pedestrian Interaction</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2018<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->12<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3173574.3174003" target="_blank">https://doi.org/10.1145/3173574.3174003</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/08OEKuz93dY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/08OEKuz93dY?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/08OEKuz93dY/maxresdefault.jpg src=https://img.youtube.com/vi/08OEKuz93dY/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-neustaedter" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2018-neustaedter/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2018-neustaedter</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2018-neustaedter cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2018-neustaedter.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2018-neustaedter.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-neustaedter" target="_blank">The Benefits and Challenges of Video Calling for Emergency Situations</a></h1><p class="meta"><a href="/people/carman-neustaedter"><img alt="carman-neustaedter photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/carman-neustaedter.jpg 1x" src="/pr-preview/pr-121/static/images/people/carman-neustaedter.jpg"/><strong>Carman Neustaedter</strong></a> , <a href="/people/brennan-jones"><img alt="brennan-jones photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/brennan-jones.jpg 1x" src="/pr-preview/pr-121/static/images/people/brennan-jones.jpg"/><strong>Brennan Jones</strong></a> , <span>Kenton O&#x27;Hara</span> , <span>Abigail Sellen</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2018-neustaedter.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2018-neustaedter.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In the coming years, emergency calling services in North America will begin to incorporate new modalities for reporting emergencies, including video-based calling. The challenge is that we know little of how video calling systems should be designed and what benefits or challenges video calling might bring. We conducted observations and contextual interviews within three emergency response call centres to investigate these points. We focused on the work practices of call takers and dispatchers. Results show that video calls could provide valuable contextual information about a situation and help to overcome call taker challenges with information ambiguity, location, deceit, and communication issues. Yet video calls have the potential to introduce issues around control, information overload, and privacy if systems are not designed well. These results point to the need to think about emergency video calling along a continuum of visual modalities ranging from audio calls accompanied with images or video clips to one-way video streams to two-way video streams where camera control and camera work need to be carefully designed.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Collaboration</span><span class="ui brown basic label">Situation Awareness</span><span class="ui brown basic label">Emergency Calling</span><span class="ui brown basic label">Call Takers</span><span class="ui brown basic label">Mobile Video Calling</span><span class="ui brown basic label">Dispatchers</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Carman Neustaedter<!-- -->, <!-- -->Brennan Jones<!-- -->, <!-- -->Kenton O&#x27;Hara<!-- -->, <!-- -->Abigail Sellen<!-- -->. <b>The Benefits and Challenges of Video Calling for Emergency Situations</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2018<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3173574.3174231" target="_blank">https://doi.org/10.1145/3173574.3174231</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-wuertz" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2018-wuertz/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2018-wuertz</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2018-wuertz cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2018-wuertz.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2018-wuertz.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-wuertz" target="_blank">A Design Framework for Awareness Cues in Distributed Multiplayer Games</a></h1><p class="meta"><span>Jason Wuertz</span> , <span>Sultan A. Alharthi</span> , <span>William A. Hamilton</span> , <span>Scott Bateman</span> , <a href="/people/carl-gutwin"><img alt="carl-gutwin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/carl-gutwin.jpg 1x" src="/pr-preview/pr-121/static/images/people/carl-gutwin.jpg"/><strong>Carl Gutwin</strong></a> , <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a> , <span>Zachary O. Toups</span> , <span>Jessica Hammer</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2018-wuertz.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2018-wuertz.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In the physical world, teammates develop situation awareness about each other&#x27;s location, status, and actions through cues such as gaze direction and ambient noise. To support situation awareness, distributed multiplayer games provide awareness cues - information that games automatically make available to players to support cooperative gameplay. The design of awareness cues can be extremely complex, impacting how players experience games and work with teammates. Despite the importance of awareness cues, designers have little beyond experiential knowledge to guide their design. In this work, we describe a design framework for awareness cues, providing insight into what information they provide, how they communicate this information, and how design choices can impact play experience. Our research, based on a grounded theory analysis of current games, is the first to provide a characterization of awareness cues, providing a palette for game designers to improve design practice and a starting point for deeper research into collaborative play.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Workspace Awareness</span><span class="ui brown basic label">Situation Awareness</span><span class="ui brown basic label">Game Design</span><span class="ui brown basic label">Distributed Multiplayer Games</span><span class="ui brown basic label">Awareness Cues</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jason Wuertz<!-- -->, <!-- -->Sultan A. Alharthi<!-- -->, <!-- -->William A. Hamilton<!-- -->, <!-- -->Scott Bateman<!-- -->, <!-- -->Carl Gutwin<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Zachary O. Toups<!-- -->, <!-- -->Jessica Hammer<!-- -->. <b>A Design Framework for Awareness Cues in Distributed Multiplayer Games</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2018<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->14<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3173574.3173817" target="_blank">https://doi.org/10.1145/3173574.3173817</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="hri-2018-feick" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/hri-2018-feick/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>hri-2018-feick</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">HRI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="hri-2018-feick cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/hri-2018-feick.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/hri-2018-feick.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/hri-2018-feick" target="_blank">The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</a></h1><p class="meta"><a href="/people/martin-feick"><img alt="martin-feick photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/martin-feick.jpg 1x" src="/pr-preview/pr-121/static/images/people/martin-feick.jpg"/><strong>Martin Feick</strong></a> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a> , <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a> , <span>André Miede</span> , <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/hri-2018-feick.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>hri-2018-feick.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this paper, we discuss the role of the movement trajectory and velocity enabled by our tele-robotic system (ReMa) for remote collaboration on physical tasks. Our system reproduces changes in object orientation and position at a remote location using a humanoid robotic arm. However, even minor kinematics differences between robot and human arm can result in awkward or exaggerated robot movements. As a result, user communication with the robotic system can become less efficient, less fluent and more time intensive.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Movement Trajectory Velocity</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Robot Surrogate</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Martin Feick<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->André Miede<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</b>. <i>In <!-- -->Adjunct Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction<!-- -->(<!-- -->HRI 2018<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->2<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3173386.3176959" target="_blank">https://doi.org/10.1145/3173386.3176959</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="assets-2017-suzuki" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/assets-2017-suzuki/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>assets-2017-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">ASSETS 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="assets-2017-suzuki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/assets-2017-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/assets-2017-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/assets-2017-suzuki" target="_blank">FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-121/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a> , <span>Abigale Stangl</span> , <span>Mark D. Gross</span> , <span>Tom Yeh</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/assets-2017-suzuki.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>assets-2017-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/VbwIZ9V6i_g" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/VbwIZ9V6i_g?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/VbwIZ9V6i_g/maxresdefault.jpg src=https://img.youtube.com/vi/VbwIZ9V6i_g/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>For people with visual impairments, tactile graphics are an important means to learn and explore information. However, raised line tactile graphics created with traditional materials such as embossing are static. While available refreshable displays can dynamically change the content, they are still too expensive for many users, and are limited in size. These factors limit wide-spread adoption and the representation of large graphics or data sets. In this paper, we present FluxMaker, an inexpensive scalable system that renders dynamic information on top of static tactile graphics with movable tactile markers. These dynamic tactile markers can be easily reconfigured and used to annotate static raised line tactile graphics, including maps, graphs, and diagrams. We developed a hardware prototype that actuates magnetic tactile markers driven by low-cost and scalable electromagnetic coil arrays, which can be fabricated with standard printed circuit board manufacturing. We evaluate our prototype with six participants with visual impairments and found positive results across four application areas: location finding or navigating on tactile maps, data analysis, and physicalization, feature identification for tactile graphics, and drawing support. The user study confirms advantages in application domains such as education and data exploration.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Visual Impairment</span><span class="ui brown basic label">Dynamic Tactile Markers</span><span class="ui brown basic label">Tangible Interfaces</span><span class="ui brown basic label">Interactive Tactile Graphics</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Abigale Stangl<!-- -->, <!-- -->Mark D. Gross<!-- -->, <!-- -->Tom Yeh<!-- -->. <b>FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers</b>. <i>(<!-- -->ASSETS 2020<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->10<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3132525.3132548" target="_blank">https://doi.org/10.1145/3132525.3132548</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="sui-2017-li" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/sui-2017-li/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>sui-2017-li</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">SUI 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="sui-2017-li cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/sui-2017-li.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/sui-2017-li.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/sui-2017-li" target="_blank">Visibility Perception and Dynamic Viewsheds for Topographic Maps and Models</a></h1><p class="meta"><span>Nico Li</span> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a> , <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a> , <span>Mario Costa Sousa</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/sui-2017-li.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>sui-2017-li.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/275404995" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/275404995?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://i.vimeocdn.com/video/707686230_640.webp src=https://i.vimeocdn.com/video/707686230_640.webp&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We compare the effectiveness of 2D maps and 3D terrain models for visibility tasks and demonstrate how interactive dynamic viewsheds can improve performance for both types of terrain representations. In general, the two-dimensional nature of classic topographic maps limits their legibility and can make complex yet typical cartographic tasks like determining the visibility between locations difficult. Both 3D physical models and interactive techniques like dynamic viewsheds have the potential to improve viewers&#x27; understanding of topography, but their impact has not been deeply explored. We evaluate the effectiveness of 2D maps, 3D models, and interactive viewsheds for both simple and complex visibility tasks. Our results demonstrate the benefits of the dynamic viewshed technique and highlight opportunities for additional tactile interactions. Based on these findings we present guidelines for improving the design and usability of future topographic maps and models.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Terrain Visualization</span><span class="ui brown basic label">Geospatial Visualization</span><span class="ui brown basic label">Dynamic Viewshed</span><span class="ui brown basic label">Topographic Maps</span><span class="ui brown basic label">Tangible User Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nico Li<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Mario Costa Sousa<!-- -->. <b>Visibility Perception and Dynamic Viewsheds for Topographic Maps and Models</b>. <i>(<!-- -->SUI 2017<!-- -->)</i>. <!-- --> <!-- -->Page: 1-<!-- -->9<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3131277.3132178" target="_blank">https://doi.org/10.1145/3131277.3132178</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/aVXUojoQF60" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/aVXUojoQF60?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/aVXUojoQF60/maxresdefault.jpg src=https://img.youtube.com/vi/aVXUojoQF60/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2017-mok" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/dis-2017-mok/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2017-mok</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2017-mok cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2017-mok.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2017-mok.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2017-mok" target="_blank">Critiquing Physical Prototypes for a Remote Audience</a></h1><p class="meta"><a href="/people/terrance-mok"><img alt="terrance-mok photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/terrance-mok.jpg 1x" src="/pr-preview/pr-121/static/images/people/terrance-mok.jpg"/><strong>Terrance Mok</strong></a> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/dis-2017-mok.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>dis-2017-mok.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present an observational study of physical prototype critique that highlights some of the challenges of communicating physical behaviors and materiality at a distance. Geographically distributed open hardware communities often conduct user feedback and peer critique sessions via video conference. However, people have difficulty using current video conferencing tools to demonstrate and critique physical designs. To examine the challenges of remote critique, we conducted an observational lab study in which participants critiqued pairs of physical prototypes (prosthetic hands) for a face-to-face or remote collaborator. In both conditions, participants&#x27; material experiences were an important part of their critique, however their attention was divided between interacting with the prototype and finding strategies to communicate `invisible&#x27; features. Based on our findings, we propose design implications for remote collaboration tools that support the sharing of material experiences and prototype critique.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Design Review</span><span class="ui brown basic label">Prototype Critique</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Material Experience</span><span class="ui brown basic label">Open Hardware</span><span class="ui brown basic label">Video Conferencing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Terrance Mok<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Critiquing Physical Prototypes for a Remote Audience</b>. <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- -->(<!-- -->DIS 2017<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3064663.3064722" target="_blank">https://doi.org/10.1145/3064663.3064722</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2017-aoki" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2017-aoki/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2017-aoki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2017-aoki cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2017-aoki.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2017-aoki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2017-aoki" target="_blank">Environmental Protection and Agency: Motivations, Capacity, and Goals in Participatory Sensing</a></h1><p class="meta"><span>Paul Aoki</span> , <span>Allison Woodruff</span> , <span>Baladitya Yellapragada</span> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2017-aoki.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2017-aoki.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this paper we consider various genres of citizen science from the perspective of citizen participants. As a mode of scientific inquiry, citizen science has the potential to &quot;scale up&quot; scientific data collection efforts and increase lay engagement with science. However, current technological directions risk losing sight of the ways in which citizen science is actually practiced. As citizen science is increasingly used to describe a wide range of activities, we begin by presenting a framework of citizen science genres. We then present findings from four interlocking qualitative studies and technological interventions of community air quality monitoring efforts, examining the motivations and capacities of citizen participants and characterizing their alignment with different types of citizen science. Based on these studies, we suggest that data acquisition involves complex multi-dimensional tradeoffs, and the commonly held view that citizen science systems are a win-win for citizens and science may be overstated.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Citizen Science</span><span class="ui brown basic label">Environmental Sensing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Paul Aoki<!-- -->, <!-- -->Allison Woodruff<!-- -->, <!-- -->Baladitya Yellapragada<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Environmental Protection and Agency: Motivations, Capacity, and Goals in Participatory Sensing</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2017<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3025453.3025667" target="_blank">https://doi.org/10.1145/3025453.3025667</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2017-hull" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2017-hull/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2017-hull</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2017-hull cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2017-hull.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2017-hull.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2017-hull" target="_blank">Building with Data: Architectural Models as Inspiration for Data Physicalization</a></h1><p class="meta"><a href="/people/carmen-hull"><img alt="carmen-hull photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/carmen-hull.jpg 1x" src="/pr-preview/pr-121/static/images/people/carmen-hull.jpg"/><strong>Carmen Hull</strong></a> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/bkqLNgYIXek" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/bkqLNgYIXek?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/bkqLNgYIXek/maxresdefault.jpg src=https://img.youtube.com/vi/bkqLNgYIXek/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>In this paper we analyze the role of physical scale models in the architectural design process and apply insights from architecture for the creation and use of data physicalizations. Based on a survey of the architecture literature on model making and ten interviews with practicing architects, we describe the role of physical models as a tool for exploration and communication. From these observations, we identify trends in the use of physical models in architecture, which have the potential to inform the design of data physicalizations. We identify four functions of architectural modeling that can be directly adapted for use in the process of building rich data models. Finally, we discuss how the visualization community can apply observations from architecture to the design of new data physicalizations.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Design Process</span><span class="ui brown basic label">Architectural Models</span><span class="ui brown basic label">Data Physicalization</span><span class="ui brown basic label">Embodied Interaction</span><span class="ui brown basic label">Data Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Carmen Hull<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Building with Data: Architectural Models as Inspiration for Data Physicalization</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2017<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->12<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3025453.3025850" target="_blank">https://doi.org/10.1145/3025453.3025850</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2017-ledo" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2017-ledo/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2017-ledo</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2017-ledo cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2017-ledo.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2017-ledo.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2017-ledo" target="_blank">Pineal: Bringing Passive Objects to Life with Embedded Mobile Devices</a></h1><p class="meta"><a href="/people/david-ledo"><img alt="david-ledo photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/david-ledo.jpg 1x" src="/pr-preview/pr-121/static/images/people/david-ledo.jpg"/><strong>David Ledo</strong></a> , <span>Fraser Anderson</span> , <span>Ryan Schmidt</span> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a> , <a href="/people/saul-greenberg"><img alt="saul-greenberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/saul-greenberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/saul-greenberg.jpg"/><strong>Saul Greenberg</strong></a> , <span>Tovi Grossman</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2017-ledo.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2017-ledo.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/ORN9jljPncc" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/ORN9jljPncc?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/ORN9jljPncc/maxresdefault.jpg src=https://img.youtube.com/vi/ORN9jljPncc/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Interactive, smart objects – customized to individuals and uses – are central to many movements, such as tangibles, the internet of things (IoT), and ubiquitous computing. Yet, rapid prototyping both the form and function of these custom objects can be problematic, particularly for those with limited electronics or programming experience. Designers often need to embed custom circuitry; program its workings; and create a form factor that not only reflects the desired user experience but can also house the required circuitry and electronics. To mitigate this, we created Pineal, a design tool that lets end-users: (1) modify 3D models to include a smart watch or phone as its heart; (2) specify high-level interactive behaviours through visual programming; and (3) have the phone or watch act out such behaviours as the objects&#x27; &quot;smarts&quot;. Furthermore, a series of prototypes show how Pineal exploits mobile sensing and output, and automatically generates 3D printed form-factors for rich, interactive, objects.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Fabrication</span><span class="ui brown basic label">3 D Printing</span><span class="ui brown basic label">Smart Objects</span><span class="ui brown basic label">Rapid Prototyping</span><span class="ui brown basic label">Toolkits</span><span class="ui brown basic label">Prototyping Tool</span><span class="ui brown basic label">Interaction Design</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Ledo<!-- -->, <!-- -->Fraser Anderson<!-- -->, <!-- -->Ryan Schmidt<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Saul Greenberg<!-- -->, <!-- -->Tovi Grossman<!-- -->. <b>Pineal: Bringing Passive Objects to Life with Embedded Mobile Devices</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2017<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->10<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3025453.3025652" target="_blank">https://doi.org/10.1145/3025453.3025652</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/QB0kbcu_CsY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/QB0kbcu_CsY?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/QB0kbcu_CsY/maxresdefault.jpg src=https://img.youtube.com/vi/QB0kbcu_CsY/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2017-somanath" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2017-somanath/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2017-somanath</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2017-somanath cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2017-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2017-somanath.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2017-somanath" target="_blank">&#x27;Maker&#x27; within Constraints: Exploratory Study of Young Learners using Arduino at a High School in India</a></h1><p class="meta"><a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg 1x" src="/pr-preview/pr-121/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a> , <span>Janette Hughes</span> , <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a> , <span>Mario Costa Sousa</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/NpIME1h1mH8" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/NpIME1h1mH8?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/NpIME1h1mH8/maxresdefault.jpg src=https://img.youtube.com/vi/NpIME1h1mH8/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Do-it-yourself (DIY) inspired activities have gained popularity as a means of creative expression and self-directed learning. However, DIY culture is difficult to implement in places with limited technology infrastructure and traditional learning cultures. Our goal is to understand how learners in such a setting react to DIY activities. We present observations from a physical computing workshop with 12 students (13-15 years old) conducted at a high school in India. We observed unique challenges for these students when tackling DIY activities: a high monetary and psychological cost to exploration, limited independent learning resources, difficulties with finding intellectual courage and assumed technical language proficiency. Our participants, however, overcome some of these challenges by adopting their own local strategies: resilience, nonverbal and verbal learning techniques, and creating documentation and fallback circuit versions. Based on our findings, we discuss a set of lessons learned about makerspaces in a context with socio-technical challenges.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">India</span><span class="ui brown basic label">HCI 4 D</span><span class="ui brown basic label">Physical Computing</span><span class="ui brown basic label">DIY</span><span class="ui brown basic label">Young Learners</span><span class="ui brown basic label">Maker Culture</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sowmya Somanath<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Janette Hughes<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Mario Costa Sousa<!-- -->. <b>&#x27;Maker&#x27; within Constraints: Exploratory Study of Young Learners using Arduino at a High School in India</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2017<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/3025453.3025849" target="_blank">https://doi.org/10.1145/3025453.3025849</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tvcg-2017-goffin" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/tvcg-2017-goffin/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>tvcg-2017-goffin</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TVCG 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="tvcg-2017-goffin cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tvcg-2017-goffin.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tvcg-2017-goffin.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tvcg-2017-goffin" target="_blank">An Exploratory Study of Word-Scale Graphics in Data-Rich Text Documents</a></h1><p class="meta"><span>Pascal Goffin</span> , <span>Jeremy Boy</span> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a> , <a href="/people/petra-isenberg"><img alt="petra-isenberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/petra-isenberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/petra-isenberg.jpg"/><strong>Petra Isenberg</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/230834366" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/230834366?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://i.vimeocdn.com/video/651490206_640.webp src=https://i.vimeocdn.com/video/651490206_640.webp&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We contribute an investigation of the design and function of word-scale graphics and visualizations embedded in text documents. Word-scale graphics include both data-driven representations such as word-scale visualizations and sparklines, and non-data-driven visual marks. Their design, function, and use has so far received little research attention. We present the results of an open ended exploratory study with nine graphic designers. The study resulted in a rich collection of different types of graphics, data provenance, and relationships between text, graphics, and data. Based on this corpus, we present a systematic overview of word-scale graphic designs, and examine how designers used them. We also discuss the designers&#x27; goals in creating their graphics, and characterize how they used word-scale graphics to visualize data, add emphasis, and create alternative narratives. Building on these examples, we discuss implications for the design of authoring tools for word-scale graphics and visualizations, and explore how new authoring environments could make it easier for designers to integrate them into documents.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Word Scale Visualization</span><span class="ui brown basic label">Word Scale Graphic</span><span class="ui brown basic label">Text Visualization</span><span class="ui brown basic label">Sparklines</span><span class="ui brown basic label">Authoring Tool</span><span class="ui brown basic label">Information Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Pascal Goffin<!-- -->, <!-- -->Jeremy Boy<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Petra Isenberg<!-- -->. <b>An Exploratory Study of Word-Scale Graphics in Data-Rich Text Documents</b>. <i>In <!-- -->IEEE Transactions on Visualization and Computer Graphics<!-- -->(<!-- -->TVCG 2017<!-- -->)</i>. <!-- -->IEEE, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1109/TVCG.2016.2618797" target="_blank">https://doi.org/10.1109/TVCG.2016.2618797</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tvcg-2017-willett" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/tvcg-2017-willett/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>tvcg-2017-willett</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TVCG 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="tvcg-2017-willett cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/tvcg-2017-willett.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/tvcg-2017-willett.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tvcg-2017-willett" target="_blank">Embedded Data Representations</a></h1><p class="meta"><a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a> , <span>Yvonne Jansen</span> , <span>Pierre Dragicevic</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/tvcg-2017-willett.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>tvcg-2017-willett.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/182971005" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/182971005?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://i.vimeocdn.com/video/592033369_640.webp src=https://i.vimeocdn.com/video/592033369_640.webp&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce embedded data representations, the use of visual and physical representations of data that are deeply integrated with the physical spaces, objects, and entities to which the data refers. Technologies like lightweight wireless displays, mixed reality hardware, and autonomous vehicles are making it increasingly easier to display data in-context. While researchers and artists have already begun to create embedded data representations, the benefits, trade-offs, and even the language necessary to describe and compare these approaches remain unexplored. In this paper, we formalize the notion of physical data referents – the real-world entities and spaces to which data corresponds – and examine the relationship between referents and the visual and physical representations of their data. We differentiate situated representations, which display data in proximity to data referents, and embedded representations, which display data so that it spatially coincides with data referents. Drawing on examples from visualization, ubiquitous computing, and art, we explore the role of spatial indirection, scale, and interaction for embedded representations. We also examine the tradeoffs between non-situated, situated, and embedded data displays, including both visualizations and physicalizations. Based on our observations, we identify a variety of design challenges for embedded data representation, and suggest opportunities for future research and applications.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Data Physicalization</span><span class="ui brown basic label">Ambient Displays</span><span class="ui brown basic label">Ubiquitous Computing</span><span class="ui brown basic label">Augmented Reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Wesley Willett<!-- -->, <!-- -->Yvonne Jansen<!-- -->, <!-- -->Pierre Dragicevic<!-- -->. <b>Embedded Data Representations</b>. <i>In <!-- -->IEEE Transactions on Visualization and Computer Graphics<!-- -->(<!-- -->TVCG 2017<!-- -->)</i>. <!-- -->IEEE, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->10<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1109/TVCG.2016.2598608" target="_blank">https://doi.org/10.1109/TVCG.2016.2598608</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/ZS7lU60xChI" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/ZS7lU60xChI?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/ZS7lU60xChI/maxresdefault.jpg src=https://img.youtube.com/vi/ZS7lU60xChI/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2016-jones" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/dis-2016-jones/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2016-jones</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2016</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2016-jones cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/dis-2016-jones.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/dis-2016-jones.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2016-jones" target="_blank">Elevating Communication, Collaboration, and Shared Experiences in Mobile Video through Drones</a></h1><p class="meta"><a href="/people/brennan-jones"><img alt="brennan-jones photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/brennan-jones.jpg 1x" src="/pr-preview/pr-121/static/images/people/brennan-jones.jpg"/><strong>Brennan Jones</strong></a> , <span>Kody Dillman</span> , <span>Richard Tang</span> , <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a> , <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-121/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a> , <a href="/people/carman-neustaedter"><img alt="carman-neustaedter photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/carman-neustaedter.jpg 1x" src="/pr-preview/pr-121/static/images/people/carman-neustaedter.jpg"/><strong>Carman Neustaedter</strong></a> , <span>Scott Bateman</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/dis-2016-jones.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>dis-2016-jones.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/10hbJHIQVX8" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/10hbJHIQVX8?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/10hbJHIQVX8/maxresdefault.jpg src=https://img.youtube.com/vi/10hbJHIQVX8/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>People are increasingly using mobile video to communicate, collaborate, and share experiences while on the go. Yet this presents challenges in adequately sharing camera views with remote users. In this paper, we study the use of semi-autonomous drones for video conferencing, where an outdoor user (using a smartphone) is connected to a desktop user who can explore the environment from the drone&#x27;s perspective. We describe findings from a study where pairs collaborated to complete shared navigation and search tasks. We illustrate the benefits of providing the desktop user with a view that is elevated, manipulable, and decoupled from the outdoor user. In addition, we articulate how participants overcame challenges in communicating environmental information and navigational cues, negotiated control of the view, and used the drone as a tool for sharing experiences. This provides a new way of thinking about mobile video conferencing where cameras that are decoupled from both users play an integral role in communication, collaboration, and sharing experiences.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Cscw</span><span class="ui brown basic label">Telepresence</span><span class="ui brown basic label">Video Communication</span><span class="ui brown basic label">Shared Experiences</span><span class="ui brown basic label">Teleoperation</span><span class="ui brown basic label">Drones</span><span class="ui brown basic label">Collaboration</span><span class="ui brown basic label">Hri</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Brennan Jones<!-- -->, <!-- -->Kody Dillman<!-- -->, <!-- -->Richard Tang<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Carman Neustaedter<!-- -->, <!-- -->Scott Bateman<!-- -->. <b>Elevating Communication, Collaboration, and Shared Experiences in Mobile Video through Drones</b>. <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- -->(<!-- -->DIS 2016<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/2901790.2901847" target="_blank">https://doi.org/10.1145/2901790.2901847</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tvcg-2016-lopez" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/tvcg-2016-lopez/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>tvcg-2016-lopez</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TVCG 2016</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/publications/tvcg-2016-lopez" target="_blank">Towards An Understanding of Mobile Touch Navigation in a Stereoscopic Viewing Environment for 3D Data Exploration</a></h1><p class="meta"><span>David Lopez</span> , <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a> , <span>Candemir Doger</span> , <a href="/people/tobias-isenberg"><img alt="tobias-isenberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/tobias-isenberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/tobias-isenberg.jpg"/><strong>Tobias Isenberg</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/tvcg-2016-lopez.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>tvcg-2016-lopez.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/jBtHgTYpJl0" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/jBtHgTYpJl0?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/jBtHgTYpJl0/maxresdefault.jpg src=https://img.youtube.com/vi/jBtHgTYpJl0/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We discuss touch-based navigation of 3D visualizations in a combined monoscopic and stereoscopic viewing environment. We identify a set of interaction modes, and a workflow that helps users transition between these modes to improve their interaction experience. In our discussion we analyze, in particular, the control-display space mapping between the different reference frames of the stereoscopic and monoscopic displays. We show how this mapping supports interactive data exploration, but may also lead to conflicts between the stereoscopic and monoscopic views due to users&#x27; movement in space; we resolve these problems through synchronization. To support our discussion, we present results from an exploratory observational evaluation with domain experts in fluid mechanics and structural biology. These experts explored domain-specific datasets using variations of a system that embodies the interaction modes and workflows; we report on their interactions and qualitative feedback on the system and its workflow.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Visualization Of 3 D Data</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Expert Interaction</span><span class="ui brown basic label">Direct Touch Input</span><span class="ui brown basic label">Mobile Displays</span><span class="ui brown basic label">Stereoscopic Environments</span><span class="ui brown basic label">VR</span><span class="ui brown basic label">AR</span><span class="ui brown basic label">Conceptual Model Of Interaction</span><span class="ui brown basic label">Interaction Reference Frame Mapping</span><span class="ui brown basic label">Observational Study</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Lopez<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Candemir Doger<!-- -->, <!-- -->Tobias Isenberg<!-- -->. <b>Towards An Understanding of Mobile Touch Navigation in a Stereoscopic Viewing Environment for 3D Data Exploration</b>. <i>In <!-- -->IEEE Transactions on Visualization and Computer Graphics<!-- -->(<!-- -->TVCG 2016<!-- -->)</i>. <!-- -->IEEE, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->13<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1109/TVCG.2015.2440233" target="_blank">https://doi.org/10.1109/TVCG.2015.2440233</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/245846750" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/245846750?autoplay=1&gt;&lt;Image width={0} height={0} alt=undefined src=undefined&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="mobilehci-2015-ledo" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/mobilehci-2015-ledo/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>mobilehci-2015-ledo</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MobileHCI 2015</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="mobilehci-2015-ledo cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/mobilehci-2015-ledo.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/mobilehci-2015-ledo.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/mobilehci-2015-ledo" target="_blank">Proxemic-Aware Controls: Designing Remote Controls for Ubiquitous Computing Ecologies</a></h1><p class="meta"><a href="/people/david-ledo"><img alt="david-ledo photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/david-ledo.jpg 1x" src="/pr-preview/pr-121/static/images/people/david-ledo.jpg"/><strong>David Ledo</strong></a> , <a href="/people/saul-greenberg"><img alt="saul-greenberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/saul-greenberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/saul-greenberg.jpg"/><strong>Saul Greenberg</strong></a> , <a href="/people/nicolai-marquardt"><img alt="nicolai-marquardt photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/nicolai-marquardt.jpg 1x" src="/pr-preview/pr-121/static/images/people/nicolai-marquardt.jpg"/><strong>Nicolai Marquardt</strong></a> , <span>Sebastian Boring</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/mobilehci-2015-ledo.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>mobilehci-2015-ledo.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/1AlMUmD6E3U" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/1AlMUmD6E3U?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/1AlMUmD6E3U/maxresdefault.jpg src=https://img.youtube.com/vi/1AlMUmD6E3U/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Remote controls facilitate interactions at-a-distance with appliances. However, the complexity, diversity, and increasing number of digital appliances in ubiquitous computing ecologies make it increasingly difficult to: (1) discover which appliances are controllable; (2) select a particular appliance from the large number available; (3) view information about its status; and (4) control the appliance in a pertinent manner. To mitigate these problems we contribute proxemic-aware controls, which exploit the spatial relationships between a person&#x27;s handheld device and all surrounding appliances to create a dynamic appliance control interface. Specifically, a person can discover and select an appliance by the way one orients a mobile device around the room, and then progressively view the appliance&#x27;s status and control its features in increasing detail by simply moving towards it. We illustrate proxemic-aware controls of assorted appliances through various scenarios. We then provide a generalized conceptual framework that informs future designs of proxemic-aware controls.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Ubiquitous Computing</span><span class="ui brown basic label">Proxemic Interaction</span><span class="ui brown basic label">Mobile Interaction</span><span class="ui brown basic label">Control Of Appliances</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Ledo<!-- -->, <!-- -->Saul Greenberg<!-- -->, <!-- -->Nicolai Marquardt<!-- -->, <!-- -->Sebastian Boring<!-- -->. <b>Proxemic-Aware Controls: Designing Remote Controls for Ubiquitous Computing Ecologies</b>. <i>In <!-- -->Proceedings of the International Conference on Human-Computer Interaction with Mobile Devices and Services<!-- -->(<!-- -->MobileHCI 2015<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->10<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/2785830.2785871" target="_blank">https://doi.org/10.1145/2785830.2785871</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2015-oehlberg" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2015-oehlberg/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2015-oehlberg</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2015</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2015-oehlberg cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2015-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2015-oehlberg.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2015-oehlberg" target="_blank">Patterns of Physical Design Remixing in Online Maker Communities</a></h1><p class="meta"><a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a> , <span>Wendy E. Mackay</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2015-oehlberg.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2015-oehlberg.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Makers participate in remixing culture by drawing inspiration from, combining, and adapting designs for physical objects. To examine how makers remix each others&#x27; designs on a community scale, we analyzed metadata from over 175,000 digital designs from Thingiverse, the largest online design community for digital fabrication. Remixed designs on Thingiverse are predominantly generated designs from Customizer a built-in web app for adjusting parametric designs. However, we find that these designs do not elicit subsequent user activity and the authors who generate them tend not to contribute additional content to Thingiverse. Outside of Customizer, influential sources of remixing include complex assemblies and design primitives, as well as non-physical resources posing as physical designs. Building on our findings, we discuss ways in which online maker communities could become more than just design repositories and better support collaborative remixing.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Customization</span><span class="ui brown basic label">Maker Communities</span><span class="ui brown basic label">User Innovation</span><span class="ui brown basic label">Collaboration</span><span class="ui brown basic label">Hacking</span><span class="ui brown basic label">Remixing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Lora Oehlberg<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Wendy E. Mackay<!-- -->. <b>Patterns of Physical Design Remixing in Online Maker Communities</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2015<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->12<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/2702123.2702175" target="_blank">https://doi.org/10.1145/2702123.2702175</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/vJrVjH04nGc" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/vJrVjH04nGc?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/vJrVjH04nGc/maxresdefault.jpg src=https://img.youtube.com/vi/vJrVjH04nGc/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2015-aseniero" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2015-aseniero/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2015-aseniero</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2015</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2015-aseniero cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2015-aseniero.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2015-aseniero.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2015-aseniero" target="_blank">Stratos: Using Visualization to Support Decisions in Strategic Software Release Planning</a></h1><p class="meta"><a href="/people/bon-adriel-aseniero"><img alt="bon-adriel-aseniero photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/bon-adriel-aseniero.jpg 1x" src="/pr-preview/pr-121/static/images/people/bon-adriel-aseniero.jpg"/><strong>Bon Adriel Aseniero</strong></a> , <span>Tiffany Wun</span> , <a href="/people/david-ledo"><img alt="david-ledo photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/david-ledo.jpg 1x" src="/pr-preview/pr-121/static/images/people/david-ledo.jpg"/><strong>David Ledo</strong></a> , <span>Guenther Ruhe</span> , <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a> , <a href="/people/sheelagh-carpendale"><img alt="sheelagh-carpendale photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-121/static/images/people/sheelagh-carpendale.jpg"/><strong>Sheelagh Carpendale</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2015-aseniero.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2015-aseniero.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/qm57aHjTAYc" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/qm57aHjTAYc?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/qm57aHjTAYc/maxresdefault.jpg src=https://img.youtube.com/vi/qm57aHjTAYc/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Software is typically developed incrementally and released in stages. Planning these releases involves deciding which features of the system should be implemented for each release. This is a complex planning process involving numerous trade-offs-constraints and factors that often make decisions difficult. Since the success of a product depends on this plan, it is important to understand the trade-offs between different release plans in order to make an informed choice. We present STRATOS, a tool that simultaneously visualizes several software release plans. The visualization shows several attributes about each plan that are important to planners. Multiple plans are shown in a single layout to help planners find and understand the trade-offs between alternative plans. We evaluated our tool via a qualitative study and found that STRATOS enables a range of decision-making processes, helping participants decide on which plan is most optimal.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Software Engineering</span><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Release Planning</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Bon Adriel Aseniero<!-- -->, <!-- -->Tiffany Wun<!-- -->, <!-- -->David Ledo<!-- -->, <!-- -->Guenther Ruhe<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->. <b>Stratos: Using Visualization to Support Decisions in Strategic Software Release Planning</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2015<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->10<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/2702123.2702426" target="_blank">https://doi.org/10.1145/2702123.2702426</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2015-jones" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2015-jones/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2015-jones</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2015</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2015-jones cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2015-jones.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2015-jones.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2015-jones" target="_blank">Mechanics of Camera Work in Mobile Video Collaboration</a></h1><p class="meta"><a href="/people/brennan-jones"><img alt="brennan-jones photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/brennan-jones.jpg 1x" src="/pr-preview/pr-121/static/images/people/brennan-jones.jpg"/><strong>Brennan Jones</strong></a> , <span>Anna Witcraft</span> , <span>Scott Bateman</span> , <a href="/people/carman-neustaedter"><img alt="carman-neustaedter photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/carman-neustaedter.jpg 1x" src="/pr-preview/pr-121/static/images/people/carman-neustaedter.jpg"/><strong>Carman Neustaedter</strong></a> , <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-121/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2015-jones.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2015-jones.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/V133YGkLxC8" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/V133YGkLxC8?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/V133YGkLxC8/maxresdefault.jpg src=https://img.youtube.com/vi/V133YGkLxC8/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Mobile video conferencing, where one or more participants are moving about in the real world, enables entirely new interaction scenarios (e.g., asking for help to construct or repair an object, or showing a physical location). While we have a good understanding of the challenges of video conferencing in office or home environments, we do not fully understand the mechanics of camera work-how people use mobile devices to communicate with one another-during mobile video calls. To provide an understanding of what people do in mobile video collaboration, we conducted an observational study where pairs of participants completed tasks using a mobile video conferencing system. Our analysis suggests that people use the camera view deliberately to support their interactions-for example, to convey a message or to ask questions-but the limited field of view, and the lack of camera control can make it a frustrating experience.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Video Communication</span><span class="ui brown basic label">Collaboration</span><span class="ui brown basic label">Mobile Computing</span><span class="ui brown basic label">Handheld Devices</span><span class="ui brown basic label">Cscw</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Brennan Jones<!-- -->, <!-- -->Anna Witcraft<!-- -->, <!-- -->Scott Bateman<!-- -->, <!-- -->Carman Neustaedter<!-- -->, <!-- -->Anthony Tang<!-- -->. <b>Mechanics of Camera Work in Mobile Video Collaboration</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2015<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->10<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/2702123.2702345" target="_blank">https://doi.org/10.1145/2702123.2702345</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2015-willett" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-121/publications/chi-2015-willett/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2015-willett</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-121/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2015</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2015-willett cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/publications/cover/chi-2015-willett.jpg 1x" src="/pr-preview/pr-121/static/images/publications/cover/chi-2015-willett.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2015-willett" target="_blank">Lightweight Relief Shearing for Enhanced Terrain Perception on Interactive Maps</a></h1><p class="meta"><a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-121/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a> , <span>Bernhard Jenny</span> , <a href="/people/tobias-isenberg"><img alt="tobias-isenberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-121/static/images/people/tobias-isenberg.jpg 1x" src="/pr-preview/pr-121/static/images/people/tobias-isenberg.jpg"/><strong>Tobias Isenberg</strong></a> , <span>Pierre Dragicevic</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2015-willett.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2015-willett.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/YW31lmzQzpc" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/YW31lmzQzpc?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/YW31lmzQzpc/maxresdefault.jpg src=https://img.youtube.com/vi/YW31lmzQzpc/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We explore interactive relief shearing, a set of non-intrusive, direct manipulation interactions that expose depth and shape information in terrain maps using ephemeral animations. Reading and interpreting topography and relief on terrain maps is an important aspect of map use, but extracting depth information from 2D maps is notoriously difficult. Modern mapping software attempts to alleviate this limitation by presenting digital terrain using 3D views. However, 3D views introduce occlusion, complicate distance estimations, and typically require more complex interactions. In contrast, our approach reveals depth information via shearing animations on 2D maps, and can be paired with existing interactions such as pan and zoom. We examine explicit, integrated, and hybrid interactions for triggering relief shearing and present a version that uses device tilt to control depth effects. Our evaluation shows that these interactive techniques improve depth perception when compared to standard 2D and perspective views.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Plan Oblique Relief</span><span class="ui brown basic label">Interaction</span><span class="ui brown basic label">Depth Perception</span><span class="ui brown basic label">Terrain Maps</span><span class="ui brown basic label">Relief Shearing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Wesley Willett<!-- -->, <!-- -->Bernhard Jenny<!-- -->, <!-- -->Tobias Isenberg<!-- -->, <!-- -->Pierre Dragicevic<!-- -->. <b>Lightweight Relief Shearing for Enhanced Terrain Perception on Interactive Maps</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2015<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->Page: 1-<!-- -->10<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.1145/2702123.2702172" target="_blank">https://doi.org/10.1145/2702123.2702172</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><div class="content"><a href="https://ucalgary.ca"><img alt="University of Calgary logo" loading="lazy" width="200" height="0" decoding="async" data-nimg="1" style="color:transparent;max-width:200px;margin:0px auto;height:auto" srcSet="/pr-preview/pr-121/static/images/logo-4.png 1x, /pr-preview/pr-121/static/images/logo-4.png 2x" src="/pr-preview/pr-121/static/images/logo-4.png"/></a><div class="sub header"><a class="item" href="https://cpsc.ucalgary.ca">Department of Computer Science</a></div></div></div></footer></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{},"__N_SSG":true},"page":"/publications","query":{},"buildId":"gasA4LnMQu4cOXKl1aJ2_","assetPrefix":"/pr-preview/pr-121","runtimeConfig":{"basePath":"/pr-preview/pr-121"},"isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>