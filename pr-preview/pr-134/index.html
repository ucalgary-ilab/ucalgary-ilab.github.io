<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><title data-next-head="">Interactions Lab - University of Calgary HCI Group</title><meta name="keywords" content="Human-Computer Interaction, HCI, Information Visualization, University of Calgary, CHI, UIST" data-next-head=""/><meta name="description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" data-next-head=""/><meta property="og:title" content="Interactions Lab - University of Calgary HCI Group" data-next-head=""/><meta property="og:description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" data-next-head=""/><meta property="og:site_name" content="University of Calgary Interactions Lab" data-next-head=""/><meta property="og:url" content="https://ilab.ucalgary.ca/" data-next-head=""/><meta property="og:image" content="https://ilab.ucalgary.ca/static/images/cover.jpg" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:title" content="Interactions Lab - University of Calgary HCI Group" data-next-head=""/><meta name="twitter:description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" data-next-head=""/><meta name="twitter:image" content="https://ilab.ucalgary.ca/static/images/cover.jpg" data-next-head=""/><meta name="twitter:card" content="summary" data-next-head=""/><meta name="twitter:site" content="@ucalgary" data-next-head=""/><meta name="twitter:url" content="https://ilab.ucalgary.ca/" data-next-head=""/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link rel="preload" href="/pr-preview/pr-134/_next/static/media/dc84b505c4b06e35-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/pr-preview/pr-134/_next/static/css/0d5b9578ae57a1af.css" as="style"/><link rel="preload" href="/pr-preview/pr-134/_next/static/css/a1a0497113412518.css" as="style"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              // $('.ui.sidebar')
              //   .sidebar('attach events', '.sidebar.icon')

              $('.sidebar.icon').on('click', function(event) {
                $('.ui.sidebar')
                  .sidebar('toggle')
              })

              $('.project').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })

              $('.thesis').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><link rel="stylesheet" href="/pr-preview/pr-134/_next/static/css/0d5b9578ae57a1af.css" data-n-g=""/><link rel="stylesheet" href="/pr-preview/pr-134/_next/static/css/a1a0497113412518.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/pr-preview/pr-134/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/pr-preview/pr-134/_next/static/chunks/webpack-2b74b38220333ed6.js" defer=""></script><script src="/pr-preview/pr-134/_next/static/chunks/340-642ca131732656d2.js" defer=""></script><script src="/pr-preview/pr-134/_next/static/chunks/main-237fefe93cf728ed.js" defer=""></script><script src="/pr-preview/pr-134/_next/static/chunks/vendor-styles-342403e4c5479d33.js" defer=""></script><script src="/pr-preview/pr-134/_next/static/chunks/505-fcabfd5164abe7a7.js" defer=""></script><script src="/pr-preview/pr-134/_next/static/chunks/pages/_app-ef1f9bbf0259e57f.js" defer=""></script><script src="/pr-preview/pr-134/_next/static/chunks/347-501ace96f6678428.js" defer=""></script><script src="/pr-preview/pr-134/_next/static/chunks/590-50156f4a47cbbfdc.js" defer=""></script><script src="/pr-preview/pr-134/_next/static/chunks/718-68d763da4f2ae6f1.js" defer=""></script><script src="/pr-preview/pr-134/_next/static/chunks/404-8046d737a7cb0047.js" defer=""></script><script src="/pr-preview/pr-134/_next/static/chunks/pages/index-256f8fab11f7705f.js" defer=""></script><script src="/pr-preview/pr-134/_next/static/9o_HcBSBizKAuJDjxl5_g/_buildManifest.js" defer=""></script><script src="/pr-preview/pr-134/_next/static/9o_HcBSBizKAuJDjxl5_g/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_01c5fd"><div><div class="ui stackable grid"><div class="eleven wide column centered"><div class="ui container"><div class="ui basic segment"><img alt="Interactions Lab logo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui large centered image" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/ilab-logo-3d.gif 1x" src="/pr-preview/pr-134/static/images/ilab-logo-3d.gif"/></div></div><div class="ui two column centered grid"><div class="ui text container left aligned" id="about"><p style="font-size:1.6em"> The <span style="font-size:2em;font-weight:900;color:#e56f50">INTERACTIONS LAB</span><br/> is the <span style="font-weight:600">University of Calgaryâ€™s</span> Human-Computer Interaction research collective.</p><p style="font-size:1.15em">Based in the <a href="https://cpsc.ucalgary.ca">Department of Computer Science</a>, our collocated research space hosts an integrated set of <strong>7 research groups</strong> with over <strong>50 members</strong> including post-docs, graduate students, and undergraduate researchers focused on a diverse range of future-looking interaction and human-centered computing research.</p><p style="font-size:1.1em">The ILab space, along with faculty offices for the principal researchers, are located in <a href="https://maps.app.goo.gl/N1GoY4sAp69r1pFt6"> Math Science 680</a> on the main University of Calgary campus.</p></div></div><div id="labs" class="ui two column centered grid"><div class="ui text container left aligned"><div class="card" style="padding:15px"><div style="display:flex"><a href="/pr-preview/pr-134/labs/curio/"><div style="display:flex"><div style="background:#2095ba;z-index:2;border-radius:50%;min-height:6vw;height:6vw;min-width:6vw;width:6vw;display:flex;align-items:center"><img alt="curio" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;height:auto;width:100%;object-fit:contain;padding:0" srcSet="/pr-preview/pr-134/static/images/labs/curio.png 1x" src="/pr-preview/pr-134/static/images/labs/curio.png"/></div><img alt="lora-oehlberg&#x27;s photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;transform:translateX(-1vw);border-radius:50%;min-height:6vw;height:6vw;min-width:6vw;width:6vw;padding:0px" srcSet="/pr-preview/pr-134/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-134/static/images/people/lora-oehlberg.jpg"/></div></a><div style="display:flex;align-items:center"><p style="margin-left:5px"><span style="font-size:1.15em">The </span><a href="/pr-preview/pr-134/labs/curio/"><span style="color:#2095ba;font-weight:700;font-size:1.7em"> <!-- -->Curio Lab<!-- --> </span></a><span style="font-size:1.15em"> (<a href="/pr-preview/pr-134/people/lora-oehlberg/"><span style="font-weight:600">Prof. <!-- -->Lora Oehlberg</span></a>) <!-- -->examines human-centered design for creativity &amp; curiosity.<!-- --> </span> </p></div></div></div><div class="card" style="padding:15px"><div style="display:flex"><a href="/pr-preview/pr-134/labs/data-experience/"><div style="display:flex"><div style="background:#c14824;z-index:2;border-radius:50%;min-height:6vw;height:6vw;min-width:6vw;width:6vw;display:flex;align-items:center"><img alt="data-experience" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;height:auto;width:100%;object-fit:contain;padding:0" srcSet="/pr-preview/pr-134/static/images/labs/data-experience.png 1x" src="/pr-preview/pr-134/static/images/labs/data-experience.png"/></div><img alt="wesley-willett&#x27;s photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;transform:translateX(-1vw);border-radius:50%;min-height:6vw;height:6vw;min-width:6vw;width:6vw;padding:0px" srcSet="/pr-preview/pr-134/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-134/static/images/people/wesley-willett.jpg"/></div></a><div style="display:flex;align-items:center"><p style="margin-left:5px"><span style="font-size:1.15em">The </span><a href="/pr-preview/pr-134/labs/data-experience/"><span style="color:#c14824;font-weight:700;font-size:1.7em"> <!-- -->Data Experience Lab<!-- --> </span></a><span style="font-size:1.15em"> (<a href="/pr-preview/pr-134/people/wesley-willett/"><span style="font-weight:600">Prof. <!-- -->Wesley Willett</span></a>) <!-- -->develops new data visualizations, interactions, and experiences.<!-- --> </span> </p></div></div></div><div class="card" style="padding:15px"><div style="display:flex"><a href="/pr-preview/pr-134/labs/diff/"><div style="display:flex"><div style="background:#57a793;z-index:2;border-radius:50%;min-height:6vw;height:6vw;min-width:6vw;width:6vw;display:flex;align-items:center"><img alt="diff" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;height:auto;width:100%;object-fit:contain;padding:0" srcSet="/pr-preview/pr-134/static/images/labs/diff.png 1x" src="/pr-preview/pr-134/static/images/labs/diff.png"/></div><img alt="aditya-shekhar-nittala&#x27;s photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;transform:translateX(-1vw);border-radius:50%;min-height:6vw;height:6vw;min-width:6vw;width:6vw;padding:0px" srcSet="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg"/></div></a><div style="display:flex;align-items:center"><p style="margin-left:5px"><span style="font-size:1.15em">The </span><a href="/pr-preview/pr-134/labs/diff/"><span style="color:#57a793;font-weight:700;font-size:1.7em"> <!-- -->DIFF Lab<!-- --> </span></a><span style="font-size:1.15em"> (<a href="/pr-preview/pr-134/people/aditya-shekhar-nittala/"><span style="font-weight:600">Prof. <!-- -->Aditya Shekhar Nittala</span></a>) <!-- -->creates devices, interactions, and fabrication for the future.<!-- --> </span> </p></div></div></div><div class="card" style="padding:15px"><div style="display:flex"><a href="https://fatemerajabi.github.io/HealthVisFutures/"><div style="display:flex"><div style="background:#185b79;z-index:2;border-radius:50%;min-height:6vw;height:6vw;min-width:6vw;width:6vw;display:flex;align-items:center"><img alt="health-vis" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;height:auto;width:100%;object-fit:contain;padding:0" srcSet="/pr-preview/pr-134/static/images/labs/health-vis.png 1x" src="/pr-preview/pr-134/static/images/labs/health-vis.png"/></div><img alt="fateme-rajabiyazdi&#x27;s photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;transform:translateX(-1vw);border-radius:50%;min-height:6vw;height:6vw;min-width:6vw;width:6vw;padding:0px" srcSet="/pr-preview/pr-134/static/images/people/fateme-rajabiyazdi.jpg 1x" src="/pr-preview/pr-134/static/images/people/fateme-rajabiyazdi.jpg"/></div></a><div style="display:flex;align-items:center"><p style="margin-left:5px"><span style="font-size:1.15em"></span><a href="https://fatemerajabi.github.io/HealthVisFutures/"><span style="color:#185b79;font-weight:700;font-size:1.7em"> <!-- -->HealthVisFutures<!-- --> </span></a><span style="font-size:1.15em"> (<a href="/pr-preview/pr-134/people/fateme-rajabiyazdi/"><span style="font-weight:600">Prof. <!-- -->Fateme Rajabiyazdi</span></a>) <!-- -->explores new visual tools for patient-engaged healthcare.<!-- --> </span> </p></div></div></div><div class="card" style="padding:15px"><div style="display:flex"><a href="/pr-preview/pr-134/labs/shivers/"><div style="display:flex"><div style="background:#a3b86c;z-index:2;border-radius:50%;min-height:6vw;height:6vw;min-width:6vw;width:6vw;display:flex;align-items:center"><img alt="shivers" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;height:auto;width:100%;object-fit:contain;padding:0" srcSet="/pr-preview/pr-134/static/images/labs/shivers.png 1x" src="/pr-preview/pr-134/static/images/labs/shivers.png"/></div><img alt="christian-frisson&#x27;s photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;transform:translateX(-1vw);border-radius:50%;min-height:6vw;height:6vw;min-width:6vw;width:6vw;padding:0px" srcSet="/pr-preview/pr-134/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-134/static/images/people/christian-frisson.jpg"/></div></a><div style="display:flex;align-items:center"><p style="margin-left:5px"><span style="font-size:1.15em"></span><a href="/pr-preview/pr-134/labs/shivers/"><span style="color:#a3b86c;font-weight:700;font-size:1.7em"> <!-- -->SHIVERS<!-- --> </span></a><span style="font-size:1.15em"> (<a href="/pr-preview/pr-134/people/christian-frisson/"><span style="font-weight:600">Prof. <!-- -->Christian Frisson</span></a>) <!-- -->crafts new tools for multisensory (haptic) and multimedia (sonic) interaction.<!-- --> </span> </p></div></div></div><div class="card" style="padding:15px"><div style="display:flex"><a href="/pr-preview/pr-134/labs/prpl/"><div style="display:flex"><div style="background:#a765e9;z-index:2;border-radius:50%;min-height:6vw;height:6vw;min-width:6vw;width:6vw;display:flex;align-items:center"><img alt="prpl" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;height:auto;width:100%;object-fit:contain;padding:0" srcSet="/pr-preview/pr-134/static/images/labs/prpl.png 1x" src="/pr-preview/pr-134/static/images/labs/prpl.png"/></div><img alt="matthew-lakier&#x27;s photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;transform:translateX(-1vw);border-radius:50%;min-height:6vw;height:6vw;min-width:6vw;width:6vw;padding:0px" srcSet="/pr-preview/pr-134/static/images/people/matthew-lakier.jpg 1x" src="/pr-preview/pr-134/static/images/people/matthew-lakier.jpg"/></div></a><div style="display:flex;align-items:center"><p style="margin-left:5px"><span style="font-size:1.15em">The </span><a href="/pr-preview/pr-134/labs/prpl/"><span style="color:#a765e9;font-weight:700;font-size:1.7em"> <!-- -->prpl Lab<!-- --> </span></a><span style="font-size:1.15em"> (<a href="/pr-preview/pr-134/people/matthew-lakier/"><span style="font-weight:600">Prof. <!-- -->Matthew Lakier</span></a>) <!-- -->investigates and designs non-game forms of playfulness in user interfaces.<!-- --> </span> </p></div></div></div><div class="card" style="padding:15px"><div style="display:flex"><a href="/pr-preview/pr-134/labs/utouch/"><div style="display:flex"><div style="background:#ecaa35;z-index:2;border-radius:50%;min-height:6vw;height:6vw;min-width:6vw;width:6vw;display:flex;align-items:center"><img alt="utouch" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;height:auto;width:100%;object-fit:contain;padding:0" srcSet="/pr-preview/pr-134/static/images/labs/utouch.png 1x" src="/pr-preview/pr-134/static/images/labs/utouch.png"/></div><img alt="ehud-sharlin&#x27;s photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;transform:translateX(-1vw);border-radius:50%;min-height:6vw;height:6vw;min-width:6vw;width:6vw;padding:0px" srcSet="/pr-preview/pr-134/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-134/static/images/people/ehud-sharlin.jpg"/></div></a><div style="display:flex;align-items:center"><p style="margin-left:5px"><span style="font-size:1.15em"></span><a href="/pr-preview/pr-134/labs/utouch/"><span style="color:#ecaa35;font-weight:700;font-size:1.7em"> <!-- -->uTouch<!-- --> </span></a><span style="font-size:1.15em"> (<a href="/pr-preview/pr-134/people/ehud-sharlin/"><span style="font-weight:600">Prof. <!-- -->Ehud Sharlin</span></a>) <!-- -->explores physical and human-robot interaction and autonomy.<!-- --> </span> </p></div></div></div><h2>Associated Research Groups</h2><div class="associatedGroups" style="display:flex;flex-wrap:wrap;margin-top:5vh"><div class="card" style="flex:50%;margin-bottom:3vh"><div style="display:flex"><a href="http://grouplab.cpsc.ucalgary.ca/"><div style="display:flex"><div style="background:#57a793;z-index:2;border-radius:50%;min-height:4.5vw;height:4.5vw;min-width:4.5vw;width:4.5vw;display:flex;align-items:center"><img alt="grouplab" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;height:auto;width:100%;object-fit:contain;padding:0" srcSet="/pr-preview/pr-134/static/images/labs/grouplab.png 1x" src="/pr-preview/pr-134/static/images/labs/grouplab.png"/></div><img alt="saul-greenberg&#x27;s photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;transform:translateX(-1vw);border-radius:50%;min-height:4.5vw;height:4.5vw;min-width:4.5vw;width:4.5vw;padding:0px" srcSet="/pr-preview/pr-134/static/images/people/saul-greenberg.jpg 1x" src="/pr-preview/pr-134/static/images/people/saul-greenberg.jpg"/></div></a><div style="display:flex;align-items:center"><p> <a href="http://grouplab.cpsc.ucalgary.ca/"><span style="color:#57a793;font-weight:700;font-size:1.15em"> <!-- -->GroupLab<!-- --> </span></a><span style="font-size:0.8em"> (<a href="/pr-preview/pr-134/people/saul-greenberg/">Dr. <!-- -->Saul Greenberg</a> - <!-- -->Emeritus Professor<!-- -->) <!-- -->Research in HCI, CSCW, and UbiComp<!-- -->. </span> </p></div></div></div><div class="card" style="flex:50%;margin-bottom:3vh"><div style="display:flex"><a href="http://sheelaghcarpendale.ca/"><div style="display:flex"><div style="background:#185b79;z-index:2;border-radius:50%;min-height:4.5vw;height:4.5vw;min-width:4.5vw;width:4.5vw;display:flex;align-items:center"><img alt="innovis" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;height:auto;width:100%;object-fit:contain;padding:0" srcSet="/pr-preview/pr-134/static/images/labs/innovis.png 1x" src="/pr-preview/pr-134/static/images/labs/innovis.png"/></div><img alt="sheelagh-carpendale&#x27;s photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;transform:translateX(-1vw);border-radius:50%;min-height:4.5vw;height:4.5vw;min-width:4.5vw;width:4.5vw;padding:0px" srcSet="/pr-preview/pr-134/static/images/people/sheelagh-carpendale.jpg 1x" src="/pr-preview/pr-134/static/images/people/sheelagh-carpendale.jpg"/></div></a><div style="display:flex;align-items:center"><p> <a href="http://sheelaghcarpendale.ca/"><span style="color:#185b79;font-weight:700;font-size:1.15em"> <!-- -->InnoVis<!-- --> </span></a><span style="font-size:0.8em"> (<a href="/pr-preview/pr-134/people/sheelagh-carpendale/">Dr. <!-- -->Sheelagh Carpendale</a> - <!-- -->Adjunct Professor (Simon Fraser University)<!-- -->) <!-- -->Innovations in Visualization Laboratory<!-- -->. </span> </p></div></div></div><div class="card" style="flex:50%;margin-bottom:3vh"><div style="display:flex"><a href="https://programmable-reality-lab.github.io/"><div style="display:flex"><div style="background:#a3b86c;z-index:2;border-radius:50%;min-height:4.5vw;height:4.5vw;min-width:4.5vw;width:4.5vw;display:flex;align-items:center"><img alt="programmable-reality" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;height:auto;width:100%;object-fit:contain;padding:0" srcSet="/pr-preview/pr-134/static/images/labs/programmable-reality.png 1x" src="/pr-preview/pr-134/static/images/labs/programmable-reality.png"/></div><img alt="ryo-suzuki&#x27;s photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;transform:translateX(-1vw);border-radius:50%;min-height:4.5vw;height:4.5vw;min-width:4.5vw;width:4.5vw;padding:0px" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/></div></a><div style="display:flex;align-items:center"><p> <a href="https://programmable-reality-lab.github.io/"><span style="color:#a3b86c;font-weight:700;font-size:1.15em"> <!-- -->Programmable Reality Lab<!-- --> </span></a><span style="font-size:0.8em"> (<a href="/pr-preview/pr-134/people/ryo-suzuki/">Dr. <!-- -->Ryo Suzuki</a> - <!-- -->Adjunct Assistant Professor (CU Boulder)<!-- -->) <!-- -->Programmable Reality Lab - Tangible, AR/VR, and Robotics<!-- -->. </span> </p></div></div></div><div class="card" style="flex:50%;margin-bottom:3vh"><div style="display:flex"><a href="https://ricelab.github.io/"><div style="display:flex"><div style="background:#2095ba;z-index:2;border-radius:50%;min-height:4.5vw;height:4.5vw;min-width:4.5vw;width:4.5vw;display:flex;align-items:center"><img alt="ricelab" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;height:auto;width:100%;object-fit:contain;padding:0" srcSet="/pr-preview/pr-134/static/images/labs/ricelab.png 1x" src="/pr-preview/pr-134/static/images/labs/ricelab.png"/></div><img alt="anthony-tang&#x27;s photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;transform:translateX(-1vw);border-radius:50%;min-height:4.5vw;height:4.5vw;min-width:4.5vw;width:4.5vw;padding:0px" srcSet="/pr-preview/pr-134/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-134/static/images/people/anthony-tang.jpg"/></div></a><div style="display:flex;align-items:center"><p> <a href="https://ricelab.github.io/"><span style="color:#2095ba;font-weight:700;font-size:1.15em"> <!-- -->RICELab<!-- --> </span></a><span style="font-size:0.8em"> (<a href="/pr-preview/pr-134/people/anthony-tang/">Dr. <!-- -->Anthony Tang</a> - <!-- -->Adjunct Associate Professor (Singapore Management University)<!-- -->) <!-- -->Rethinking Interaction, Collaboration, &amp; Engagement<!-- -->. </span> </p></div></div></div></div></div></div><div id="people" class="category ui container"><h1 class="ui horizontal divider header"><svg data-prefix="fas" data-icon="child-reaching" class="svg-inline--fa fa-child-reaching" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M256 64a64 64 0 1 0 -128 0 64 64 0 1 0 128 0zM152.9 169.3c-23.7-8.4-44.5-24.3-58.8-45.8L74.6 94.2C64.8 79.5 45 75.6 30.3 85.4S11.6 115 21.4 129.8L40.9 159c18.1 27.1 42.8 48.4 71.1 62.4L112 480c0 17.7 14.3 32 32 32s32-14.3 32-32l0-96 32 0 0 96c0 17.7 14.3 32 32 32s32-14.3 32-32l0-258.4c29.1-14.2 54.4-36.2 72.7-64.2l18.2-27.9c9.6-14.8 5.4-34.6-9.4-44.3s-34.6-5.5-44.3 9.4L291 122.4c-21.8 33.4-58.9 53.6-98.8 53.6-12.6 0-24.9-2-36.6-5.8-.9-.3-1.8-.7-2.7-.9z"></path></svg>Researchers</h1><div>Students are part of the <a title="Computer Science" href="https://science.ucalgary.ca/computer-science">CS</a> or <a title="Computational Media Design" href="https://science.ucalgary.ca/computational-media-design">CMD</a> programs. <a class="ui button" href="/pr-preview/pr-134/people/">See all <!-- -->114<!-- --> alumni</a></div><div class="people-category eleven wide column centered"><div class="ui grid"><a class="four wide column person" href="/pr-preview/pr-134/people/ashratuz-zavin-asha/"><img alt="Ashratuz Zavin Asha photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ashratuz-zavin-asha.jpg 1x" src="/pr-preview/pr-134/static/images/people/ashratuz-zavin-asha.jpg"/><p><b>Ashratuz Zavin Asha</b></p><p>PhD</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/christopher-smith/"><img alt="Christopher Smith photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/christopher-smith.jpg 1x" src="/pr-preview/pr-134/static/images/people/christopher-smith.jpg"/><p><b>Christopher Smith</b></p><p>CS PhD</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/clara-xi/"><img alt="Clara Xi photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/clara-xi.jpg 1x" src="/pr-preview/pr-134/static/images/people/clara-xi.jpg"/><p><b>Clara Xi</b></p><p>PhD</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/georgina-freeman/"><img alt="Georgina Freeman photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/georgina-freeman.jpg 1x" src="/pr-preview/pr-134/static/images/people/georgina-freeman.jpg"/><p><b>Georgina Freeman</b></p><p>PhD</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/kathryn-blair/"><img alt="Kathryn Blair photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/kathryn-blair.jpg 1x" src="/pr-preview/pr-134/static/images/people/kathryn-blair.jpg"/><p><b>Kathryn Blair</b></p><p>CMD PhD</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/tania-villalobos-lujan/"><img alt="Tania Villalobos Lujan photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/tania-villalobos-lujan.jpg 1x" src="/pr-preview/pr-134/static/images/people/tania-villalobos-lujan.jpg"/><p><b>Tania Villalobos Lujan</b></p><p>PhD</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/terrance-mok/"><img alt="Terrance Mok photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/terrance-mok.jpg 1x" src="/pr-preview/pr-134/static/images/people/terrance-mok.jpg"/><p><b>Terrance Mok</b></p><p>PhD</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/tim-au-yeung/"><img alt="Tim Au Yeung photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/tim-au-yeung.jpg 1x" src="/pr-preview/pr-134/static/images/people/tim-au-yeung.jpg"/><p><b>Tim Au Yeung</b></p><p>PhD</p></a></div></div><div class="people-category eleven wide column centered"><div class="ui grid"><a class="four wide column person" href="/pr-preview/pr-134/people/aditya-gunturu/"><img alt="Aditya Gunturu photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-gunturu.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-gunturu.jpg"/><p><b>Aditya Gunturu</b></p><p>MSc</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/anand-kumar/"><img alt="Anand Kumar photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/anand-kumar.jpg 1x" src="/pr-preview/pr-134/static/images/people/anand-kumar.jpg"/><p><b>Anand Kumar</b></p><p>MSc</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/ben-pearman/"><img alt="Ben Pearman photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ben-pearman.jpg 1x" src="/pr-preview/pr-134/static/images/people/ben-pearman.jpg"/><p><b>Ben Pearman</b></p><p>MSc</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/bonnie-wu/"><img alt="Bonnie Wu photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/bonnie-wu.jpg 1x" src="/pr-preview/pr-134/static/images/people/bonnie-wu.jpg"/><p><b>Bonnie Wu</b></p><p>MSc</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/carson-witts/"><img alt="Carson Witts photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/carson-witts.jpg 1x" src="/pr-preview/pr-134/static/images/people/carson-witts.jpg"/><p><b>Carson Witts</b></p><p>MSc</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/christian-salvador/"><img alt="Christian Salvador photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/christian-salvador.jpg 1x" src="/pr-preview/pr-134/static/images/people/christian-salvador.jpg"/><p><b>Christian Salvador</b></p><p>MSc</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/colin-auyeung/"><img alt="Colin Au Yeung photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/colin-auyeung.jpg 1x" src="/pr-preview/pr-134/static/images/people/colin-auyeung.jpg"/><p><b>Colin Au Yeung</b></p><p>MSc</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/huanjun-zhao/"><img alt="Huann Zhao photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/huanjun-zhao.jpg 1x" src="/pr-preview/pr-134/static/images/people/huanjun-zhao.jpg"/><p><b>Huann Zhao</b></p><p>MSc</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/isaac-ng/"><img alt="Isaac Ng photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/isaac-ng.jpg 1x" src="/pr-preview/pr-134/static/images/people/isaac-ng.jpg"/><p><b>Isaac Ng</b></p><p>MSc</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/jane-shen/"><img alt="Jane Shen photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/jane-shen.jpg 1x" src="/pr-preview/pr-134/static/images/people/jane-shen.jpg"/><p><b>Jane Shen</b></p><p>MSc</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/karly-ross/"><img alt="Karly Ross photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/karly-ross.jpg 1x" src="/pr-preview/pr-134/static/images/people/karly-ross.jpg"/><p><b>Karly Ross</b></p><p>MSc</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/shanna-hollingworth/"><img alt="Shanna Hollingworth photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/shanna-hollingworth.jpg 1x" src="/pr-preview/pr-134/static/images/people/shanna-hollingworth.jpg"/><p><b>Shanna Hollingworth</b></p><p>MSc</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/sutirtha-roy/"><img alt="Sutirtha Roy photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/sutirtha-roy.jpg 1x" src="/pr-preview/pr-134/static/images/people/sutirtha-roy.jpg"/><p><b>Sutirtha Roy</b></p><p>MSc</p></a></div></div><div class="people-category eleven wide column centered"><div class="ui grid"><a class="four wide column person" href="/pr-preview/pr-134/people/aidan-gaede-janke/"><img alt="Aidan Gaede-Janke photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aidan-gaede-janke.jpg 1x" src="/pr-preview/pr-134/static/images/people/aidan-gaede-janke.jpg"/><p><b>Aidan Gaede-Janke</b></p><p>Ugrad</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/ebube-anachebe/"><img alt="Ebube Anachebe photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ebube-anachebe.jpg 1x" src="/pr-preview/pr-134/static/images/people/ebube-anachebe.jpg"/><p><b>Ebube Anachebe</b></p><p>Ugrad</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/godwin-saure/"><img alt="Godwin Saure photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/godwin-saure.jpg 1x" src="/pr-preview/pr-134/static/images/people/godwin-saure.jpg"/><p><b>Godwin Saure</b></p><p>Ugrad</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/isabella-huang/"><img alt="Isabella Huang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/isabella-huang.jpg 1x" src="/pr-preview/pr-134/static/images/people/isabella-huang.jpg"/><p><b>Isabella Huang</b></p><p>Ugrad</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/lychelle-pham/"><img alt="Lychelle Pham photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/lychelle-pham.jpg 1x" src="/pr-preview/pr-134/static/images/people/lychelle-pham.jpg"/><p><b>Lychelle Pham</b></p><p>Ugrad</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/nadeem-moosa/"><img alt="Nadeem Moosa photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/nadeem-moosa.jpg 1x" src="/pr-preview/pr-134/static/images/people/nadeem-moosa.jpg"/><p><b>Nadeem Moosa</b></p><p>Ugrad</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/sebastian-gil/"><img alt="Sebastian Gil photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/sebastian-gil.jpg 1x" src="/pr-preview/pr-134/static/images/people/sebastian-gil.jpg"/><p><b>Sebastian Gil</b></p><p>Ugrad</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/victoria-wong/"><img alt="Victoria Wong photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/victoria-wong.jpg 1x" src="/pr-preview/pr-134/static/images/people/victoria-wong.jpg"/><p><b>Victoria Wong</b></p><p>Ugrad</p></a><a class="four wide column person" href="/pr-preview/pr-134/people/yaseen-rashid/"><img alt="Yaseen Rashid photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image medium-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/yaseen-rashid.jpg 1x" src="/pr-preview/pr-134/static/images/people/yaseen-rashid.jpg"/><p><b>Yaseen Rashid</b></p><p>Ugrad</p></a></div></div></div><div id="publications" class="category ui container"><h1 class="ui horizontal divider header"><svg data-prefix="far" data-icon="file-lines" class="svg-inline--fa fa-file-lines" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M64 48l112 0 0 88c0 39.8 32.2 72 72 72l88 0 0 240c0 8.8-7.2 16-16 16L64 464c-8.8 0-16-7.2-16-16L48 64c0-8.8 7.2-16 16-16zM224 67.9l92.1 92.1-68.1 0c-13.3 0-24-10.7-24-24l0-68.1zM64 0C28.7 0 0 28.7 0 64L0 448c0 35.3 28.7 64 64 64l256 0c35.3 0 64-28.7 64-64l0-261.5c0-17-6.7-33.3-18.7-45.3L242.7 18.7C230.7 6.7 214.5 0 197.5 0L64 0zm56 256c-13.3 0-24 10.7-24 24s10.7 24 24 24l144 0c13.3 0 24-10.7 24-24s-10.7-24-24-24l-144 0zm0 96c-13.3 0-24 10.7-24 24s10.7 24 24 24l144 0c13.3 0 24-10.7 24-24s-10.7-24-24-24l-144 0z"></path></svg>Recent Publications</h1><div class="ui segment" style="margin-top:50px"><div class="publication ui vertical segment stackable grid" data-id="chi-2025-ghaneezabadi"><div class="three wide column" style="margin:auto"><img alt="chi-2025-ghaneezabadi cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-2025-ghaneezabadi.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-2025-ghaneezabadi.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2025</span></p><p class="color" style="font-size:1.3em"><b>IntelliLining: Activity Sensing through Textile Interlining Sensors Using TENGs</b></p><p><span>Mahdie GhaneEzabadi</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/aditya-shekhar-nittala/"><img alt="Aditya Shekhar Nittala picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg"/><span class="author-link">Aditya Shekhar Nittala</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/xing-dong-yang/"><img alt="Xing-Dong Yang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/xing-dong-yang.jpg 1x" src="/pr-preview/pr-134/static/images/people/xing-dong-yang.jpg"/><span class="author-link">Xing-Dong Yang</span></a><span class="role"></span>, <span>Te-yen Wu</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Interactive Textile</span><span class="ui brown basic label">TEN Gs</span><span class="ui brown basic label">Machine Learning</span><span class="ui brown basic label">Vibration Sensing</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2025-madill"><div class="three wide column" style="margin:auto"><img alt="chi-2025-madill cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-2025-madill.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-2025-madill.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2025</span></p><p class="color" style="font-size:1.3em"><b>Playing with Robots: Performing Arts Techniques for Designing and Understanding Robot Group Movement</b></p><p><span>Philippa Madill</span><span class="role"></span>, <span>Matthew Newton</span><span class="role"></span>, <span>Huanjun Zhao</span><span class="role"></span>, <span>Yichen Lian</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/zachary-mckendrick/"><img alt="Zachary McKendrick picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/zachary-mckendrick.jpg 1x" src="/pr-preview/pr-134/static/images/people/zachary-mckendrick.jpg"/><span class="author-link">Zachary McKendrick</span></a><span class="role"></span>, <span>Patrick Finn</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/aditya-shekhar-nittala/"><img alt="Aditya Shekhar Nittala picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg"/><span class="author-link">Aditya Shekhar Nittala</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-134/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Humanities</span><span class="ui brown basic label">Art</span><span class="ui brown basic label">Robots</span><span class="ui brown basic label">Method</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2025-shiokawa"><div class="three wide column" style="margin:auto"><img alt="chi-2025-shiokawa cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-2025-shiokawa.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-2025-shiokawa.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2025</span></p><p class="color" style="font-size:1.3em"><b>Beyond Vacuuming: How Can We Exploit Domestic Robots&#x27; Idle Time?</b></p><p><span>Yoshiaki Shiokawa</span><span class="role"></span>, <span>Winnie Chen</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/aditya-shekhar-nittala/"><img alt="Aditya Shekhar Nittala picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg"/><span class="author-link">Aditya Shekhar Nittala</span></a><span class="role"></span>, <span>Jason Alexander</span><span class="role"></span>, <span>Adwait Sharma</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Domestic Robots</span><span class="ui brown basic label">Ubiquitous</span><span class="ui brown basic label">Interaction</span><span class="ui brown basic label">Design Space</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="httf-2024-blair"><div class="three wide column" style="margin:auto"><img alt="httf-2024-blair cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/httf-2024-blair.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/httf-2024-blair.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">HTTF 2024</span></p><p class="color" style="font-size:1.3em"><b>Weaving Perspectives into Practice: A Manifesto for Combining Epistemological and Dissemination Strategies</b></p><p><a href="/pr-preview/pr-134/people/kathryn-blair/"><img alt="Kathryn Blair picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/kathryn-blair.jpg 1x" src="/pr-preview/pr-134/static/images/people/kathryn-blair.jpg"/><span class="author-link">Kathryn Blair</span></a><span class="role"></span>, <span>Pil Hansen</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-134/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Fine Arts</span><span class="ui brown basic label">Methodology</span><span class="ui brown basic label">Emergent Ontology</span><span class="ui brown basic label">Epistemology</span><span class="ui brown basic label">Practice Based Research</span><span class="ui brown basic label">Qualitative Research</span><span class="ui brown basic label">Dissemination</span><span class="ui brown basic label">Knowledge Generation</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="mdpi-actuators-2024-piao"><div class="three wide column" style="margin:auto"><img alt="mdpi-actuators-2024-piao cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/mdpi-actuators-2024-piao.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/mdpi-actuators-2024-piao.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MDPI Actuators 2024 (Special Issue &quot;Actuators for Haptic and Tactile Stimulation Applications&quot;)</span></p><p class="color" style="font-size:1.3em"><b>Assessing the Impact of Force Feedback in Musical Knobs on Performance and User Experience</b></p><p><span>Ziyue Piao</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/christian-frisson/"><img alt="Christian Frisson picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-134/static/images/people/christian-frisson.jpg"/><span class="author-link">Christian Frisson</span></a><span class="role"></span>, <span>Bavo Van Kerrebroeck</span><span class="role"></span>, <span>Marcelo M. Wanderley</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Rotary Force Feedback</span><span class="ui brown basic label">Digital Musical Instruments</span><span class="ui brown basic label">Knobs</span><span class="ui brown basic label">Torque Tuner</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2024-gunturu"><div class="three wide column" style="margin:auto"><img alt="uist-2024-gunturu cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/uist-2024-gunturu.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/uist-2024-gunturu.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2024</span><span class="ui big basic pink label"><b><svg data-prefix="fas" data-icon="trophy" class="svg-inline--fa fa-trophy" role="img" viewBox="0 0 512 512" aria-hidden="true"><path fill="currentColor" d="M144.3 0l224 0c26.5 0 48.1 21.8 47.1 48.2-.2 5.3-.4 10.6-.7 15.8l49.6 0c26.1 0 49.1 21.6 47.1 49.8-7.5 103.7-60.5 160.7-118 190.5-15.8 8.2-31.9 14.3-47.2 18.8-20.2 28.6-41.2 43.7-57.9 51.8l0 73.1 64 0c17.7 0 32 14.3 32 32s-14.3 32-32 32l-192 0c-17.7 0-32-14.3-32-32s14.3-32 32-32l64 0 0-73.1c-16-7.7-35.9-22-55.3-48.3-18.4-4.8-38.4-12.1-57.9-23.1-54.1-30.3-102.9-87.4-109.9-189.9-1.9-28.1 21-49.7 47.1-49.7l49.6 0c-.3-5.2-.5-10.4-.7-15.8-1-26.5 20.6-48.2 47.1-48.2zM101.5 112l-52.4 0c6.2 84.7 45.1 127.1 85.2 149.6-14.4-37.3-26.3-86-32.8-149.6zM380 256.8c40.5-23.8 77.1-66.1 83.3-144.8L411 112c-6.2 60.9-17.4 108.2-31 144.8z"></path></svg> Best Paper</b></span></p><p class="color" style="font-size:1.3em"><b>Augmented Physics: Creating Interactive and Embedded Physics Simulations from Static Textbook Diagrams</b></p><p><a href="/pr-preview/pr-134/people/aditya-gunturu/"><img alt="Aditya Gunturu picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-gunturu.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-gunturu.jpg"/><span class="author-link">Aditya Gunturu</span></a><span class="role"></span>, <span>Yi Wen</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/nandi-zhang/"><img alt="Nandi Zhang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/nandi-zhang.jpg 1x" src="/pr-preview/pr-134/static/images/people/nandi-zhang.jpg"/><span class="author-link">Nandi Zhang</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/jarin-thundathil/"><img alt="Jarin Thundathil picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/jarin-thundathil.jpg 1x" src="/pr-preview/pr-134/static/images/people/jarin-thundathil.jpg"/><span class="author-link">Jarin Thundathil</span></a><span class="role"></span>, <span>Rubaiat Habib Kazi</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Physics Education</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Authoring Interfaces</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2024-roy"><div class="three wide column" style="margin:auto"><img alt="uist-2024-roy cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/uist-2024-roy.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/uist-2024-roy.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2024</span></p><p class="color" style="font-size:1.3em"><b>HoloChemie - Sustainable Fabrication of Soft Biochemical Holographic Devices for Ubiquitous Sensing</b></p><p><a href="/pr-preview/pr-134/people/sutirtha-roy/"><img alt="Sutirtha Roy picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/sutirtha-roy.jpg 1x" src="/pr-preview/pr-134/static/images/people/sutirtha-roy.jpg"/><span class="author-link">Sutirtha Roy</span></a><span class="role"></span>, <span>Moshfiq-Us-Saleheen Chowdhury</span><span class="role"></span>, <span>Jurjaan Onayza Noim</span><span class="role"></span>, <span>Richa Pandey</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/aditya-shekhar-nittala/"><img alt="Aditya Shekhar Nittala picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg"/><span class="author-link">Aditya Shekhar Nittala</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Physiological Sensing</span><span class="ui brown basic label">Biochemical Devices Sensing</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2024-danyluk"><div class="three wide column" style="margin:auto"><img alt="dis-2024-danyluk cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/dis-2024-danyluk.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/dis-2024-danyluk.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2024</span></p><p class="color" style="font-size:1.3em"><b>Understanding Gesture and Microgesture Inputs for Augmented Reality Maps</b></p><p><a href="/pr-preview/pr-134/people/kurtis-danyluk/"><img alt="Kurtis Danyluk picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-134/static/images/people/kurtis-danyluk.jpg"/><span class="author-link">Kurtis Danyluk</span></a><span class="role"></span>, <span>Simon Klueber</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/aditya-shekhar-nittala/"><img alt="Aditya Shekhar Nittala picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg"/><span class="author-link">Aditya Shekhar Nittala</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-134/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Gestural Input</span><span class="ui brown basic label">Microgestures</span><span class="ui brown basic label">AR</span><span class="ui brown basic label">Maps</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2024-bressa"><div class="three wide column" style="margin:auto"><img alt="chi-2024-bressa cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-2024-bressa.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-2024-bressa.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2024</span></p><p class="color" style="font-size:1.3em"><b>Input Visualization: Collecting and Modifying Data with Visual Representations</b></p><p><a href="/pr-preview/pr-134/people/nathalie-bressa/"><img alt="Nathalie Bressa picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/nathalie-bressa.jpg 1x" src="/pr-preview/pr-134/static/images/people/nathalie-bressa.jpg"/><span class="author-link">Nathalie Bressa</span></a><span class="role"></span>, <span>Jordan Louis</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-134/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span>, <span>Samuel Huron</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Input Visualization</span><span class="ui brown basic label">Data Physicalization</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2024-dhawka"><div class="three wide column" style="margin:auto"><img alt="chi-2024-dhawka cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-2024-dhawka.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-2024-dhawka.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2024</span></p><p class="color" style="font-size:1.3em"><b>Better Little People Pictures: Generative Creation of Demographically Diverse Anthropographics</b></p><p><a href="/pr-preview/pr-134/people/priya-dhawka/"><img alt="Priya Dhawka picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/priya-dhawka.jpg 1x" src="/pr-preview/pr-134/static/images/people/priya-dhawka.jpg"/><span class="author-link">Priya Dhawka</span></a><span class="role"></span>, <span>Lauren Perera</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-134/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Anthropographics</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Marginalized Populations</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2024-panigrahy"><div class="three wide column" style="margin:auto"><img alt="chi-2024-panigrahy cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-2024-panigrahy.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-2024-panigrahy.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2024</span></p><p class="color" style="font-size:1.3em"><b>ecSkin: Low-Cost Fabrication of Epidermal Electrochemical Sensors for Detecting Biomarkers in Sweat</b></p><p><span>Sai Nandan Panigrahy*</span><span class="role"></span>, <span>Chang Hyeon Lee*</span><span class="role"></span>, <span>Vrahant Nagoria</span><span class="role"></span>, <span>Mohammad Janghorban</span><span class="role"></span>, <span>Richa Pandey</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/aditya-shekhar-nittala/"><img alt="Aditya Shekhar Nittala picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg"/><span class="author-link">Aditya Shekhar Nittala</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Physiological Sensing</span><span class="ui brown basic label">Electrochemical Devices</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2023-chulpongsatorn"><div class="three wide column" style="margin:auto"><img alt="uist-2023-chulpongsatorn cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/uist-2023-chulpongsatorn.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/uist-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2023</span></p><p class="color" style="font-size:1.3em"><b>Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks</b></p><p><a href="/pr-preview/pr-134/people/neil-chulpongsatorn/"><img alt="Neil Chulpongsatorn picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-134/static/images/people/neil-chulpongsatorn.jpg"/><span class="author-link">Neil Chulpongsatorn</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/mille-skovhus-lunding/"><img alt="Mille Skovhus Lunding picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/mille-skovhus-lunding.jpg 1x" src="/pr-preview/pr-134/static/images/people/mille-skovhus-lunding.jpg"/><span class="author-link">Mille Skovhus Lunding</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/nishan-soni/"><img alt="Nishan Soni picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/no-profile-2.jpg 1x" src="/pr-preview/pr-134/static/images/people/no-profile-2.jpg"/><span class="author-link">Nishan Soni</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Authoring Interfaces</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2023-ihara"><div class="three wide column" style="margin:auto"><img alt="uist-2023-ihara cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/uist-2023-ihara.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/uist-2023-ihara.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2023</span></p><p class="color" style="font-size:1.3em"><b>HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality</b></p><p><a href="/pr-preview/pr-134/people/keiichi-ihara/"><img alt="Keiichi Ihara picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/keiichi-ihara.jpg 1x" src="/pr-preview/pr-134/static/images/people/keiichi-ihara.jpg"/><span class="author-link">Keiichi Ihara</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/mehrad-faridan/"><img alt="Mehrad Faridan picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/mehrad-faridan.jpg 1x" src="/pr-preview/pr-134/static/images/people/mehrad-faridan.jpg"/><span class="author-link">Mehrad Faridan</span></a><span class="role"></span>, <span>Ayumi Ichikawa</span><span class="role"></span>, <span>Ikkaku Kawaguchi</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Physical Telepresence</span><span class="ui brown basic label">Mobile Robots</span><span class="ui brown basic label">Actuated Tangible Ui</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2023-xia"><div class="three wide column" style="margin:auto"><img alt="uist-2023-xia cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/uist-2023-xia.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/uist-2023-xia.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2023</span></p><p class="color" style="font-size:1.3em"><b>RealityCanvas: Augmented Reality Sketching for Embedded and Responsive Scribble Animation Effects</b></p><p><a href="/pr-preview/pr-134/people/zhijie-xia/"><img alt="Zhijie Xia picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/zhijie-xia.jpg 1x" src="/pr-preview/pr-134/static/images/people/zhijie-xia.jpg"/><span class="author-link">Zhijie Xia</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/kyzyl-monteiro/"><img alt="Kyzyl Monteiro picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/kyzyl-monteiro.jpg 1x" src="/pr-preview/pr-134/static/images/people/kyzyl-monteiro.jpg"/><span class="author-link">Kyzyl Monteiro</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/kevin-van/"><img alt="Kevin Van picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/kevin-van.jpg 1x" src="/pr-preview/pr-134/static/images/people/kevin-van.jpg"/><span class="author-link">Kevin Van</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Sketching Interfaces</span><span class="ui brown basic label">Scribble Animation</span><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Real Time Authoring</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="assets-2023-mok"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">ASSETS 2023</span></p><p class="color" style="font-size:1.3em"><b>Experiences of Autistic Twitch Livestreamers: â€œI have made easily the most meaningful and impactful relationshipsâ€</b></p><p><a href="/pr-preview/pr-134/people/terrance-mok/"><img alt="Terrance Mok picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/terrance-mok.jpg 1x" src="/pr-preview/pr-134/static/images/people/terrance-mok.jpg"/><span class="author-link">Terrance Mok</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-134/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a><span class="role"></span>, <span>Adam McCrimmon</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-134/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Autism</span><span class="ui brown basic label">Live Streaming</span><span class="ui brown basic label">Autistic</span><span class="ui brown basic label">Twitch</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2023-mukashev"><div class="three wide column" style="margin:auto"><img alt="uist-2023-mukashev cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/uist-2023-mukashev.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/uist-2023-mukashev.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2023</span></p><p class="color" style="font-size:1.3em"><b>TactTongue: Prototyping ElectroTactile Stimulations on the Tongue</b></p><p><a href="/pr-preview/pr-134/people/dinmukhammed-mukashev/"><img alt="Dinmukhammed Mukashev picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/dinmukhammed-mukashev.jpg 1x" src="/pr-preview/pr-134/static/images/people/dinmukhammed-mukashev.jpg"/><span class="author-link">Dinmukhammed Mukashev</span></a><span class="role"></span>, <span>Nimesha Ranasinghe</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/aditya-shekhar-nittala/"><img alt="Aditya Shekhar Nittala picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg"/><span class="author-link">Aditya Shekhar Nittala</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Electrotactile Actuation</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2023-xia2"><div class="three wide column" style="margin:auto"><img alt="uist-2023-xia2 cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/uist-2023-xia2.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/uist-2023-xia2.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2023</span></p><p class="color" style="font-size:1.3em"><b>CrossTalk: Intelligent Substrates for Language-Oriented Interaction in Video-Based Communication and Collaboration</b></p><p><span>Haijun Xia</span><span class="role"></span>, <span>Tony Wang</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/aditya-gunturu/"><img alt="Aditya Gunturu picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-gunturu.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-gunturu.jpg"/><span class="author-link">Aditya Gunturu</span></a><span class="role"></span>, <span>Peiling Jiang</span><span class="role"></span>, <span>William Duan</span><span class="role"></span>, <span>Xiaoshuo Yao</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Videoconferencing</span><span class="ui brown basic label">Natural Language Interface</span><span class="ui brown basic label">Language Oriented Interaction</span><span class="ui brown basic label">Context Aware Computing</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="mdpi-arts-2023-frisson"><div class="three wide column" style="margin:auto"><img alt="mdpi-arts-2023-frisson cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/mdpi-arts-2023-frisson.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/mdpi-arts-2023-frisson.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MDPI Arts 2023 (Special Issue Feeling the Futureâ€”Haptic Audio)</span></p><p class="color" style="font-size:1.3em"><b>Challenges and Opportunities of Force Feedback in Music</b></p><p><a href="/pr-preview/pr-134/people/christian-frisson/"><img alt="Christian Frisson picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-134/static/images/people/christian-frisson.jpg"/><span class="author-link">Christian Frisson</span></a><span class="role"></span>, <span>Marcelo M. Wanderley</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Force Feedback</span><span class="ui brown basic label">Digital Musical Instruments</span><span class="ui brown basic label">Torque Tuner</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="siggraph-labs-2023-seta"><div class="three wide column" style="margin:auto"><img alt="siggraph-labs-2023-seta cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/siggraph-labs-2023-seta.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/siggraph-labs-2023-seta.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">SIGGRAPH 2023 Labs</span></p><p class="color" style="font-size:1.3em"><b>Sketching Pipelines for Ephemeral Immersive Spaces</b></p><p><span>MichaÅ‚ Seta</span><span class="role"></span>, <span>Eduardo A. L. Meneses</span><span class="role"></span>, <span>Emmanuel Durand</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/christian-frisson/"><img alt="Christian Frisson picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-134/static/images/people/christian-frisson.jpg"/><span class="author-link">Christian Frisson</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Art</span><span class="ui brown basic label">Interaction</span><span class="ui brown basic label">Pipeline</span><span class="ui brown basic label">Production</span><span class="ui brown basic label">Ui Tools</span><span class="ui brown basic label">VR AR MR</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2023-li"><div class="three wide column" style="margin:auto"><img alt="dis-2023-li cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/dis-2023-li.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/dis-2023-li.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2023</span></p><p class="color" style="font-size:1.3em"><b>Physica: Interactive Tangible Physics Simulation based on Tabletop Mobile Robots Towards Explorable Physics Education</b></p><p><span>Jiatong Li</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"></span>, <span>Ken Nakagaki</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Physics Simulation</span><span class="ui brown basic label">Actuated Tangible Ui</span><span class="ui brown basic label">Swarm Ui</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2023-dhawka"><div class="three wide column" style="margin:auto"><img alt="chi-2023-dhawka cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-2023-dhawka.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-2023-dhawka.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2023</span></p><p class="color" style="font-size:1.3em"><b>We are the Data: Challenges and Opportunities for Creating Demographically Diverse Anthropographics</b></p><p><a href="/pr-preview/pr-134/people/priya-dhawka/"><img alt="Priya Dhawka picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/priya-dhawka.jpg 1x" src="/pr-preview/pr-134/static/images/people/priya-dhawka.jpg"/><span class="author-link">Priya Dhawka</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/helen-ai-he/"><img alt="Helen Ai He picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/helen-ai-he.jpg 1x" src="/pr-preview/pr-134/static/images/people/helen-ai-he.jpg"/><span class="author-link">Helen Ai He</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-134/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Anthropographics</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Marginalized Populations</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2023-faridan"><div class="three wide column" style="margin:auto"><img alt="chi-2023-faridan cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-2023-faridan.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-2023-faridan.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2023</span></p><p class="color" style="font-size:1.3em"><b>ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms</b></p><p><a href="/pr-preview/pr-134/people/mehrad-faridan/"><img alt="Mehrad Faridan picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/mehrad-faridan.jpg 1x" src="/pr-preview/pr-134/static/images/people/mehrad-faridan.jpg"/><span class="author-link">Mehrad Faridan</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/bheesha-kumari/"><img alt="Bheesha Kumari picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/bheesha-kumari.jpg 1x" src="/pr-preview/pr-134/static/images/people/bheesha-kumari.jpg"/><span class="author-link">Bheesha Kumari</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Visual Cue</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Telepresence</span><span class="ui brown basic label">Remote Guidance</span><span class="ui brown basic label">Human Surrogates</span><span class="ui brown basic label">Hands On Training</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2023-monteiro"><div class="three wide column" style="margin:auto"><img alt="chi-2023-monteiro cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-2023-monteiro.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-2023-monteiro.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2023</span></p><p class="color" style="font-size:1.3em"><b>Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching</b></p><p><a href="/pr-preview/pr-134/people/kyzyl-monteiro/"><img alt="Kyzyl Monteiro picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/kyzyl-monteiro.jpg 1x" src="/pr-preview/pr-134/static/images/people/kyzyl-monteiro.jpg"/><span class="author-link">Kyzyl Monteiro</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/ritik-vatsal/"><img alt="Ritik Vatsal picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ritik-vatsal.jpg 1x" src="/pr-preview/pr-134/static/images/people/ritik-vatsal.jpg"/><span class="author-link">Ritik Vatsal</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/neil-chulpongsatorn/"><img alt="Neil Chulpongsatorn picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-134/static/images/people/neil-chulpongsatorn.jpg"/><span class="author-link">Neil Chulpongsatorn</span></a><span class="role"></span>, <span>Aman Parnami</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Prototyping Tools</span><span class="ui brown basic label">Tangible Interactions</span><span class="ui brown basic label">Everyday Objects</span><span class="ui brown basic label">Interactive Machine Teaching</span><span class="ui brown basic label">Human Centered Machine Learning</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-ea-2023-chulpongsatorn"><div class="three wide column" style="margin:auto"><img alt="chi-ea-2023-chulpongsatorn cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-ea-2023-chulpongsatorn.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-ea-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI EA 2023</span></p><p class="color" style="font-size:1.3em"><b>HoloTouch: Interacting with Mixed Reality Visualizations Through Smartphone Proxies</b></p><p><a href="/pr-preview/pr-134/people/neil-chulpongsatorn/"><img alt="Neil Chulpongsatorn picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-134/static/images/people/neil-chulpongsatorn.jpg"/><span class="author-link">Neil Chulpongsatorn</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-134/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Embedded Data Visualization</span><span class="ui brown basic label">Tangible Interaction</span><span class="ui brown basic label">Cross Device Interaction</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-ea-2023-fang"><div class="three wide column" style="margin:auto"><img alt="chi-ea-2023-fang cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-ea-2023-fang.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-ea-2023-fang.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI EA 2023</span></p><p class="color" style="font-size:1.3em"><b>VR Haptics at Home: Repurposing Everyday Objects and Environment for Casual and On-Demand VR Haptic Experiences</b></p><p><span>Cathy Mengying Fang</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"></span>, <span>Daniel Leithinger</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Interaction Techniques</span><span class="ui brown basic label">Passive Haptics</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="vrst-2022-frisson"><div class="three wide column" style="margin:auto"><img alt="vrst-2022-frisson cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/vrst-2022-frisson.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/vrst-2022-frisson.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">VRST 2022 Poster</span></p><p class="color" style="font-size:1.3em"><b>LivePose: Democratizing Pose Detection for Multimedia Arts and Telepresence Applications on Open Edge Devices</b></p><p><a href="/pr-preview/pr-134/people/christian-frisson/"><img alt="Christian Frisson picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-134/static/images/people/christian-frisson.jpg"/><span class="author-link">Christian Frisson</span></a><span class="role"></span>, <span>Gabriel N. Downs</span><span class="role"></span>, <span>Marie-Ãˆve Dumas</span><span class="role"></span>, <span>Farzaneh Askari</span><span class="role"></span>, <span>Emmanuel Durand</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Multimedia Arts</span><span class="ui brown basic label">Telepresence</span><span class="ui brown basic label">Edge Computing</span><span class="ui brown basic label">Pose Detection</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="tochi-2022-nittala"><div class="three wide column" style="margin:auto"><img alt="tochi-2022-nittala cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/tochi-2022-nittala.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/tochi-2022-nittala.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TOCHI 2022</span></p><p class="color" style="font-size:1.3em"><b>SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures</b></p><p><span>Adwait Sharma</span><span class="role"></span>, <span>Christina Salchow-HÃ¶mmen</span><span class="role"></span>, <span>Vimal Suresh Mollyn</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/aditya-shekhar-nittala/"><img alt="Aditya Shekhar Nittala picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg"/><span class="author-link">Aditya Shekhar Nittala</span></a><span class="role"></span>, <span>Michael A. Hedderich</span><span class="role"></span>, <span>Marion Koelle</span><span class="role"></span>, <span>Thomas Seel</span><span class="role"></span>, <span>JÃ¼rgen Steimle</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Gesture Recognition</span><span class="ui brown basic label">Hand Gestures</span><span class="ui brown basic label">Sensor Placement</span><span class="ui brown basic label">IMU</span><span class="ui brown basic label">Objects</span><span class="ui brown basic label">Design Tool</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2022-kaimoto"><div class="three wide column" style="margin:auto"><img alt="uist-2022-kaimoto cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/uist-2022-kaimoto.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/uist-2022-kaimoto.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2022</span></p><p class="color" style="font-size:1.3em"><b>Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI</b></p><p><a href="/pr-preview/pr-134/people/hiroki-kaimoto/"><img alt="Hiroki Kaimoto picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/hiroki-kaimoto.jpg 1x" src="/pr-preview/pr-134/static/images/people/hiroki-kaimoto.jpg"/><span class="author-link">Hiroki Kaimoto</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/kyzyl-monteiro/"><img alt="Kyzyl Monteiro picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/kyzyl-monteiro.jpg 1x" src="/pr-preview/pr-134/static/images/people/kyzyl-monteiro.jpg"/><span class="author-link">Kyzyl Monteiro</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/mehrad-faridan/"><img alt="Mehrad Faridan picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/mehrad-faridan.jpg 1x" src="/pr-preview/pr-134/static/images/people/mehrad-faridan.jpg"/><span class="author-link">Mehrad Faridan</span></a><span class="role"></span>, <span>Jiatong Li</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/samin-farajian/"><img alt="Samin Farajian picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/samin-farajian.jpg 1x" src="/pr-preview/pr-134/static/images/people/samin-farajian.jpg"/><span class="author-link">Samin Farajian</span></a><span class="role"></span>, <span>Yasuaki Kakehi</span><span class="role"></span>, <span>Ken Nakagaki</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Actuated Tangible Interfaces</span><span class="ui brown basic label">Swarm User Interfaces</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2022-liao"><div class="three wide column" style="margin:auto"><img alt="uist-2022-liao cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/uist-2022-liao.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/uist-2022-liao.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2022</span></p><p class="color" style="font-size:1.3em"><b>RealityTalk: Real-time Speech-driven Augmented Presentation for AR Live Storytelling</b></p><p><a href="/pr-preview/pr-134/people/jian-liao/"><img alt="Jian Liao picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/jian-liao.jpg 1x" src="/pr-preview/pr-134/static/images/people/jian-liao.jpg"/><span class="author-link">Jian Liao</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/adnan-karim/"><img alt="Adnan Karim picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/adnan-karim.jpg 1x" src="/pr-preview/pr-134/static/images/people/adnan-karim.jpg"/><span class="author-link">Adnan Karim</span></a><span class="role"></span>, <a href="/pr-preview/pr-134/people/shivesh-jadon/"><img alt="Shivesh Jadon picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/shivesh-jadon.jpg 1x" src="/pr-preview/pr-134/static/images/people/shivesh-jadon.jpg"/><span class="author-link">Shivesh Jadon</span></a><span class="role"></span>, <span>Rubaiat Habib Kazi</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Augmented Presentation</span><span class="ui brown basic label">Natural Language Processing</span><span class="ui brown basic label">Gestural And Speech Input</span><span class="ui brown basic label">Video</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2022-nisser"><div class="three wide column" style="margin:auto"><img alt="uist-2022-nisser cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/uist-2022-nisser.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/uist-2022-nisser.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2022</span></p><p class="color" style="font-size:1.3em"><b>Mixels: Fabricating Interfaces using Programmable Magnetic Pixels</b></p><p><span>Martin Nisser</span><span class="role"></span>, <span>Yashaswini Makaram</span><span class="role"></span>, <span>Lucian Covarrubias</span><span class="role"></span>, <span>Amadou Bah</span><span class="role"></span>, <span>Faraz Faruqi</span><span class="role"></span>, <a href="/pr-preview/pr-134/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"></span>, <span>Stefanie Mueller</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Programmable Materials</span><span class="ui brown basic label">Magnetic Interfaces</span><span class="ui brown basic label">Fabrication</span></div></div></div></div></div><div id="publications-modal"><div id="chi-2025-ghaneezabadi" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/chi-2025-ghaneezabadi/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2025-ghaneezabadi</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2025</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2025-ghaneezabadi cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-2025-ghaneezabadi.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-2025-ghaneezabadi.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2025-ghaneezabadi" target="_blank">IntelliLining: Activity Sensing through Textile Interlining Sensors Using TENGs</a></h1><p class="meta"><span>Mahdie GhaneEzabadi<!-- --> <span class="role"></span></span>, <a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a><span class="role"></span>, <a href="/people/xing-dong-yang"><img alt="xing-dong-yang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/xing-dong-yang.jpg 1x" src="/pr-preview/pr-134/static/images/people/xing-dong-yang.jpg"/><strong>Xing-Dong Yang</strong></a><span class="role"></span>, <span>Te-yen Wu<!-- --> <span class="role"></span></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2025-ghaneezabadi.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2025-ghaneezabadi.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We introduce a novel component for smart garments: smart interlining, and validate its technical feasibility through a series of experiments. Our work involved the implementation of a prototype that employs a textile vibration sensor based on Triboelectric Nanogenerators (TENGs), commonly used for activity detection. We explore several unique features of smart interlining, including how sensor signals and patterns are influenced by factors such as the size and shape of the interlining sensor, the location of the vibration source within the sensor area, and various propagation media, such as airborne and surface vibrations. We present our study results and discuss how these findings support the feasibility of smart interlining. Additionally, we demonstrate that smart interlinings on a shirt can detect a variety of user activities involving the hand, mouth, and upper body, achieving an accuracy rate of 93.9% in the tested activities.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Interactive Textile</span><span class="ui brown basic label">TEN Gs</span><span class="ui brown basic label">Machine Learning</span><span class="ui brown basic label">Vibration Sensing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Mahdie GhaneEzabadi<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Xing-Dong Yang<!-- -->, <!-- -->Te-yen Wu<!-- -->.Â <b>IntelliLining: Activity Sensing through Textile Interlining Sensors Using TENGs</b>.Â <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2025<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->DOI: <a href="https://doi.org/10.1145/3706598.3713167" target="_blank">https://doi.org/10.1145/3706598.3713167</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2025-madill" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/chi-2025-madill/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2025-madill</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2025</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2025-madill cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-2025-madill.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-2025-madill.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2025-madill" target="_blank">Playing with Robots: Performing Arts Techniques for Designing and Understanding Robot Group Movement</a></h1><p class="meta"><span>Philippa Madill<!-- --> <span class="role"></span></span>, <span>Matthew Newton<!-- --> <span class="role"></span></span>, <span>Huanjun Zhao<!-- --> <span class="role"></span></span>, <span>Yichen Lian<!-- --> <span class="role"></span></span>, <a href="/people/zachary-mckendrick"><img alt="zachary-mckendrick photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/zachary-mckendrick.jpg 1x" src="/pr-preview/pr-134/static/images/people/zachary-mckendrick.jpg"/><strong>Zachary McKendrick</strong></a><span class="role"></span>, <span>Patrick Finn<!-- --> <span class="role"></span></span>, <a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a><span class="role"></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ehud-sharlin.jpg 1x" src="/pr-preview/pr-134/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2025-madill.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2025-madill.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this work, we introduce a formal design approach derived from the performing arts to design robot group behaviour. In our first experiment, we worked with professional actors, directors, and non-specialists using a participatory design approach to identify common group behaviour patterns. In a follow-up studio work, we identified twelve common group movement patterns, transposed them into a performance script, built a scale model to support the performance process, and evaluated the patterns with a senior actor under studio conditions. We evaluated our refined models with 20 volunteers in a user study in the third experiment. Results from our affective circumplex modelling suggest that the patterns elicit positive emotional responses from the users. Also, participants performed better than chance in identifying the motion patterns without prior training. Based on our results, we propose design guidelines for social robotsâ€™ behaviour and movement design to improve their overall comprehensibility in interaction.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Humanities</span><span class="ui brown basic label">Art</span><span class="ui brown basic label">Robots</span><span class="ui brown basic label">Method</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Philippa Madill<!-- -->, <!-- -->Matthew Newton<!-- -->, <!-- -->Huanjun Zhao<!-- -->, <!-- -->Yichen Lian<!-- -->, <!-- -->Zachary McKendrick<!-- -->, <!-- -->Patrick Finn<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Ehud Sharlin<!-- -->.Â <b>Playing with Robots: Performing Arts Techniques for Designing and Understanding Robot Group Movement</b>.Â <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2025<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->DOI: <a href="https://dl.acm.org/doi/10.1145/3706598.3713996" target="_blank">https://dl.acm.org/doi/10.1145/3706598.3713996</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2025-shiokawa" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/chi-2025-shiokawa/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2025-shiokawa</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2025</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2025-shiokawa cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-2025-shiokawa.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-2025-shiokawa.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2025-shiokawa" target="_blank">Beyond Vacuuming: How Can We Exploit Domestic Robots&#x27; Idle Time?</a></h1><p class="meta"><span>Yoshiaki Shiokawa<!-- --> <span class="role"></span></span>, <span>Winnie Chen<!-- --> <span class="role"></span></span>, <a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a><span class="role"></span>, <span>Jason Alexander<!-- --> <span class="role"></span></span>, <span>Adwait Sharma<!-- --> <span class="role"></span></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2025-shiokawa.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2025-shiokawa.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/npYIDenYb2Y" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/npYIDenYb2Y?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/npYIDenYb2Y/maxresdefault.jpg src=https://img.youtube.com/vi/npYIDenYb2Y/maxresdefault.jpg&gt;&lt;span&gt;â–¶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We are increasingly adopting domestic robots (e.g., Roomba) that provide relief from mundane household tasks. However, these robots usually only spend little time executing their specific task and remain idle for long periods. They typically possess advanced mobility and sensing capabilities, and therefore have significant potential applications beyond their designed use. Our work explores this untapped potential of domestic robots in ubiquitous computing, focusing on how they can improve and support modern lifestyles. We conducted two studies: an online survey (n=50) to understand current usage patterns of these robots within homes and an exploratory study (n=12) with HCI and HRI experts. Our thematic analysis revealed 12 key dimensions for developing interactions with domestic robots and outlined over 100 use cases, illustrating how these robots can offer proactive assistance and provide privacy. Finally, we implemented a proof-of-concept prototype to demonstrate the feasibility of reappropriating domestic robots for diverse ubiquitous computing applications.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Domestic Robots</span><span class="ui brown basic label">Ubiquitous</span><span class="ui brown basic label">Interaction</span><span class="ui brown basic label">Design Space</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Yoshiaki Shiokawa<!-- -->, <!-- -->Winnie Chen<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Jason Alexander<!-- -->, <!-- -->Adwait Sharma<!-- -->.Â <b>Beyond Vacuuming: How Can We Exploit Domestic Robots&#x27; Idle Time?</b>.Â <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2025<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->DOI: <a href="https://dl.acm.org/doi/10.1145/3706598.3714266" target="_blank">https://dl.acm.org/doi/10.1145/3706598.3714266</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="httf-2024-blair" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/httf-2024-blair/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>httf-2024-blair</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">HTTF 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="httf-2024-blair cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/httf-2024-blair.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/httf-2024-blair.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/httf-2024-blair" target="_blank">Weaving Perspectives into Practice: A Manifesto for Combining Epistemological and Dissemination Strategies</a></h1><p class="meta"><a href="/people/kathryn-blair"><img alt="kathryn-blair photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/kathryn-blair.jpg 1x" src="/pr-preview/pr-134/static/images/people/kathryn-blair.jpg"/><strong>Kathryn Blair</strong></a><span class="role"></span>, <span>Pil Hansen<!-- --> <span class="role"></span></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-134/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/httf-2024-blair.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>httf-2024-blair.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>This paper is an invitation to HCI designers and artist-researchers to weave together practice-based ways of knowing with othersâ€™ experiences with their work, focused on understanding cognitive and aesthetic impressions garnered during exhibitions. It describes the first author&#x27;s process for weaving together the warp (practice-based ways of knowing) and weft (interview-based insights into othersâ€™ experiences) by leveraging the exhibition site as a place to generate knowledge with attendees experiencing their work. Understanding othersâ€™ experiences informs the experimentation that practice-based knowledge generation is founded on, deepening and enriching the resulting work.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Fine Arts</span><span class="ui brown basic label">Methodology</span><span class="ui brown basic label">Emergent Ontology</span><span class="ui brown basic label">Epistemology</span><span class="ui brown basic label">Practice Based Research</span><span class="ui brown basic label">Qualitative Research</span><span class="ui brown basic label">Dissemination</span><span class="ui brown basic label">Knowledge Generation</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kathryn Blair<!-- -->, <!-- -->Pil Hansen<!-- -->, <!-- -->Lora Oehlberg<!-- -->.Â <b>Weaving Perspectives into Practice: A Manifesto for Combining Epistemological and Dissemination Strategies</b>.Â <i>(<!-- -->HTTF 2024<!-- -->)</i>Â <!-- -->Page: 1-<!-- -->4<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.1145/3686169.3686205" target="_blank">https://doi.org/10.1145/3686169.3686205</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="mdpi-actuators-2024-piao" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/mdpi-actuators-2024-piao/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>mdpi-actuators-2024-piao</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MDPI Actuators 2024 (Special Issue &quot;Actuators for Haptic and Tactile Stimulation Applications&quot;)</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="mdpi-actuators-2024-piao cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/mdpi-actuators-2024-piao.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/mdpi-actuators-2024-piao.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/mdpi-actuators-2024-piao" target="_blank">Assessing the Impact of Force Feedback in Musical Knobs on Performance and User Experience</a></h1><p class="meta"><span>Ziyue Piao<!-- --> <span class="role"></span></span>, <a href="/people/christian-frisson"><img alt="christian-frisson photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-134/static/images/people/christian-frisson.jpg"/><strong>Christian Frisson</strong></a><span class="role"></span>, <span>Bavo Van Kerrebroeck<!-- --> <span class="role"></span></span>, <span>Marcelo M. Wanderley<!-- --> <span class="role"></span></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/mdpi-actuators-2024-piao.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>mdpi-actuators-2024-piao.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>This paper examined how rotary force feedback in knobs can enhance control over musical techniques, focusing on both performance and user experience. To support our study, we developed the Bend-aid system, a web-based sequencer with pre-designed haptic modes for pitch modulation, integrated with TorqueTuner, a rotary haptic device that controls pitch through programmable haptic effects. Then, twenty musically trained participants evaluated three haptic modes (No-force feedback (No-FF), Spring, and Detent) by performing a vibrato mimicry task, rating their experience on a Likert scale, and providing qualitative feedback in post-experiment interviews. The study assessed objective performance metrics (Pitch Error and Pitch Deviation) and subjective user experience ratings (Comfort, Ease of Control, and Helpfulness) of each haptic mode. User experience results showed that participants found force feedback helpful. Performance results showed that the Detent mode significantly improved pitch accuracy and vibrato stability compared to No-FF, while the Spring mode did not show a similar improvement. Post-experiment interviews showed that preferences for Spring and Detent modes varied, and the applicants provided suggestions for future knob designs. These findings suggest that force feedback may enhance both control and the experience of control in rotary knobs, with potential applications for more nuanced control in DMIs.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Rotary Force Feedback</span><span class="ui brown basic label">Digital Musical Instruments</span><span class="ui brown basic label">Knobs</span><span class="ui brown basic label">Torque Tuner</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ziyue Piao<!-- -->, <!-- -->Christian Frisson<!-- -->, <!-- -->Bavo Van Kerrebroeck<!-- -->, <!-- -->Marcelo M. Wanderley<!-- -->.Â <b>Assessing the Impact of Force Feedback in Musical Knobs on Performance and User Experience</b>.Â <i>(<!-- -->MDPI Actuators 2024 (Special Issue &quot;Actuators for Haptic and Tactile Stimulation Applications&quot;)<!-- -->)</i>Â <!-- -->Page: 1-<!-- -->15<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.3390/act13110462" target="_blank">https://doi.org/10.3390/act13110462</a></p></div></div><div class="block"><h1>Materials</h1><div class="ui horizontal small divided link list"><div class="item"><a href="https://github.com/piaoziyue/Bend-aid-Actuators-2024" target="_blank" style="font-size:1.2em"><svg data-prefix="fab" data-icon="github-alt" class="svg-inline--fa fa-github-alt" role="img" viewBox="0 0 512 512" aria-hidden="true"><path fill="currentColor" d="M202.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM496 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3l48.2 0c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"></path></svg>https://github.com/piaoziyue/Bend-aid-Actuators-2024</a></div></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2024-gunturu" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/uist-2024-gunturu/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2024-gunturu</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2024-gunturu cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/uist-2024-gunturu.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/uist-2024-gunturu.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2024-gunturu" target="_blank">Augmented Physics: Creating Interactive and Embedded Physics Simulations from Static Textbook Diagrams</a></h1><p class="meta"><a href="/people/aditya-gunturu"><img alt="aditya-gunturu photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-gunturu.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-gunturu.jpg"/><strong>Aditya Gunturu</strong></a><span class="role"></span>, <span>Yi Wen<!-- --> <span class="role"></span></span>, <a href="/people/nandi-zhang"><img alt="nandi-zhang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/nandi-zhang.jpg 1x" src="/pr-preview/pr-134/static/images/people/nandi-zhang.jpg"/><strong>Nandi Zhang</strong></a><span class="role"></span>, <a href="/people/jarin-thundathil"><img alt="jarin-thundathil photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/jarin-thundathil.jpg 1x" src="/pr-preview/pr-134/static/images/people/jarin-thundathil.jpg"/><strong>Jarin Thundathil</strong></a><span class="role"></span>, <span>Rubaiat Habib Kazi<!-- --> <span class="role"></span></span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2024-gunturu.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2024-gunturu.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/MOdSeUp8YcE" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/MOdSeUp8YcE?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/MOdSeUp8YcE/maxresdefault.jpg src=https://img.youtube.com/vi/MOdSeUp8YcE/maxresdefault.jpg&gt;&lt;span&gt;â–¶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce Augmented Physics, a machine learning-integrated authoring tool designed for creating embedded interactive physics simulations from static textbook diagrams. Leveraging recent advancements in computer vision, such as Segment Anything and Multi-modal LLMs, our web-based system enables users to semi-automatically extract diagrams from physics textbooks and generate interactive simulations based on the extracted content. These interactive diagrams are seamlessly integrated into scanned textbook pages, facilitating interactive and personalized learning experiences across various physics concepts, such as optics, circuits, and kinematics. Drawing from an elicitation study with seven physics instructors, we explore four key augmentation strategies: 1) augmented experiments, 2) animated diagrams, 3) bi-directional binding, and 4) parameter visualization. We evaluate our system through technical evaluation, a usability study (N=12), and expert interviews (N=12). Study findings suggest that our system can facilitate more engaging and personalized learning experiences in physics education.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Physics Education</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Authoring Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Aditya Gunturu<!-- -->, <!-- -->Yi Wen<!-- -->, <!-- -->Nandi Zhang<!-- -->, <!-- -->Jarin Thundathil<!-- -->, <!-- -->Rubaiat Habib Kazi<!-- -->, <!-- -->Ryo Suzuki<!-- -->.Â <b>Augmented Physics: Creating Interactive and Embedded Physics Simulations from Static Textbook Diagrams</b>.Â <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- --> <!-- -->(<!-- -->UIST 2024<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->Page: 1-<!-- -->12<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.1145/3654777.3676392" target="_blank">https://doi.org/10.1145/3654777.3676392</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2024-roy" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/uist-2024-roy/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2024-roy</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2024-roy cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/uist-2024-roy.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/uist-2024-roy.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2024-roy" target="_blank">HoloChemie - Sustainable Fabrication of Soft Biochemical Holographic Devices for Ubiquitous Sensing</a></h1><p class="meta"><a href="/people/sutirtha-roy"><img alt="sutirtha-roy photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/sutirtha-roy.jpg 1x" src="/pr-preview/pr-134/static/images/people/sutirtha-roy.jpg"/><strong>Sutirtha Roy</strong></a><span class="role"></span>, <span>Moshfiq-Us-Saleheen Chowdhury<!-- --> <span class="role"></span></span>, <span>Jurjaan Onayza Noim<!-- --> <span class="role"></span></span>, <span>Richa Pandey<!-- --> <span class="role"></span></span>, <a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2024-roy.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2024-roy.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/0FrYD1xInNs" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/0FrYD1xInNs?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/0FrYD1xInNs/maxresdefault.jpg src=https://img.youtube.com/vi/0FrYD1xInNs/maxresdefault.jpg&gt;&lt;span&gt;â–¶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Sustainable fabrication approaches and biomaterials are increasingly being used in HCI to fabricate interactive devices. However, the majority of the work has focused on integrating electronics. This paper takes a sustainable approach to exploring the fabrication of biochemical sensing devices. Firstly, we contribute a set of biochemical formulations for biological and environmental sensing with bio-sourced and environment-friendly substrate materials. Our formulations are based on a combination of enzymes derived from bacteria and fungi, plant extracts and commercially available chemicals to sense both liquid and gaseous analytes: glucose, lactic acid, pH levels and carbon dioxide. Our novel holographic sensing scheme allows for detecting the presence of analytes and enables quantitative estimation of the analyte levels. We present a set of application scenarios that demonstrate the versatility of our approach and discuss the sustainability aspects, its limitations, and the implications for bio-chemical systems in HCI.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Physiological Sensing</span><span class="ui brown basic label">Biochemical Devices Sensing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sutirtha Roy<!-- -->, <!-- -->Moshfiq-Us-Saleheen Chowdhury<!-- -->, <!-- -->Jurjaan Onayza Noim<!-- -->, <!-- -->Richa Pandey<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->.Â <b>HoloChemie - Sustainable Fabrication of Soft Biochemical Holographic Devices for Ubiquitous Sensing</b>.Â <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- --> <!-- -->(<!-- -->UIST 2024<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->Page: 1-<!-- -->19<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.1145/3654777.3676448" target="_blank">https://doi.org/10.1145/3654777.3676448</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2024-danyluk" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/dis-2024-danyluk/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2024-danyluk</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2024-danyluk cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/dis-2024-danyluk.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/dis-2024-danyluk.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2024-danyluk" target="_blank">Understanding Gesture and Microgesture Inputs for Augmented Reality Maps</a></h1><p class="meta"><a href="/people/kurtis-danyluk"><img alt="kurtis-danyluk photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/kurtis-danyluk.jpg 1x" src="/pr-preview/pr-134/static/images/people/kurtis-danyluk.jpg"/><strong>Kurtis Danyluk</strong></a><span class="role"></span>, <span>Simon Klueber<!-- --> <span class="role"></span></span>, <a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a><span class="role"></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-134/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>We explore the potential for subtle on-hand gesture and microgesture interactions for map navigation with augmented reality (AR) devices. We describe a design exercise and follow-up elicitation study in which we identified on-hand gestures for cartographic interaction primitives. Microgestures and on-hand interactions are a promising space for AR map navigation as they offers always-available, tactile, and memorable spaces for interaction. Our findings show a clear set of microgesture interaction patterns that are well suited for supporting map navigation and manipulation. In particular, we highlight how the properties of various microgestures align with particular cartographic interaction tasks. We also describe our experience creating an exploratory proof-of-concept AR map prototype which helped us identify new opportunities and practical challenges for microgesture control. Finally, we discuss how future AR map systems could benefit from on-hand and microgesture input schemes.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Gestural Input</span><span class="ui brown basic label">Microgestures</span><span class="ui brown basic label">AR</span><span class="ui brown basic label">Maps</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kurtis Danyluk<!-- -->, <!-- -->Simon Klueber<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Wesley Willett<!-- -->.Â <b>Understanding Gesture and Microgesture Inputs for Augmented Reality Maps</b>.Â <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- --> <!-- -->(<!-- -->DIS 2024<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->DOI: <a href="https://doi.org/10.1145/3643834.3661630" target="_blank">https://doi.org/10.1145/3643834.3661630</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2024-bressa" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/chi-2024-bressa/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2024-bressa</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2024-bressa cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-2024-bressa.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-2024-bressa.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2024-bressa" target="_blank">Input Visualization: Collecting and Modifying Data with Visual Representations</a></h1><p class="meta"><a href="/people/nathalie-bressa"><img alt="nathalie-bressa photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/nathalie-bressa.jpg 1x" src="/pr-preview/pr-134/static/images/people/nathalie-bressa.jpg"/><strong>Nathalie Bressa</strong></a><span class="role"></span>, <span>Jordan Louis<!-- --> <span class="role"></span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-134/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span>, <span>Samuel Huron<!-- --> <span class="role"></span></span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/RAfv2quE6nA" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/RAfv2quE6nA?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/RAfv2quE6nA/maxresdefault.jpg src=https://img.youtube.com/vi/RAfv2quE6nA/maxresdefault.jpg&gt;&lt;span&gt;â–¶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We examine input visualizations, visual representations that are designed to collect (and represent) new data rather than encode preexisting datasets. Information visualization is commonly used to reveal insights and stories within existing data. As a result, most contemporary visualization approaches assume existing datasets as the starting point for design, through which that data is mapped to visual encodings. Meanwhile, the implications of visualizations as inputs and as data sources have received little attentionâ€”despite the existence of visual and physical examples stretching back centuries. In this paper, we present a design space of 50 input visualizations analyzing their visual representation, data, artifact, context, and input. Based on this, we identify input modalities, purposes of input visualizations, and a set of design considerations. Finally, we discuss the relationship between input visualization and traditional visualization design and suggest opportunities for future research to better understand these visual representations and their potential.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Input Visualization</span><span class="ui brown basic label">Data Physicalization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nathalie Bressa<!-- -->, <!-- -->Jordan Louis<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Samuel Huron<!-- -->.Â <b>Input Visualization: Collecting and Modifying Data with Visual Representations</b>.Â <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2024<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->DOI: <a href="https://doi.org/10.1145/3613904.3642808" target="_blank">https://doi.org/10.1145/3613904.3642808</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2024-dhawka" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/chi-2024-dhawka/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2024-dhawka</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2024-dhawka cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-2024-dhawka.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-2024-dhawka.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2024-dhawka" target="_blank">Better Little People Pictures: Generative Creation of Demographically Diverse Anthropographics</a></h1><p class="meta"><a href="/people/priya-dhawka"><img alt="priya-dhawka photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/priya-dhawka.jpg 1x" src="/pr-preview/pr-134/static/images/people/priya-dhawka.jpg"/><strong>Priya Dhawka</strong></a><span class="role"></span>, <span>Lauren Perera<!-- --> <span class="role"></span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-134/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/dCEFvx4AqIo" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/dCEFvx4AqIo?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/dCEFvx4AqIo/maxresdefault.jpg src=https://img.youtube.com/vi/dCEFvx4AqIo/maxresdefault.jpg&gt;&lt;span&gt;â–¶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We explore the potential of generative AI text-to-image models to help designers efficiently craft unique, representative, and demographically diverse anthropographics that visualize data about people. Currently, creating data-driven iconic images to represent individuals in a dataset often requires considerable design effort. Generative text-to-image models can streamline the process of creating these images, but risk perpetuating designer biases in addition to stereotypes latent in the models. In response, we outline a conceptual workflow for crafting anthropographic assets for visualizations, highlighting possible sources of risk and bias as well as opportunities for reflection and refinement by a human designer. Using an implementation of this workflow with Stable Diffusion and Google Colab, we illustrate a variety of new anthropographic designs that showcase the visual expressiveness and scalability of these generative approaches. Based on our experiments, we also identify challenges and research opportunities for new AI-enabled anthropographic visualization tools.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Anthropographics</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Marginalized Populations</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Priya Dhawka<!-- -->, <!-- -->Lauren Perera<!-- -->, <!-- -->Wesley Willett<!-- -->.Â <b>Better Little People Pictures: Generative Creation of Demographically Diverse Anthropographics</b>.Â <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2024<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->DOI: <a href="https://doi.org/10.1145/3613904.3641957" target="_blank">https://doi.org/10.1145/3613904.3641957</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2024-panigrahy" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/chi-2024-panigrahy/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2024-panigrahy</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2024-panigrahy cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-2024-panigrahy.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-2024-panigrahy.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2024-panigrahy" target="_blank">ecSkin: Low-Cost Fabrication of Epidermal Electrochemical Sensors for Detecting Biomarkers in Sweat</a></h1><p class="meta"><span>Sai Nandan Panigrahy*<!-- --> <span class="role"></span></span>, <span>Chang Hyeon Lee*<!-- --> <span class="role"></span></span>, <span>Vrahant Nagoria<!-- --> <span class="role"></span></span>, <span>Mohammad Janghorban<!-- --> <span class="role"></span></span>, <span>Richa Pandey<!-- --> <span class="role"></span></span>, <a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2024-panigrahy.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2024-panigrahy.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/OqW3owQyMk8" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/OqW3owQyMk8?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/OqW3owQyMk8/maxresdefault.jpg src=https://img.youtube.com/vi/OqW3owQyMk8/maxresdefault.jpg&gt;&lt;span&gt;â–¶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>The development of low-cost and non-invasive biosensors for monitoring electrochemical biomarkers in sweat holds great promise for personalized healthcare and early disease detection. In this work, we present ecSkin, a novel fabrication approach for realizing epidermal electrochemical sensors that can detect two vital biomarkers in sweat: glucose and cortisol. We contribute the synthesis of functional reusable inks, that can be formulated using simple household materials. Electrical characterization of inks indicates that they outperform commercially available carbon inks. Cyclic voltammetry experiments show that our inks are electrochemically active and detect glucose and cortisol at activation voltages of -0.36 V and -0.22 V, respectively. Chronoamperometry experiments show that the sensors can detect the full range of glucose and cortisol levels typically found in sweat. Results from a user evaluation show that ecSkin sensors successfully function on the skin. Finally, we demonstrate three applications to illustrate how ecSkin devices can be deployed for various interactive applications.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Physiological Sensing</span><span class="ui brown basic label">Electrochemical Devices</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sai Nandan Panigrahy*<!-- -->, <!-- -->Chang Hyeon Lee*<!-- -->, <!-- -->Vrahant Nagoria<!-- -->, <!-- -->Mohammad Janghorban<!-- -->, <!-- -->Richa Pandey<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->.Â <b>ecSkin: Low-Cost Fabrication of Epidermal Electrochemical Sensors for Detecting Biomarkers in Sweat</b>.Â <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2024<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->Page: 1-<!-- -->20<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.1145/3613904.3642232" target="_blank">https://doi.org/10.1145/3613904.3642232</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2023-chulpongsatorn" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/uist-2023-chulpongsatorn/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2023-chulpongsatorn</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2023-chulpongsatorn cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/uist-2023-chulpongsatorn.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/uist-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2023-chulpongsatorn" target="_blank">Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks</a></h1><p class="meta"><a href="/people/neil-chulpongsatorn"><img alt="neil-chulpongsatorn photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-134/static/images/people/neil-chulpongsatorn.jpg"/><strong>Neil Chulpongsatorn</strong></a><span class="role"></span>, <a href="/people/mille-skovhus-lunding"><img alt="mille-skovhus-lunding photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/mille-skovhus-lunding.jpg 1x" src="/pr-preview/pr-134/static/images/people/mille-skovhus-lunding.jpg"/><strong>Mille Skovhus Lunding</strong></a><span class="role"></span>, <a href="/people/nishan-soni"><img alt="nishan-soni photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/nishan-soni.jpg 1x" src="/pr-preview/pr-134/static/images/people/nishan-soni.jpg"/><strong>Nishan Soni</strong></a><span class="role"></span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2023-chulpongsatorn.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2023-chulpongsatorn.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Zv6JQ5T-qn0" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Zv6JQ5T-qn0?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/Zv6JQ5T-qn0/maxresdefault.jpg src=https://img.youtube.com/vi/Zv6JQ5T-qn0/maxresdefault.jpg&gt;&lt;span&gt;â–¶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce Augmented Math, a machine learning-based approach to authoring AR explorable explanations by augmenting static math textbooks without programming. To augment a static document, our system first extracts mathematical formulas and figures from a given document using optical character recognition (OCR) and computer vision. By binding and manipulating these extracted contents, the user can see the interactive animation overlaid onto the document through mobile AR interfaces. This empowers non-technical users, such as teachers or students, to transform existing math textbooks and handouts into on-demand and personalized explorable explanations. To design our system, we first analyzed existing explorable math explanations to identify common design strategies. Based on the findings, we developed a set of augmentation techniques that can be automatically generated based on the extracted content, which are 1) dynamic values, 2) interactive figures, 3) relationship highlights, 4) concrete examples, and 5) step-by-step hints. To evaluate our system, we conduct two user studies: preliminary user testing and expert interviews. The study results confirm that our system allows more engaging experiences for learning math concepts.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Authoring Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Neil Chulpongsatorn<!-- -->, <!-- -->Mille Skovhus Lunding<!-- -->, <!-- -->Nishan Soni<!-- -->, <!-- -->Ryo Suzuki<!-- -->.Â <b>Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks</b>.Â <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- --> <!-- -->(<!-- -->UIST 2023<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->Page: 1-<!-- -->16<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.1145/3586183.3606827" target="_blank">https://doi.org/10.1145/3586183.3606827</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2023-ihara" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/uist-2023-ihara/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2023-ihara</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2023-ihara cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/uist-2023-ihara.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/uist-2023-ihara.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2023-ihara" target="_blank">HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality</a></h1><p class="meta"><a href="/people/keiichi-ihara"><img alt="keiichi-ihara photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/keiichi-ihara.jpg 1x" src="/pr-preview/pr-134/static/images/people/keiichi-ihara.jpg"/><strong>Keiichi Ihara</strong></a><span class="role"></span>, <a href="/people/mehrad-faridan"><img alt="mehrad-faridan photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/mehrad-faridan.jpg 1x" src="/pr-preview/pr-134/static/images/people/mehrad-faridan.jpg"/><strong>Mehrad Faridan</strong></a><span class="role"></span>, <span>Ayumi Ichikawa<!-- --> <span class="role"></span></span>, <span>Ikkaku Kawaguchi<!-- --> <span class="role"></span></span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2023-ihara.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2023-ihara.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/KSBPtiXy8Hg" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/KSBPtiXy8Hg?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/KSBPtiXy8Hg/maxresdefault.jpg src=https://img.youtube.com/vi/KSBPtiXy8Hg/maxresdefault.jpg&gt;&lt;span&gt;â–¶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces HoloBots, a mixed reality remote collaboration system that augments holographic telepresence with synchronized mobile robots. Beyond existing mixed reality telepresence, HoloBots lets remote users not only be visually and spatially present, but also physically engage with local users and their environment. HoloBots allows the users to touch, grasp, manipulate, and interact with the remote physical environment as if they were co-located in the same shared space. We achieve this by synchronizing holographic user motion (Hololens 2 and Azure Kinect) with tabletop mobile robots (Sony Toio). Beyond the existing physical telepresence, HoloBots contributes to an exploration of broader design space, such as object actuation, virtual hand physicalization, world-in-miniature exploration, shared tangible interfaces, embodied guidance, and haptic communication. We evaluate our system with twelve participants by comparing it with hologram-only and robot-only conditions. Both quantitative and qualitative results confirm that our system significantly enhances the level of co-presence and shared experience, compared to the other conditions.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Physical Telepresence</span><span class="ui brown basic label">Mobile Robots</span><span class="ui brown basic label">Actuated Tangible Ui</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Keiichi Ihara<!-- -->, <!-- -->Mehrad Faridan<!-- -->, <!-- -->Ayumi Ichikawa<!-- -->, <!-- -->Ikkaku Kawaguchi<!-- -->, <!-- -->Ryo Suzuki<!-- -->.Â <b>HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality</b>.Â <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- --> <!-- -->(<!-- -->UIST 2023<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->Page: 1-<!-- -->12<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.1145/3586183.3606727" target="_blank">https://doi.org/10.1145/3586183.3606727</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2023-xia" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/uist-2023-xia/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2023-xia</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2023-xia cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/uist-2023-xia.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/uist-2023-xia.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2023-xia" target="_blank">RealityCanvas: Augmented Reality Sketching for Embedded and Responsive Scribble Animation Effects</a></h1><p class="meta"><a href="/people/zhijie-xia"><img alt="zhijie-xia photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/zhijie-xia.jpg 1x" src="/pr-preview/pr-134/static/images/people/zhijie-xia.jpg"/><strong>Zhijie Xia</strong></a><span class="role"></span>, <a href="/people/kyzyl-monteiro"><img alt="kyzyl-monteiro photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/kyzyl-monteiro.jpg 1x" src="/pr-preview/pr-134/static/images/people/kyzyl-monteiro.jpg"/><strong>Kyzyl Monteiro</strong></a><span class="role"></span>, <a href="/people/kevin-van"><img alt="kevin-van photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/kevin-van.jpg 1x" src="/pr-preview/pr-134/static/images/people/kevin-van.jpg"/><strong>Kevin Van</strong></a><span class="role"></span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2023-xia.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2023-xia.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/HVOgH1quDsc" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/HVOgH1quDsc?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/HVOgH1quDsc/maxresdefault.jpg src=https://img.youtube.com/vi/HVOgH1quDsc/maxresdefault.jpg&gt;&lt;span&gt;â–¶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce RealityCanvas, a mobile AR sketching tool that can easily augment real-world physical motion with responsive hand-drawn animation. Recent research in AR sketching tools has enabled users to not only embed static drawings into the real world but also dynamically animate them with physical motion. However, existing tools often lack the flexibility and expressiveness of possible animations, as they primarily support simple line-based geometry. To address this limitation, we explore both expressive and improvisational AR sketched animation by introducing a set of responsive scribble animation techniques that can be directly embedded through sketching interactions: 1) object binding, 2) flip-book animation, 3) action trigger, 4) particle effects, 5) motion trajectory, and 6) contour highlight. These six animation effects were derived from the analysis of 172 existing video-edited scribble animations. We showcase these techniques through various applications, such as video creation, augmented education, storytelling, and AR prototyping. The results of our user study and expert interviews confirm that our tool can lower the barrier to creating AR-based sketched animation, while allowing creative, expressive, and improvisational AR sketching experiences.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Sketching Interfaces</span><span class="ui brown basic label">Scribble Animation</span><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Real Time Authoring</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Zhijie Xia<!-- -->, <!-- -->Kyzyl Monteiro<!-- -->, <!-- -->Kevin Van<!-- -->, <!-- -->Ryo Suzuki<!-- -->.Â <b>RealityCanvas: Augmented Reality Sketching for Embedded and Responsive Scribble Animation Effects</b>.Â <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- --> <!-- -->(<!-- -->UIST 2023<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->Page: 1-<!-- -->14<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.1145/3586183.3606716" target="_blank">https://doi.org/10.1145/3586183.3606716</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="assets-2023-mok" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/assets-2023-mok/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>assets-2023-mok</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">ASSETS 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/publications/assets-2023-mok" target="_blank">Experiences of Autistic Twitch Livestreamers: â€œI have made easily the most meaningful and impactful relationshipsâ€</a></h1><p class="meta"><a href="/people/terrance-mok"><img alt="terrance-mok photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/terrance-mok.jpg 1x" src="/pr-preview/pr-134/static/images/people/terrance-mok.jpg"/><strong>Terrance Mok</strong></a><span class="role"></span>, <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/anthony-tang.jpg 1x" src="/pr-preview/pr-134/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a><span class="role"></span>, <span>Adam McCrimmon<!-- --> <span class="role"></span></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/lora-oehlberg.jpg 1x" src="/pr-preview/pr-134/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/assets-2023-mok.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>assets-2023-mok.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present perspectives from 10 autistic Twitch streamers regarding their experiences as livestreamers and how autism uniquely colors their experiences. Livestreaming offers a social online experience distinct from in-person, face-to-face communication, where autistic people tend to encounter challenges. Our reflexive thematic analysis of interviews with 10 participants showcases autistic livestreamersâ€™ perspectives in their own words. Our findings center on the importance of having streamers establishing connections with other, sharing autistic identities, controlling a space for social interaction, personal growth, and accessibility challenges. In our discussion, we highlight the crucial value of having a medium for autistic representation, as well as design opportunities for streaming platforms to onboard autistic livestreamers and to facilitate livestreamers communication with their audience.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Autism</span><span class="ui brown basic label">Live Streaming</span><span class="ui brown basic label">Autistic</span><span class="ui brown basic label">Twitch</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Terrance Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Adam McCrimmon<!-- -->, <!-- -->Lora Oehlberg<!-- -->.Â <b>Experiences of Autistic Twitch Livestreamers: â€œI have made easily the most meaningful and impactful relationshipsâ€</b>.Â <i>(<!-- -->ASSETS 2023<!-- -->)</i>Â <!-- -->DOI: <a href="https://doi.org/10.1145/3597638.3608416" target="_blank">https://doi.org/10.1145/3597638.3608416</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2023-mukashev" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/uist-2023-mukashev/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2023-mukashev</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2023-mukashev cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/uist-2023-mukashev.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/uist-2023-mukashev.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2023-mukashev" target="_blank">TactTongue: Prototyping ElectroTactile Stimulations on the Tongue</a></h1><p class="meta"><a href="/people/dinmukhammed-mukashev"><img alt="dinmukhammed-mukashev photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/dinmukhammed-mukashev.jpg 1x" src="/pr-preview/pr-134/static/images/people/dinmukhammed-mukashev.jpg"/><strong>Dinmukhammed Mukashev</strong></a><span class="role"></span>, <span>Nimesha Ranasinghe<!-- --> <span class="role"></span></span>, <a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2023-mukashev.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2023-mukashev.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/zCUdJNNRz5s" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/zCUdJNNRz5s?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/zCUdJNNRz5s/maxresdefault.jpg src=https://img.youtube.com/vi/zCUdJNNRz5s/maxresdefault.jpg&gt;&lt;span&gt;â–¶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>The tongue is a remarkable human organ with a high concentration of taste receptors and an exceptional ability to sense touch. This work uses electro-tactile stimulation to explore the intricate interplay between tactile perception and taste rendering on the tongue. To facilitate this exploration, we utilized a fexible, high-resolution electro-tactile prototyping platform that can be administered in the mouth. We have created a design tool that abstracts users from the low-level stimulation parameters, enabling them to focus on higher-level design objectives. Through this platform, we present the results of three studies. Our frst study evaluates the design toolâ€™s qualitative and formative aspects. In contrast, the second study measures the qualitative attributes of the sensations produced by our device, including tactile sensations and taste. In the third study, we demonstrate the ability of our device to sense touch input through the tongue when placed on the hard palate region in the mouth. Finally, we present a range of application demonstrators that span diverse domains, including accessibility, medical surgeries, and extended reality. These demonstrators showcase the versatility and potential of our platform, highlighting its ability to enable researchers and practitioners to explore new ways of leveraging the tongueâ€™s unique capabilities. Overall, this work presents new opportunities to deploy tongue interfaces and has broad implications for designing interfaces that incorporate the tongue as a sensory organ.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Electrotactile Actuation</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Dinmukhammed Mukashev<!-- -->, <!-- -->Nimesha Ranasinghe<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->.Â <b>TactTongue: Prototyping ElectroTactile Stimulations on the Tongue</b>.Â <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- --> <!-- -->(<!-- -->UIST 2023<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->Page: 1-<!-- -->14<!-- -->.Â <!-- -->DOI: <a href="https://dl.acm.org/doi/10.1145/3586183.3606829" target="_blank">https://dl.acm.org/doi/10.1145/3586183.3606829</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2023-xia2" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/uist-2023-xia2/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2023-xia2</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2023-xia2 cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/uist-2023-xia2.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/uist-2023-xia2.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2023-xia2" target="_blank">CrossTalk: Intelligent Substrates for Language-Oriented Interaction in Video-Based Communication and Collaboration</a></h1><p class="meta"><span>Haijun Xia<!-- --> <span class="role"></span></span>, <span>Tony Wang<!-- --> <span class="role"></span></span>, <a href="/people/aditya-gunturu"><img alt="aditya-gunturu photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-gunturu.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-gunturu.jpg"/><strong>Aditya Gunturu</strong></a><span class="role"></span>, <span>Peiling Jiang<!-- --> <span class="role"></span></span>, <span>William Duan<!-- --> <span class="role"></span></span>, <span>Xiaoshuo Yao<!-- --> <span class="role"></span></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2023-xia2.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2023-xia2.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/8I1yXNRcm54" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/8I1yXNRcm54?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/8I1yXNRcm54/maxresdefault.jpg src=https://img.youtube.com/vi/8I1yXNRcm54/maxresdefault.jpg&gt;&lt;span&gt;â–¶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Despite the advances and ubiquity of digital communication media such as videoconferencing and virtual reality, they remain oblivious to the rich intentions expressed by users. Beyond transmitting audio, videos, and messages, we envision digital communication media as proactive facilitators that can provide unobtrusive assistance to enhance communication and collaboration. Informed by the results of a formative study, we propose three key design concepts to explore the systematic integration of intelligence into communication and collaboration, including the panel substrate, language-based intent recognition, and lightweight interaction techniques. We developed CrossTalk, a videoconferencing system that instantiates these concepts, which was found to enable a more fluid and flexible communication and collaboration experience.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Videoconferencing</span><span class="ui brown basic label">Natural Language Interface</span><span class="ui brown basic label">Language Oriented Interaction</span><span class="ui brown basic label">Context Aware Computing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Haijun Xia<!-- -->, <!-- -->Tony Wang<!-- -->, <!-- -->Aditya Gunturu<!-- -->, <!-- -->Peiling Jiang<!-- -->, <!-- -->William Duan<!-- -->, <!-- -->Xiaoshuo Yao<!-- -->.Â <b>CrossTalk: Intelligent Substrates for Language-Oriented Interaction in Video-Based Communication and Collaboration</b>.Â <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- --> <!-- -->(<!-- -->UIST 2023<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->Page: 1-<!-- -->16<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.1145/3586183.3606773" target="_blank">https://doi.org/10.1145/3586183.3606773</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="mdpi-arts-2023-frisson" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/mdpi-arts-2023-frisson/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>mdpi-arts-2023-frisson</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MDPI Arts 2023 (Special Issue Feeling the Futureâ€”Haptic Audio)</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="mdpi-arts-2023-frisson cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/mdpi-arts-2023-frisson.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/mdpi-arts-2023-frisson.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/mdpi-arts-2023-frisson" target="_blank">Challenges and Opportunities of Force Feedback in Music</a></h1><p class="meta"><a href="/people/christian-frisson"><img alt="christian-frisson photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-134/static/images/people/christian-frisson.jpg"/><strong>Christian Frisson</strong></a><span class="role"></span>, <span>Marcelo M. Wanderley<!-- --> <span class="role"></span></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/mdpi-arts-2023-frisson.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>mdpi-arts-2023-frisson.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>A growing body of work on musical haptics focuses on vibrotactile feedback, while musical applications of force feedback, though more than four decades old, are sparser. This paper reviews related work combining music and haptics, focusing on force feedback. We then discuss the limitations of these works and elicit the main challenges in current applications of force feedback and music (FF&amp;M), which are as follows: modularity; replicability; affordability; and usability. We call for the following opportunities in future research works on FF&amp;M: embedding audio and haptic software into hardware modules, networking multiple modules with distributed control, and authoring with audio-inspired and audio-coupled tools. We illustrate our review with recent efforts to develop an affordable, open-source and self-contained 1-Degree-of-Freedom (DoF) rotary force-feedback device for musical applications, i.e., the TorqueTuner, and to embed audio and haptic processing and authoring in module firmware, with ForceHost, and examine their advantages and drawbacks in light of the opportunities presented in the text.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Force Feedback</span><span class="ui brown basic label">Digital Musical Instruments</span><span class="ui brown basic label">Torque Tuner</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Christian Frisson<!-- -->, <!-- -->Marcelo M. Wanderley<!-- -->.Â <b>Challenges and Opportunities of Force Feedback in Music</b>.Â <i>(<!-- -->MDPI Arts 2023 (Special Issue Feeling the Futureâ€”Haptic Audio)<!-- -->)</i>Â <!-- -->Page: 1-<!-- -->13<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.3390/arts12040147" target="_blank">https://doi.org/10.3390/arts12040147</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="siggraph-labs-2023-seta" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/siggraph-labs-2023-seta/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>siggraph-labs-2023-seta</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">SIGGRAPH 2023 Labs</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="siggraph-labs-2023-seta cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/siggraph-labs-2023-seta.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/siggraph-labs-2023-seta.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/siggraph-labs-2023-seta" target="_blank">Sketching Pipelines for Ephemeral Immersive Spaces</a></h1><p class="meta"><span>MichaÅ‚ Seta<!-- --> <span class="role"></span></span>, <span>Eduardo A. L. Meneses<!-- --> <span class="role"></span></span>, <span>Emmanuel Durand<!-- --> <span class="role"></span></span>, <a href="/people/christian-frisson"><img alt="christian-frisson photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-134/static/images/people/christian-frisson.jpg"/><strong>Christian Frisson</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/siggraph-labs-2023-seta.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>siggraph-labs-2023-seta.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>This hands-on class will allow artists to use open-source tools to create interactive and immersive experiences. These tools have been created and incubated at the Society for Arts and Technology (SAT), a unique non-profit organization in Canada whose mission is to democratize technologies to enable people to experience and author multisensory immersions. During the class we invite participants to use their favorite software on platforms they are already familiar with, to interface with our tools. The toolset will include transmission protocols, video mapping tools, sound spatialization software, and gestural control using pose detection. The class will be organized in two parts: a presentation of the tools and context involving the development and applications, and a hands-on session with an ephemeral immersive space. This event is designed for art researchers, artists, designers, content creators, and other creatives interested in creating immersive spaces using research-developed tools. Participants will learn how to employ open-source tools for different artistic tasks so that they will be able to deploy their own immersive spaces after the class.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Art</span><span class="ui brown basic label">Interaction</span><span class="ui brown basic label">Pipeline</span><span class="ui brown basic label">Production</span><span class="ui brown basic label">Ui Tools</span><span class="ui brown basic label">VR AR MR</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">MichaÅ‚ Seta<!-- -->, <!-- -->Eduardo A. L. Meneses<!-- -->, <!-- -->Emmanuel Durand<!-- -->, <!-- -->Christian Frisson<!-- -->.Â <b>Sketching Pipelines for Ephemeral Immersive Spaces</b>.Â <i>(<!-- -->SIGGRAPH 2023 Labs<!-- -->)</i>Â <!-- -->Page: 1-<!-- -->2<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.1145/3588029.3599740" target="_blank">https://doi.org/10.1145/3588029.3599740</a></p></div></div><div class="block"><h1>Materials</h1><div class="ui horizontal small divided link list"><div class="item"><a href="https://sat-mtl.gitlab.io/tools/" target="_blank" style="font-size:1.2em"><svg data-prefix="fab" data-icon="gitlab" class="svg-inline--fa fa-gitlab" role="img" viewBox="0 0 512 512" aria-hidden="true"><path fill="currentColor" d="M504 204.6l-.7-1.8-69.7-181.8c-1.4-3.6-3.9-6.6-7.2-8.6-2.4-1.6-5.1-2.5-8-2.8s-5.7 .1-8.4 1.1-5.1 2.7-7.1 4.8c-1.9 2.1-3.3 4.7-4.1 7.4l-47 144-190.5 0-47.1-144c-.8-2.8-2.2-5.3-4.1-7.4-2-2.1-4.4-3.7-7.1-4.8-2.6-1-5.5-1.4-8.4-1.1s-5.6 1.2-8 2.8c-3.2 2-5.8 5.1-7.2 8.6L9.8 202.8 9 204.6c-10 26.2-11.3 55-3.5 82 7.7 26.9 24 50.7 46.4 67.6l.3 .2 .6 .4 106 79.5c38.5 29.1 66.7 50.3 84.6 63.9 3.7 1.9 8.3 4.3 13 4.3s9.3-2.4 13-4.3c17.9-13.5 46.1-34.9 84.6-63.9l106.7-79.9 .3-.3c22.4-16.9 38.7-40.6 45.6-67.5 8.6-27 7.4-55.8-2.6-82z"></path></svg>https://sat-mtl.gitlab.io/tools/</a></div></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2023-li" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/dis-2023-li/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2023-li</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2023-li cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/dis-2023-li.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/dis-2023-li.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2023-li" target="_blank">Physica: Interactive Tangible Physics Simulation based on Tabletop Mobile Robots Towards Explorable Physics Education</a></h1><p class="meta"><span>Jiatong Li<!-- --> <span class="role"></span></span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"></span>, <span>Ken Nakagaki<!-- --> <span class="role"></span></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/dis-2023-li.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>dis-2023-li.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/7DKpq52282g" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/7DKpq52282g?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/7DKpq52282g/maxresdefault.jpg src=https://img.youtube.com/vi/7DKpq52282g/maxresdefault.jpg&gt;&lt;span&gt;â–¶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>In this paper, we introduce Physica, a tangible physics simulation system and approach based on tabletop mobile robots. In Physica, each tabletop robot can physically represent distinct simulated objects that are controlled through an underlying physics simulation, such as gravitational force, molecular movement, and spring force. It aims to bring the benefits of tangible and haptic interaction into explorable physics learning, which was traditionally only available on screen-based interfaces. The system utilizes off-the-shelf mobile robots (Sony Toio) and an open-source physics simulation tool (Teilchen). Built on top of them, we implement the interaction software pipeline that consists of 1) an event detector to reflect tangible interaction by users, and 2) target speed control to minimize the gap between the robot motion and simulated moving objects. To present the potential for physics education, we demonstrate various application scenarios that illustrate different forms of learning using Physica. In our user study, we investigate the effect and the potential of our approach through a perception study and interviews with physics educators.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Physics Simulation</span><span class="ui brown basic label">Actuated Tangible Ui</span><span class="ui brown basic label">Swarm Ui</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jiatong Li<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Ken Nakagaki<!-- -->.Â <b>Physica: Interactive Tangible Physics Simulation based on Tabletop Mobile Robots Towards Explorable Physics Education</b>.Â <i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- --> <!-- -->(<!-- -->DIS 2023<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->Page: 1-<!-- -->15<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.1145/3526113.3545626" target="_blank">https://doi.org/10.1145/3526113.3545626</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2023-dhawka" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/chi-2023-dhawka/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2023-dhawka</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2023-dhawka cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-2023-dhawka.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-2023-dhawka.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2023-dhawka" target="_blank">We are the Data: Challenges and Opportunities for Creating Demographically Diverse Anthropographics</a></h1><p class="meta"><a href="/people/priya-dhawka"><img alt="priya-dhawka photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/priya-dhawka.jpg 1x" src="/pr-preview/pr-134/static/images/people/priya-dhawka.jpg"/><strong>Priya Dhawka</strong></a><span class="role"></span>, <a href="/people/helen-ai-he"><img alt="helen-ai-he photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/helen-ai-he.jpg 1x" src="/pr-preview/pr-134/static/images/people/helen-ai-he.jpg"/><strong>Helen Ai He</strong></a><span class="role"></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-134/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2023-dhawka.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2023-dhawka.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/iBzv2jS3ECM" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/iBzv2jS3ECM?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/iBzv2jS3ECM/maxresdefault.jpg src=https://img.youtube.com/vi/iBzv2jS3ECM/maxresdefault.jpg&gt;&lt;span&gt;â–¶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Anthropographics are human-shaped visualizations that aim to emphasize the human importance of datasets and the people behind them. However, current anthropographics tend to employ homogeneous human shapes to encode data about diverse demographic groups. Such anthropographics can obscure important differences between groups and contemporary designs exemplify the lack of inclusive approaches for representing human diversity in visualizations. In response, we explore the creation of demographically diverse anthropographics that communicate the visible diversity of demographically distinct populations. Building on previous anthropographics research, we explore strategies for visualizing datasets about people in ways that explicitly encode diversityâ€”illustrating these approaches with examples in a variety of visual styles. We also critically reflect on strategies for creating diverse anthropographics, identifying social and technical challenges that can result in harmful representations. Finally, we highlight a set of forward-looking research opportunities for advancing the design and understanding of diverse anthropographics.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Anthropographics</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Marginalized Populations</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Priya Dhawka<!-- -->, <!-- -->Helen Ai He<!-- -->, <!-- -->Wesley Willett<!-- -->.Â <b>We are the Data: Challenges and Opportunities for Creating Demographically Diverse Anthropographics</b>.Â <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2023<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->Page: 1-<!-- -->14<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.1145/3544548.3581086" target="_blank">https://doi.org/10.1145/3544548.3581086</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2023-faridan" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/chi-2023-faridan/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2023-faridan</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2023-faridan cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-2023-faridan.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-2023-faridan.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2023-faridan" target="_blank">ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms</a></h1><p class="meta"><a href="/people/mehrad-faridan"><img alt="mehrad-faridan photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/mehrad-faridan.jpg 1x" src="/pr-preview/pr-134/static/images/people/mehrad-faridan.jpg"/><strong>Mehrad Faridan</strong></a><span class="role"></span>, <a href="/people/bheesha-kumari"><img alt="bheesha-kumari photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/bheesha-kumari.jpg 1x" src="/pr-preview/pr-134/static/images/people/bheesha-kumari.jpg"/><strong>Bheesha Kumari</strong></a><span class="role"></span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2023-faridan.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2023-faridan.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/VOe3fETd3sk" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/VOe3fETd3sk?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/VOe3fETd3sk/maxresdefault.jpg src=https://img.youtube.com/vi/VOe3fETd3sk/maxresdefault.jpg&gt;&lt;span&gt;â–¶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present ChameleonControl, a real-human teleoperation system for scalable remote instruction in hands-on classrooms. In contrast to the existing video or AR/VR-based remote hands-on education, ChameleonControl uses a real human as a surrogate of a remote instructor. Building on existing human-based telepresence approaches (e.g. ChameleonMask), we contribute a novel method to teleoperate a human surrogate through synchronized mixed reality (MR) hand gestural navigation and verbal communication. By overlaying the remote instructor&#x27;s virtual hands in the local user&#x27;s MR view, the remote instructor can guide and control the local user as if they were physically present. This allows the local user/surrogate to synchronize their hand movements and gestures with the remote instructor, effectively ``teleoperating&#x27;&#x27; a real human. We evaluate our system through the in-the-wild deployment for physiotherapy classrooms, as well as lab-based experiments for other application domains such as mechanical assembly, sign language, and cooking lessons. The study results confirm that our approach can increase engagement and the sense of co-presence, showing potential for the future of remote hands-on classrooms.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Visual Cue</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Telepresence</span><span class="ui brown basic label">Remote Guidance</span><span class="ui brown basic label">Human Surrogates</span><span class="ui brown basic label">Hands On Training</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Mehrad Faridan<!-- -->, <!-- -->Bheesha Kumari<!-- -->, <!-- -->Ryo Suzuki<!-- -->.Â <b>ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms</b>.Â <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2023<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->Page: 1-<!-- -->13<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.1145/3544548.3581381" target="_blank">https://doi.org/10.1145/3544548.3581381</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2023-monteiro" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/chi-2023-monteiro/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2023-monteiro</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2023-monteiro cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-2023-monteiro.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-2023-monteiro.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2023-monteiro" target="_blank">Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching</a></h1><p class="meta"><a href="/people/kyzyl-monteiro"><img alt="kyzyl-monteiro photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/kyzyl-monteiro.jpg 1x" src="/pr-preview/pr-134/static/images/people/kyzyl-monteiro.jpg"/><strong>Kyzyl Monteiro</strong></a><span class="role"></span>, <a href="/people/ritik-vatsal"><img alt="ritik-vatsal photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ritik-vatsal.jpg 1x" src="/pr-preview/pr-134/static/images/people/ritik-vatsal.jpg"/><strong>Ritik Vatsal</strong></a><span class="role"></span>, <a href="/people/neil-chulpongsatorn"><img alt="neil-chulpongsatorn photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-134/static/images/people/neil-chulpongsatorn.jpg"/><strong>Neil Chulpongsatorn</strong></a><span class="role"></span>, <span>Aman Parnami<!-- --> <span class="role"></span></span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2023-monteiro.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2023-monteiro.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/JssiyfrhIJw" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/JssiyfrhIJw?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/JssiyfrhIJw/maxresdefault.jpg src=https://img.youtube.com/vi/JssiyfrhIJw/maxresdefault.jpg&gt;&lt;span&gt;â–¶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces Teachable Reality, an augmented reality (AR) prototyping tool for creating interactive tangible AR applications with arbitrary everyday objects. Teachable Reality leverages vision-based interactive machine teaching (e.g., Teachable Machine), which captures real-world interactions for AR prototyping. It identifies the user-defined tangible and gestural interactions using an on-demand computer vision model. Based on this, the user can easily create functional AR prototypes without programming, enabled by a trigger-action authoring interface. Therefore, our approach allows the flexibility, customizability, and generalizability of tangible AR applications that can address the limitation of current marker-based approaches. We explore the design space and demonstrate various AR prototypes, which include tangible and deformable interfaces, context-aware assistants, and body-driven AR applications. The results of our user study and expert interviews confirm that our approach can lower the barrier to creating functional AR prototypes while also allowing flexible and general-purpose prototyping experiences.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Prototyping Tools</span><span class="ui brown basic label">Tangible Interactions</span><span class="ui brown basic label">Everyday Objects</span><span class="ui brown basic label">Interactive Machine Teaching</span><span class="ui brown basic label">Human Centered Machine Learning</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kyzyl Monteiro<!-- -->, <!-- -->Ritik Vatsal<!-- -->, <!-- -->Neil Chulpongsatorn<!-- -->, <!-- -->Aman Parnami<!-- -->, <!-- -->Ryo Suzuki<!-- -->.Â <b>Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching</b>.Â <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2023<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->Page: 1-<!-- -->15<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.1145/3544548.3581449" target="_blank">https://doi.org/10.1145/3544548.3581449</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-ea-2023-chulpongsatorn" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/chi-ea-2023-chulpongsatorn/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-ea-2023-chulpongsatorn</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI EA 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-ea-2023-chulpongsatorn cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-ea-2023-chulpongsatorn.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-ea-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-ea-2023-chulpongsatorn" target="_blank">HoloTouch: Interacting with Mixed Reality Visualizations Through Smartphone Proxies</a></h1><p class="meta"><a href="/people/neil-chulpongsatorn"><img alt="neil-chulpongsatorn photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/neil-chulpongsatorn.jpg 1x" src="/pr-preview/pr-134/static/images/people/neil-chulpongsatorn.jpg"/><strong>Neil Chulpongsatorn</strong></a><span class="role"></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-134/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-ea-2023-chulpongsatorn.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-ea-2023-chulpongsatorn.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We contribute interaction techniques for augmenting mixed reality (MR) visualizations with smartphone proxies. By combining head-mounted displays (HMDs) with mobile touchscreens, we can augment low-resolution holographic 3D charts with precise touch input, haptics feedback, high-resolution 2D graphics, and physical manipulation. Our approach aims to complement both MR and physical visualizations. Most current MR visualizations suffer from unreliable tracking, low visual resolution, and imprecise input. Data physicalizations on the other hand, although allowing for natural physical manipulation, are limited in dynamic and interactive modification. We demonstrate how mobile devices such as smartphones or tablets can serve as physical proxies for MR data interactions, creating dynamic visualizations that support precise manipulation and rich input and output. We describe 6 interaction techniques that leverage the combined physicality, sensing, and output capabilities of HMDs and smartphones, and demonstrate those interactions via a prototype system. Based on an evaluation, we outline opportunities for combining the advantages of both MR and physical charts.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Embedded Data Visualization</span><span class="ui brown basic label">Tangible Interaction</span><span class="ui brown basic label">Cross Device Interaction</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Neil Chulpongsatorn<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Ryo Suzuki<!-- -->.Â <b>HoloTouch: Interacting with Mixed Reality Visualizations Through Smartphone Proxies</b>.Â <i>In <!-- -->Extended Abstracts of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI EA 2023<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->Page: 1-<!-- -->8<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.1145/3544549.3585738" target="_blank">https://doi.org/10.1145/3544549.3585738</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-ea-2023-fang" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/chi-ea-2023-fang/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-ea-2023-fang</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI EA 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-ea-2023-fang cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/chi-ea-2023-fang.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/chi-ea-2023-fang.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-ea-2023-fang" target="_blank">VR Haptics at Home: Repurposing Everyday Objects and Environment for Casual and On-Demand VR Haptic Experiences</a></h1><p class="meta"><span>Cathy Mengying Fang<!-- --> <span class="role"></span></span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"></span>, <span>Daniel Leithinger<!-- --> <span class="role"></span></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-ea-2023-fang.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-ea-2023-fang.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces VR Haptics at Home, a method of repurposing everyday objects in the home to provide casual and on-demand haptic experiences. Current VR haptic devices are often expensive, complex, and unreliable, which limits the opportunities for rich haptic experiences outside research labs. In contrast, we envision that, by repurposing everyday objects as passive haptics props, we can create engaging VR experiences for casual uses with minimal cost and setup. To explore and evaluate this idea, we conducted an in-the-wild study with eight participants, in which they used our proof-of-concept system to turn their surrounding objects such as chairs, tables, and pillows at their own homes into haptic props. The study results show that our method can be adapted to different homes and environments, enabling more engaging VR experiences without the need for complex setup process. Based on our findings, we propose a possible design space to showcase the potential for future investigation.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Interaction Techniques</span><span class="ui brown basic label">Passive Haptics</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Cathy Mengying Fang<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Daniel Leithinger<!-- -->.Â <b>VR Haptics at Home: Repurposing Everyday Objects and Environment for Casual and On-Demand VR Haptic Experiences</b>.Â <i>In <!-- -->Extended Abstracts of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI EA 2023<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->Page: 1-<!-- -->7<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.1145/3544549.3585871" target="_blank">https://doi.org/10.1145/3544549.3585871</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="vrst-2022-frisson" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/vrst-2022-frisson/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>vrst-2022-frisson</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">VRST 2022 Poster</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="vrst-2022-frisson cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/vrst-2022-frisson.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/vrst-2022-frisson.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/vrst-2022-frisson" target="_blank">LivePose: Democratizing Pose Detection for Multimedia Arts and Telepresence Applications on Open Edge Devices</a></h1><p class="meta"><a href="/people/christian-frisson"><img alt="christian-frisson photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/christian-frisson.jpg 1x" src="/pr-preview/pr-134/static/images/people/christian-frisson.jpg"/><strong>Christian Frisson</strong></a><span class="role"></span>, <span>Gabriel N. Downs<!-- --> <span class="role"></span></span>, <span>Marie-Ãˆve Dumas<!-- --> <span class="role"></span></span>, <span>Farzaneh Askari<!-- --> <span class="role"></span></span>, <span>Emmanuel Durand<!-- --> <span class="role"></span></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/vrst-2022-frisson.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>vrst-2022-frisson.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/604196712" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/604196712?autoplay=1&gt;&lt;Image width={0} height={0} alt=undefined src=undefined&gt;&lt;span&gt;â–¶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present LivePose: an open-source (GPL license) tool that democratizes pose detection for multimedia arts and telepresence applications, optimized for and distributed on open edge devices. We designed the architecture of LivePose with a 5-stage pipeline (frame capture, pose estimation, dimension mapping, filtering, output) sharing streams of data flow, distributable on networked nodes. We distribute LivePose and dependencies packages and filesystem images optimized for edge devices (NVIDIA Jetson). We showcase multimedia arts and telepresence applications enabled by LivePose.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Multimedia Arts</span><span class="ui brown basic label">Telepresence</span><span class="ui brown basic label">Edge Computing</span><span class="ui brown basic label">Pose Detection</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Christian Frisson<!-- -->, <!-- -->Gabriel N. Downs<!-- -->, <!-- -->Marie-Ãˆve Dumas<!-- -->, <!-- -->Farzaneh Askari<!-- -->, <!-- -->Emmanuel Durand<!-- -->.Â <b>LivePose: Democratizing Pose Detection for Multimedia Arts and Telepresence Applications on Open Edge Devices</b>.Â <i>(<!-- -->VRST 2022 Poster<!-- -->)</i>Â <!-- -->Page: 1-<!-- -->2<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.1145/3562939.3565660" target="_blank">https://doi.org/10.1145/3562939.3565660</a></p></div></div><div class="block"><h1>Materials</h1><div class="ui horizontal small divided link list"><div class="item"><a href="https://gitlab.com/sat-mtl/tools/livepose" target="_blank" style="font-size:1.2em"><svg data-prefix="fab" data-icon="gitlab" class="svg-inline--fa fa-gitlab" role="img" viewBox="0 0 512 512" aria-hidden="true"><path fill="currentColor" d="M504 204.6l-.7-1.8-69.7-181.8c-1.4-3.6-3.9-6.6-7.2-8.6-2.4-1.6-5.1-2.5-8-2.8s-5.7 .1-8.4 1.1-5.1 2.7-7.1 4.8c-1.9 2.1-3.3 4.7-4.1 7.4l-47 144-190.5 0-47.1-144c-.8-2.8-2.2-5.3-4.1-7.4-2-2.1-4.4-3.7-7.1-4.8-2.6-1-5.5-1.4-8.4-1.1s-5.6 1.2-8 2.8c-3.2 2-5.8 5.1-7.2 8.6L9.8 202.8 9 204.6c-10 26.2-11.3 55-3.5 82 7.7 26.9 24 50.7 46.4 67.6l.3 .2 .6 .4 106 79.5c38.5 29.1 66.7 50.3 84.6 63.9 3.7 1.9 8.3 4.3 13 4.3s9.3-2.4 13-4.3c17.9-13.5 46.1-34.9 84.6-63.9l106.7-79.9 .3-.3c22.4-16.9 38.7-40.6 45.6-67.5 8.6-27 7.4-55.8-2.6-82z"></path></svg>https://gitlab.com/sat-mtl/tools/livepose</a></div></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tochi-2022-nittala" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/tochi-2022-nittala/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>tochi-2022-nittala</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TOCHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="tochi-2022-nittala cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/tochi-2022-nittala.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/tochi-2022-nittala.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tochi-2022-nittala" target="_blank">SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures</a></h1><p class="meta"><span>Adwait Sharma<!-- --> <span class="role"></span></span>, <span>Christina Salchow-HÃ¶mmen<!-- --> <span class="role"></span></span>, <span>Vimal Suresh Mollyn<!-- --> <span class="role"></span></span>, <a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/pr-preview/pr-134/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a><span class="role"></span>, <span>Michael A. Hedderich<!-- --> <span class="role"></span></span>, <span>Marion Koelle<!-- --> <span class="role"></span></span>, <span>Thomas Seel<!-- --> <span class="role"></span></span>, <span>JÃ¼rgen Steimle<!-- --> <span class="role"></span></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/tochi-2022-nittala.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>tochi-2022-nittala.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Gestural interaction with freehands and while grasping an everyday object enables always-available input. To sense such gestures, minimal instrumentation of the userâ€™s hand is desirable. However, the choice of an effective but minimal IMU layout remains challenging, due to the complexity of the multi-factorial space that comprises diverse finger gestures, objects and grasps. We present SparseIMU, a rapid method for selecting minimal inertial sensor-based layouts for effective gesture recognition. Furthermore, we contribute a computational tool to guide designers with optimal sensor placement. Our approach builds on an extensive microgestures dataset that we collected with a dense network of 17 inertial measurement units (IMUs). We performed a series of analyses, including an evaluation of the entire combinatorial space for freehand and grasping microgestures (393K layouts), and quantified the performance across different layout choices, revealing new gesture detection opportunities with IMUs. Finally, we demonstrate the versatility of our method with four scenarios.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Gesture Recognition</span><span class="ui brown basic label">Hand Gestures</span><span class="ui brown basic label">Sensor Placement</span><span class="ui brown basic label">IMU</span><span class="ui brown basic label">Objects</span><span class="ui brown basic label">Design Tool</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Adwait Sharma<!-- -->, <!-- -->Christina Salchow-HÃ¶mmen<!-- -->, <!-- -->Vimal Suresh Mollyn<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Michael A. Hedderich<!-- -->, <!-- -->Marion Koelle<!-- -->, <!-- -->Thomas Seel<!-- -->, <!-- -->JÃ¼rgen Steimle<!-- -->.Â <b>SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures</b>.Â <i>(<!-- -->TOCHI 2022<!-- -->)</i>Â <!-- -->Page: 1-<!-- -->40<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.1145/3569894" target="_blank">https://doi.org/10.1145/3569894</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2022-kaimoto" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/uist-2022-kaimoto/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2022-kaimoto</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2022-kaimoto cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/uist-2022-kaimoto.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/uist-2022-kaimoto.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2022-kaimoto" target="_blank">Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI</a></h1><p class="meta"><a href="/people/hiroki-kaimoto"><img alt="hiroki-kaimoto photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/hiroki-kaimoto.jpg 1x" src="/pr-preview/pr-134/static/images/people/hiroki-kaimoto.jpg"/><strong>Hiroki Kaimoto</strong></a><span class="role"></span>, <a href="/people/kyzyl-monteiro"><img alt="kyzyl-monteiro photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/kyzyl-monteiro.jpg 1x" src="/pr-preview/pr-134/static/images/people/kyzyl-monteiro.jpg"/><strong>Kyzyl Monteiro</strong></a><span class="role"></span>, <a href="/people/mehrad-faridan"><img alt="mehrad-faridan photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/mehrad-faridan.jpg 1x" src="/pr-preview/pr-134/static/images/people/mehrad-faridan.jpg"/><strong>Mehrad Faridan</strong></a><span class="role"></span>, <span>Jiatong Li<!-- --> <span class="role"></span></span>, <a href="/people/samin-farajian"><img alt="samin-farajian photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/samin-farajian.jpg 1x" src="/pr-preview/pr-134/static/images/people/samin-farajian.jpg"/><strong>Samin Farajian</strong></a><span class="role"></span>, <span>Yasuaki Kakehi<!-- --> <span class="role"></span></span>, <span>Ken Nakagaki<!-- --> <span class="role"></span></span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2022-kaimoto.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2022-kaimoto.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/xy-IeVgoEpY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/xy-IeVgoEpY?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/xy-IeVgoEpY/maxresdefault.jpg src=https://img.youtube.com/vi/xy-IeVgoEpY/maxresdefault.jpg&gt;&lt;span&gt;â–¶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces Sketched Reality, an approach that com- bines AR sketching and actuated tangible user interfaces (TUI) for bi-directional sketching interaction. Bi-directional sketching enables virtual sketches and physical objects to affect each other through physical actuation and digital computation. In the existing AR sketching, the relationship between virtual and physical worlds is only one-directional --- while physical interaction can affect virtual sketches, virtual sketches have no return effect on the physical objects or environment. In contrast, bi-directional sketching interaction allows the seamless coupling between sketches and actuated TUIs. In this paper, we employ tabletop-size small robots (Sony Toio) and an iPad-based AR sketching tool to demonstrate the concept. In our system, virtual sketches drawn and simulated on an iPad (e.g., lines, walls, pendulums, and springs) can move, actuate, collide, and constrain physical Toio robots, as if virtual sketches and the physical objects exist in the same space through seamless coupling between AR and robot motion. This paper contributes a set of novel interactions and a design space of bi-directional AR sketching. We demonstrate a series of potential applications, such as tangible physics education, explorable mechanism, tangible gaming for children, and in-situ robot programming via sketching.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Actuated Tangible Interfaces</span><span class="ui brown basic label">Swarm User Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Hiroki Kaimoto<!-- -->, <!-- -->Kyzyl Monteiro<!-- -->, <!-- -->Mehrad Faridan<!-- -->, <!-- -->Jiatong Li<!-- -->, <!-- -->Samin Farajian<!-- -->, <!-- -->Yasuaki Kakehi<!-- -->, <!-- -->Ken Nakagaki<!-- -->, <!-- -->Ryo Suzuki<!-- -->.Â <b>Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI</b>.Â <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- --> <!-- -->(<!-- -->UIST 2022<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->Page: 1-<!-- -->12<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.1145/3526113.3545626" target="_blank">https://doi.org/10.1145/3526113.3545626</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2022-liao" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/uist-2022-liao/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2022-liao</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2022-liao cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/uist-2022-liao.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/uist-2022-liao.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2022-liao" target="_blank">RealityTalk: Real-time Speech-driven Augmented Presentation for AR Live Storytelling</a></h1><p class="meta"><a href="/people/jian-liao"><img alt="jian-liao photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/jian-liao.jpg 1x" src="/pr-preview/pr-134/static/images/people/jian-liao.jpg"/><strong>Jian Liao</strong></a><span class="role"></span>, <a href="/people/adnan-karim"><img alt="adnan-karim photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/adnan-karim.jpg 1x" src="/pr-preview/pr-134/static/images/people/adnan-karim.jpg"/><strong>Adnan Karim</strong></a><span class="role"></span>, <a href="/people/shivesh-jadon"><img alt="shivesh-jadon photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/shivesh-jadon.jpg 1x" src="/pr-preview/pr-134/static/images/people/shivesh-jadon.jpg"/><strong>Shivesh Jadon</strong></a><span class="role"></span>, <span>Rubaiat Habib Kazi<!-- --> <span class="role"></span></span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2022-liao.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2022-liao.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/vfIMeICV-7c" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/vfIMeICV-7c?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/vfIMeICV-7c/maxresdefault.jpg src=https://img.youtube.com/vi/vfIMeICV-7c/maxresdefault.jpg&gt;&lt;span&gt;â–¶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present RealityTalk, a system that augments real-time live presentations with speech-driven interactive virtual elements. Augmented presentations leverage embedded visuals and animation for engaging and expressive storytelling. However, existing tools for live presentations often lack interactivity and improvisation, while creating such effects in video editing tools require significant time and expertise. RealityTalk enables users to create live augmented presentations with real-time speech-driven interactions. The user can interactively prompt, move, and manipulate graphical elements through real-time speech and supporting modalities. Based on our analysis of 177 existing video-edited augmented presentations, we propose a novel set of interaction techniques and then incorporated them into RealityTalk. We evaluate our tool from a presenterâ€™s perspective to demonstrate the effectiveness of our system.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Augmented Presentation</span><span class="ui brown basic label">Natural Language Processing</span><span class="ui brown basic label">Gestural And Speech Input</span><span class="ui brown basic label">Video</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jian Liao<!-- -->, <!-- -->Adnan Karim<!-- -->, <!-- -->Shivesh Jadon<!-- -->, <!-- -->Rubaiat Habib Kazi<!-- -->, <!-- -->Ryo Suzuki<!-- -->.Â <b>RealityTalk: Real-time Speech-driven Augmented Presentation for AR Live Storytelling</b>.Â <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- --> <!-- -->(<!-- -->UIST 2022<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->Page: 1-<!-- -->12<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.1145/3526113.3545702" target="_blank">https://doi.org/10.1145/3526113.3545702</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2022-nisser" class="ui large modal"><div class="header"><a target="_blank" href="/pr-preview/pr-134/publications/uist-2022-nisser/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>uist-2022-nisser</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-134/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">UIST 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="uist-2022-nisser cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/publications/cover/uist-2022-nisser.jpg 1x" src="/pr-preview/pr-134/static/images/publications/cover/uist-2022-nisser.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2022-nisser" target="_blank">Mixels: Fabricating Interfaces using Programmable Magnetic Pixels</a></h1><p class="meta"><span>Martin Nisser<!-- --> <span class="role"></span></span>, <span>Yashaswini Makaram<!-- --> <span class="role"></span></span>, <span>Lucian Covarrubias<!-- --> <span class="role"></span></span>, <span>Amadou Bah<!-- --> <span class="role"></span></span>, <span>Faraz Faruqi<!-- --> <span class="role"></span></span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-134/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"></span>, <span>Stefanie Mueller<!-- --> <span class="role"></span></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/uist-2022-nisser.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>uist-2022-nisser.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/6SvFCQkVFtw" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/6SvFCQkVFtw?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/6SvFCQkVFtw/maxresdefault.jpg src=https://img.youtube.com/vi/6SvFCQkVFtw/maxresdefault.jpg&gt;&lt;span&gt;â–¶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>In this paper, we present Mixels, programmable magnetic pixels that can be rapidly fabricated using an electromagnetic printhead mounted on an off-the-shelve 3-axis CNC machine. The ability to program magnetic material pixel-wise with varying magnetic force enables Mixels to create new tangible, tactile, and haptic interfaces. To facilitate the creation of interactive objects with Mixels, we provide a user interface that lets users specify the high-level magnetic behavior and that then computes the underlying magnetic pixel assignments and fabrication instructions to program the magnetic surface. Our custom hardware add-on based on an electromagnetic printhead and hall effect sensor clips onto a standard 3-axis CNC machine and can both write and read magnetic pixel values from magnetic material. Our evaluation shows that our system can reliably program and read magnetic pixels of various strengths, that we can predict the behavior of two interacting magnetic surfaces before programming them, that our electromagnet is strong enough to create pixels that utilize the maximum magnetic strength of the material being programmed, and that this material remains magnetized when removed from the magnetic plotter.</p><div class="ui large basic labels">Keywords: Â <span class="ui brown basic label">Programmable Materials</span><span class="ui brown basic label">Magnetic Interfaces</span><span class="ui brown basic label">Fabrication</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Martin Nisser<!-- -->, <!-- -->Yashaswini Makaram<!-- -->, <!-- -->Lucian Covarrubias<!-- -->, <!-- -->Amadou Bah<!-- -->, <!-- -->Faraz Faruqi<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Stefanie Mueller<!-- -->.Â <b>Mixels: Fabricating Interfaces using Programmable Magnetic Pixels</b>.Â <i>In <!-- -->Proceedings of the Annual ACM Symposium on User Interface Software and Technology<!-- --> <!-- -->(<!-- -->UIST 2022<!-- -->)</i>ACM, New York, NY, USA<!-- -->Â <!-- -->Page: 1-<!-- -->12<!-- -->.Â <!-- -->DOI: <a href="https://doi.org/10.1145/3526113.3545698" target="_blank">https://doi.org/10.1145/3526113.3545698</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div></div><div class="ui vertical segment stackable" style="text-align:center"><a class="ui button" href="/pr-preview/pr-134/publications/">+ 30 more publications</a></div></div></div></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><div class="content"><a href="https://ucalgary.ca"><img alt="University of Calgary logo" loading="lazy" width="200" height="0" decoding="async" data-nimg="1" style="color:transparent;max-width:200px;margin:0px auto;height:auto" srcSet="/pr-preview/pr-134/static/images/logo-4.png 1x, /pr-preview/pr-134/static/images/logo-4.png 2x" src="/pr-preview/pr-134/static/images/logo-4.png"/></a><div class="sub header"><a class="item" href="https://cpsc.ucalgary.ca">Department of Computer Science</a></div></div></div></footer></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"peopleStaticProps":{"people":[{"name":"Ehud Sharlin","type":"faculty","title":"Professor","labs":["utouch"],"keywords":["HRI","Robots","Drones"],"order":1,"url":"http://contacts.ucalgary.ca/info/cpsc/profiles/102-3264","scholar":"https://scholar.google.ca/citations?hl=en\u0026user=eAFxlZIAAAAJ","dir":"content/output/people","base":"ehud-sharlin.json","ext":".json","sourceBase":"ehud-sharlin.yaml","sourceExt":".yaml","id":"ehud-sharlin","photo":"/static/images/people/ehud-sharlin.jpg"},{"name":"Lora Oehlberg","type":"faculty","title":"Associate Professor","labs":["curio"],"keywords":["Tangible","Design Tools"],"order":2,"url":"https://pages.cpsc.ucalgary.ca/~lora.oehlberg/","scholar":"https://scholar.google.ca/citations?hl=en\u0026user=8GzaBdwAAAAJ","dir":"content/output/people","base":"lora-oehlberg.json","ext":".json","sourceBase":"lora-oehlberg.yaml","sourceExt":".yaml","id":"lora-oehlberg","photo":"/static/images/people/lora-oehlberg.jpg"},{"name":"Wesley Willett","alias":"Wesley J. Willett","type":"faculty","title":"Associate Professor","labs":["data-experience"],"keywords":["Data Visualization","Data Phyz","AR"],"order":3,"url":"https://dataexperience.cpsc.ucalgary.ca/","scholar":"https://scholar.google.ca/citations?user=Q17-rckAAAAJ","dir":"content/output/people","base":"wesley-willett.json","ext":".json","sourceBase":"wesley-willett.yaml","sourceExt":".yaml","id":"wesley-willett","photo":"/static/images/people/wesley-willett.jpg"},{"name":"Aditya Shekhar Nittala","type":"faculty","title":"Assistant Professor","labs":["diff"],"keywords":["Wearable Computing","Fabrication","Interaction Techniques"],"order":4,"url":"https://sites.google.com/site/adityanittala/","scholar":"https://scholar.google.com/citations?user=pDSbjBsAAAAJ","email":"anittala@ucalgary.ca","linkedin":"https://www.linkedin.com/in/adityashekharn","dir":"content/output/people","base":"aditya-shekhar-nittala.json","ext":".json","sourceBase":"aditya-shekhar-nittala.yaml","sourceExt":".yaml","id":"aditya-shekhar-nittala","photo":"/static/images/people/aditya-shekhar-nittala.jpg"},{"name":"Christian Frisson","type":"faculty","title":"Assistant Professor","labs":["shivers"],"keywords":["Haptics","Multisensory \u0026 Multimedia","Immersive Arts"],"order":5,"url":"https://frisson.re","cv":"https://frisson.re","scholar":"https://scholar.google.com/citations?user=sZVn1V4AAAAJ","twitter":"https://twitter.com/TuyleriDikenli","facebook":"https://www.facebook.com/christian.frisson","linkedin":"https://www.linkedin.com/in/christianfrisson","github":"https://github.com/ChristianFrisson","gitlab":"https://gitlab.com/ChristianFrisson","email":"christian.frisson@ucalgary.ca","dir":"content/output/people","base":"christian-frisson.json","ext":".json","sourceBase":"christian-frisson.yaml","sourceExt":".yaml","id":"christian-frisson","photo":"/static/images/people/christian-frisson.jpg"},{"name":"Fateme Rajabiyazdi","type":"faculty","title":"Assistant Professor","labs":["health-vis"],"keywords":["Data Visualization","Health"],"order":6,"url":"https://fatemerajabi.github.io/HealthVisFutures/","scholar":"https://scholar.google.com/citations?user=mvoc_5AAAAAJ\u0026hl=en","email":"fateme.rajabiyazdi@ucalgary.ca","github":"https://github.com/FatemeRajabi","linkedin":"https://www.linkedin.com/in/rajabiyazdi","dir":"content/output/people","base":"fateme-rajabiyazdi.json","ext":".json","sourceBase":"fateme-rajabiyazdi.yaml","sourceExt":".yaml","id":"fateme-rajabiyazdi","photo":"/static/images/people/fateme-rajabiyazdi.jpg"},{"name":"Matthew Lakier","type":"faculty","title":"Assistant Professor","labs":["prpl"],"keywords":["Playful Interfaces","Games \u0026 Play","VR/AR"],"order":7,"url":"https://matthewlakier.com","scholar":"https://scholar.google.ca/citations?hl=en\u0026user=RYrb8kwAAAAJ","email":"matthew.lakier@ucalgary.ca","github":"https://github.com/spamalot","dir":"content/output/people","base":"matthew-lakier.json","ext":".json","sourceBase":"matthew-lakier.yaml","sourceExt":".yaml","id":"matthew-lakier","photo":"/static/images/people/matthew-lakier.jpg"},{"name":"Saul Greenberg","type":"faculty","title":"Emeritus Professor","keywords":["UbiComp","CSCW"],"order":10,"url":"http://saul.cpsc.ucalgary.ca/","scholar":"https://scholar.google.com/citations?user=TthhUuoAAAAJ","dir":"content/output/people","base":"saul-greenberg.json","ext":".json","sourceBase":"saul-greenberg.yaml","sourceExt":".yaml","id":"saul-greenberg","photo":"/static/images/people/saul-greenberg.jpg"},{"name":"Ryo Suzuki","type":"faculty","title":"Adjunct Assistant Professor (CU Boulder)","keywords":["Tangible","AR x AI","Robots"],"order":11,"url":"https://ryosuzuki.org","scholar":"https://scholar.google.com/citations?user=klWjaQIAAAAJ","twitter":"https://twitter.com/ryosuzk","facebook":"https://www.facebook.com/ryosuzk","email":"ryo.suzuki@ucalgary.ca","github":"https://github.com/ryosuzuki","linkedin":"https://www.linkedin.com/in/ryosuzuki/","dir":"content/output/people","base":"ryo-suzuki.json","ext":".json","sourceBase":"ryo-suzuki.yaml","sourceExt":".yaml","id":"ryo-suzuki","photo":"/static/images/people/ryo-suzuki.jpg"},{"name":"Anthony Tang","type":"faculty","title":"Adjunct Associate Professor (Singapore Management University)","keywords":["Mixed Reality","CSCW"],"order":12,"url":"https://hcitang.github.io/","scholar":"https://scholar.google.com/citations?user=RG1EQowAAAAJ","twitter":"https://twitter.com/proclubboy","github":"http://github.com/hcitang","dir":"content/output/people","base":"anthony-tang.json","ext":".json","sourceBase":"anthony-tang.yaml","sourceExt":".yaml","id":"anthony-tang","photo":"/static/images/people/anthony-tang.jpg"},{"name":"Sheelagh Carpendale","type":"faculty","title":"Adjunct Professor (Simon Fraser University)","keywords":["Data Viz","Data Phyz"],"order":13,"url":"https://www.cs.sfu.ca/~sheelagh/","scholar":"https://scholar.google.com/citations?user=43LLX2kAAAAJ","dir":"content/output/people","base":"sheelagh-carpendale.json","ext":".json","sourceBase":"sheelagh-carpendale.yaml","sourceExt":".yaml","id":"sheelagh-carpendale","photo":"/static/images/people/sheelagh-carpendale.jpg"},{"name":"Abhinav Pillai","type":"alumni","past":"undergrad","email":"abhinav.arunpillai@ucalgary.ca","linkedin":"https://www.linkedin.com/in/abhinav-pillai/","dir":"content/output/people","base":"abhinav-pillai.json","ext":".json","sourceBase":"abhinav-pillai.yaml","sourceExt":".yaml","id":"abhinav-pillai","photo":"/static/images/people/abhinav-pillai.jpg"},{"name":"Aditya Gunturu","type":"master","url":"https://adigunturu.com/","email":"aditya.gunturu@ucalgary.ca","linkedin":"https://www.linkedin.com/in/adigunturu/","github":"https://github.com/adigunturu","labs":["programmable-reality"],"dir":"content/output/people","base":"aditya-gunturu.json","ext":".json","sourceBase":"aditya-gunturu.yaml","sourceExt":".yaml","id":"aditya-gunturu","photo":"/static/images/people/aditya-gunturu.jpg","title":"MSc"},{"name":"Adnan Karim","type":"alumni","past":"master","now":"Attabotics","url":"https://sites.google.com/view/adnankarim/","email":"adnan.karim@ucalgary.ca","linkedin":"https://www.linkedin.com/in/adnan-karim-757b98101/","dir":"content/output/people","base":"adnan-karim.json","ext":".json","sourceBase":"adnan-karim.yaml","sourceExt":".yaml","id":"adnan-karim","photo":"/static/images/people/adnan-karim.jpg"},{"name":"Ahmed Elshabasi","type":"alumni","past":"master","labs":["utouch","data-experience"],"dir":"content/output/people","base":"ahmed-elshabasi.json","ext":".json","sourceBase":"ahmed-elshabasi.yaml","sourceExt":".yaml","id":"ahmed-elshabasi","photo":"/static/images/people/ahmed-elshabasi.jpg"},{"name":"Aidan Gaede-Janke","type":"undergrad","labs":["shivers"],"linkedin":"https://www.linkedin.com/in/aidan-gaede-janke-9202991b2","github":"https://github.com/AidanGJ","email":"aidan.gaedejanke@ucalgary.ca","dir":"content/output/people","base":"aidan-gaede-janke.json","ext":".json","sourceBase":"aidan-gaede-janke.yaml","sourceExt":".yaml","id":"aidan-gaede-janke","photo":"/static/images/people/aidan-gaede-janke.jpg","title":"Ugrad"},{"name":"Akashdeep Singh","type":"alumni","past":"undergrad","now":"Hexagon AB","email":"akashdeep.singh4@ucalgary.ca","linkedin":"https://www.linkedin.com/in/akash02ita/","labs":["diff"],"dir":"content/output/people","base":"akashdeep-singh.json","ext":".json","sourceBase":"akashdeep-singh.yaml","sourceExt":".yaml","id":"akashdeep-singh","photo":"/static/images/people/akashdeep-singh.jpg"},{"name":"Anand Kumar","type":"master","url":"https://anandkumarrajpal.github.io/","email":"anand.kumar1@ucalgary.ca","linkedin":"https://www.linkedin.com/in/anand-kumar-rajpal/","github":"https://github.com/AnandKumarRajpal/","scholar":"https://scholar.google.com/citations?user=1_qfoBUAAAAJ\u0026hl=en","labs":["diff"],"dir":"content/output/people","base":"anand-kumar.json","ext":".json","sourceBase":"anand-kumar.yaml","sourceExt":".yaml","id":"anand-kumar","photo":"/static/images/people/anand-kumar.jpg","title":"MSc"},{"name":"Annie Tat","type":"alumni","past":"master","linkedin":"https://www.linkedin.com/in/annie-tat/","now":"Canada Energy Regulator","dir":"content/output/people","base":"annie-tat.json","ext":".json","sourceBase":"annie-tat.yaml","sourceExt":".yaml","id":"annie-tat","photo":"/static/images/people/annie-tat.jpg"},{"name":"April Zhang","type":"alumni","past":"master","url":"https://aprilzhang.design/","linkedin":"https://www.linkedin.com/in/aprilxczhang/","dir":"content/output/people","base":"april-zhang.json","ext":".json","sourceBase":"april-zhang.yaml","sourceExt":".yaml","id":"april-zhang","photo":"/static/images/people/april-zhang.jpg"},{"name":"Ashratuz Zavin Asha","type":"phd","url":"https://sites.google.com/view/ashratuzzavinasha","scholar":"https://scholar.google.com/citations?user=E7gtMMoAAAAJ","labs":["utouch"],"dir":"content/output/people","base":"ashratuz-zavin-asha.json","ext":".json","sourceBase":"ashratuz-zavin-asha.yaml","sourceExt":".yaml","id":"ashratuz-zavin-asha","photo":"/static/images/people/ashratuz-zavin-asha.jpg","title":"PhD"},{"name":"Ben Pearman","type":"master","url":"https://benpearman.xyz","linkedin":"https://www.linkedin.com/in/benpearman","scholar":"https://scholar.google.com/citations?user=QJ5G87UAAAAJ\u0026hl=en","labs":["data-experience"],"dir":"content/output/people","base":"ben-pearman.json","ext":".json","sourceBase":"ben-pearman.yaml","sourceExt":".yaml","id":"ben-pearman","photo":"/static/images/people/ben-pearman.jpg","title":"MSc"},{"name":"Bheesha Kumari","type":"alumni","past":"undergrad","email":"bheesha.kumari@ucalgary.ca","linkedin":"https://www.linkedin.com/in/bheesha-kumari-/","dir":"content/output/people","base":"bheesha-kumari.json","ext":".json","sourceBase":"bheesha-kumari.yaml","sourceExt":".yaml","id":"bheesha-kumari","photo":"/static/images/people/bheesha-kumari.jpg"},{"name":"Bon Adriel Aseniero","type":"alumni","past":"phd","now":"Autodesk Research","url":"http://bonadriel.com/","scholar":"https://scholar.google.com/citations?user=V4nRMoMAAAAJ","twitter":"https://twitter.com/HexenKoenig","facebook":"https://www.facebook.com/bonadriel","linkedin":"https://www.linkedin.com/in/bon-adriel-aseniero-47140560/","labs":["utouch"],"dir":"content/output/people","base":"bon-adriel-aseniero.json","ext":".json","sourceBase":"bon-adriel-aseniero.yaml","sourceExt":".yaml","id":"bon-adriel-aseniero","photo":"/static/images/people/bon-adriel-aseniero.jpg"},{"name":"Bonnie Wu","type":"master","linkedin":"https://www.linkedin.com/in/bonniewfwu","labs":["data-experience"],"dir":"content/output/people","base":"bonnie-wu.json","ext":".json","sourceBase":"bonnie-wu.yaml","sourceExt":".yaml","id":"bonnie-wu","photo":"/static/images/people/bonnie-wu.jpg","title":"MSc"},{"name":"Brennan Jones","type":"alumni","past":"phd","now":"Xi'an Jiaotong-Liverpool University","url":"https://brennanjones.com/","scholar":"https://scholar.google.ca/citations?user=yzxiadIAAAAJ","linkedin":"https://www.linkedin.com/in/bdgjones/","dir":"content/output/people","base":"brennan-jones.json","ext":".json","sourceBase":"brennan-jones.yaml","sourceExt":".yaml","id":"brennan-jones","photo":"/static/images/people/brennan-jones.jpg"},{"name":"Carl Gutwin","type":"alumni","past":"phd","now":"University of Saskatchewan","url":"https://www.cs.usask.ca/~gutwin/","scholar":"https://scholar.google.ca/citations?user=ZWsIEYcAAAAJ","email":"gutwin@cs.usask.ca","dir":"content/output/people","base":"carl-gutwin.json","ext":".json","sourceBase":"carl-gutwin.yaml","sourceExt":".yaml","id":"carl-gutwin","photo":"/static/images/people/carl-gutwin.jpg"},{"name":"Carman Neustaedter","type":"alumni","past":"phd","now":"Simon Fraser University","url":"https://carmster.com/","twitter":"https://mobile.twitter.com/dr_carmster","scholar":"https://scholar.google.ca/citations?user=krQJJwIAAAAJ","linkedin":"https://www.linkedin.com/in/dr-carman-neustaedter-3666591","dir":"content/output/people","base":"carman-neustaedter.json","ext":".json","sourceBase":"carman-neustaedter.yaml","sourceExt":".yaml","id":"carman-neustaedter","photo":"/static/images/people/carman-neustaedter.jpg"},{"name":"Carmen Hull","type":"alumni","past":"phd","now":"Northeastern University","url":"https://www.carmenhull.com/","linkedin":"https://www.linkedin.com/in/carmen-hull-2083ab20/","scholar":"https://scholar.google.com/citations?user=eJ-d2wsAAAAJ\u0026hl=en","labs":["data-experience"],"dir":"content/output/people","base":"carmen-hull.json","ext":".json","sourceBase":"carmen-hull.yaml","sourceExt":".yaml","id":"carmen-hull","photo":"/static/images/people/carmen-hull.jpg"},{"name":"Carson Witts","type":"master","email":"carson.witts@ucalgary.ca","linkedin":"https://www.linkedin.com/in/carson-witts","labs":["data-experience"],"dir":"content/output/people","base":"carson-witts.json","ext":".json","sourceBase":"carson-witts.yaml","sourceExt":".yaml","id":"carson-witts","photo":"/static/images/people/carson-witts.jpg","title":"MSc"},{"name":"Charlotte Tang","type":"alumni","past":"phd","now":"University of Michigan","url":"https://charlottetang.ca/","scholar":"https://scholar.google.com/citations?user=RYkK_owAAAAJ","linkedin":"https://www.linkedin.com/in/charlottetang/","dir":"content/output/people","base":"charlotte-tang.json","ext":".json","sourceBase":"charlotte-tang.yaml","sourceExt":".yaml","id":"charlotte-tang","photo":"/static/images/people/charlotte-tang.jpg"},{"name":"Christian Salvador","type":"master","url":"https://www.csalt.dev/","email":"christian.salvador@ucalgary.ca","linkedin":"https://www.linkedin.com/in/cl-salvador/","labs":["curio"],"dir":"content/output/people","base":"christian-salvador.json","ext":".json","sourceBase":"christian-salvador.yaml","sourceExt":".yaml","id":"christian-salvador","photo":"/static/images/people/christian-salvador.jpg","title":"MSc"},{"name":"Christopher Collins","type":"alumni","past":"phd","now":"Ontario Tech University","url":"https://vialab.ca/","scholar":"https://scholar.google.ca/citations?user=yHRw4eMAAAAJ\u0026hl=en","linkedin":"https://www.linkedin.com/in/christophercollins/","dir":"content/output/people","base":"christopher-collins.json","ext":".json","sourceBase":"christopher-collins.yaml","sourceExt":".yaml","id":"christopher-collins","photo":"/static/images/people/christopher-collins.jpg"},{"name":"Christopher Rodriguez","type":"alumni","past":"undergrad","email":"christopher.rodrigue@ucalgary.ca","linkedin":"https://www.linkedin.com/in/christopher-rodriguez-74259217a/","dir":"content/output/people","base":"christopher-rodriguez.json","ext":".json","sourceBase":"christopher-rodriguez.yaml","sourceExt":".yaml","id":"christopher-rodriguez","photo":"/static/images/people/christopher-rodriguez.jpg"},{"name":"Christopher Smith","type":"phd","program":"cs","linkedin":"https://www.linkedin.com/in/christopher-smith-uofc/","labs":["utouch"],"dir":"content/output/people","base":"christopher-smith.json","ext":".json","sourceBase":"christopher-smith.yaml","sourceExt":".yaml","id":"christopher-smith","photo":"/static/images/people/christopher-smith.jpg","title":"CS PhD"},{"name":"Clara Xi","alias":"Clara Y. Xi","type":"phd","labs":["curio"],"url":"https://cspages.ucalgary.ca/~clara.xi/","email":"clara.xi@ucalgary.ca","scholar":"https://scholar.google.ca/citations?user=4iYXktYAAAAJ","dir":"content/output/people","base":"clara-xi.json","ext":".json","sourceBase":"clara-xi.yaml","sourceExt":".yaml","id":"clara-xi","photo":"/static/images/people/clara-xi.jpg","title":"PhD"},{"name":"Colin Au Yeung","type":"master","url":"https://colinauyeung.github.io/","email":"colinauyeung@ucalgary.ca","twitter":"https://x.com/yandereyuuko","linkedin":"https://www.linkedin.com/in/colin-au-yeung-348293312","labs":["curio"],"dir":"content/output/people","base":"colin-auyeung.json","ext":".json","sourceBase":"colin-auyeung.yaml","sourceExt":".yaml","id":"colin-auyeung","photo":"/static/images/people/colin-auyeung.jpg","title":"MSc"},{"name":"Dane Bertram","type":"alumni","past":"master","now":"YNAB","url":"https://danebertram.com/","linkedin":"https://www.linkedin.com/in/danebertram/","github":"https://github.com/dbertram","dir":"content/output/people","base":"dane-bertram.json","ext":".json","sourceBase":"dane-bertram.yaml","sourceExt":".yaml","id":"dane-bertram","photo":"/static/images/people/dane-bertram.jpg"},{"name":"Danissa Sandykbayeva","past":"master","type":"alumni","email":"danissa.sandykbay1@ucalgary.ca","scholar":"https://scholar.google.com/citations?user=tWhlB7YAAAAJ","linkedin":"https://www.linkedin.com/in/danissa-sandykbayeva/","labs":["diff"],"dir":"content/output/people","base":"danissa-sandykbayeva.json","ext":".json","sourceBase":"danissa-sandykbayeva.yaml","sourceExt":".yaml","id":"danissa-sandykbayeva","photo":"/static/images/people/danissa-sandykbayeva.jpg"},{"name":"D'Arcy Norman","type":"alumni","past":"phd","now":"University of Calgary","labs":["utouch"],"url":"https://darcynorman.net/","twitter":"https://twitter.com/realdlnorman","linkedin":"https://www.linkedin.com/in/d-arcy-norman-phd-48648272/","scholar":"https://scholar.google.ca/citations?user=s6C2YEIAAAAJ\u0026hl=en","dir":"content/output/people","base":"darcy-norman.json","ext":".json","sourceBase":"darcy-norman.yaml","sourceExt":".yaml","id":"darcy-norman","photo":"/static/images/people/darcy-norman.jpg"},{"name":"David Ledo","type":"alumni","past":"phd","now":"Autodesk Research","url":"https://www.davidledo.com/","scholar":"https://scholar.google.com/citations?user=V_2BZDoAAAAJ","linkedin":"https://www.linkedin.com/in/david-ledo-75914249","labs":["curio"],"dir":"content/output/people","base":"david-ledo.json","ext":".json","sourceBase":"david-ledo.yaml","sourceExt":".yaml","id":"david-ledo","photo":"/static/images/people/david-ledo.jpg"},{"name":"Desmond Larsen-Rosner","type":"alumni","past":"master","github":"https://github.com/desmondlr","linkedin":"https://www.linkedin.com/in/desmondlr","labs":["utouch"],"dir":"content/output/people","base":"desmond-larsen-rosner.json","ext":".json","sourceBase":"desmond-larsen-rosner.yaml","sourceExt":".yaml","id":"desmond-larsen-rosner","photo":"/static/images/people/desmond-larsen-rosner.jpg"},{"name":"Dinmukhammed Mukashev","type":"alumni","past":"master","email":"dimash.mukashev@ucalgary.ca","scholar":"https://scholar.google.com/citations?user=rUEDTOsAAAAJ","dir":"content/output/people","base":"dinmukhammed-mukashev.json","ext":".json","sourceBase":"dinmukhammed-mukashev.yaml","sourceExt":".yaml","id":"dinmukhammed-mukashev","photo":"/static/images/people/dinmukhammed-mukashev.jpg"},{"name":"Donald Cox","type":"alumni","past":"master","now":"Ping Identity","linkedin":"https://www.linkedin.com/in/donald-cox-6455435/","dir":"content/output/people","base":"donald-cox.json","ext":".json","sourceBase":"donald-cox.yaml","sourceExt":".yaml","id":"donald-cox","photo":"/static/images/people/donald-cox.jpg"},{"name":"Doug Schaeffer","type":"alumni","past":"master","now":"R2 Solutions","linkedin":"https://www.linkedin.com/in/dougschaffer/","dir":"content/output/people","base":"doug-schaeffer.json","ext":".json","sourceBase":"doug-schaeffer.yaml","sourceExt":".yaml","id":"doug-schaeffer","photo":"/static/images/people/doug-schaeffer.jpg"},{"name":"Ebube Anachebe","type":"undergrad","labs":["shivers"],"linkedin":"https://www.linkedin.com/in/ebubea","github":"https://github.com/ebubea1","email":"chukwudiebube.anache@ucalgary.ca","dir":"content/output/people","base":"ebube-anachebe.json","ext":".json","sourceBase":"ebube-anachebe.yaml","sourceExt":".yaml","id":"ebube-anachebe","photo":"/static/images/people/ebube-anachebe.jpg","title":"Ugrad"},{"name":"Edward Tse","type":"alumni","past":"phd","now":"Ai Parenting","url":"https://edwardtse.com/","linkedin":"https://ca.linkedin.com/in/edward-tse","twitter":"https://twitter.com/DoctorET","dir":"content/output/people","base":"edward-tse.json","ext":".json","sourceBase":"edward-tse.yaml","sourceExt":".yaml","id":"edward-tse","photo":"/static/images/people/edward-tse.jpg"},{"name":"Faiz Marsad","type":"alumni","past":"undergrad","email":"faiz.marsad@ucalgary.ca","dir":"content/output/people","base":"faiz-marsad.json","ext":".json","sourceBase":"faiz-marsad.yaml","sourceExt":".yaml","id":"faiz-marsad","photo":"/static/images/people/no-profile.jpg"},{"name":"Georgina Freeman","type":"phd","url":"https://www.ginafreeman.com/about","cv":"https://static1.squarespace.com/static/5d6eff51c4807e0001982813/t/611401518943c1218b053432/1628701009534/Georgina+Freeman+CV+AUG+2021+NO+ID.pdf","scholar":"https://scholar.google.com/citations?user=kkZQiusAAAAJ\u0026hl=en","linkedin":"https://www.linkedin.com/in/georgina-gina-freeman-1072b081","labs":["curio"],"dir":"content/output/people","base":"georgina-freeman.json","ext":".json","sourceBase":"georgina-freeman.yaml","sourceExt":".yaml","id":"georgina-freeman","photo":"/static/images/people/georgina-freeman.jpg","title":"PhD"},{"name":"Godwin Saure","type":"undergrad","email":"godwin.saure@gmail.com","linkedin":"https://www.linkedin.com/in/godwin-saure/","labs":["data-experience"],"dir":"content/output/people","base":"godwin-saure.json","ext":".json","sourceBase":"godwin-saure.yaml","sourceExt":".yaml","id":"godwin-saure","photo":"/static/images/people/godwin-saure.jpg","title":"Ugrad"},{"name":"Grace Ferguson","type":"alumni","past":"undergrad","now":"Insignia Software","linkedin":"https://www.linkedin.com/in/grace-ferguson-219982193","dir":"content/output/people","base":"grace-ferguson.json","ext":".json","sourceBase":"grace-ferguson.yaml","sourceExt":".yaml","id":"grace-ferguson","photo":"/static/images/people/grace-ferguson.jpg"},{"name":"Gregor McEwan","type":"alumni","past":"master","now":"Modail Mara","url":"https://www.modailmara.ca/whoweare","scholar":"https://scholar.google.com/citations?user=6xdmrPgAAAAJ","linkedin":"https://www.linkedin.com/in/gregor-mcewan-5076992/","dir":"content/output/people","base":"gregor-mcewan.json","ext":".json","sourceBase":"gregor-mcewan.yaml","sourceExt":".yaml","id":"gregor-mcewan","photo":"/static/images/people/gregor-mcewan.jpg"},{"name":"Harrison Chen","type":"alumni","past":"undergrad","url":"https://harrisoneverettchen.com","email":"hechen@ucalgary.ca","github":"https://github.com/Hechen93","linkedin":"https://www.linkedin.com/in/harrison-chen-a90b5569/","dir":"content/output/people","base":"harrison-chen.json","ext":".json","sourceBase":"harrison-chen.yaml","sourceExt":".yaml","id":"harrison-chen","photo":"/static/images/people/harrison-chen.jpg"},{"name":"Helen Ai He","type":"alumni","past":"faculty","title":"Assistant Professor","keywords":null,"url":"https://helenaihe.com/research","scholar":"https://scholar.google.com/citations?user=FlMKf0gAAAAJ","twitter":"https://twitter.com/helenaihe","email":"helen.he1@ucalgary.ca","labs":["data-experience"],"dir":"content/output/people","base":"helen-ai-he.json","ext":".json","sourceBase":"helen-ai-he.yaml","sourceExt":".yaml","id":"helen-ai-he","photo":"/static/images/people/helen-ai-he.jpg"},{"name":"Hiroki Kaimoto","type":"alumni","past":"visiting","now":"University of Tokyo","url":"https://hirokikaimoto.com","scholar":"https://scholar.google.co.jp/citations?user=BPL36T0AAAAJ","twitter":"https://twitter.com/open_origin","email":"hkaimoto@xlab.iii.u-tokyo.ac.jp","dir":"content/output/people","base":"hiroki-kaimoto.json","ext":".json","sourceBase":"hiroki-kaimoto.yaml","sourceExt":".yaml","id":"hiroki-kaimoto","photo":"/static/images/people/hiroki-kaimoto.jpg"},{"name":"Huann Zhao","type":"master","url":"https://contacts.ucalgary.ca/info/cpsc/profiles/1-12783408","email":"huanjun.zhao@ucalgary.ca","linkedin":"https://www.linkedin.com/in/huanjun-zhao-00b281230/","labs":["diff"],"dir":"content/output/people","base":"huanjun-zhao.json","ext":".json","sourceBase":"huanjun-zhao.yaml","sourceExt":".yaml","id":"huanjun-zhao","photo":"/static/images/people/huanjun-zhao.jpg","title":"MSc"},{"name":"Iryna Luchak","type":"alumni","past":"master","linkedin":"https://www.linkedin.com/in/irynaluchak/","labs":["utouch"],"dir":"content/output/people","base":"iryna-luchak.json","ext":".json","sourceBase":"iryna-luchak.yaml","sourceExt":".yaml","id":"iryna-luchak","photo":"/static/images/people/iryna-luchak.jpg"},{"name":"Isaac Ng","type":"master","labs":["shivers"],"email":"isaac.ng@ucalgary.ca","linkedin":"https://www.linkedin.com/in/isaac-ng-273810254/","github":"https://github.com/nkh6601","dir":"content/output/people","base":"isaac-ng.json","ext":".json","sourceBase":"isaac-ng.yaml","sourceExt":".yaml","id":"isaac-ng","photo":"/static/images/people/isaac-ng.jpg","title":"MSc"},{"name":"Isabella Huang","type":"undergrad","labs":["shivers"],"linkedin":"https://www.linkedin.com/in/isabella-huang-667a02269/","github":"https://github.com/IsabellaH537","email":"isabella.huang@ucalgary.ca","dir":"content/output/people","base":"isabella-huang.json","ext":".json","sourceBase":"isabella-huang.yaml","sourceExt":".yaml","id":"isabella-huang","photo":"/static/images/people/isabella-huang.jpg","title":"Ugrad"},{"name":"Jagoda Walny","type":"alumni","past":"phd","now":"Canada Energy Regulator (CER)","url":"https://www.fwd50.com/speaker/2395/jagoda-walny-nix","scholar":"https://scholar.google.ca/citations?user=EK99E-8AAAAJ\u0026hl=en","linkedin":"https://www.linkedin.com/in/jagoda/?originalSubdomain=ca","labs":["data-experience"],"dir":"content/output/people","base":"jagoda-walny.json","ext":".json","sourceBase":"jagoda-walny.yaml","sourceExt":".yaml","id":"jagoda-walny","photo":"/static/images/people/jagoda-walny.jpg"},{"name":"James Tam","type":"alumni","past":"master","now":"University of Calgary","url":"http://pages.cpsc.ucalgary.ca/~tamj/index.html","email":"tamj@cpsc.ucalgary.ca","dir":"content/output/people","base":"james-tam.json","ext":".json","sourceBase":"james-tam.yaml","sourceExt":".yaml","id":"james-tam","photo":"/static/images/people/no-profile.jpg"},{"name":"Jane Shen","type":"master","linkedin":"https://www.linkedin.com/in/shen-jane/","email":"jane.shen1@ucalgary.ca","labs":["curio"],"dir":"content/output/people","base":"jane-shen.json","ext":".json","sourceBase":"jane-shen.yaml","sourceExt":".yaml","id":"jane-shen","photo":"/static/images/people/jane-shen.jpg","title":"MSc"},{"name":"Jarin Thundathil","type":"alumni","past":"undergrad","email":"jarin.thundathil@ucalgary.ca","linkedin":"https://www.linkedin.com/in/jarin-thundathil-170616150/","labs":["programmable-reality"],"dir":"content/output/people","base":"jarin-thundathil.json","ext":".json","sourceBase":"jarin-thundathil.yaml","sourceExt":".yaml","id":"jarin-thundathil","photo":"/static/images/people/jarin-thundathil.jpg"},{"name":"Jessi Stark","type":"alumni","past":"master","now":"University of Toronto","url":"https://jtstark.com/","scholar":"https://scholar.google.com/citations?user=aRkKN5UAAAAJ","twitter":"https://twitter.com/_jessistark","labs":["utouch"],"dir":"content/output/people","base":"jessi-stark.json","ext":".json","sourceBase":"jessi-stark.yaml","sourceExt":".yaml","id":"jessi-stark","photo":"/static/images/people/jessi-stark.jpg"},{"name":"Jian Liao","type":"alumni","past":"undergrad","email":"jian.liao1@ucalgary.ca","twitter":"https://twitter.com/imjliao","linkedin":"https://www.linkedin.com/in/jian-liao/","github":"https://github.com/jlia0","dir":"content/output/people","base":"jian-liao.json","ext":".json","sourceBase":"jian-liao.yaml","sourceExt":".yaml","id":"jian-liao","photo":"/static/images/people/jian-liao.jpg"},{"name":"Jiannan Li","type":"alumni","past":"master","now":"University of Toronto","url":"https://www.dgp.toronto.edu/~jiannanli/","scholar":"https://scholar.google.com/citations?user=wyW0rJQAAAAJ","labs":["utouch"],"dir":"content/output/people","base":"jiannan-li.json","ext":".json","sourceBase":"jiannan-li.yaml","sourceExt":".yaml","id":"jiannan-li","photo":"/static/images/people/jiannan-li.jpg"},{"name":"Joshua Lee","type":"alumni","past":"undergrad","email":"chang.lee@ucalgary.ca","linkedin":"https://www.linkedin.com/in/chang-lee/","labs":["diff"],"dir":"content/output/people","base":"joshua-lee.json","ext":".json","sourceBase":"joshua-lee.yaml","sourceExt":".yaml","id":"joshua-lee","photo":"/static/images/people/joshua-lee.jpg"},{"name":"Karly Ross","type":"master","labs":["data-experience"],"linkedin":"https://www.linkedin.com/in/karly-j-ross/","dir":"content/output/people","base":"karly-ross.json","ext":".json","sourceBase":"karly-ross.yaml","sourceExt":".yaml","id":"karly-ross","photo":"/static/images/people/karly-ross.jpg","title":"MSc"},{"name":"Karthik Mahadevan","type":"alumni","past":"master","now":"University of Toronto","url":"https://karthikm0.github.io/","scholar":"https://scholar.google.ca/citations?user=aK4siPkAAAAJ","labs":["utouch"],"dir":"content/output/people","base":"karthik-mahadevan.json","ext":".json","sourceBase":"karthik-mahadevan.yaml","sourceExt":".yaml","id":"karthik-mahadevan","photo":"/static/images/people/karthik-mahadevan.jpg"},{"name":"Katherine Currier","type":"alumni","past":"master","now":"Insulet Corporation","dir":"content/output/people","base":"katherine-currier.json","ext":".json","sourceBase":"katherine-currier.yaml","sourceExt":".yaml","id":"katherine-currier","photo":"/static/images/people/katherine-currier.jpg"},{"name":"Kathryn Blair","type":"phd","program":"cmd","url":"http://kathrynblair.com/","twitter":"https://twitter.com/kathblair","email":"kathryn.blair@ucalgary.ca","linkedin":"https://www.linkedin.com/in/kmblair/","labs":["curio"],"dir":"content/output/people","base":"kathryn-blair.json","ext":".json","sourceBase":"kathryn-blair.yaml","sourceExt":".yaml","id":"kathryn-blair","photo":"/static/images/people/kathryn-blair.jpg","title":"CMD PhD"},{"name":"Kathryn Elliot-Rounding","type":"alumni","past":"master","now":"YNAB","linkedin":"https://www.linkedin.com/in/kathrynkrounding","dir":"content/output/people","base":"kathryn-elliot-rounding.json","ext":".json","sourceBase":"kathryn-elliot-rounding.yaml","sourceExt":".yaml","id":"kathryn-elliot-rounding","photo":"/static/images/people/kathryn-elliot-rounding.jpg"},{"name":"Kaynen Mitchell","type":"alumni","past":"undergrad","email":"kaynen.mitchell1@ucalgary.ca","linkedin":"https://www.linkedin.com/in/kaynen-mitchell/","dir":"content/output/people","base":"kaynen-mitchell.json","ext":".json","sourceBase":"kaynen-mitchell.yaml","sourceExt":".yaml","id":"kaynen-mitchell","photo":"/static/images/people/kaynen-mitchell.jpg"},{"name":"Keiichi Ihara","type":"alumni","past":"visiting","url":"https://www.iplab.cs.tsukuba.ac.jp/~kihara/","email":"kihara@iplab.cs.tsukuba.ac.jp","dir":"content/output/people","base":"keiichi-ihara.json","ext":".json","sourceBase":"keiichi-ihara.yaml","sourceExt":".yaml","id":"keiichi-ihara","photo":"/static/images/people/keiichi-ihara.jpg"},{"name":"Kendra Wannamaker","type":"alumni","past":"master","now":"Autodesk Research","url":"http://kendrawannamaker.com","scholar":"https://scholar.google.com/citations?user=fC1WI8cAAAAJ\u0026hl=en","linkedin":"https://www.linkedin.com/in/kendra-wannamaker-88622aa4/","labs":["data-experience"],"dir":"content/output/people","base":"kendra-wannamaker.json","ext":".json","sourceBase":"kendra-wannamaker.yaml","sourceExt":".yaml","id":"kendra-wannamaker","photo":"/static/images/people/kendra-wannamaker.jpg"},{"name":"Kevin Van","type":"alumni","past":"undergrad","email":"kevin.van@ucalgary.ca","url":"https://kevin-van.github.io/Website/","github":"https://github.com/kevin-van","linkedin":"https://www.linkedin.com/in/kevin-van-a66130204/","labs":["utouch"],"dir":"content/output/people","base":"kevin-van.json","ext":".json","sourceBase":"kevin-van.yaml","sourceExt":".yaml","id":"kevin-van","photo":"/static/images/people/kevin-van.jpg"},{"name":"Kimberly Tee","type":"alumni","past":"master","now":"Shopify","scholar":"https://scholar.google.com/citations?user=srTy2voAAAAJ","linkedin":"https://www.linkedin.com/in/kimberly-tee-7305b159/","dir":"content/output/people","base":"kimberly-tee.json","ext":".json","sourceBase":"kimberly-tee.yaml","sourceExt":".yaml","id":"kimberly-tee","photo":"/static/images/people/kimberly-tee.jpg"},{"name":"Kurtis Danyluk","alias":"Kurtis Thorvald Danyluk","type":"alumni","past":"phd","scholar":"https://scholar.google.com/citations?user=vr-EF5IAAAAJ","labs":["data-experience"],"dir":"content/output/people","base":"kurtis-danyluk.json","ext":".json","sourceBase":"kurtis-danyluk.yaml","sourceExt":".yaml","id":"kurtis-danyluk","photo":"/static/images/people/kurtis-danyluk.jpg"},{"name":"Kyzyl Monteiro","type":"alumni","past":"visiting","now":"CMU","url":"https://kyzyl.me","scholar":"https://scholar.google.ca/citations?user=A9IqYNoAAAAJ","twitter":"https://twitter.com/kyzylmonteiro","email":"kyzylmonteiro@gmail.com","github":"https://github.com/kyzylmonteiro","linkedin":"https://www.linkedin.com/in/kyzylmonteiro/","dir":"content/output/people","base":"kyzyl-monteiro.json","ext":".json","sourceBase":"kyzyl-monteiro.yaml","sourceExt":".yaml","id":"kyzyl-monteiro","photo":"/static/images/people/kyzyl-monteiro.jpg"},{"name":"Linda Tauscher","type":"alumni","past":"master","now":"NetStart Consulting Ltd.","linkedin":"https://www.linkedin.com/in/ltauscher","url":"https://netstart.com","dir":"content/output/people","base":"linda-tauscher.json","ext":".json","sourceBase":"linda-tauscher.yaml","sourceExt":".yaml","id":"linda-tauscher","photo":"/static/images/people/linda-tauscher.jpg"},{"name":"Lychelle Pham","type":"undergrad","labs":["shivers","data-experience"],"linkedin":"https://www.linkedin.com/in/lychelle-pham-4b70332b9","github":"https://github.com/LychellePham","email":"lychelle.pham@ucalgary.ca","dir":"content/output/people","base":"lychelle-pham.json","ext":".json","sourceBase":"lychelle-pham.yaml","sourceExt":".yaml","id":"lychelle-pham","photo":"/static/images/people/lychelle-pham.jpg","title":"Ugrad"},{"name":"Mackenzie Bowal","type":"alumni","past":"undergrad","email":"mackenzie.bowal@ucalgary.ca","linkedin":"https://www.linkedin.com/in/mackenzie-bowal/","labs":["utouch"],"dir":"content/output/people","base":"mackenzie-bowal.json","ext":".json","sourceBase":"mackenzie-bowal.yaml","sourceExt":".yaml","id":"mackenzie-bowal","photo":"/static/images/people/mackenzie-bowal.jpg"},{"name":"Mackenzie Hisako Dalton","type":"alumni","past":"undergrad","url":"https://hisak00.weebly.com/","labs":["data-experience"],"dir":"content/output/people","base":"mackenzie-hisako-dalton.json","ext":".json","sourceBase":"mackenzie-hisako-dalton.yaml","sourceExt":".yaml","id":"mackenzie-hisako-dalton","photo":"/static/images/people/mackenzie-hisako-dalton.jpg"},{"name":"Maham Fatima","type":"alumni","past":"undergrad","now":"University of Calgary","cv":"https://linktr.ee/mahamf","github":"https://github.com/maham000","linkedin":"https://www.linkedin.com/in/maham-f/","email":"maham.fatima2@ucalgary.ca","url":"https://mahamf567.wixsite.com/maham-2","labs":["curio"],"dir":"content/output/people","base":"maham-fatima.json","ext":".json","sourceBase":"maham-fatima.yaml","sourceExt":".yaml","id":"maham-fatima","photo":"/static/images/people/maham-fatima.jpg"},{"name":"Manjot Khangura","type":"alumni","past":"undergrad","email":"manjot.khangura@ucalgary.ca","linkedin":"https://www.linkedin.com/in/manjot-khangura/","dir":"content/output/people","base":"manjot-khangura.json","ext":".json","sourceBase":"manjot-khangura.yaml","sourceExt":".yaml","id":"manjot-khangura","photo":"/static/images/people/manjot-khangura.jpg"},{"name":"Manuel Rodriguez","type":"alumni","past":"undergrad","email":"manuel.rodriguez@ucalgary.ca","linkedin":"https://www.linkedin.com/in/manuel-rodriguez/","dir":"content/output/people","base":"manuel-rodriguez.json","ext":".json","sourceBase":"manuel-rodriguez.yaml","sourceExt":".yaml","id":"manuel-rodriguez","photo":"/static/images/people/manuel-rodriguez.jpg"},{"name":"Marcus Friedel","type":"alumni","past":"master","labs":["utouch"],"url":"https://marcusfriedel.com","facebook":"https://www.facebook.com/marcus.friedel.3","email":"marcus.friedel@ucalgary.ca","linkedin":"https://www.linkedin.com/in/marcusfriedel/","dir":"content/output/people","base":"marcus-friedel.json","ext":".json","sourceBase":"marcus-friedel.yaml","sourceExt":".yaml","id":"marcus-friedel","photo":"/static/images/people/marcus-friedel.jpg"},{"name":"Marian DÃ¶rk","type":"alumni","past":"phd","now":"University of Applied Sciences Potsdam","url":"https://mariandoerk.de/","email":"marian.doerk@fh-potsdam.de","scholar":"https://scholar.google.com/citations?user=pDg8ZMAAAAAJ\u0026hl=en","dir":"content/output/people","base":"marian-doerk.json","ext":".json","sourceBase":"marian-doerk.yaml","sourceExt":".yaml","id":"marian-doerk","photo":"/static/images/people/marian-doerk.jpg"},{"name":"Mark Hancock","type":"alumni","past":"phd","now":"University of Waterloo","url":"https://uwaterloo.ca/touchlab","email":"mark.hancock@uwaterloo.ca","linkedin":"https://www.linkedin.com/in/mark-hancock-55439925","scholar":"https://scholar.google.ca/citations?user=PYvcN3cAAAAJ\u0026hl=en","dir":"content/output/people","base":"mark-hancock.json","ext":".json","sourceBase":"mark-hancock.yaml","sourceExt":".yaml","id":"mark-hancock","photo":"/static/images/people/mark-hancock.jpg"},{"name":"Mark Roseman","type":"alumni","past":"master","url":"https://markroseman.com/","linkedin":"https://www.linkedin.com/in/mroseman/","dir":"content/output/people","base":"mark-roseman.json","ext":".json","sourceBase":"mark-roseman.yaml","sourceExt":".yaml","id":"mark-roseman","photo":"/static/images/people/mark-roseman.jpg"},{"name":"Martin Feick","type":"alumni","past":"phd","now":"Saarland University","url":"http://martinfeick.com/","scholar":"https://scholar.google.de/citations?user=az0GkfQAAAAJ","github":"https://github.com/MartinFk","twitter":"https://twitter.com/mafeick","labs":["utouch"],"dir":"content/output/people","base":"martin-feick.json","ext":".json","sourceBase":"martin-feick.yaml","sourceExt":".yaml","id":"martin-feick","photo":"/static/images/people/martin-feick.jpg"},{"name":"Matthew Dunlap","type":"alumni","past":"master","now":"UNC Chapel Hill","url":"https://odum.unc.edu/people/dunlap/","linkedin":"https://www.linkedin.com/in/matthew-dunlap-47a7b3b3/","github":"https://github.com/matthew-a-dunlap","dir":"content/output/people","base":"matthew-dunlap.json","ext":".json","sourceBase":"matthew-dunlap.yaml","sourceExt":".yaml","id":"matthew-dunlap","photo":"/static/images/people/matthew-dunlap.jpg"},{"name":"Mehrad Faridan","type":"alumni","past":"undergrad","now":"University of Tokyo and MaKami College","url":"https://www.mehradfaridan.com/","email":"mehrad.faridan1@ucalgary.ca","scholar":"https://scholar.google.com/citations?user=amh7v2EAAAAJ","twitter":"https://twitter.com/MehradFaridan","github":"https://github.com/mehradFaridan","linkedin":"https://www.linkedin.com/in/mehrad-f-a34462164/","labs":["programmable-reality"],"dir":"content/output/people","base":"mehrad-faridan.json","ext":".json","sourceBase":"mehrad-faridan.yaml","sourceExt":".yaml","id":"mehrad-faridan","photo":"/static/images/people/mehrad-faridan.jpg"},{"name":"Melissa Hoang","type":"alumni","past":"undergrad","email":"melissa.hoang@ucalgary.ca","linkedin":"https://www.linkedin.com/in/melissa-e-hoang/","dir":"content/output/people","base":"melissa-hoang.json","ext":".json","sourceBase":"melissa-hoang.yaml","sourceExt":".yaml","id":"melissa-hoang","photo":"/static/images/people/melissa-hoang.jpg"},{"name":"Miaosen Wang","type":"alumni","past":"master","now":"DeepMind","scholar":"https://scholar.google.com/citations?user=j99nvycAAAAJ\u0026hl=en","linkedin":"https://www.linkedin.com/in/miaosen-wang-2607a317/","twitter":"https://x.com/MiaosenWang","dir":"content/output/people","base":"miaosen-wang.json","ext":".json","sourceBase":"miaosen-wang.yaml","sourceExt":".yaml","id":"miaosen-wang","photo":"/static/images/people/miaosen-wang.jpg"},{"name":"Michael Hung","type":"alumni","past":"master","now":"Open Cycle Technologies Inc.","url":"https://michael-hung.ca","email":"myshung@ucalgary.ca","github":"https://github.com/murrrkle","linkedin":"https://www.linkedin.com/in/hungyukshing","labs":["curio"],"dir":"content/output/people","base":"michael-hung.json","ext":".json","sourceBase":"michael-hung.yaml","sourceExt":".yaml","id":"michael-hung","photo":"/static/images/people/michael-hung.jpg"},{"name":"Micheal Boyle","type":"alumni","past":"phd","dir":"content/output/people","base":"micheal-boyle.json","ext":".json","sourceBase":"micheal-boyle.yaml","sourceExt":".yaml","id":"micheal-boyle","photo":"/static/images/people/no-profile.jpg"},{"name":"Micheal Nunes","type":"alumni","past":"master","dir":"content/output/people","base":"micheal-nunes.json","ext":".json","sourceBase":"micheal-nunes.yaml","sourceExt":".yaml","id":"micheal-nunes","photo":"/static/images/people/no-profile.jpg"},{"name":"Micheal Roudning","type":"alumni","past":"master","now":"Curve Dental","linkedin":"https://www.linkedin.com/in/mlroundi/","dir":"content/output/people","base":"micheal-rounding.json","ext":".json","sourceBase":"micheal-rounding.yaml","sourceExt":".yaml","id":"micheal-rounding","photo":"/static/images/people/micheal-rounding.jpg"},{"name":"Mille Skovhus Lunding","type":"alumni","past":"visiting","url":"https://pure.au.dk/portal/en/persons/mille-skovhus-lunding(806b2f20-b7ae-4ebc-a32f-98ee39a53380).html","email":"milledsk@cs.au.dk","scholar":"https://scholar.google.com/citations?user=kt6cN_UAAAAJ","linkedin":"https://www.linkedin.com/in/mille-skovhus-lunding-1b419583/","dir":"content/output/people","base":"mille-skovhus-lunding.json","ext":".json","sourceBase":"mille-skovhus-lunding.yaml","sourceExt":".yaml","id":"mille-skovhus-lunding","photo":"/static/images/people/mille-skovhus-lunding.jpg"},{"name":"Muhammad Mahian","type":"alumni","past":"undergrad","email":"muhammad.mahian@ucalgary.ca","dir":"content/output/people","base":"muhammad-mahian.json","ext":".json","sourceBase":"muhammad-mahian.yaml","sourceExt":".yaml","id":"muhammad-mahian","photo":"/static/images/people/no-profile.jpg"},{"name":"Nadeem Moosa","type":"undergrad","labs":["shivers"],"linkedin":"https://www.linkedin.com/in/nadeem-moosa","github":"https://github.com/DeemDeem52","email":"nadeem.moosa@ucalgary.ca","dir":"content/output/people","base":"nadeem-moosa.json","ext":".json","sourceBase":"nadeem-moosa.yaml","sourceExt":".yaml","id":"nadeem-moosa","photo":"/static/images/people/nadeem-moosa.jpg","title":"Ugrad"},{"name":"Nandi Zhang","type":"alumni","past":"master","now":"University of Rochester","email":"nandi.zhang@ucalgary.ca","url":"https://nandi-zhang.github.io/","linkedin":"https://www.linkedin.com/in/nandi-zhang-022ab8184/","github":"https://github.com/nandi-zhang","labs":["programmable-reality"],"dir":"content/output/people","base":"nandi-zhang.json","ext":".json","sourceBase":"nandi-zhang.yaml","sourceExt":".yaml","id":"nandi-zhang","photo":"/static/images/people/nandi-zhang.jpg"},{"name":"Nathalie Bressa","type":"alumni","past":"visiting","now":"Aarhus University","labs":["data-experience"],"dir":"content/output/people","base":"nathalie-bressa.json","ext":".json","sourceBase":"nathalie-bressa.yaml","sourceExt":".yaml","id":"nathalie-bressa","photo":"/static/images/people/nathalie-bressa.jpg"},{"name":"Neil Chulpongsatorn","type":"alumni","past":"master","url":"https://thobthai.wixsite.com/neilchulpongsatorn","scholar":"https://scholar.google.com/citations?user=D0KbikIAAAAJ\u0026hl=en","linkedin":"https://www.linkedin.com/in/neil-chulpongsatorn-187273204/","email":"thobthai.chulpongsat@ucalgary.ca","labs":["data-experience","programmable-reality"],"dir":"content/output/people","base":"neil-chulpongsatorn.json","ext":".json","sourceBase":"neil-chulpongsatorn.yaml","sourceExt":".yaml","id":"neil-chulpongsatorn","photo":"/static/images/people/neil-chulpongsatorn.jpg"},{"name":"Nelson Wong","type":"alumni","past":"master","dir":"content/output/people","base":"nelson-wong.json","ext":".json","sourceBase":"nelson-wong.yaml","sourceExt":".yaml","id":"nelson-wong","photo":"/static/images/people/nelson-wong.jpg"},{"name":"Nicolai Marquardt","type":"alumni","past":"phd","now":"Microsoft Research","url":"http://www.nicolaimarquardt.com/","scholar":"https://scholar.google.com/citations?user=PXeN0RsAAAAJ","dir":"content/output/people","base":"nicolai-marquardt.json","ext":".json","sourceBase":"nicolai-marquardt.yaml","sourceExt":".yaml","id":"nicolai-marquardt","photo":"/static/images/people/nicolai-marquardt.jpg"},{"name":"Nishan Soni","type":"alumni","past":"undergrad","email":"nishan.soni@ucalgary.ca","linkedin":"https://www.linkedin.com/in/nishan-soni/","github":"https://github.com/nishan-soni","labs":["programmable-reality"],"dir":"content/output/people","base":"nishan-soni.json","ext":".json","sourceBase":"nishan-soni.yaml","sourceExt":".yaml","id":"nishan-soni","photo":"/static/images/people/no-profile.jpg"},{"name":"Nour Hammad","type":"alumni","past":"undergrad","labs":["utouch"],"dir":"content/output/people","base":"nour-hammad.json","ext":".json","sourceBase":"nour-hammad.yaml","sourceExt":".yaml","id":"nour-hammad","photo":"/static/images/people/nour-hammad.jpg"},{"name":"Paige So'Brien","type":"alumni","past":"undergrad","labs":["data-experience"],"dir":"content/output/people","base":"paige-sobrien.json","ext":".json","sourceBase":"paige-sobrien.yaml","sourceExt":".yaml","id":"paige-sobrien","photo":"/static/images/people/paige-sobrien.jpg"},{"name":"Paul Lapides","type":"alumni","past":"phd","url":"http://www.paullapides.com/","labs":["utouch"],"dir":"content/output/people","base":"paul-lapides.json","ext":".json","sourceBase":"paul-lapides.yaml","sourceExt":".yaml","id":"paul-lapides","photo":"/static/images/people/paul-lapides.jpg"},{"name":"Paul Saulnier","type":"alumni","past":"master","now":"Bravura Security","url":"http://paulsaulnier.com/","linkedin":"https://www.linkedin.com/in/paulsaulnier/","labs":["utouch"],"dir":"content/output/people","base":"paul-saulnier.json","ext":".json","sourceBase":"paul-saulnier.yaml","sourceExt":".yaml","id":"paul-saulnier","photo":"/static/images/people/paul-saulnier.jpg"},{"name":"Petra Isenberg","type":"alumni","past":"phd","now":"Inria Saclay","url":"https://petra.isenberg.cc/","linkedin":"https://www.linkedin.com/in/petraisenberg","scholar":"https://scholar.google.com/citations?user=mw0_aLoAAAAJ\u0026hl=en","dir":"content/output/people","base":"petra-isenberg.json","ext":".json","sourceBase":"petra-isenberg.yaml","sourceExt":".yaml","id":"petra-isenberg","photo":"/static/images/people/petra-isenberg.jpg"},{"name":"Priya Dhawka","type":"alumni","past":"master","email":"priyadarshinee.dhawk@ucalgary.ca","github":"https://github.com/dhawkap","linkedin":"https://www.linkedin.com/in/priya-d-15b743112/","labs":["data-experience"],"dir":"content/output/people","base":"priya-dhawka.json","ext":".json","sourceBase":"priya-dhawka.yaml","sourceExt":".yaml","id":"priya-dhawka","photo":"/static/images/people/priya-dhawka.jpg"},{"name":"Ran Zhou","type":"alumni","past":"visiting","url":"https://www.ranzhourobot.com/","email":"ran.zhou@colorado.edu","scholar":"https://scholar.google.com/citations?user=QDCBu-cAAAAJ","linkedin":"https://www.linkedin.com/in/ran-zhou-096b8812b/","twitter":"https://twitter.com/ranzhoubot","dir":"content/output/people","base":"ran-zhou.json","ext":".json","sourceBase":"ran-zhou.yaml","sourceExt":".yaml","id":"ran-zhou","photo":"/static/images/people/ran-zhou.jpg"},{"name":"Rasmus Lunding","type":"alumni","past":"visiting","url":"https://pure.au.dk/portal/en/persons/rasmus-skovhus-lunding(a1b1400c-dffa-4227-ac5b-0218dbd72de9).html","email":"rsl@cs.au.dk","linkedin":"https://www.linkedin.com/in/rlunding/","dir":"content/output/people","base":"rasmus-lunding.json","ext":".json","sourceBase":"rasmus-lunding.yaml","sourceExt":".yaml","id":"rasmus-lunding","photo":"/static/images/people/rasmus-lunding.jpg"},{"name":"Ritik Vatsal","type":"alumni","past":"visiting","email":"vatrit@gmail.com","github":"https://github.com/RitikVatsal-2019321","linkedin":"https://www.linkedin.com/in/ritik-vatsal/","dir":"content/output/people","base":"ritik-vatsal.json","ext":".json","sourceBase":"ritik-vatsal.yaml","sourceExt":".yaml","id":"ritik-vatsal","photo":"/static/images/people/ritik-vatsal.jpg"},{"name":"Roberta Cabral Mota","type":"alumni","past":"phd","url":"https://www.robertacrmota.com/","labs":["utouch"],"dir":"content/output/people","base":"roberta-cabral-mota.json","ext":".json","sourceBase":"roberta-cabral-mota.yaml","sourceExt":".yaml","id":"roberta-cabral-mota","photo":"/static/images/people/roberta-cabral-mota.jpg"},{"name":"Roberto Diaz Marino","type":"alumni","past":"master","now":"SMART Technologies","linkedin":"https://www.linkedin.com/in/rob-diaz-marino-b7a575bb","labs":["grouplab"],"dir":"content/output/people","base":"roberto-diaz-marino.json","ext":".json","sourceBase":"roberto-diaz-marino.yaml","sourceExt":".yaml","id":"roberto-diaz-marino","photo":"/static/images/people/roberto-diaz-marino.jpg"},{"name":"Ryota Gomi","type":"alumni","past":"visiting","url":"https://www.icd.riec.tohoku.ac.jp/en/about/member/","email":"ryota.gomi.s7@dc.tohoku.ac.jp","linkedin":"https://www.linkedin.com/in/ryota-gomi-6a6029203/","dir":"content/output/people","base":"ryota-gomi.json","ext":".json","sourceBase":"ryota-gomi.yaml","sourceExt":".yaml","id":"ryota-gomi","photo":"/static/images/people/ryota-gomi.jpg"},{"name":"Sabrina Lakhdhir","type":"alumni","past":"undergrad","now":"University of Victoria","labs":["utouch"],"dir":"content/output/people","base":"sabrina-lakhdhir.json","ext":".json","sourceBase":"sabrina-lakhdhir.yaml","sourceExt":".yaml","id":"sabrina-lakhdhir","photo":"/static/images/people/sabrina-lakhdhir.jpg"},{"name":"Saja Abufarha","type":"alumni","past":"undergrad","email":"saja.abufarha@ucalgary.ca","linkedin":"https://www.linkedin.com/in/saja-abufarha/","dir":"content/output/people","base":"saja-abufarha.json","ext":".json","sourceBase":"saja-abufarha.yaml","sourceExt":".yaml","id":"saja-abufarha","photo":"/static/images/people/saja-abufarha.jpg"},{"name":"Samin Farajian","type":"alumni","past":"master","url":"https://farajiansamin.github.io","linkedin":"https://www.linkedin.com/in/samin-farajian-736018140/","dir":"content/output/people","base":"samin-farajian.json","ext":".json","sourceBase":"samin-farajian.yaml","sourceExt":".yaml","id":"samin-farajian","photo":"/static/images/people/samin-farajian.jpg"},{"name":"Sasha Ivanov","alias":"Alexander Ivanov","type":"alumni","past":"master","now":"Meta Reality Labs","labs":["data-experience"],"dir":"content/output/people","base":"sasha-ivanov.json","ext":".json","sourceBase":"sasha-ivanov.yaml","sourceExt":".yaml","id":"sasha-ivanov","photo":"/static/images/people/sasha-ivanov.jpg"},{"name":"Sebastian Gil","type":"undergrad","labs":["shivers"],"linkedin":"https://www.linkedin.com/in/sebastian-gil-268a3b22a","github":"https://github.com/EmpOfBol","email":"sebastian.gil@ucalgary.ca","dir":"content/output/people","base":"sebastian-gil.json","ext":".json","sourceBase":"sebastian-gil.yaml","sourceExt":".yaml","id":"sebastian-gil","photo":"/static/images/people/sebastian-gil.jpg","title":"Ugrad"},{"name":"Setareh Manesh","type":"alumni","past":"master","now":"Maple Scan","linkedin":"https://www.linkedin.com/in/setareh-m","scholar":"https://scholar.google.com/citations?user=Cb-_dmIAAAAJ\u0026hl=en","labs":["utouch"],"dir":"content/output/people","base":"setareh-manesh.json","ext":".json","sourceBase":"setareh-manesh.yaml","sourceExt":".yaml","id":"setareh-manesh","photo":"/static/images/people/setareh-manesh.jpg"},{"name":"Shamim Seyson","type":"alumni","past":"undergrad","email":"shamim.seyson@ucalgary.ca","linkedin":"https://www.linkedin.com/in/shamimseyson/","labs":["data-experience"],"dir":"content/output/people","base":"shamim-seyson.json","ext":".json","sourceBase":"shamim-seyson.yaml","sourceExt":".yaml","id":"shamim-seyson","photo":"/static/images/people/shamim-seyson.jpg"},{"name":"Shanna Hollingworth","alias":"Shanna Li Ching Hollingworth","type":"master","email":"shanna.hollingwor1@ucalgary.ca","url":"https://shanna1408.github.io/","linkedin":"https://www.linkedin.com/in/shanna-hollingworth/","labs":["data-experience"],"dir":"content/output/people","base":"shanna-hollingworth.json","ext":".json","sourceBase":"shanna-hollingworth.yaml","sourceExt":".yaml","id":"shanna-hollingworth","photo":"/static/images/people/shanna-hollingworth.jpg","title":"MSc"},{"name":"Shaun Kaasten","type":"alumni","past":"master","dir":"content/output/people","base":"shaun-kaasten.json","ext":".json","sourceBase":"shaun-kaasten.yaml","sourceExt":".yaml","id":"shaun-kaasten","photo":"/static/images/people/no-profile.jpg"},{"name":"Shivesh Jadon","type":"alumni","past":"master","now":"Apple","url":"https://shivesh.dev/","scholar":"https://scholar.google.com/citations?user=EwqYU2sAAAAJ\u0026hl=en","linkedin":"https://www.linkedin.com/in/shiveshjadon/","twitter":"https://twitter.com/ShiveshJadon","labs":["data-experience"],"dir":"content/output/people","base":"shivesh-jadon.json","ext":".json","sourceBase":"shivesh-jadon.yaml","sourceExt":".yaml","id":"shivesh-jadon","photo":"/static/images/people/shivesh-jadon.jpg"},{"name":"Shrivatsa Mishra","type":"alumni","past":"visiting","email":"shrivatsamishra@gmail.com","github":"https://github.com/ShrivatsaMishra","linkedin":"https://www.linkedin.com/in/shrivatsa-mishra/","dir":"content/output/people","base":"shrivatsa-mishra.json","ext":".json","sourceBase":"shrivatsa-mishra.yaml","sourceExt":".yaml","id":"shrivatsa-mishra","photo":"/static/images/people/shrivatsa-mishra.jpg"},{"name":"Simon KlÃ¼ber","type":"alumni","past":"visiting","labs":["data-experience"],"dir":"content/output/people","base":"simon-kluber.json","ext":".json","sourceBase":"simon-kluber.yaml","sourceExt":".yaml","id":"simon-kluber","photo":"/static/images/people/no-profile.jpg"},{"name":"SÃ¸ren Knudsen","type":"alumni","past":"postdoc","now":"University of Copenhagen","url":"http://sorenknudsen.com/","labs":["data-experience"],"dir":"content/output/people","base":"soren-knudsen.json","ext":".json","sourceBase":"soren-knudsen.yaml","sourceExt":".yaml","id":"soren-knudsen","photo":"/static/images/people/soren-knudsen.jpg"},{"name":"Sowmya Somanath","type":"alumni","past":"phd","now":"University of Victoria","labs":["utouch"],"url":"http://pages.cpsc.ucalgary.ca/~ssomanat/","scholar":"https://scholar.google.ca/citations?user=R9ar1NkAAAAJ","twitter":"https://twitter.com/sowmyasomanath","dir":"content/output/people","base":"sowmya-somanath.json","ext":".json","sourceBase":"sowmya-somanath.yaml","sourceExt":".yaml","id":"sowmya-somanath","photo":"/static/images/people/sowmya-somanath.jpg"},{"name":"Stacey Scott","type":"alumni","past":"phd","now":"University of Guelph","url":"https://csl.uwaterloo.ca/","linkedin":"https://www.linkedin.com/in/staceydscott","scholar":"https://scholar.google.ca/citations?user=fB_xzi8AAAAJ\u0026hl=en","dir":"content/output/people","base":"stacey-scott.json","ext":".json","sourceBase":"stacey-scott.yaml","sourceExt":".yaml","id":"stacey-scott","photo":"/static/images/people/stacey-scott.jpg"},{"name":"Stephanie Smale","type":"alumni","past":"master","dir":"content/output/people","base":"stephanie-smale.json","ext":".json","sourceBase":"stephanie-smale.yaml","sourceExt":".yaml","id":"stephanie-smale","photo":"/static/images/people/no-profile.jpg"},{"name":"Sutirtha Roy","type":"master","labs":["diff"],"email":"sutirtha.roy@ucalgary.ca","scholar":"https://scholar.google.com/citations?user=i_4V4oAAAAAJ","linkedin":"https://www.linkedin.com/in/sutirtha-roy-320586199/","dir":"content/output/people","base":"sutirtha-roy.json","ext":".json","sourceBase":"sutirtha-roy.yaml","sourceExt":".yaml","id":"sutirtha-roy","photo":"/static/images/people/sutirtha-roy.jpg","title":"MSc"},{"name":"Sydney Pratte","type":"alumni","past":"phd","url":"https://www.sydneypratte.ca/","labs":["curio"],"dir":"content/output/people","base":"sydney-pratte.json","ext":".json","sourceBase":"sydney-pratte.yaml","sourceExt":".yaml","id":"sydney-pratte","photo":"/static/images/people/sydney-pratte.jpg"},{"name":"Tania Villalobos Lujan","type":"phd","linkedin":"https://www.linkedin.com/in/tania-villalobos-lujan","labs":["curio"],"dir":"content/output/people","base":"tania-villalobos-lujan.json","ext":".json","sourceBase":"tania-villalobos-lujan.yaml","sourceExt":".yaml","id":"tania-villalobos-lujan","photo":"/static/images/people/tania-villalobos-lujan.jpg","title":"PhD"},{"name":"Teale Masrani","type":"alumni","past":"master","now":"University of Calgary","scholar":"https://scholar.google.ca/citations?user=sGEgYE0AAAAJ\u0026hl=en","email":"teale.masrani2@ucalgary.ca","linkedin":"https://www.linkedin.com/in/teale-masrani-876151127/","dir":"content/output/people","base":"teale-masrani.json","ext":".json","sourceBase":"teale-masrani.yaml","sourceExt":".yaml","id":"teale-masrani","photo":"/static/images/people/teale-masrani.jpg"},{"name":"Teddy Seyed","type":"alumni","past":"phd","now":"Microsoft Research","url":"https://www.microsoft.com/en-us/research/people/teddy/","scholar":"https://scholar.google.com/citations?user=A8VSir8AAAAJ","dir":"content/output/people","base":"teddy-seyed.json","ext":".json","sourceBase":"teddy-seyed.yaml","sourceExt":".yaml","id":"teddy-seyed","photo":"/static/images/people/teddy-seyed.jpg"},{"name":"Terrance Mok","type":"phd","url":"http://terrancemok.com/","scholar":"https://scholar.google.ca/citations?user=nHkJDSEAAAAJ","twitter":"https://twitter.com/terrancem","linkedin":"https://www.linkedin.com/in/terrance-mok-22421011","labs":["curio"],"dir":"content/output/people","base":"terrance-mok.json","ext":".json","sourceBase":"terrance-mok.yaml","sourceExt":".yaml","id":"terrance-mok","photo":"/static/images/people/terrance-mok.jpg","title":"PhD"},{"name":"Theodore (Ted) O'Grady","type":"alumni","past":"master","now":"Fillip","linkedin":"https://www.linkedin.com/in/ogradyt","dir":"content/output/people","base":"theodore-ogrady.json","ext":".json","sourceBase":"theodore-ogrady.yaml","sourceExt":".yaml","id":"theodore-ogrady","photo":"/static/images/people/theodore-ogrady.jpg"},{"name":"Tian Xia","type":"alumni","past":"undergrad","linkedin":"https://www.linkedin.com/in/tianxiacs/","email":"tian.xia2@ucalgary.ca","labs":["utouch"],"dir":"content/output/people","base":"tian-xia.json","ext":".json","sourceBase":"tian-xia.yaml","sourceExt":".yaml","id":"tian-xia","photo":"/static/images/people/tian-xia.jpg"},{"name":"Tim Au Yeung","type":"phd","linkedin":"https://www.linkedin.com/in/tim-au-yeung-3a29816","labs":["utouch"],"dir":"content/output/people","base":"tim-au-yeung.json","ext":".json","sourceBase":"tim-au-yeung.yaml","sourceExt":".yaml","id":"tim-au-yeung","photo":"/static/images/people/tim-au-yeung.jpg","title":"PhD"},{"name":"Tobias Isenberg","type":"alumni","past":"postdoc","now":"Inria Saclay","url":"https://tobias.isenberg.cc/","linkedin":"https://www.linkedin.com/in/tobiasisenberg","scholar":"https://scholar.google.com/citations?user=e09cpQUAAAAJ","dir":"content/output/people","base":"tobias-isenberg.json","ext":".json","sourceBase":"tobias-isenberg.yaml","sourceExt":".yaml","id":"tobias-isenberg","photo":"/static/images/people/no-profile.jpg"},{"name":"Uta Hinrichs","type":"alumni","past":"phd","now":"University of Edinburgh","url":"http://www.utahinrichs.de/","scholar":"https://scholar.google.ca/citations?user=kGPAR0YAAAAJ\u0026hl=en","linkedin":"https://www.linkedin.com/in/uta-hinrichs-506a5560","dir":"content/output/people","base":"uta-hinrichs.json","ext":".json","sourceBase":"uta-hinrichs.yaml","sourceExt":".yaml","id":"uta-hinrichs","photo":"/static/images/people/uta-hinrichs.jpg"},{"name":"Victoria Wong","type":"undergrad","linkedin":"https://www.linkedin.com/in/victoria-wong-63509723b","labs":["data-experience"],"dir":"content/output/people","base":"victoria-wong.json","ext":".json","sourceBase":"victoria-wong.yaml","sourceExt":".yaml","id":"victoria-wong","photo":"/static/images/people/victoria-wong.jpg","title":"Ugrad"},{"name":"Wei Wei","type":"alumni","past":"master","url":"http://www.weiweiff.com/","email":"wei.wei2@ucalgary.ca","linkedin":"https://www.linkedin.com/in/wei-wei-961a211ba/","labs":["utouch","data-experience"],"dir":"content/output/people","base":"wei-wei.json","ext":".json","sourceBase":"wei-wei.yaml","sourceExt":".yaml","id":"wei-wei","photo":"/static/images/people/wei-wei.jpg"},{"name":"William Wright","type":"masters","url":"http://pages.cpsc.ucalgary.ca/~wwright/","scholar":"https://scholar.google.com/citations?user=V4nRMoMAAAAJ","twitter":"https://twitter.com/HexenKoenig","facebook":"https://www.facebook.com/bonadriel","linkedin":"https://www.linkedin.com/in/bon-adriel-aseniero-47140560/","dir":"content/output/people","base":"william-wright.json","ext":".json","sourceBase":"william-wright.yaml","sourceExt":".yaml","id":"william-wright","photo":"/static/images/people/no-profile.jpg"},{"name":"Xiang 'Anthony' Chen","type":"alumni","past":"master","now":"UCLA","url":"https://xac.is/","scholar":"https://scholar.google.com/citations?user=I2W13z0AAAAJ","twitter":"https://twitter.com/_xiang_chen_","dir":"content/output/people","base":"xiang-anthony-chen.json","ext":".json","sourceBase":"xiang-anthony-chen.yaml","sourceExt":".yaml","id":"xiang-anthony-chen","photo":"/static/images/people/xiang-anthony-chen.jpg"},{"name":"Xing-Dong Yang","type":"alumni","past":"postdoc","now":"Simon Fraser University","url":"https://www.sfu.ca/~xingdong/","scholar":"https://scholar.google.ca/citations?user=9WfDSNwAAAAJ\u0026hl=en","linkedin":"https://www.linkedin.com/in/xing-dong-yang-648a1663/","dir":"content/output/people","base":"xing-dong-yang.json","ext":".json","sourceBase":"xing-dong-yang.yaml","sourceExt":".yaml","id":"xing-dong-yang","photo":"/static/images/people/xing-dong-yang.jpg"},{"name":"Yaseen Rashid","type":"undergrad","labs":["shivers"],"linkedin":"https://www.linkedin.com/in/yaseenmrashid","github":"https://github.com/YaseeenMR","email":"yaseen.rashid@ucalgary.ca","dir":"content/output/people","base":"yaseen-rashid.json","ext":".json","sourceBase":"yaseen-rashid.yaml","sourceExt":".yaml","id":"yaseen-rashid","photo":"/static/images/people/yaseen-rashid.jpg","title":"Ugrad"},{"name":"Yibo Sun","type":"alumni","past":"master","linkedin":"https://www.linkedin.com/in/yibo-sun-30800334/?originalSubdomain=ca","dir":"content/output/people","base":"yibo-sun.json","ext":".json","sourceBase":"yibo-sun.yaml","sourceExt":".yaml","id":"yibo-sun","photo":"/static/images/people/no-profile.jpg"},{"name":"Zachary McKendrick","type":"alumni","past":"phd","now":"University of Waterloo","url":"https://uwaterloo.ca/postdoctoral-scholars/blog/2024-provosts-program-interdisciplinary-postdoctoral-scholar-2","linkedin":"https://www.linkedin.com/in/zach-mckendrick-7a24bb3b","labs":["utouch"],"dir":"content/output/people","base":"zachary-mckendrick.json","ext":".json","sourceBase":"zachary-mckendrick.yaml","sourceExt":".yaml","id":"zachary-mckendrick","photo":"/static/images/people/zachary-mckendrick.jpg"},{"name":"Zhijie Xia","type":"alumni","past":"undergrad","now":"Huawei","url":"https://zhijiexia.dev/","linkedin":"https://www.linkedin.com/in/zhijie-xia-678b331b5/","dir":"content/output/people","base":"zhijie-xia.json","ext":".json","sourceBase":"zhijie-xia.yaml","sourceExt":".yaml","id":"zhijie-xia","photo":"/static/images/people/zhijie-xia.jpg"}]},"labsStaticProps":{"labs":[{"id":"curio","name":"Curio Lab","description":"Human-Centered Design for Creativity \u0026 Curiosity","statement":"examines human-centered design for creativity \u0026 curiosity.","prof":"lora-oehlberg","url":"http://pages.cpsc.ucalgary.ca/~lora.oehlberg/","colour":"#2095ba","profilePhoto":"/static/images/people/lora-oehlberg.jpg","person":{"name":"Lora Oehlberg","type":"faculty","title":"Associate Professor","labs":["curio"],"keywords":["Tangible","Design Tools"],"order":2,"url":"https://pages.cpsc.ucalgary.ca/~lora.oehlberg/","scholar":"https://scholar.google.ca/citations?hl=en\u0026user=8GzaBdwAAAAJ","dir":"content/output/people","base":"lora-oehlberg.json","ext":".json","sourceBase":"lora-oehlberg.yaml","sourceExt":".yaml"},"picture":"/static/images/labs/curio.png"},{"id":"data-experience","name":"Data Experience Lab","description":"Visual Data-driven Tools and Experiences","statement":"develops new data visualizations, interactions, and experiences.","prof":"wesley-willett","url":"https://dataexperience.cpsc.ucalgary.ca/","colour":"#c14824","profilePhoto":"/static/images/people/wesley-willett.jpg","person":{"name":"Wesley Willett","alias":"Wesley J. Willett","type":"faculty","title":"Associate Professor","labs":["data-experience"],"keywords":["Data Visualization","Data Phyz","AR"],"order":3,"url":"https://dataexperience.cpsc.ucalgary.ca/","scholar":"https://scholar.google.ca/citations?user=Q17-rckAAAAJ","dir":"content/output/people","base":"wesley-willett.json","ext":".json","sourceBase":"wesley-willett.yaml","sourceExt":".yaml"},"picture":"/static/images/labs/data-experience.png"},{"id":"diff","name":"DIFF Lab","description":"Devices, Interaction and Fabrication for the Future","statement":"creates devices, interactions, and fabrication for the future.","prof":"aditya-shekhar-nittala","url":"https://difflab.org","colour":"#57a793","profilePhoto":"/static/images/people/aditya-shekhar-nittala.jpg","person":{"name":"Aditya Shekhar Nittala","type":"faculty","title":"Assistant Professor","labs":["diff"],"keywords":["Wearable Computing","Fabrication","Interaction Techniques"],"order":4,"url":"https://sites.google.com/site/adityanittala/","scholar":"https://scholar.google.com/citations?user=pDSbjBsAAAAJ","email":"anittala@ucalgary.ca","linkedin":"https://www.linkedin.com/in/adityashekharn","dir":"content/output/people","base":"aditya-shekhar-nittala.json","ext":".json","sourceBase":"aditya-shekhar-nittala.yaml","sourceExt":".yaml"},"picture":"/static/images/labs/diff.png"},{"id":"health-vis","name":"HealthVisFutures","description":"Health Data Visualization","statement":"explores new visual tools for patient-engaged healthcare.","prof":"fateme-rajabiyazdi","url":"https://fatemerajabi.github.io/HealthVisFutures/","colour":"#185b79","redirect":true,"profilePhoto":"/static/images/people/fateme-rajabiyazdi.jpg","person":{"name":"Fateme Rajabiyazdi","type":"faculty","title":"Assistant Professor","labs":["health-vis"],"keywords":["Data Visualization","Health"],"order":6,"url":"https://fatemerajabi.github.io/HealthVisFutures/","scholar":"https://scholar.google.com/citations?user=mvoc_5AAAAAJ\u0026hl=en","email":"fateme.rajabiyazdi@ucalgary.ca","github":"https://github.com/FatemeRajabi","linkedin":"https://www.linkedin.com/in/rajabiyazdi","dir":"content/output/people","base":"fateme-rajabiyazdi.json","ext":".json","sourceBase":"fateme-rajabiyazdi.yaml","sourceExt":".yaml"},"picture":"/static/images/labs/health-vis.png"},{"id":"shivers","name":"SHIVERS","description":null,"statement":"crafts new tools for multisensory (haptic) and multimedia (sonic) interaction.","prof":"christian-frisson","url":"https://ilab.ucalgary.ca/labs/shivers","colour":"#a3b86c","profilePhoto":"/static/images/people/christian-frisson.jpg","person":{"name":"Christian Frisson","type":"faculty","title":"Assistant Professor","labs":["shivers"],"keywords":["Haptics","Multisensory \u0026 Multimedia","Immersive Arts"],"order":5,"url":"https://frisson.re","cv":"https://frisson.re","scholar":"https://scholar.google.com/citations?user=sZVn1V4AAAAJ","twitter":"https://twitter.com/TuyleriDikenli","facebook":"https://www.facebook.com/christian.frisson","linkedin":"https://www.linkedin.com/in/christianfrisson","github":"https://github.com/ChristianFrisson","gitlab":"https://gitlab.com/ChristianFrisson","email":"christian.frisson@ucalgary.ca","dir":"content/output/people","base":"christian-frisson.json","ext":".json","sourceBase":"christian-frisson.yaml","sourceExt":".yaml"},"picture":"/static/images/labs/shivers.png"},{"id":"prpl","name":"prpl Lab","description":null,"statement":"investigates and designs non-game forms of playfulness in user interfaces.","prof":"matthew-lakier","url":"https://ilab.ucalgary.ca/labs/prpl","colour":"#a765e9","profilePhoto":"/static/images/people/matthew-lakier.jpg","person":{"name":"Matthew Lakier","type":"faculty","title":"Assistant Professor","labs":["prpl"],"keywords":["Playful Interfaces","Games \u0026 Play","VR/AR"],"order":7,"url":"https://matthewlakier.com","scholar":"https://scholar.google.ca/citations?hl=en\u0026user=RYrb8kwAAAAJ","email":"matthew.lakier@ucalgary.ca","github":"https://github.com/spamalot","dir":"content/output/people","base":"matthew-lakier.json","ext":".json","sourceBase":"matthew-lakier.yaml","sourceExt":".yaml"},"picture":"/static/images/labs/prpl.png"},{"id":"utouch","name":"uTouch","description":"Physical Interaction and Human-Robot Interaction","statement":"explores physical and human-robot interaction and autonomy.","prof":"ehud-sharlin","url":"https://utouch.cpsc.ucalgary.ca/","colour":"#ecaa35","profilePhoto":"/static/images/people/ehud-sharlin.jpg","person":{"name":"Ehud Sharlin","type":"faculty","title":"Professor","labs":["utouch"],"keywords":["HRI","Robots","Drones"],"order":1,"url":"http://contacts.ucalgary.ca/info/cpsc/profiles/102-3264","scholar":"https://scholar.google.ca/citations?hl=en\u0026user=eAFxlZIAAAAJ","dir":"content/output/people","base":"ehud-sharlin.json","ext":".json","sourceBase":"ehud-sharlin.yaml","sourceExt":".yaml"},"picture":"/static/images/labs/utouch.png"},{"id":"grouplab","name":"GroupLab","description":"Research in HCI, CSCW, and UbiComp","prof":"saul-greenberg","url":"http://grouplab.cpsc.ucalgary.ca/","colour":"#57a793","status":"associated","profilePhoto":"/static/images/people/saul-greenberg.jpg","person":{"name":"Saul Greenberg","type":"faculty","title":"Emeritus Professor","keywords":["UbiComp","CSCW"],"order":10,"url":"http://saul.cpsc.ucalgary.ca/","scholar":"https://scholar.google.com/citations?user=TthhUuoAAAAJ","dir":"content/output/people","base":"saul-greenberg.json","ext":".json","sourceBase":"saul-greenberg.yaml","sourceExt":".yaml"},"picture":"/static/images/labs/grouplab.png"},{"id":"innovis","name":"InnoVis","description":"Innovations in Visualization Laboratory","prof":"sheelagh-carpendale","url":"http://sheelaghcarpendale.ca/","colour":"#185b79","status":"associated","profilePhoto":"/static/images/people/sheelagh-carpendale.jpg","person":{"name":"Sheelagh Carpendale","type":"faculty","title":"Adjunct Professor (Simon Fraser University)","keywords":["Data Viz","Data Phyz"],"order":13,"url":"https://www.cs.sfu.ca/~sheelagh/","scholar":"https://scholar.google.com/citations?user=43LLX2kAAAAJ","dir":"content/output/people","base":"sheelagh-carpendale.json","ext":".json","sourceBase":"sheelagh-carpendale.yaml","sourceExt":".yaml"},"picture":"/static/images/labs/innovis.png"},{"id":"programmable-reality","name":"Programmable Reality Lab","description":"Programmable Reality Lab - Tangible, AR/VR, and Robotics","prof":"ryo-suzuki","url":"https://programmable-reality-lab.github.io/","colour":"#a3b86c","status":"associated","profilePhoto":"/static/images/people/ryo-suzuki.jpg","person":{"name":"Ryo Suzuki","type":"faculty","title":"Adjunct Assistant Professor (CU Boulder)","keywords":["Tangible","AR x AI","Robots"],"order":11,"url":"https://ryosuzuki.org","scholar":"https://scholar.google.com/citations?user=klWjaQIAAAAJ","twitter":"https://twitter.com/ryosuzk","facebook":"https://www.facebook.com/ryosuzk","email":"ryo.suzuki@ucalgary.ca","github":"https://github.com/ryosuzuki","linkedin":"https://www.linkedin.com/in/ryosuzuki/","dir":"content/output/people","base":"ryo-suzuki.json","ext":".json","sourceBase":"ryo-suzuki.yaml","sourceExt":".yaml"},"picture":"/static/images/labs/programmable-reality.png"},{"id":"ricelab","name":"RICELab","description":"Rethinking Interaction, Collaboration, \u0026 Engagement","prof":"anthony-tang","url":"https://ricelab.github.io/","colour":"#2095ba","status":"associated","profilePhoto":"/static/images/people/anthony-tang.jpg","person":{"name":"Anthony Tang","type":"faculty","title":"Adjunct Associate Professor (Singapore Management University)","keywords":["Mixed Reality","CSCW"],"order":12,"url":"https://hcitang.github.io/","scholar":"https://scholar.google.com/citations?user=RG1EQowAAAAJ","twitter":"https://twitter.com/proclubboy","github":"http://github.com/hcitang","dir":"content/output/people","base":"anthony-tang.json","ext":".json","sourceBase":"anthony-tang.yaml","sourceExt":".yaml"},"picture":"/static/images/labs/ricelab.png"}]}},"__N_SSG":true},"page":"/","query":{},"buildId":"9o_HcBSBizKAuJDjxl5_g","assetPrefix":"/pr-preview/pr-134","runtimeConfig":{"basePath":"/pr-preview/pr-134"},"isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>