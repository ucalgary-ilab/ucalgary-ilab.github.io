<!DOCTYPE html><html><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="format-detection" content="telephone=no"/><link href="https://use.fontawesome.com/releases/v5.1.1/css/all.css" rel="stylesheet"/><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.css" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/css/lightbox.css" rel="stylesheet"/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link href="/pr-preview/pr-86/static/css/style.css" rel="stylesheet"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox-plus-jquery.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              $('.ui.sidebar')
                .sidebar('attach events', '.sidebar.icon')

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1" class="next-head"/><meta charSet="utf-8" class="next-head"/><title class="next-head">IntelliLining: Activity Sensing through Textile Interlining Sensors Using TENGs | Interactions Lab - University of Calgary HCI Group</title><meta name="keywords" content="Interactive textile, TENGs, Machine learning, Vibration sensing" class="next-head"/><meta name="description" content="We introduce a novel component for smart garments: smart interlining, and validate its technical feasibility through a series of experiments. Our work involved the implementation of a prototype that employs a textile vibration sensor based on Triboelectric Nanogenerators (TENGs), commonly used for activity detection. We explore several unique features of smart interlining, including how sensor signals and patterns are influenced by factors such as the size and shape of the interlining sensor, the location of the vibration source within the sensor area, and various propagation media, such as airborne and surface vibrations. We present our study results and discuss how these findings support the feasibility of smart interlining. Additionally, we demonstrate that smart interlinings on a shirt can detect a variety of user activities involving the hand, mouth, and upper body, achieving an accuracy rate of 93.9% in the tested activities." class="next-head"/><meta property="og:title" content="IntelliLining: Activity Sensing through Textile Interlining Sensors Using TENGs | Interactions Lab - University of Calgary HCI Group" class="next-head"/><meta property="og:description" content="We introduce a novel component for smart garments: smart interlining, and validate its technical feasibility through a series of experiments. Our work involved the implementation of a prototype that employs a textile vibration sensor based on Triboelectric Nanogenerators (TENGs), commonly used for activity detection. We explore several unique features of smart interlining, including how sensor signals and patterns are influenced by factors such as the size and shape of the interlining sensor, the location of the vibration source within the sensor area, and various propagation media, such as airborne and surface vibrations. We present our study results and discuss how these findings support the feasibility of smart interlining. Additionally, we demonstrate that smart interlinings on a shirt can detect a variety of user activities involving the hand, mouth, and upper body, achieving an accuracy rate of 93.9% in the tested activities." class="next-head"/><meta property="og:site_name" content="University of Calgary Interactions Lab" class="next-head"/><meta property="og:url" content="https://ilab.ucalgary.ca/" class="next-head"/><meta property="og:image" content="/pr-preview/pr-86/static/images/publications/cover/chi-2025-ghaneezabadi.jpg" class="next-head"/><meta property="og:type" content="website" class="next-head"/><meta name="twitter:title" content="IntelliLining: Activity Sensing through Textile Interlining Sensors Using TENGs | Interactions Lab - University of Calgary HCI Group" class="next-head"/><meta name="twitter:description" content="We introduce a novel component for smart garments: smart interlining, and validate its technical feasibility through a series of experiments. Our work involved the implementation of a prototype that employs a textile vibration sensor based on Triboelectric Nanogenerators (TENGs), commonly used for activity detection. We explore several unique features of smart interlining, including how sensor signals and patterns are influenced by factors such as the size and shape of the interlining sensor, the location of the vibration source within the sensor area, and various propagation media, such as airborne and surface vibrations. We present our study results and discuss how these findings support the feasibility of smart interlining. Additionally, we demonstrate that smart interlinings on a shirt can detect a variety of user activities involving the hand, mouth, and upper body, achieving an accuracy rate of 93.9% in the tested activities." class="next-head"/><meta name="twitter:image" content="/pr-preview/pr-86/static/images/publications/cover/chi-2025-ghaneezabadi.jpg" class="next-head"/><meta name="twitter:card" content="summary" class="next-head"/><meta name="twitter:site" content="@ucalgary" class="next-head"/><meta name="twitter:url" content="https://ilab.ucalgary.ca/" class="next-head"/><link rel="preload" href="/pr-preview/pr-86/_next/static/OGNG4_ObTU4gkZ-2ULkiR/pages/publication.js" as="script"/><link rel="preload" href="/pr-preview/pr-86/_next/static/OGNG4_ObTU4gkZ-2ULkiR/pages/_app.js" as="script"/><link rel="preload" href="/pr-preview/pr-86/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" as="script"/><link rel="preload" href="/pr-preview/pr-86/_next/static/chunks/commons.0c93cb8d3516282dd2c4.js" as="script"/><link rel="preload" href="/pr-preview/pr-86/_next/static/runtime/main-563fdde58a64bdca21c4.js" as="script"/></head><body><div id="__next"><div><div><div class="ui right vertical sidebar menu"><a class="item" href="/pr-preview/pr-86/">Home</a><a class="item active" href="/pr-preview/pr-86/publications">Publications</a><a class="item" href="/pr-preview/pr-86/people">People</a><a class="item" href="/pr-preview/pr-86/courses">Courses</a><a class="item" href="/pr-preview/pr-86/facility">Facility</a><a class="item" href="/pr-preview/pr-86/seminar">Seminar</a><a class="item" href="/pr-preview/pr-86/location">Location</a></div><div class="ui stackable secondary pointing container menu" style="border-bottom:none;margin-right:15%;font-size:1.1em"><div class="left menu"><a class="item" href="/pr-preview/pr-86/"><b>UCalgary iLab</b></a></div><div class="right menu"><a class="item active" href="/pr-preview/pr-86/publications">Publications</a><a class="item" href="/pr-preview/pr-86/people">People</a><a class="item" href="/pr-preview/pr-86/courses">Courses</a><a class="item" href="/pr-preview/pr-86/facility">Facility</a><a class="item" href="/pr-preview/pr-86/seminar">Seminar</a><a class="item" href="/pr-preview/pr-86/location">Location</a><div class="toc item"><a href="/pr-preview/pr-86/"><b>UCalgary iLab</b></a><i style="float:right" class="sidebar icon"></i></div></div></div></div><div class="ui stackable grid"><div class="one wide column"></div><div class="ten wide column centered" style="margin-top:30px"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-86/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2025</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/pr-preview/pr-86/static/images/publications/cover/chi-2025-ghaneezabadi.jpg"/></div><div class="thirteen wide column"><h1>IntelliLining: Activity Sensing through Textile Interlining Sensors Using TENGs</h1><p class="meta"><span>Mahdie GhaneEzabadi</span> , <a href="/pr-preview/pr-86/people/aditya-shekhar-nittala"><img src="/pr-preview/pr-86/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><strong>Aditya Shekhar Nittala</strong></a> , <a href="/pr-preview/pr-86/people/xing-dong-yang"><img src="/pr-preview/pr-86/static/images/people/xing-dong-yang.jpg" class="ui circular spaced image mini-profile"/><strong>Xing-Dong Yang</strong></a> , <span>Te-yen Wu</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/chi-2025-ghaneezabadi.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2025-ghaneezabadi.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We introduce a novel component for smart garments: smart interlining, and validate its technical feasibility through a series of experiments. Our work involved the implementation of a prototype that employs a textile vibration sensor based on Triboelectric Nanogenerators (TENGs), commonly used for activity detection. We explore several unique features of smart interlining, including how sensor signals and patterns are influenced by factors such as the size and shape of the interlining sensor, the location of the vibration source within the sensor area, and various propagation media, such as airborne and surface vibrations. We present our study results and discuss how these findings support the feasibility of smart interlining. Additionally, we demonstrate that smart interlinings on a shirt can detect a variety of user activities involving the hand, mouth, and upper body, achieving an accuracy rate of 93.9% in the tested activities.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Interactive Textile</span><span class="ui brown basic label">TEN Gs</span><span class="ui brown basic label">Machine Learning</span><span class="ui brown basic label">Vibration Sensing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Mahdie GhaneEzabadi<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Xing-Dong Yang<!-- -->, <!-- -->Te-yen Wu<!-- -->. <b>IntelliLining: Activity Sensing through Textile Interlining Sensors Using TENGs</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;25)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->.  DOI: <a href="https://doi.org/10.1145/3706598.3713167" target="_blank">https://doi.org/10.1145/3706598.3713167</a></p></div></div></div></div><div class="one wide column"></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><img style="max-width:180px;margin:30px auto" src="/pr-preview/pr-86/static/images/logo-6.png"/><div class="content"><img style="max-width:200px;margin:0px auto" src="/pr-preview/pr-86/static/images/logo-4.png"/><div class="sub header">Department of Computer Science</div></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"dataManager":"[]","props":{"pageProps":{"id":"chi-2025-ghaneezabadi"}},"page":"/publication","query":{"id":"chi-2025-ghaneezabadi"},"buildId":"OGNG4_ObTU4gkZ-2ULkiR","dynamicBuildId":false,"nextExport":true}</script><script async="" id="__NEXT_PAGE__/publication" src="/pr-preview/pr-86/_next/static/OGNG4_ObTU4gkZ-2ULkiR/pages/publication.js"></script><script async="" id="__NEXT_PAGE__/_app" src="/pr-preview/pr-86/_next/static/OGNG4_ObTU4gkZ-2ULkiR/pages/_app.js"></script><script src="/pr-preview/pr-86/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" async=""></script><script src="/pr-preview/pr-86/_next/static/chunks/commons.0c93cb8d3516282dd2c4.js" async=""></script><script src="/pr-preview/pr-86/_next/static/runtime/main-563fdde58a64bdca21c4.js" async=""></script></body></html>