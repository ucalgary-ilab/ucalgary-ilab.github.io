<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><title data-next-head="">Mixed Reality Interfaces for Augmented Text and Speech | Interactions Lab - University of Calgary HCI Group</title><meta name="keywords" content="augmented reality, natural language processing, speech recognition, keyword extraction, mixed reality" data-next-head=""/><meta name="description" content="While technology plays a vital role in human communication, there still remain many significant challenges when using them in everyday life. Modern computing technologies, such as smartphones, offer convenient and swift access to information, facilitating tasks like reading documents or communicating with friends. However, these tools frequently lack adaptability, become distracting, consume excessive time, and impede interactions with people and contextual information. Furthermore, they often require numerous steps and significant time investment to gather pertinent information. We want to explore an efficient process of contextual information gathering for mixed reality (MR) interfaces that provide information directly in the user’s view. This approach allows for a seamless and flexible transition between language and subsequent contextual references, without disrupting the flow of communication. ’Augmented Language’ can be defined as the integration of language and communication with mixed reality to enhance, transform, or manipulate language-related aspects and various forms of linguistic augmentations (such as annotation/referencing, aiding social interactions, translation, localization, etc.). In this thesis, our broad objective is to explore mixed reality interfaces and their potential to enhance augmented language, particularly in the domains of speech and text. Our aim is to create interfaces that offer a more natural, generalizable, on-demand, and real-time experience of accessing contextually relevant information and providing adaptive interactions. To better address this broader objective, we systematically break it down to focus on two instances of augmented language. First, enhancing augmented conversation to support on-the-fly, co-located in-person conversations using embedded references. And second, enhancing digital and physical documents using MR to provide on-demand reading support in the form of different summarization techniques. To examine the effectiveness of these speech and text interfaces, we conducted two studies in which we asked the participants to evaluate our system prototype in different use cases. The exploratory usability study for the first exploration confirms that our system decreases distraction and friction in conversation compared to smartphone search while providing highly useful and relevant information. For the second project, we conducted an exploratory design workshop to identify categories of document enhancements. We later conducted a user study with a mixed-reality prototype to highlight five board themes to discuss the benefits of MR document enhancement." data-next-head=""/><meta property="og:title" content="Mixed Reality Interfaces for Augmented Text and Speech | Interactions Lab - University of Calgary HCI Group" data-next-head=""/><meta property="og:description" content="While technology plays a vital role in human communication, there still remain many significant challenges when using them in everyday life. Modern computing technologies, such as smartphones, offer convenient and swift access to information, facilitating tasks like reading documents or communicating with friends. However, these tools frequently lack adaptability, become distracting, consume excessive time, and impede interactions with people and contextual information. Furthermore, they often require numerous steps and significant time investment to gather pertinent information. We want to explore an efficient process of contextual information gathering for mixed reality (MR) interfaces that provide information directly in the user’s view. This approach allows for a seamless and flexible transition between language and subsequent contextual references, without disrupting the flow of communication. ’Augmented Language’ can be defined as the integration of language and communication with mixed reality to enhance, transform, or manipulate language-related aspects and various forms of linguistic augmentations (such as annotation/referencing, aiding social interactions, translation, localization, etc.). In this thesis, our broad objective is to explore mixed reality interfaces and their potential to enhance augmented language, particularly in the domains of speech and text. Our aim is to create interfaces that offer a more natural, generalizable, on-demand, and real-time experience of accessing contextually relevant information and providing adaptive interactions. To better address this broader objective, we systematically break it down to focus on two instances of augmented language. First, enhancing augmented conversation to support on-the-fly, co-located in-person conversations using embedded references. And second, enhancing digital and physical documents using MR to provide on-demand reading support in the form of different summarization techniques. To examine the effectiveness of these speech and text interfaces, we conducted two studies in which we asked the participants to evaluate our system prototype in different use cases. The exploratory usability study for the first exploration confirms that our system decreases distraction and friction in conversation compared to smartphone search while providing highly useful and relevant information. For the second project, we conducted an exploratory design workshop to identify categories of document enhancements. We later conducted a user study with a mixed-reality prototype to highlight five board themes to discuss the benefits of MR document enhancement." data-next-head=""/><meta property="og:site_name" content="University of Calgary Interactions Lab" data-next-head=""/><meta property="og:url" content="https://ilab.ucalgary.ca/" data-next-head=""/><meta property="og:image" content="https://ilab.ucalgary.ca/static/images/theses/cover/msc-2023-jadon.jpg" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:title" content="Mixed Reality Interfaces for Augmented Text and Speech | Interactions Lab - University of Calgary HCI Group" data-next-head=""/><meta name="twitter:description" content="While technology plays a vital role in human communication, there still remain many significant challenges when using them in everyday life. Modern computing technologies, such as smartphones, offer convenient and swift access to information, facilitating tasks like reading documents or communicating with friends. However, these tools frequently lack adaptability, become distracting, consume excessive time, and impede interactions with people and contextual information. Furthermore, they often require numerous steps and significant time investment to gather pertinent information. We want to explore an efficient process of contextual information gathering for mixed reality (MR) interfaces that provide information directly in the user’s view. This approach allows for a seamless and flexible transition between language and subsequent contextual references, without disrupting the flow of communication. ’Augmented Language’ can be defined as the integration of language and communication with mixed reality to enhance, transform, or manipulate language-related aspects and various forms of linguistic augmentations (such as annotation/referencing, aiding social interactions, translation, localization, etc.). In this thesis, our broad objective is to explore mixed reality interfaces and their potential to enhance augmented language, particularly in the domains of speech and text. Our aim is to create interfaces that offer a more natural, generalizable, on-demand, and real-time experience of accessing contextually relevant information and providing adaptive interactions. To better address this broader objective, we systematically break it down to focus on two instances of augmented language. First, enhancing augmented conversation to support on-the-fly, co-located in-person conversations using embedded references. And second, enhancing digital and physical documents using MR to provide on-demand reading support in the form of different summarization techniques. To examine the effectiveness of these speech and text interfaces, we conducted two studies in which we asked the participants to evaluate our system prototype in different use cases. The exploratory usability study for the first exploration confirms that our system decreases distraction and friction in conversation compared to smartphone search while providing highly useful and relevant information. For the second project, we conducted an exploratory design workshop to identify categories of document enhancements. We later conducted a user study with a mixed-reality prototype to highlight five board themes to discuss the benefits of MR document enhancement." data-next-head=""/><meta name="twitter:image" content="https://ilab.ucalgary.ca/static/images/theses/cover/msc-2023-jadon.jpg" data-next-head=""/><meta name="twitter:card" content="summary" data-next-head=""/><meta name="twitter:site" content="@ucalgary" data-next-head=""/><meta name="twitter:url" content="https://ilab.ucalgary.ca/" data-next-head=""/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link rel="preload" href="/pr-preview/pr-142/_next/static/media/dc84b505c4b06e35-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/pr-preview/pr-142/_next/static/css/281ff1f186fbce6b.css" as="style"/><link rel="preload" href="/pr-preview/pr-142/_next/static/css/a1a0497113412518.css" as="style"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              // $('.ui.sidebar')
              //   .sidebar('attach events', '.sidebar.icon')

              $('.sidebar.icon').on('click', function(event) {
                $('.ui.sidebar')
                  .sidebar('toggle')
              })

              $('.project').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })

              $('.thesis').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><link rel="stylesheet" href="/pr-preview/pr-142/_next/static/css/281ff1f186fbce6b.css" data-n-g=""/><link rel="stylesheet" href="/pr-preview/pr-142/_next/static/css/a1a0497113412518.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/pr-preview/pr-142/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/pr-preview/pr-142/_next/static/chunks/webpack-7899779395be340d.js" defer=""></script><script src="/pr-preview/pr-142/_next/static/chunks/340-795c2935822720c4.js" defer=""></script><script src="/pr-preview/pr-142/_next/static/chunks/main-f5a95b893c70049f.js" defer=""></script><script src="/pr-preview/pr-142/_next/static/chunks/vendor-styles-218505cf05882012.js" defer=""></script><script src="/pr-preview/pr-142/_next/static/chunks/505-a6927c13d0d92549.js" defer=""></script><script src="/pr-preview/pr-142/_next/static/chunks/pages/_app-09ca72778dd825e1.js" defer=""></script><script src="/pr-preview/pr-142/_next/static/chunks/347-6f14004506bc7107.js" defer=""></script><script src="/pr-preview/pr-142/_next/static/chunks/590-a0ec5d328c626dc7.js" defer=""></script><script src="/pr-preview/pr-142/_next/static/chunks/330-04233cc3010a7fa2.js" defer=""></script><script src="/pr-preview/pr-142/_next/static/chunks/960-4b0ff261179f7f73.js" defer=""></script><script src="/pr-preview/pr-142/_next/static/chunks/pages/theses/%5Bid%5D-64656ddd92f422f0.js" defer=""></script><script src="/pr-preview/pr-142/_next/static/48P8pCDlCxLVf6dR0GH9O/_buildManifest.js" defer=""></script><script src="/pr-preview/pr-142/_next/static/48P8pCDlCxLVf6dR0GH9O/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_728be9"><div class="ui center aligned container"><div class="ui secondary huge compact menu"><a class="item" href="/pr-preview/pr-142/"><img loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui tiny image" style="color:transparent" srcSet="/pr-preview/pr-142/static/images/ilab-logo-3d.gif 1x" src="/pr-preview/pr-142/static/images/ilab-logo-3d.gif"/></a><a class="item" href="/pr-preview/pr-142/people/">People</a><a class="item" href="/pr-preview/pr-142/publications/">Research</a></div></div><div class="pusher"><div class="ui stackable grid"><div class="one wide column"></div><div class="ten wide column centered" style="margin-top:30px"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-142/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1>Mixed Reality Interfaces for Augmented Text and Speech</h1><p class="meta"><a href="/people/shivesh-jadon"><img alt="shivesh-jadon photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-142/static/images/people/shivesh-jadon.jpg 1x" src="/pr-preview/pr-142/static/images/people/shivesh-jadon.jpg"/><strong>Shivesh Singh Jadon</strong></a><span class="role"> (author)</span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-142/static/images/people/ryo-suzuki.jpg 1x" src="/pr-preview/pr-142/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"> (supervisor)</span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-142/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-142/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"> (supervisor)</span>, <span>Frank Maurer<!-- --> <span class="role"> (committee)</span></span>, <span>Ruofei Du<!-- --> <span class="role"> (committee)</span></span>, <span>Kangsoo Kim<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>While technology plays a vital role in human communication, there still remain many significant challenges when using them in everyday life. Modern computing technologies, such as smartphones, offer convenient and swift access to information, facilitating tasks like reading documents or communicating with friends. However, these tools frequently lack adaptability, become distracting, consume excessive time, and impede interactions with people and contextual information. Furthermore, they often require numerous steps and significant time investment to gather pertinent information. We want to explore an efficient process of contextual information gathering for mixed reality (MR) interfaces that provide information directly in the user’s view. This approach allows for a seamless and flexible transition between language and subsequent contextual references, without disrupting the flow of communication. ’Augmented Language’ can be defined as the integration of language and communication with mixed reality to enhance, transform, or manipulate language-related aspects and various forms of linguistic augmentations (such as annotation/referencing, aiding social interactions, translation, localization, etc.). In this thesis, our broad objective is to explore mixed reality interfaces and their potential to enhance augmented language, particularly in the domains of speech and text. Our aim is to create interfaces that offer a more natural, generalizable, on-demand, and real-time experience of accessing contextually relevant information and providing adaptive interactions. To better address this broader objective, we systematically break it down to focus on two instances of augmented language. First, enhancing augmented conversation to support on-the-fly, co-located in-person conversations using embedded references. And second, enhancing digital and physical documents using MR to provide on-demand reading support in the form of different summarization techniques. To examine the effectiveness of these speech and text interfaces, we conducted two studies in which we asked the participants to evaluate our system prototype in different use cases. The exploratory usability study for the first exploration confirms that our system decreases distraction and friction in conversation compared to smartphone search while providing highly useful and relevant information. For the second project, we conducted an exploratory design workshop to identify categories of document enhancements. We later conducted a user study with a mixed-reality prototype to highlight five board themes to discuss the benefits of MR document enhancement.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Natural Language Processing</span><span class="ui brown basic label">Speech Recognition</span><span class="ui brown basic label">Keyword Extraction</span><span class="ui brown basic label">Mixed Reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Shivesh Singh Jadon<!-- -->. <b>Mixed Reality Interfaces for Augmented Text and Speech</b>. <i></i> <!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2023-08<!-- -->. <!-- -->DOI: <a href="https://dx.doi.org/10.11575/PRISM/41696" target="_blank">https://dx.doi.org/10.11575/PRISM/41696</a>URL: <a href="https://hdl.handle.net/1880/116854" target="_blank">https://hdl.handle.net/1880/116854</a></p></div></div></div></div><div class="one wide column"></div></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><div class="content"><a href="https://ucalgary.ca"><img alt="University of Calgary logo" loading="lazy" width="200" height="0" decoding="async" data-nimg="1" style="color:transparent;max-width:200px;margin:0px auto;height:auto" srcSet="/pr-preview/pr-142/static/images/logo-4.png 1x, /pr-preview/pr-142/static/images/logo-4.png 2x" src="/pr-preview/pr-142/static/images/logo-4.png"/></a><div class="sub header"><a class="item" href="https://cpsc.ucalgary.ca">Department of Computer Science</a></div></div></div></footer></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"ids":[{"id":"mmus-2012-pon"},{"id":"msc-2008-guo"},{"id":"msc-2011-harris"},{"id":"msc-2011-sultanum"},{"id":"msc-2012-lapides"},{"id":"msc-2015-li"},{"id":"msc-2017-hu"},{"id":"msc-2017-mok"},{"id":"msc-2017-payne"},{"id":"msc-2018-cartwright"},{"id":"msc-2018-ta"},{"id":"msc-2019-danyluk"},{"id":"msc-2019-kollannur"},{"id":"msc-2019-kuzabaviciute"},{"id":"msc-2019-mahadevan"},{"id":"msc-2019-mikalauskas"},{"id":"msc-2019-wun"},{"id":"msc-2021-asha"},{"id":"msc-2021-hung"},{"id":"msc-2021-wannamaker"},{"id":"msc-2023-dhawka"},{"id":"msc-2023-jadon"},{"id":"msc-2023-smith"},{"id":"msc-2023-wei"},{"id":"msc-2024-friedel"},{"id":"msc-2025-chulpongsatorn"},{"id":"phd-2010-young"},{"id":"phd-2017-somanath"},{"id":"phd-2018-mostafa"},{"id":"phd-2018-rajabiyazdi"},{"id":"phd-2019-li"},{"id":"phd-2020-ledo-maira"},{"id":"phd-2022-hull"},{"id":"phd-2024-mckendrick"},{"id":"phd-2025-cabral-mota"},{"id":"phd-2025-poostchi"},{"id":"phd-2025-pratte"}]},"__N_SSG":true},"page":"/theses/[id]","query":{"id":"msc-2023-jadon"},"buildId":"48P8pCDlCxLVf6dR0GH9O","assetPrefix":"/pr-preview/pr-142","runtimeConfig":{"basePath":"/pr-preview/pr-142"},"isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>