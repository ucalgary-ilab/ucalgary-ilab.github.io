<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><title data-next-head="">Better Little People Pictures: Generative Creation of Demographically Diverse Anthropographics | Interactions Lab - University of Calgary HCI Group</title><meta name="keywords" content="anthropographics, demographic data, diversity, marginalized populations" data-next-head=""/><meta name="description" content="We explore the potential of generative AI text-to-image models to help designers efficiently craft unique, representative, and demographically diverse anthropographics that visualize data about people. Currently, creating data-driven iconic images to represent individuals in a dataset often requires considerable design effort. Generative text-to-image models can streamline the process of creating these images, but risk perpetuating designer biases in addition to stereotypes latent in the models. In response, we outline a conceptual workflow for crafting anthropographic assets for visualizations, highlighting possible sources of risk and bias as well as opportunities for reflection and refinement by a human designer. Using an implementation of this workflow with Stable Diffusion and Google Colab, we illustrate a variety of new anthropographic designs that showcase the visual expressiveness and scalability of these generative approaches. Based on our experiments, we also identify challenges and research opportunities for new AI-enabled anthropographic visualization tools." data-next-head=""/><meta property="og:title" content="Better Little People Pictures: Generative Creation of Demographically Diverse Anthropographics | Interactions Lab - University of Calgary HCI Group" data-next-head=""/><meta property="og:description" content="We explore the potential of generative AI text-to-image models to help designers efficiently craft unique, representative, and demographically diverse anthropographics that visualize data about people. Currently, creating data-driven iconic images to represent individuals in a dataset often requires considerable design effort. Generative text-to-image models can streamline the process of creating these images, but risk perpetuating designer biases in addition to stereotypes latent in the models. In response, we outline a conceptual workflow for crafting anthropographic assets for visualizations, highlighting possible sources of risk and bias as well as opportunities for reflection and refinement by a human designer. Using an implementation of this workflow with Stable Diffusion and Google Colab, we illustrate a variety of new anthropographic designs that showcase the visual expressiveness and scalability of these generative approaches. Based on our experiments, we also identify challenges and research opportunities for new AI-enabled anthropographic visualization tools." data-next-head=""/><meta property="og:site_name" content="University of Calgary Interactions Lab" data-next-head=""/><meta property="og:url" content="https://ilab.ucalgary.ca/" data-next-head=""/><meta property="og:image" content="https://ilab.ucalgary.ca/static/images/publications/cover/chi-2024-dhawka.jpg" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:title" content="Better Little People Pictures: Generative Creation of Demographically Diverse Anthropographics | Interactions Lab - University of Calgary HCI Group" data-next-head=""/><meta name="twitter:description" content="We explore the potential of generative AI text-to-image models to help designers efficiently craft unique, representative, and demographically diverse anthropographics that visualize data about people. Currently, creating data-driven iconic images to represent individuals in a dataset often requires considerable design effort. Generative text-to-image models can streamline the process of creating these images, but risk perpetuating designer biases in addition to stereotypes latent in the models. In response, we outline a conceptual workflow for crafting anthropographic assets for visualizations, highlighting possible sources of risk and bias as well as opportunities for reflection and refinement by a human designer. Using an implementation of this workflow with Stable Diffusion and Google Colab, we illustrate a variety of new anthropographic designs that showcase the visual expressiveness and scalability of these generative approaches. Based on our experiments, we also identify challenges and research opportunities for new AI-enabled anthropographic visualization tools." data-next-head=""/><meta name="twitter:image" content="https://ilab.ucalgary.ca/static/images/publications/cover/chi-2024-dhawka.jpg" data-next-head=""/><meta name="twitter:card" content="summary" data-next-head=""/><meta name="twitter:site" content="@ucalgary" data-next-head=""/><meta name="twitter:url" content="https://ilab.ucalgary.ca/" data-next-head=""/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link rel="preload" href="/pr-preview/pr-115/_next/static/media/e807dee2426166ad-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/pr-preview/pr-115/_next/static/css/e9b4dac809082ca6.css" as="style"/><link rel="preload" href="/pr-preview/pr-115/_next/static/css/fa1543fdff44676b.css" as="style"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              // $('.ui.sidebar')
              //   .sidebar('attach events', '.sidebar.icon')

              $('.sidebar.icon').on('click', function(event) {
                $('.ui.sidebar')
                  .sidebar('toggle')
              })

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><link rel="stylesheet" href="/pr-preview/pr-115/_next/static/css/e9b4dac809082ca6.css" data-n-g=""/><link rel="stylesheet" href="/pr-preview/pr-115/_next/static/css/fa1543fdff44676b.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/pr-preview/pr-115/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/pr-preview/pr-115/_next/static/chunks/webpack-e927f1861161db82.js" defer=""></script><script src="/pr-preview/pr-115/_next/static/chunks/340-c11cf6ad79ea01d6.js" defer=""></script><script src="/pr-preview/pr-115/_next/static/chunks/main-237fefe93cf728ed.js" defer=""></script><script src="/pr-preview/pr-115/_next/static/chunks/vendor-styles-23869fcdb44eebb4.js" defer=""></script><script src="/pr-preview/pr-115/_next/static/chunks/110-2c2224f87be24bba.js" defer=""></script><script src="/pr-preview/pr-115/_next/static/chunks/pages/_app-a1a52e1bf24d014b.js" defer=""></script><script src="/pr-preview/pr-115/_next/static/chunks/723-9071df5d30f1ec6a.js" defer=""></script><script src="/pr-preview/pr-115/_next/static/chunks/900-3d5847e785319f79.js" defer=""></script><script src="/pr-preview/pr-115/_next/static/chunks/230-149465347cb3e206.js" defer=""></script><script src="/pr-preview/pr-115/_next/static/chunks/829-1c30adaf55f71841.js" defer=""></script><script src="/pr-preview/pr-115/_next/static/chunks/296-9deb1a681c3c979d.js" defer=""></script><script src="/pr-preview/pr-115/_next/static/chunks/696-a5335f570829c66d.js" defer=""></script><script src="/pr-preview/pr-115/_next/static/chunks/969-3080c5672f210b14.js" defer=""></script><script src="/pr-preview/pr-115/_next/static/chunks/pages/publications/%5Bid%5D-43c48fefab462daf.js" defer=""></script><script src="/pr-preview/pr-115/_next/static/WbVIuZ5asJ5nO_SIy0XCg/_buildManifest.js" defer=""></script><script src="/pr-preview/pr-115/_next/static/WbVIuZ5asJ5nO_SIy0XCg/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_91dcd6"><div class="ui right vertical sidebar menu"><a class="item" href="/pr-preview/pr-115/">Home</a><a class="item active" href="/pr-preview/pr-115/publications/">Publications</a><a class="item" href="/pr-preview/pr-115/people/">People</a><a class="item" href="/pr-preview/pr-115/courses/">Courses</a><a class="item" href="/pr-preview/pr-115/facility/">Facility</a><a class="item" href="/pr-preview/pr-115/seminar/">Seminar</a><a class="item" href="/pr-preview/pr-115/location/">Location</a></div><div class="ui stackable secondary pointing container menu" style="border-bottom:none;margin-right:15%;font-size:1.1em"><div class="left menu"><a class="item" href="/pr-preview/pr-115/"><b>UCalgary iLab</b></a></div><div class="right menu"><a class="item active" href="/pr-preview/pr-115/publications/">Publications</a><a class="item" href="/pr-preview/pr-115/people/">People</a><a class="item" href="/pr-preview/pr-115/courses/">Courses</a><a class="item" href="/pr-preview/pr-115/facility/">Facility</a><a class="item" href="/pr-preview/pr-115/seminar/">Seminar</a><a class="item" href="/pr-preview/pr-115/location/">Location</a><div class="toc item"><a href="/pr-preview/pr-115/"><b>UCalgary iLab</b></a><i style="float:right" class="sidebar icon"></i></div></div></div><div class="pusher"><div class="ui stackable grid"><div class="one wide column"></div><div class="ten wide column centered" style="margin-top:30px"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/pr-preview/pr-115/publications/">Publications</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2024-dhawka cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/pr-preview/pr-115/static/images/publications/cover/chi-2024-dhawka.jpg 1x" src="/pr-preview/pr-115/static/images/publications/cover/chi-2024-dhawka.jpg"/></div><div class="thirteen wide column"><h1>Better Little People Pictures: Generative Creation of Demographically Diverse Anthropographics</h1><p class="meta"><a href="/people/priya-dhawka"><img alt="priya-dhawka photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-115/static/images/people/priya-dhawka.jpg 1x" src="/pr-preview/pr-115/static/images/people/priya-dhawka.jpg"/><strong>Priya Dhawka</strong></a> , <span>Lauren Perera</span> , <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/pr-preview/pr-115/static/images/people/wesley-willett.jpg 1x" src="/pr-preview/pr-115/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/dCEFvx4AqIo" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/dCEFvx4AqIo?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/dCEFvx4AqIo/maxresdefault.jpg src=https://img.youtube.com/vi/dCEFvx4AqIo/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We explore the potential of generative AI text-to-image models to help designers efficiently craft unique, representative, and demographically diverse anthropographics that visualize data about people. Currently, creating data-driven iconic images to represent individuals in a dataset often requires considerable design effort. Generative text-to-image models can streamline the process of creating these images, but risk perpetuating designer biases in addition to stereotypes latent in the models. In response, we outline a conceptual workflow for crafting anthropographic assets for visualizations, highlighting possible sources of risk and bias as well as opportunities for reflection and refinement by a human designer. Using an implementation of this workflow with Stable Diffusion and Google Colab, we illustrate a variety of new anthropographic designs that showcase the visual expressiveness and scalability of these generative approaches. Based on our experiments, we also identify challenges and research opportunities for new AI-enabled anthropographic visualization tools.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Anthropographics</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Marginalized Populations</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Priya Dhawka<!-- -->, <!-- -->Lauren Perera<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Better Little People Pictures: Generative Creation of Demographically Diverse Anthropographics</b>. <i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- -->(<!-- -->CHI 2024<!-- -->)</i>. <!-- -->ACM, New York, NY, USA<!-- --> <!-- -->DOI: <a href="https://doi.org/10.1145/3613904.3641957" target="_blank">https://doi.org/10.1145/3613904.3641957</a></p></div></div></div></div><div class="one wide column"></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><img alt="Interactions Lab logo" loading="lazy" width="180" height="0" decoding="async" data-nimg="1" style="color:transparent;max-width:180px;margin:30px auto;height:auto" srcSet="/pr-preview/pr-115/static/images/logo-6.png 1x, /pr-preview/pr-115/static/images/logo-6.png 2x" src="/pr-preview/pr-115/static/images/logo-6.png"/><div class="content"><img alt="University of Calgary logo" loading="lazy" width="200" height="0" decoding="async" data-nimg="1" style="color:transparent;max-width:200px;margin:0px auto;height:auto" srcSet="/pr-preview/pr-115/static/images/logo-4.png 1x, /pr-preview/pr-115/static/images/logo-4.png 2x" src="/pr-preview/pr-115/static/images/logo-4.png"/><div class="sub header">Department of Computer Science</div></div></div></footer></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"ids":[{"id":"assets-2017-suzuki"},{"id":"assets-2023-mok"},{"id":"capstone-2025-haptic-floor-proxy"},{"id":"cga-2019-ivanov"},{"id":"cgi-2019-danyluk"},{"id":"chi-2015-aseniero"},{"id":"chi-2015-jones"},{"id":"chi-2015-oehlberg"},{"id":"chi-2015-willett"},{"id":"chi-2017-aoki"},{"id":"chi-2017-hull"},{"id":"chi-2017-ledo"},{"id":"chi-2017-somanath"},{"id":"chi-2018-dillman"},{"id":"chi-2018-feick"},{"id":"chi-2018-heshmat"},{"id":"chi-2018-ledo"},{"id":"chi-2018-mahadevan"},{"id":"chi-2018-neustaedter"},{"id":"chi-2018-oh"},{"id":"chi-2018-suzuki"},{"id":"chi-2018-wuertz"},{"id":"chi-2019-danyluk"},{"id":"chi-2019-george"},{"id":"chi-2020-anjani"},{"id":"chi-2020-asha"},{"id":"chi-2020-goffin"},{"id":"chi-2020-hou"},{"id":"chi-2020-suzuki"},{"id":"chi-2021-danyluk"},{"id":"chi-2021-ens"},{"id":"chi-2021-hammad"},{"id":"chi-2022-bressa"},{"id":"chi-2022-ivanov"},{"id":"chi-2022-nittala"},{"id":"chi-2022-suzuki"},{"id":"chi-2023-dhawka"},{"id":"chi-2023-faridan"},{"id":"chi-2023-monteiro"},{"id":"chi-2024-bressa"},{"id":"chi-2024-dhawka"},{"id":"chi-2024-panigrahy"},{"id":"chi-2025-ghaneezabadi"},{"id":"chi-2025-madill"},{"id":"chi-2025-shiokawa"},{"id":"chi-ea-2022-blair"},{"id":"chi-ea-2023-chulpongsatorn"},{"id":"chi-ea-2023-fang"},{"id":"cmj-2020-ko"},{"id":"cnc-2019-hammad"},{"id":"cupum-2021-rout"},{"id":"dis-2016-jones"},{"id":"dis-2017-mok"},{"id":"dis-2018-mikalauskas"},{"id":"dis-2018-pham"},{"id":"dis-2018-ta"},{"id":"dis-2019-blair"},{"id":"dis-2019-bressa"},{"id":"dis-2019-ledo"},{"id":"dis-2019-mahadevan"},{"id":"dis-2019-nakayama"},{"id":"dis-2019-seyed"},{"id":"dis-2021-asha"},{"id":"dis-2021-blair"},{"id":"dis-2021-wannamaker"},{"id":"dis-2023-li"},{"id":"dis-2024-danyluk"},{"id":"frobt-2022-suzuki"},{"id":"gecco-2022-ivanov"},{"id":"gi-2020-rajabiyazdi"},{"id":"gi-2021-mactavish"},{"id":"gi-2022-hull"},{"id":"haid-2020-frisson"},{"id":"hri-2018-feick"},{"id":"httf-2024-blair"},{"id":"ieee-2021-willett"},{"id":"ijac-2021-hosseini"},{"id":"imwut-2020-wang"},{"id":"imx-2020-mok"},{"id":"iros-2020-hedayati"},{"id":"iros-2022-suzuki"},{"id":"mdpi-actuators-2024-piao"},{"id":"mdpi-arts-2023-frisson"},{"id":"mobilehci-2015-ledo"},{"id":"mobilehci-2019-hung"},{"id":"nime-2020-kirkegaard"},{"id":"nime-2020-ko"},{"id":"nime-2022-frisson"},{"id":"siggraph-labs-2023-seta"},{"id":"sui-2017-li"},{"id":"tei-2016-somanath"},{"id":"tei-2019-mikalauskas"},{"id":"tei-2019-tolley"},{"id":"tei-2019-wun"},{"id":"tei-2020-suzuki"},{"id":"tei-2021-pratte"},{"id":"tochi-2022-nittala"},{"id":"tvcg-2016-lopez"},{"id":"tvcg-2017-goffin"},{"id":"tvcg-2017-willett"},{"id":"tvcg-2019-blascheck"},{"id":"tvcg-2019-walny"},{"id":"tvcg-2020-danyluk"},{"id":"uist-2018-suzuki"},{"id":"uist-2019-suzuki"},{"id":"uist-2020-suzuki"},{"id":"uist-2020-yixian"},{"id":"uist-2021-suzuki"},{"id":"uist-2022-kaimoto"},{"id":"uist-2022-liao"},{"id":"uist-2022-nisser"},{"id":"uist-2022-nittala"},{"id":"uist-2023-chulpongsatorn"},{"id":"uist-2023-ihara"},{"id":"uist-2023-mukashev"},{"id":"uist-2023-xia"},{"id":"uist-2023-xia2"},{"id":"uist-2024-gunturu"},{"id":"uist-2024-roy"},{"id":"uist-sic-2022-faridan"},{"id":"vr-2019-satriadi"},{"id":"vrst-2022-frisson"}]},"__N_SSG":true},"page":"/publications/[id]","query":{"id":"chi-2024-dhawka"},"buildId":"WbVIuZ5asJ5nO_SIy0XCg","assetPrefix":"/pr-preview/pr-115","runtimeConfig":{"basePath":"/pr-preview/pr-115"},"isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>