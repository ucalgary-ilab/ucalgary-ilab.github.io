<!DOCTYPE html><html><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="format-detection" content="telephone=no"/><link href="https://use.fontawesome.com/releases/v5.1.1/css/all.css" rel="stylesheet"/><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.css" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/css/lightbox.css" rel="stylesheet"/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link href="/static/css/style.css" rel="stylesheet"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox-plus-jquery.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              $('.ui.sidebar')
                .sidebar('attach events', '.sidebar.icon')

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1" class="next-head"/><meta charSet="utf-8" class="next-head"/><title class="next-head">Ryo Suzuki | Interactions Lab - University of Calgary HCI Group</title><meta name="keywords" content="Human-Computer Interaction, HCI, Information Visualization, University of Calgary, CHI, UIST" class="next-head"/><meta name="description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" class="next-head"/><meta property="og:title" content="Ryo Suzuki | Interactions Lab - University of Calgary HCI Group" class="next-head"/><meta property="og:description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" class="next-head"/><meta property="og:site_name" content="University of Calgary Interactions Lab" class="next-head"/><meta property="og:url" content="https://ilab.ucalgary.ca/" class="next-head"/><meta property="og:image" content="https://ilab.ucalgary.ca/static/images/people/ryo-suzuki.jpg" class="next-head"/><meta property="og:type" content="website" class="next-head"/><meta name="twitter:title" content="Ryo Suzuki | Interactions Lab - University of Calgary HCI Group" class="next-head"/><meta name="twitter:description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" class="next-head"/><meta name="twitter:image" content="https://ilab.ucalgary.ca/static/images/people/ryo-suzuki.jpg" class="next-head"/><meta name="twitter:card" content="summary" class="next-head"/><meta name="twitter:site" content="@ucalgary" class="next-head"/><meta name="twitter:url" content="https://ilab.ucalgary.ca/" class="next-head"/><link rel="preload" href="/_next/static/hVbt0deeX6DfTDZPiuYzv/pages/person.js" as="script"/><link rel="preload" href="/_next/static/hVbt0deeX6DfTDZPiuYzv/pages/_app.js" as="script"/><link rel="preload" href="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.0c93cb8d3516282dd2c4.js" as="script"/><link rel="preload" href="/_next/static/runtime/main-563fdde58a64bdca21c4.js" as="script"/></head><body><div id="__next"><div><div><div class="ui right vertical sidebar menu"><a class="item" href="/">Home</a><a class="item" href="/publications">Publications</a><a class="item active" href="/people">People</a><a class="item" href="/courses">Courses</a><a class="item" href="/facility">Facility</a><a class="item" href="/seminar">Seminar</a><a class="item" href="/location">Location</a></div><div class="ui stackable secondary pointing container menu" style="border-bottom:none;margin-right:15%;font-size:1.1em"><div class="left menu"><a class="item" href="/"><b>UCalgary iLab</b></a></div><div class="right menu"><a class="item" href="/publications">Publications</a><a class="item active" href="/people">People</a><a class="item" href="/courses">Courses</a><a class="item" href="/facility">Facility</a><a class="item" href="/seminar">Seminar</a><a class="item" href="/location">Location</a><div class="toc item"><a href="/"><b>UCalgary iLab</b></a><i style="float:right" class="sidebar icon"></i></div></div></div></div><div class="ui stackable grid"><div class="one wide column"></div><div class="eleven wide column centered"><div id="person" class="category" style="text-align:center"><img class="ui circular image large-profile" src="/static/images/people/ryo-suzuki.jpg" style="margin:auto"/><h1>Ryo Suzuki</h1><p>Assistant Professor (CU Boulder)</p><p><a href="https://ryosuzuki.org" target="_blank"><i class="fas fa-link fa-fw"></i>https://ryosuzuki.org</a></p><p><a href="https://scholar.google.com/citations?user=klWjaQIAAAAJ" target="_blank"><i class="fas fa-graduation-cap fa-fw"></i>Google Scholar</a></p><div class="ui horizontal small divided link list"><div class="item"><a href="https://www.facebook.com/ryosuzk" target="_blank" style="font-size:1.2em"><i class="fab fa-facebook-square fa-fw"></i>ryosuzk</a></div><div class="item"><a href="https://twitter.com/ryosuzk" target="_blank" style="font-size:1.2em"><i class="fab fa-twitter fa-fw"></i>ryosuzk</a></div><div class="item"><a href="https://github.com/ryosuzuki" target="_blank" style="font-size:1.2em"><i class="fab fa-github-alt fa-fw"></i>ryosuzuki</a></div><div class="item"><a href="https://www.linkedin.com/in/ryosuzuki/" target="_blank" style="font-size:1.2em"><i class="fab fa-linkedin-in fa-fw"></i>LinkedIn</a></div><div class="item"><a href="mailto:ryo.suzuki@ucalgary.ca" target="_blank" style="font-size:1.2em"><i class="far fa-envelope fa-fw"></i>ryo.suzuki@ucalgary.ca</a></div></div></div><div id="publications" class="category"><h1 class="ui horizontal divider header"><i class="file alternate outline icon"></i>Publications</h1><div class="ui segment" style="margin-top:50px"><div class="publication ui vertical segment stackable grid" data-id="uist-2024-gunturu"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2024-gunturu.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2024</span><span class="ui big basic pink label"><b><i class="fas fa-trophy"></i> Best Paper</b></span></p><p class="color" style="font-size:1.3em"><b>Augmented Physics: Creating Interactive and Embedded Physics Simulations from Static Textbook Diagrams</b></p><p><a href="/people/aditya-gunturu"><img src="/static/images/people/aditya-gunturu.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Aditya Gunturu</span></a> , <span>Yi Wen</span> , <a href="/people/nandi-zhang"><img src="/static/images/people/nandi-zhang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Nandi Zhang</span></a> , <a href="/people/jarin-thundathil"><img src="/static/images/people/jarin-thundathil.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Jarin Thundathil</span></a> , <span>Rubaiat Habib Kazi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Physics Education</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Authoring Interfaces</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2023-chulpongsatorn"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2023</span></p><p class="color" style="font-size:1.3em"><b>Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks</b></p><p><a href="/people/neil-chulpongsatorn"><img src="/static/images/people/neil-chulpongsatorn.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Neil Chulpongsatorn</span></a> , <a href="/people/mille-skovhus-lunding"><img src="/static/images/people/mille-skovhus-lunding.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Mille Skovhus Lunding</span></a> , <a href="/people/nishan-soni"><img src="/static/images/people/no-profile-2.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Nishan Soni</span></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Authoring Interfaces</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2023-ihara"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2023-ihara.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2023</span></p><p class="color" style="font-size:1.3em"><b>HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality</b></p><p><a href="/people/keiichi-ihara"><img src="/static/images/people/keiichi-ihara.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Keiichi Ihara</span></a> , <a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Mehrad Faridan</span></a> , <span>Ayumi Ichikawa</span> , <span>Ikkaku Kawaguchi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Physical Telepresence</span><span class="ui brown basic label">Mobile Robots</span><span class="ui brown basic label">Actuated Tangible Ui</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2023-xia"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2023-xia.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2023</span></p><p class="color" style="font-size:1.3em"><b>RealityCanvas: Augmented Reality Sketching for Embedded and Responsive Scribble Animation Effects</b></p><p><a href="/people/zhijie-xia"><img src="/static/images/people/zhijie-xia.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Zhijie Xia</span></a> , <a href="/people/kyzyl-monteiro"><img src="/static/images/people/kyzyl-monteiro.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kyzyl Monteiro</span></a> , <a href="/people/kevin-van"><img src="/static/images/people/kevin-van.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kevin Van</span></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Sketching Interfaces</span><span class="ui brown basic label">Scribble Animation</span><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Real Time Authoring</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2023-li"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2023-li.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2023</span></p><p class="color" style="font-size:1.3em"><b>Physica: Interactive Tangible Physics Simulation based on Tabletop Mobile Robots Towards Explorable Physics Education</b></p><p><span>Jiatong Li</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Ken Nakagaki</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Physics Simulation</span><span class="ui brown basic label">Actuated Tangible Ui</span><span class="ui brown basic label">Swarm Ui</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2023-faridan"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2023-faridan.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2023</span></p><p class="color" style="font-size:1.3em"><b>ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms</b></p><p><a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Mehrad Faridan</span></a> , <a href="/people/bheesha-kumari"><img src="/static/images/people/bheesha-kumari.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Bheesha Kumari</span></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Visual Cue</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Telepresence</span><span class="ui brown basic label">Remote Guidance</span><span class="ui brown basic label">Human Surrogates</span><span class="ui brown basic label">Hands On Training</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2023-monteiro"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2023-monteiro.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2023</span></p><p class="color" style="font-size:1.3em"><b>Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching</b></p><p><a href="/people/kyzyl-monteiro"><img src="/static/images/people/kyzyl-monteiro.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kyzyl Monteiro</span></a> , <a href="/people/ritik-vatsal"><img src="/static/images/people/ritik-vatsal.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ritik Vatsal</span></a> , <a href="/people/neil-chulpongsatorn"><img src="/static/images/people/neil-chulpongsatorn.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Neil Chulpongsatorn</span></a> , <span>Aman Parnami</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Prototyping Tools</span><span class="ui brown basic label">Tangible Interactions</span><span class="ui brown basic label">Everyday Objects</span><span class="ui brown basic label">Interactive Machine Teaching</span><span class="ui brown basic label">Human Centered Machine Learning</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-ea-2023-chulpongsatorn"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-ea-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI EA 2023</span></p><p class="color" style="font-size:1.3em"><b>HoloTouch: Interacting with Mixed Reality Visualizations Through Smartphone Proxies</b></p><p><a href="/people/neil-chulpongsatorn"><img src="/static/images/people/neil-chulpongsatorn.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Neil Chulpongsatorn</span></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Embedded Data Visualization</span><span class="ui brown basic label">Tangible Interaction</span><span class="ui brown basic label">Cross Device Interaction</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-ea-2023-fang"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-ea-2023-fang.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI EA 2023</span></p><p class="color" style="font-size:1.3em"><b>VR Haptics at Home: Repurposing Everyday Objects and Environment for Casual and On-Demand VR Haptic Experiences</b></p><p><span>Cathy Mengying Fang</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Daniel Leithinger</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Interaction Techniques</span><span class="ui brown basic label">Passive Haptics</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2022-kaimoto"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2022-kaimoto.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2022</span></p><p class="color" style="font-size:1.3em"><b>Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI</b></p><p><a href="/people/hiroki-kaimoto"><img src="/static/images/people/hiroki-kaimoto.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Hiroki Kaimoto</span></a> , <a href="/people/kyzyl-monteiro"><img src="/static/images/people/kyzyl-monteiro.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kyzyl Monteiro</span></a> , <a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Mehrad Faridan</span></a> , <span>Jiatong Li</span> , <a href="/people/samin-farajian"><img src="/static/images/people/samin-farajian.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Samin Farajian</span></a> , <span>Yasuaki Kakehi</span> , <span>Ken Nakagaki</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Actuated Tangible Interfaces</span><span class="ui brown basic label">Swarm User Interfaces</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2022-liao"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2022-liao.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2022</span></p><p class="color" style="font-size:1.3em"><b>RealityTalk: Real-time Speech-driven Augmented Presentation for AR Live Storytelling</b></p><p><a href="/people/jian-liao"><img src="/static/images/people/jian-liao.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Jian Liao</span></a> , <a href="/people/adnan-karim"><img src="/static/images/people/adnan-karim.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Adnan Karim</span></a> , <a href="/people/shivesh-jadon"><img src="/static/images/people/shivesh-jadon.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Shivesh Jadon</span></a> , <span>Rubaiat Habib Kazi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Augmented Presentation</span><span class="ui brown basic label">Natural Language Processing</span><span class="ui brown basic label">Gestural And Speech Input</span><span class="ui brown basic label">Video</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2022-nisser"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2022-nisser.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2022</span></p><p class="color" style="font-size:1.3em"><b>Mixels: Fabricating Interfaces using Programmable Magnetic Pixels</b></p><p><span>Martin Nisser</span> , <span>Yashaswini Makaram</span> , <span>Lucian Covarrubias</span> , <span>Amadou Bah</span> , <span>Faraz Faruqi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Stefanie Mueller</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Programmable Materials</span><span class="ui brown basic label">Magnetic Interfaces</span><span class="ui brown basic label">Fabrication</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-sic-2022-faridan"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-sic-2022-faridan.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST SIC 2022</span><span class="ui big basic pink label"><b><i class="fas fa-award"></i> Honorable Mention</b></span></p><p class="color" style="font-size:1.3em"><b>UltraBots: Large-Area Mid-Air Haptics for VR with Robotically Actuated Ultrasound Transducers</b></p><p><a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Mehrad Faridan</span></a> , <a href="/people/marcus-friedel"><img src="/static/images/people/marcus-friedel.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Marcus Friedel</span></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Ultrasound Transducers</span><span class="ui brown basic label">Robotics</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="iros-2022-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/iros-2022-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IROS 2022</span></p><p class="color" style="font-size:1.3em"><b>Selective Self-Assembly using Re-Programmable Magnetic Pixels</b></p><p><span>Martin Nisser</span> , <span>Yashaswini Makaram</span> , <span>Faraz Faruqi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Stefanie Mueller</span></p><p></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2022-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2022-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2022</span></p><p class="color" style="font-size:1.3em"><b>Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <a href="/people/adnan-karim"><img src="/static/images/people/adnan-karim.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Adnan Karim</span></a> , <a href="/people/tian-xia"><img src="/static/images/people/tian-xia.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Tian Xia</span></a> , <span>Hooman Hedayati</span> , <a href="/people/nicolai-marquardt"><img src="/static/images/people/nicolai-marquardt.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Nicolai Marquardt</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Robotics</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Actuated Tangible UI</span><span class="ui brown basic label">Shape Changing UI</span><span class="ui brown basic label">AR HRI</span><span class="ui brown basic label">VAM HRI</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="frobt-2022-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/frobt-2022-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Frontiers 2022</span></p><p class="color" style="font-size:1.3em"><b>Designing Expandable-Structure Robots for Human-Robot Interaction</b></p><p><span>Hooman Hedayati</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Wyatt Rees1</span> , <span>Daniel Leithinger</span> , <span>Daniel Szafir</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Deployable Robot</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Modular Robot</span><span class="ui brown basic label">Origami Robotics</span><span class="ui brown basic label">Deployable Structures</span><span class="ui brown basic label">Shape Changing Robots</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2021-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2021-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2021</span></p><p class="color" style="font-size:1.3em"><b>HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Eyal Ofek</span> , <span>Mike Sinclair</span> , <span>Daniel Leithinger</span> , <span>Mar Gonzalez-Franco</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Encountered Type Haptics</span><span class="ui brown basic label">Tabletop Mobile Robots</span><span class="ui brown basic label">Swarm User Interfaces</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2020-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2020-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2020</span><span class="ui big basic pink label"><b><i class="fas fa-award"></i> Honorable Mention</b></span></p><p class="color" style="font-size:1.3em"><b>RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Rubaiat Habib Kazi</span> , <span>Li-Yi Wei</span> , <span>Stephen DiVerdi</span> , <span>Wilmot Li</span> , <span>Daniel Leithinger</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Embedded Data Visualization</span><span class="ui brown basic label">Real Time Authoring</span><span class="ui brown basic label">Sketching Interfaces</span><span class="ui brown basic label">Tangible Interaction</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="iros-2020-hedayati"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/iros-2020-hedayati.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IROS 2020</span></p><p class="color" style="font-size:1.3em"><b>PufferBot: Actuated Expandable Structures for Aerial Robots</b></p><p><span>Hooman Hedayati</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Daniel Leithinger</span> , <span>Daniel Szafir</span></p><p></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020</span></p><p class="color" style="font-size:1.3em"><b>RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Hooman Hedayati</span> , <span>Clement Zheng</span> , <span>James Bohn</span> , <span>Daniel Szafir</span> , <span>Ellen Yi-Luen Do</span> , <span>Mark D Gross</span> , <span>Daniel Leithinger</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Room Scale Haptics</span><span class="ui brown basic label">Haptic Interfaces</span><span class="ui brown basic label">Swarm Robots</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tei-2020-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2020-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TEI 2020</span></p><p class="color" style="font-size:1.3em"><b>LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Ryosuke Nakayama</span> , <span>Dan Liu</span> , <span>Yasuaki Kakehi</span> , <span>Mark D. Gross</span> , <span>Daniel Leithinger</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Shape Changing Interfaces</span><span class="ui brown basic label">Inflatables</span><span class="ui brown basic label">Large Scale Interactions</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2019-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2019-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2019</span></p><p class="color" style="font-size:1.3em"><b>ShapeBots: Shape-changing Swarm Robots</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Clement Zheng</span> , <span>Yasuaki Kakehi</span> , <span>Tom Yeh</span> , <span>Ellen Yi-Luen Do</span> , <span>Mark D. Gross</span> , <span>Daniel Leithinger</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Swarm User Interfaces</span><span class="ui brown basic label">Shape Changing User Interfaces</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2019-nakayama"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-nakayama.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2019</span><span class="ui big basic pink label"><b><i class="fas fa-trophy"></i> Best Paper</b></span></p><p class="color" style="font-size:1.3em"><b>MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction</b></p><p><span>Ryosuke Nakayama</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Satoshi Nakamaru</span> , <span>Ryuma Niiyama</span> , <span>Yoshihiro Kawahara</span> , <span>Yasuaki Kakehi</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Shape Changing Interfaces</span><span class="ui brown basic label">Programming By Demonstration</span><span class="ui brown basic label">Soft Robots</span><span class="ui brown basic label">Pneumatic Actuation</span><span class="ui brown basic label">Tangible Interactions</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2018-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2018-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2018</span></p><p class="color" style="font-size:1.3em"><b>Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Junichi Yamaoka</span> , <span>Daniel Leithinger</span> , <span>Tom Yeh</span> , <span>Mark D. Gross</span> , <span>Yoshihiro Kawahara</span> , <span>Yasuaki Kakehi</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Digital Materials</span><span class="ui brown basic label">Dynamic 3 D Printing</span><span class="ui brown basic label">Shape Displays</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-oh"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-oh.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-Based Electronic Devices</b></p><p><span>Hyunjoo Oh</span> , <span>Tung D. Ta</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Mark D. Gross</span> , <span>Yoshihiro Kawahara</span> , <span>Lining Yao</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Paper Electronics</span><span class="ui brown basic label">3 D Sculpting</span><span class="ui brown basic label">Paper Craft</span><span class="ui brown basic label">Fabrication Techniques</span><span class="ui brown basic label">Prototyping</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Jun Kato</span> , <span>Mark D. Gross</span> , <span>Tom Yeh</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Direct Manipulation</span><span class="ui brown basic label">Tangible Programming</span><span class="ui brown basic label">Swarm User Interfaces</span><span class="ui brown basic label">Programming By Demonstration</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="assets-2017-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/assets-2017-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">ASSETS 2020</span></p><p class="color" style="font-size:1.3em"><b>FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Abigale Stangl</span> , <span>Mark D. Gross</span> , <span>Tom Yeh</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Visual Impairment</span><span class="ui brown basic label">Dynamic Tactile Markers</span><span class="ui brown basic label">Tangible Interfaces</span><span class="ui brown basic label">Interactive Tactile Graphics</span></div></p></div></div></div><div id="publications-modal"><div id="uist-2024-gunturu" class="ui large modal"><div class="header"><a href="/publications/uist-2024-gunturu" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2024-gunturu</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2024-gunturu.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2024-gunturu" target="_blank">Augmented Physics: Creating Interactive and Embedded Physics Simulations from Static Textbook Diagrams</a></h1><p class="meta"><a href="/people/aditya-gunturu"><img src="/static/images/people/aditya-gunturu.jpg" class="ui circular spaced image mini-profile"/><strong>Aditya Gunturu</strong></a> , <span>Yi Wen</span> , <a href="/people/nandi-zhang"><img src="/static/images/people/nandi-zhang.jpg" class="ui circular spaced image mini-profile"/><strong>Nandi Zhang</strong></a> , <a href="/people/jarin-thundathil"><img src="/static/images/people/jarin-thundathil.jpg" class="ui circular spaced image mini-profile"/><strong>Jarin Thundathil</strong></a> , <span>Rubaiat Habib Kazi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/uist-2024-gunturu.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2024-gunturu.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/MOdSeUp8YcE" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/MOdSeUp8YcE?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/MOdSeUp8YcE/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce Augmented Physics, a machine learning-integrated authoring tool designed for creating embedded interactive physics simulations from static textbook diagrams. Leveraging recent advancements in computer vision, such as Segment Anything and Multi-modal LLMs, our web-based system enables users to semi-automatically extract diagrams from physics textbooks and generate interactive simulations based on the extracted content. These interactive diagrams are seamlessly integrated into scanned textbook pages, facilitating interactive and personalized learning experiences across various physics concepts, such as optics, circuits, and kinematics. Drawing from an elicitation study with seven physics instructors, we explore four key augmentation strategies: 1) augmented experiments, 2) animated diagrams, 3) bi-directional binding, and 4) parameter visualization. We evaluate our system through technical evaluation, a usability study (N=12), and expert interviews (N=12). Study findings suggest that our system can facilitate more engaging and personalized learning experiences in physics education.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Physics Education</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Authoring Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Aditya Gunturu<!-- -->, <!-- -->Yi Wen<!-- -->, <!-- -->Nandi Zhang<!-- -->, <!-- -->Jarin Thundathil<!-- -->, <!-- -->Rubaiat Habib Kazi<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>Augmented Physics: Creating Interactive and Embedded Physics Simulations from Static Textbook Diagrams</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;24)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3654777.3676392" target="_blank">https://doi.org/10.1145/3654777.3676392</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2023-chulpongsatorn" class="ui large modal"><div class="header"><a href="/publications/uist-2023-chulpongsatorn" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2023-chulpongsatorn</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2023-chulpongsatorn" target="_blank">Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks</a></h1><p class="meta"><a href="/people/neil-chulpongsatorn"><img src="/static/images/people/neil-chulpongsatorn.jpg" class="ui circular spaced image mini-profile"/><strong>Neil Chulpongsatorn</strong></a> , <a href="/people/mille-skovhus-lunding"><img src="/static/images/people/mille-skovhus-lunding.jpg" class="ui circular spaced image mini-profile"/><strong>Mille Skovhus Lunding</strong></a> , <a href="/people/nishan-soni"><img src="/static/images/people/nishan-soni.jpg" class="ui circular spaced image mini-profile"/><strong>Nishan Soni</strong></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/uist-2023-chulpongsatorn.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2023-chulpongsatorn.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Zv6JQ5T-qn0" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Zv6JQ5T-qn0?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/Zv6JQ5T-qn0/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce Augmented Math, a machine learning-based approach to authoring AR explorable explanations by augmenting static math textbooks without programming. To augment a static document, our system first extracts mathematical formulas and figures from a given document using optical character recognition (OCR) and computer vision. By binding and manipulating these extracted contents, the user can see the interactive animation overlaid onto the document through mobile AR interfaces. This empowers non-technical users, such as teachers or students, to transform existing math textbooks and handouts into on-demand and personalized explorable explanations. To design our system, we first analyzed existing explorable math explanations to identify common design strategies. Based on the findings, we developed a set of augmentation techniques that can be automatically generated based on the extracted content, which are 1) dynamic values, 2) interactive figures, 3) relationship highlights, 4) concrete examples, and 5) step-by-step hints. To evaluate our system, we conduct two user studies: preliminary user testing and expert interviews. The study results confirm that our system allows more engaging experiences for learning math concepts.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Authoring Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Neil Chulpongsatorn<!-- -->, <!-- -->Mille Skovhus Lunding<!-- -->, <!-- -->Nishan Soni<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->16<!-- -->.  DOI: <a href="https://doi.org/10.1145/3586183.3606827" target="_blank">https://doi.org/10.1145/3586183.3606827</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2023-ihara" class="ui large modal"><div class="header"><a href="/publications/uist-2023-ihara" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2023-ihara</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2023-ihara.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2023-ihara" target="_blank">HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality</a></h1><p class="meta"><a href="/people/keiichi-ihara"><img src="/static/images/people/keiichi-ihara.jpg" class="ui circular spaced image mini-profile"/><strong>Keiichi Ihara</strong></a> , <a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><strong>Mehrad Faridan</strong></a> , <span>Ayumi Ichikawa</span> , <span>Ikkaku Kawaguchi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/uist-2023-ihara.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2023-ihara.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/KSBPtiXy8Hg" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/KSBPtiXy8Hg?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/KSBPtiXy8Hg/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces HoloBots, a mixed reality remote collaboration system that augments holographic telepresence with synchronized mobile robots. Beyond existing mixed reality telepresence, HoloBots lets remote users not only be visually and spatially present, but also physically engage with local users and their environment. HoloBots allows the users to touch, grasp, manipulate, and interact with the remote physical environment as if they were co-located in the same shared space. We achieve this by synchronizing holographic user motion (Hololens 2 and Azure Kinect) with tabletop mobile robots (Sony Toio). Beyond the existing physical telepresence, HoloBots contributes to an exploration of broader design space, such as object actuation, virtual hand physicalization, world-in-miniature exploration, shared tangible interfaces, embodied guidance, and haptic communication. We evaluate our system with twelve participants by comparing it with hologram-only and robot-only conditions. Both quantitative and qualitative results confirm that our system significantly enhances the level of co-presence and shared experience, compared to the other conditions.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Physical Telepresence</span><span class="ui brown basic label">Mobile Robots</span><span class="ui brown basic label">Actuated Tangible Ui</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Keiichi Ihara<!-- -->, <!-- -->Mehrad Faridan<!-- -->, <!-- -->Ayumi Ichikawa<!-- -->, <!-- -->Ikkaku Kawaguchi<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3586183.3606727" target="_blank">https://doi.org/10.1145/3586183.3606727</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2023-xia" class="ui large modal"><div class="header"><a href="/publications/uist-2023-xia" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2023-xia</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2023-xia.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2023-xia" target="_blank">RealityCanvas: Augmented Reality Sketching for Embedded and Responsive Scribble Animation Effects</a></h1><p class="meta"><a href="/people/zhijie-xia"><img src="/static/images/people/zhijie-xia.jpg" class="ui circular spaced image mini-profile"/><strong>Zhijie Xia</strong></a> , <a href="/people/kyzyl-monteiro"><img src="/static/images/people/kyzyl-monteiro.jpg" class="ui circular spaced image mini-profile"/><strong>Kyzyl Monteiro</strong></a> , <a href="/people/kevin-van"><img src="/static/images/people/kevin-van.jpg" class="ui circular spaced image mini-profile"/><strong>Kevin Van</strong></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/uist-2023-xia.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2023-xia.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/HVOgH1quDsc" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/HVOgH1quDsc?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/HVOgH1quDsc/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce RealityCanvas, a mobile AR sketching tool that can easily augment real-world physical motion with responsive hand-drawn animation. Recent research in AR sketching tools has enabled users to not only embed static drawings into the real world but also dynamically animate them with physical motion. However, existing tools often lack the flexibility and expressiveness of possible animations, as they primarily support simple line-based geometry. To address this limitation, we explore both expressive and improvisational AR sketched animation by introducing a set of responsive scribble animation techniques that can be directly embedded through sketching interactions: 1) object binding, 2) flip-book animation, 3) action trigger, 4) particle effects, 5) motion trajectory, and 6) contour highlight. These six animation effects were derived from the analysis of 172 existing video-edited scribble animations. We showcase these techniques through various applications, such as video creation, augmented education, storytelling, and AR prototyping. The results of our user study and expert interviews confirm that our tool can lower the barrier to creating AR-based sketched animation, while allowing creative, expressive, and improvisational AR sketching experiences.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Sketching Interfaces</span><span class="ui brown basic label">Scribble Animation</span><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Real Time Authoring</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Zhijie Xia<!-- -->, <!-- -->Kyzyl Monteiro<!-- -->, <!-- -->Kevin Van<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>RealityCanvas: Augmented Reality Sketching for Embedded and Responsive Scribble Animation Effects</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->14<!-- -->.  DOI: <a href="https://doi.org/10.1145/3586183.3606716" target="_blank">https://doi.org/10.1145/3586183.3606716</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2023-li" class="ui large modal"><div class="header"><a href="/publications/dis-2023-li" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2023-li</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2023-li.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2023-li" target="_blank">Physica: Interactive Tangible Physics Simulation based on Tabletop Mobile Robots Towards Explorable Physics Education</a></h1><p class="meta"><span>Jiatong Li</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Ken Nakagaki</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/dis-2023-li.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>dis-2023-li.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/7DKpq52282g" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/7DKpq52282g?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/7DKpq52282g/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>In this paper, we introduce Physica, a tangible physics simulation system and approach based on tabletop mobile robots. In Physica, each tabletop robot can physically represent distinct simulated objects that are controlled through an underlying physics simulation, such as gravitational force, molecular movement, and spring force. It aims to bring the benefits of tangible and haptic interaction into explorable physics learning, which was traditionally only available on screen-based interfaces. The system utilizes off-the-shelf mobile robots (Sony Toio) and an open-source physics simulation tool (Teilchen). Built on top of them, we implement the interaction software pipeline that consists of 1) an event detector to reflect tangible interaction by users, and 2) target speed control to minimize the gap between the robot motion and simulated moving objects. To present the potential for physics education, we demonstrate various application scenarios that illustrate different forms of learning using Physica. In our user study, we investigate the effect and the potential of our approach through a perception study and interviews with physics educators.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Physics Simulation</span><span class="ui brown basic label">Actuated Tangible Ui</span><span class="ui brown basic label">Swarm Ui</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jiatong Li<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Ken Nakagaki<!-- -->. <b>Physica: Interactive Tangible Physics Simulation based on Tabletop Mobile Robots Towards Explorable Physics Education</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->15<!-- -->.  DOI: <a href="https://doi.org/10.1145/3526113.3545626" target="_blank">https://doi.org/10.1145/3526113.3545626</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2023-faridan" class="ui large modal"><div class="header"><a href="/publications/chi-2023-faridan" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2023-faridan</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2023-faridan.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2023-faridan" target="_blank">ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms</a></h1><p class="meta"><a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><strong>Mehrad Faridan</strong></a> , <a href="/people/bheesha-kumari"><img src="/static/images/people/bheesha-kumari.jpg" class="ui circular spaced image mini-profile"/><strong>Bheesha Kumari</strong></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/chi-2023-faridan.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2023-faridan.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/VOe3fETd3sk" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/VOe3fETd3sk?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/VOe3fETd3sk/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present ChameleonControl, a real-human teleoperation system for scalable remote instruction in hands-on classrooms. In contrast to the existing video or AR/VR-based remote hands-on education, ChameleonControl uses a real human as a surrogate of a remote instructor. Building on existing human-based telepresence approaches (e.g. ChameleonMask), we contribute a novel method to teleoperate a human surrogate through synchronized mixed reality (MR) hand gestural navigation and verbal communication. By overlaying the remote instructor&#x27;s virtual hands in the local user&#x27;s MR view, the remote instructor can guide and control the local user as if they were physically present. This allows the local user/surrogate to synchronize their hand movements and gestures with the remote instructor, effectively ``teleoperating&#x27;&#x27; a real human. We evaluate our system through the in-the-wild deployment for physiotherapy classrooms, as well as lab-based experiments for other application domains such as mechanical assembly, sign language, and cooking lessons. The study results confirm that our approach can increase engagement and the sense of co-presence, showing potential for the future of remote hands-on classrooms.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Visual Cue</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Telepresence</span><span class="ui brown basic label">Remote Guidance</span><span class="ui brown basic label">Human Surrogates</span><span class="ui brown basic label">Hands On Training</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Mehrad Faridan<!-- -->, <!-- -->Bheesha Kumari<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3544548.3581381" target="_blank">https://doi.org/10.1145/3544548.3581381</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2023-monteiro" class="ui large modal"><div class="header"><a href="/publications/chi-2023-monteiro" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2023-monteiro</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2023-monteiro.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2023-monteiro" target="_blank">Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching</a></h1><p class="meta"><a href="/people/kyzyl-monteiro"><img src="/static/images/people/kyzyl-monteiro.jpg" class="ui circular spaced image mini-profile"/><strong>Kyzyl Monteiro</strong></a> , <a href="/people/ritik-vatsal"><img src="/static/images/people/ritik-vatsal.jpg" class="ui circular spaced image mini-profile"/><strong>Ritik Vatsal</strong></a> , <a href="/people/neil-chulpongsatorn"><img src="/static/images/people/neil-chulpongsatorn.jpg" class="ui circular spaced image mini-profile"/><strong>Neil Chulpongsatorn</strong></a> , <span>Aman Parnami</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/chi-2023-monteiro.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2023-monteiro.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/JssiyfrhIJw" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/JssiyfrhIJw?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/JssiyfrhIJw/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces Teachable Reality, an augmented reality (AR) prototyping tool for creating interactive tangible AR applications with arbitrary everyday objects. Teachable Reality leverages vision-based interactive machine teaching (e.g., Teachable Machine), which captures real-world interactions for AR prototyping. It identifies the user-defined tangible and gestural interactions using an on-demand computer vision model. Based on this, the user can easily create functional AR prototypes without programming, enabled by a trigger-action authoring interface. Therefore, our approach allows the flexibility, customizability, and generalizability of tangible AR applications that can address the limitation of current marker-based approaches. We explore the design space and demonstrate various AR prototypes, which include tangible and deformable interfaces, context-aware assistants, and body-driven AR applications. The results of our user study and expert interviews confirm that our approach can lower the barrier to creating functional AR prototypes while also allowing flexible and general-purpose prototyping experiences.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Prototyping Tools</span><span class="ui brown basic label">Tangible Interactions</span><span class="ui brown basic label">Everyday Objects</span><span class="ui brown basic label">Interactive Machine Teaching</span><span class="ui brown basic label">Human Centered Machine Learning</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kyzyl Monteiro<!-- -->, <!-- -->Ritik Vatsal<!-- -->, <!-- -->Neil Chulpongsatorn<!-- -->, <!-- -->Aman Parnami<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->15<!-- -->.  DOI: <a href="https://doi.org/10.1145/3544548.3581449" target="_blank">https://doi.org/10.1145/3544548.3581449</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-ea-2023-chulpongsatorn" class="ui large modal"><div class="header"><a href="/publications/chi-ea-2023-chulpongsatorn" target="_blank"><i class="fas fa-link fa-fw"></i>chi-ea-2023-chulpongsatorn</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI EA 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-ea-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-ea-2023-chulpongsatorn" target="_blank">HoloTouch: Interacting with Mixed Reality Visualizations Through Smartphone Proxies</a></h1><p class="meta"><a href="/people/neil-chulpongsatorn"><img src="/static/images/people/neil-chulpongsatorn.jpg" class="ui circular spaced image mini-profile"/><strong>Neil Chulpongsatorn</strong></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/chi-ea-2023-chulpongsatorn.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-ea-2023-chulpongsatorn.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We contribute interaction techniques for augmenting mixed reality (MR) visualizations with smartphone proxies. By combining head-mounted displays (HMDs) with mobile touchscreens, we can augment low-resolution holographic 3D charts with precise touch input, haptics feedback, high-resolution 2D graphics, and physical manipulation. Our approach aims to complement both MR and physical visualizations. Most current MR visualizations suffer from unreliable tracking, low visual resolution, and imprecise input. Data physicalizations on the other hand, although allowing for natural physical manipulation, are limited in dynamic and interactive modification. We demonstrate how mobile devices such as smartphones or tablets can serve as physical proxies for MR data interactions, creating dynamic visualizations that support precise manipulation and rich input and output. We describe 6 interaction techniques that leverage the combined physicality, sensing, and output capabilities of HMDs and smartphones, and demonstrate those interactions via a prototype system. Based on an evaluation, we outline opportunities for combining the advantages of both MR and physical charts.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Embedded Data Visualization</span><span class="ui brown basic label">Tangible Interaction</span><span class="ui brown basic label">Cross Device Interaction</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Neil Chulpongsatorn<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>HoloTouch: Interacting with Mixed Reality Visualizations Through Smartphone Proxies</b>. <i>In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->8<!-- -->.  DOI: <a href="https://doi.org/10.1145/3544549.3585738" target="_blank">https://doi.org/10.1145/3544549.3585738</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-ea-2023-fang" class="ui large modal"><div class="header"><a href="/publications/chi-ea-2023-fang" target="_blank"><i class="fas fa-link fa-fw"></i>chi-ea-2023-fang</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI EA 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-ea-2023-fang.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-ea-2023-fang" target="_blank">VR Haptics at Home: Repurposing Everyday Objects and Environment for Casual and On-Demand VR Haptic Experiences</a></h1><p class="meta"><span>Cathy Mengying Fang</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Daniel Leithinger</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/chi-ea-2023-fang.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-ea-2023-fang.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces VR Haptics at Home, a method of repurposing everyday objects in the home to provide casual and on-demand haptic experiences. Current VR haptic devices are often expensive, complex, and unreliable, which limits the opportunities for rich haptic experiences outside research labs. In contrast, we envision that, by repurposing everyday objects as passive haptics props, we can create engaging VR experiences for casual uses with minimal cost and setup. To explore and evaluate this idea, we conducted an in-the-wild study with eight participants, in which they used our proof-of-concept system to turn their surrounding objects such as chairs, tables, and pillows at their own homes into haptic props. The study results show that our method can be adapted to different homes and environments, enabling more engaging VR experiences without the need for complex setup process. Based on our findings, we propose a possible design space to showcase the potential for future investigation.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Interaction Techniques</span><span class="ui brown basic label">Passive Haptics</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Cathy Mengying Fang<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Daniel Leithinger<!-- -->. <b>VR Haptics at Home: Repurposing Everyday Objects and Environment for Casual and On-Demand VR Haptic Experiences</b>. <i>In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->7<!-- -->.  DOI: <a href="https://doi.org/10.1145/3544549.3585871" target="_blank">https://doi.org/10.1145/3544549.3585871</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2022-kaimoto" class="ui large modal"><div class="header"><a href="/publications/uist-2022-kaimoto" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2022-kaimoto</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2022-kaimoto.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2022-kaimoto" target="_blank">Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI</a></h1><p class="meta"><a href="/people/hiroki-kaimoto"><img src="/static/images/people/hiroki-kaimoto.jpg" class="ui circular spaced image mini-profile"/><strong>Hiroki Kaimoto</strong></a> , <a href="/people/kyzyl-monteiro"><img src="/static/images/people/kyzyl-monteiro.jpg" class="ui circular spaced image mini-profile"/><strong>Kyzyl Monteiro</strong></a> , <a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><strong>Mehrad Faridan</strong></a> , <span>Jiatong Li</span> , <a href="/people/samin-farajian"><img src="/static/images/people/samin-farajian.jpg" class="ui circular spaced image mini-profile"/><strong>Samin Farajian</strong></a> , <span>Yasuaki Kakehi</span> , <span>Ken Nakagaki</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/uist-2022-kaimoto.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2022-kaimoto.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/xy-IeVgoEpY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/xy-IeVgoEpY?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/xy-IeVgoEpY/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces Sketched Reality, an approach that com- bines AR sketching and actuated tangible user interfaces (TUI) for bi-directional sketching interaction. Bi-directional sketching enables virtual sketches and physical objects to affect each other through physical actuation and digital computation. In the existing AR sketching, the relationship between virtual and physical worlds is only one-directional --- while physical interaction can affect virtual sketches, virtual sketches have no return effect on the physical objects or environment. In contrast, bi-directional sketching interaction allows the seamless coupling between sketches and actuated TUIs. In this paper, we employ tabletop-size small robots (Sony Toio) and an iPad-based AR sketching tool to demonstrate the concept. In our system, virtual sketches drawn and simulated on an iPad (e.g., lines, walls, pendulums, and springs) can move, actuate, collide, and constrain physical Toio robots, as if virtual sketches and the physical objects exist in the same space through seamless coupling between AR and robot motion. This paper contributes a set of novel interactions and a design space of bi-directional AR sketching. We demonstrate a series of potential applications, such as tangible physics education, explorable mechanism, tangible gaming for children, and in-situ robot programming via sketching.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Actuated Tangible Interfaces</span><span class="ui brown basic label">Swarm User Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Hiroki Kaimoto<!-- -->, <!-- -->Kyzyl Monteiro<!-- -->, <!-- -->Mehrad Faridan<!-- -->, <!-- -->Jiatong Li<!-- -->, <!-- -->Samin Farajian<!-- -->, <!-- -->Yasuaki Kakehi<!-- -->, <!-- -->Ken Nakagaki<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3526113.3545626" target="_blank">https://doi.org/10.1145/3526113.3545626</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2022-liao" class="ui large modal"><div class="header"><a href="/publications/uist-2022-liao" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2022-liao</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2022-liao.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2022-liao" target="_blank">RealityTalk: Real-time Speech-driven Augmented Presentation for AR Live Storytelling</a></h1><p class="meta"><a href="/people/jian-liao"><img src="/static/images/people/jian-liao.jpg" class="ui circular spaced image mini-profile"/><strong>Jian Liao</strong></a> , <a href="/people/adnan-karim"><img src="/static/images/people/adnan-karim.jpg" class="ui circular spaced image mini-profile"/><strong>Adnan Karim</strong></a> , <a href="/people/shivesh-jadon"><img src="/static/images/people/shivesh-jadon.jpg" class="ui circular spaced image mini-profile"/><strong>Shivesh Jadon</strong></a> , <span>Rubaiat Habib Kazi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/uist-2022-liao.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2022-liao.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/vfIMeICV-7c" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/vfIMeICV-7c?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/vfIMeICV-7c/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present RealityTalk, a system that augments real-time live presentations with speech-driven interactive virtual elements. Augmented presentations leverage embedded visuals and animation for engaging and expressive storytelling. However, existing tools for live presentations often lack interactivity and improvisation, while creating such effects in video editing tools require significant time and expertise. RealityTalk enables users to create live augmented presentations with real-time speech-driven interactions. The user can interactively prompt, move, and manipulate graphical elements through real-time speech and supporting modalities. Based on our analysis of 177 existing video-edited augmented presentations, we propose a novel set of interaction techniques and then incorporated them into RealityTalk. We evaluate our tool from a presenter’s perspective to demonstrate the effectiveness of our system.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Augmented Presentation</span><span class="ui brown basic label">Natural Language Processing</span><span class="ui brown basic label">Gestural And Speech Input</span><span class="ui brown basic label">Video</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jian Liao<!-- -->, <!-- -->Adnan Karim<!-- -->, <!-- -->Shivesh Jadon<!-- -->, <!-- -->Rubaiat Habib Kazi<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>RealityTalk: Real-time Speech-driven Augmented Presentation for AR Live Storytelling</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3526113.3545702" target="_blank">https://doi.org/10.1145/3526113.3545702</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2022-nisser" class="ui large modal"><div class="header"><a href="/publications/uist-2022-nisser" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2022-nisser</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2022-nisser.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2022-nisser" target="_blank">Mixels: Fabricating Interfaces using Programmable Magnetic Pixels</a></h1><p class="meta"><span>Martin Nisser</span> , <span>Yashaswini Makaram</span> , <span>Lucian Covarrubias</span> , <span>Amadou Bah</span> , <span>Faraz Faruqi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Stefanie Mueller</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/uist-2022-nisser.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2022-nisser.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/6SvFCQkVFtw" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/6SvFCQkVFtw?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/6SvFCQkVFtw/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>In this paper, we present Mixels, programmable magnetic pixels that can be rapidly fabricated using an electromagnetic printhead mounted on an off-the-shelve 3-axis CNC machine. The ability to program magnetic material pixel-wise with varying magnetic force enables Mixels to create new tangible, tactile, and haptic interfaces. To facilitate the creation of interactive objects with Mixels, we provide a user interface that lets users specify the high-level magnetic behavior and that then computes the underlying magnetic pixel assignments and fabrication instructions to program the magnetic surface. Our custom hardware add-on based on an electromagnetic printhead and hall effect sensor clips onto a standard 3-axis CNC machine and can both write and read magnetic pixel values from magnetic material. Our evaluation shows that our system can reliably program and read magnetic pixels of various strengths, that we can predict the behavior of two interacting magnetic surfaces before programming them, that our electromagnet is strong enough to create pixels that utilize the maximum magnetic strength of the material being programmed, and that this material remains magnetized when removed from the magnetic plotter.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Programmable Materials</span><span class="ui brown basic label">Magnetic Interfaces</span><span class="ui brown basic label">Fabrication</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Martin Nisser<!-- -->, <!-- -->Yashaswini Makaram<!-- -->, <!-- -->Lucian Covarrubias<!-- -->, <!-- -->Amadou Bah<!-- -->, <!-- -->Faraz Faruqi<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Stefanie Mueller<!-- -->. <b>Mixels: Fabricating Interfaces using Programmable Magnetic Pixels</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3526113.3545698" target="_blank">https://doi.org/10.1145/3526113.3545698</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-sic-2022-faridan" class="ui large modal"><div class="header"><a href="/publications/uist-sic-2022-faridan" target="_blank"><i class="fas fa-link fa-fw"></i>uist-sic-2022-faridan</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST SIC 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-sic-2022-faridan.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-sic-2022-faridan" target="_blank">UltraBots: Large-Area Mid-Air Haptics for VR with Robotically Actuated Ultrasound Transducers</a></h1><p class="meta"><a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><strong>Mehrad Faridan</strong></a> , <a href="/people/marcus-friedel"><img src="/static/images/people/marcus-friedel.jpg" class="ui circular spaced image mini-profile"/><strong>Marcus Friedel</strong></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/uist-sic-2022-faridan.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-sic-2022-faridan.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/jMYAQzzQ_PI" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/jMYAQzzQ_PI?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/jMYAQzzQ_PI/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce UltraBots, a system that combines ultrasound haptic feedback and robotic actuation for large-area mid-air haptics for VR. Ultrasound haptics can provide precise mid-air haptic feedback and versatile shape rendering, but the interaction area is often limited by the small size of the ultrasound devices, restricting the possible interactions for VR. To address this problem, this paper introduces a novel approach that combines robotic actuation with ultrasound haptics. More specifically, we will attach ultrasound transducer arrays to tabletop mobile robots or robotic arms for scalable, extendable, and translatable interaction areas. We plan to use Sony Toio robots for 2D translation and/or commercially available robotic arms for 3D translation. Using robotic actuation and hand tracking measured by a VR HMD (ex: Oculus Quest), our system can keep the ultrasound transducers underneath the user’s hands to provide on-demand haptics. We demonstrate applications with workspace environments, medical training, education and entertainment.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Ultrasound Transducers</span><span class="ui brown basic label">Robotics</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Mehrad Faridan<!-- -->, <!-- -->Marcus Friedel<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>UltraBots: Large-Area Mid-Air Haptics for VR with Robotically Actuated Ultrasound Transducers</b>. <i>In Adjunct undefined (UIST SIC &#x27;22)</i>. <!-- -->  Page: 1-<!-- -->3<!-- -->.  DOI: <a href="https://doi.org/10.1145/3526114.3561350" target="_blank">https://doi.org/10.1145/3526114.3561350</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="iros-2022-suzuki" class="ui large modal"><div class="header"><a href="/publications/iros-2022-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>iros-2022-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">IROS 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/iros-2022-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/iros-2022-suzuki" target="_blank">Selective Self-Assembly using Re-Programmable Magnetic Pixels</a></h1><p class="meta"><span>Martin Nisser</span> , <span>Yashaswini Makaram</span> , <span>Faraz Faruqi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Stefanie Mueller</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/iros-2022-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>iros-2022-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/HK9_ynH6A6w" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/HK9_ynH6A6w?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/HK9_ynH6A6w/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces a method to generate highly selective encodings that can be magnetically programmed onto physical modules to enable them to self-assemble in chosen configurations. We generate these encodings based on Hadamard matrices, and show how to design the faces of modules to be maximally attractive to their intended mate, while remaining maximally agnostic to other faces. We derive guarantees on these bounds, and verify their attraction and agnosticism experimentally. Using cubic modules whose faces have been covered in soft magnetic material, we show how inexpensive, passive modules with planar faces can be used to selectively self-assemble into target shapes without geometric guides. We show that these modules can be easily re-programmed for new target shapes using a CNC-based magnetic plotter, and demonstrate self-assembly of 8 cubes in a water tank.</p></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Martin Nisser<!-- -->, <!-- -->Yashaswini Makaram<!-- -->, <!-- -->Faraz Faruqi<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Stefanie Mueller<!-- -->. <b>Selective Self-Assembly using Re-Programmable Magnetic Pixels</b>. <i>In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS &#x27;22)</i>. <!-- -->IEEE, New York, NY, USA<!-- -->  Page: 1-<!-- -->8<!-- -->.  DOI: <a target="_blank"></a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2022-suzuki" class="ui large modal"><div class="header"><a href="/publications/chi-2022-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2022-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2022-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2022-suzuki" target="_blank">Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <a href="/people/adnan-karim"><img src="/static/images/people/adnan-karim.jpg" class="ui circular spaced image mini-profile"/><strong>Adnan Karim</strong></a> , <a href="/people/tian-xia"><img src="/static/images/people/tian-xia.jpg" class="ui circular spaced image mini-profile"/><strong>Tian Xia</strong></a> , <span>Hooman Hedayati</span> , <a href="/people/nicolai-marquardt"><img src="/static/images/people/nicolai-marquardt.jpg" class="ui circular spaced image mini-profile"/><strong>Nicolai Marquardt</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/chi-2022-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2022-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/MvOWxQC_4uQ" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/MvOWxQC_4uQ?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/MvOWxQC_4uQ/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper contributes to a taxonomy of augmented reality and robotics based on a survey of 460 research papers. Augmented and mixed reality (AR/MR) have emerged as a new way to enhance human-robot interaction (HRI) and robotic interfaces (e.g., actuated and shape-changing interfaces). Recently, an increasing number of studies in HCI, HRI, and robotics have demonstrated how AR enables better interactions between people and robots. However, often research remains focused on individual explorations and key design strategies, and research questions are rarely analyzed systematically. In this paper, we synthesize and categorize this research field in the following dimensions: 1) approaches to augmenting reality; 2) characteristics of robots; 3) purposes and benefits; 4) classification of presented information; 5) design components and strategies for visual augmentation; 6) interaction techniques and modalities; 7) application domains; and 8) evaluation strategies. We formulate key challenges and opportunities to guide and inform future research in AR and robotics.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Robotics</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Actuated Tangible UI</span><span class="ui brown basic label">Shape Changing UI</span><span class="ui brown basic label">AR HRI</span><span class="ui brown basic label">VAM HRI</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Adnan Karim<!-- -->, <!-- -->Tian Xia<!-- -->, <!-- -->Hooman Hedayati<!-- -->, <!-- -->Nicolai Marquardt<!-- -->. <b>Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->33<!-- -->.  DOI: <a href="https://doi.org/10.1145/3491102.3517719" target="_blank">https://doi.org/10.1145/3491102.3517719</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="frobt-2022-suzuki" class="ui large modal"><div class="header"><a href="/publications/frobt-2022-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>frobt-2022-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">Frontiers 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/frobt-2022-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/frobt-2022-suzuki" target="_blank">Designing Expandable-Structure Robots for Human-Robot Interaction</a></h1><p class="meta"><span>Hooman Hedayati</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Wyatt Rees1</span> , <span>Daniel Leithinger</span> , <span>Daniel Szafir</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/frobt-2022-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>frobt-2022-suzuki.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this paper, we survey the emerging design space of expandable structures in robotics, with a focus on how such structures may improve human-robot interactions. We detail various implementation considerations for researchers seeking to integrate such structures in their own work and describe how expandable structures may lead to novel forms of interaction for a variety of different robots and applications, including structures that enable robots to alter their form to augment or gain entirely new capabilities, such as enhancing manipulation or navigation, structures that improve robot safety, structures that enable new forms of communication, and structures for robot swarms that enable the swarm to change shape both individually and collectively. To illustrate how these considerations may be operationalized, we also present three case studies from our own research in expandable structure robots, sharing our design process and our findings regarding how such structures enable robots to produce novel behaviors that may capture human attention, convey information, mimic emotion, and provide new types of dynamic affordances.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Deployable Robot</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Modular Robot</span><span class="ui brown basic label">Origami Robotics</span><span class="ui brown basic label">Deployable Structures</span><span class="ui brown basic label">Shape Changing Robots</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Hooman Hedayati<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Wyatt Rees1<!-- -->, <!-- -->Daniel Leithinger<!-- -->, <!-- -->Daniel Szafir<!-- -->. <b>Designing Expandable-Structure Robots for Human-Robot Interaction</b>. <i>In undefined (Frontiers &#x27;22)</i>. <!-- -->  Page: 1-<!-- -->22<!-- -->.  DOI: <a href="https://doi.org/10.3389/frobt.2022.719639" target="_blank">https://doi.org/10.3389/frobt.2022.719639</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2021-suzuki" class="ui large modal"><div class="header"><a href="/publications/uist-2021-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2021-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2021-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2021-suzuki" target="_blank">HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Eyal Ofek</span> , <span>Mike Sinclair</span> , <span>Daniel Leithinger</span> , <span>Mar Gonzalez-Franco</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/uist-2021-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2021-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/HTiZgOESJyQ" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/HTiZgOESJyQ?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/HTiZgOESJyQ/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic ap- proaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability---these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in- time touch-points on the user’s hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various ap- plications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Encountered Type Haptics</span><span class="ui brown basic label">Tabletop Mobile Robots</span><span class="ui brown basic label">Swarm User Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Eyal Ofek<!-- -->, <!-- -->Mike Sinclair<!-- -->, <!-- -->Daniel Leithinger<!-- -->, <!-- -->Mar Gonzalez-Franco<!-- -->. <b>HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;21)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->16<!-- -->.  DOI: <a href="https://doi.org/10.1145/3472749.3474821" target="_blank">https://doi.org/10.1145/3472749.3474821</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2020-suzuki" class="ui large modal"><div class="header"><a href="/publications/uist-2020-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2020-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2020-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2020-suzuki" target="_blank">RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Rubaiat Habib Kazi</span> , <span>Li-Yi Wei</span> , <span>Stephen DiVerdi</span> , <span>Wilmot Li</span> , <span>Daniel Leithinger</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/uist-2020-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2020-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/L0p-BNU9rXU" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/L0p-BNU9rXU?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/L0p-BNU9rXU/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, floating in mid air without responding to the real world. This paper introduces a new way to embed dynamic and responsive graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-defined programs and configurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Embedded Data Visualization</span><span class="ui brown basic label">Real Time Authoring</span><span class="ui brown basic label">Sketching Interfaces</span><span class="ui brown basic label">Tangible Interaction</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Rubaiat Habib Kazi<!-- -->, <!-- -->Li-Yi Wei<!-- -->, <!-- -->Stephen DiVerdi<!-- -->, <!-- -->Wilmot Li<!-- -->, <!-- -->Daniel Leithinger<!-- -->. <b>RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->16<!-- -->.  DOI: <a href="https://doi.org/10.1145/3379337.3415892" target="_blank">https://doi.org/10.1145/3379337.3415892</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="iros-2020-hedayati" class="ui large modal"><div class="header"><a href="/publications/iros-2020-hedayati" target="_blank"><i class="fas fa-link fa-fw"></i>iros-2020-hedayati</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">IROS 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/iros-2020-hedayati.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/iros-2020-hedayati" target="_blank">PufferBot: Actuated Expandable Structures for Aerial Robots</a></h1><p class="meta"><span>Hooman Hedayati</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Daniel Leithinger</span> , <span>Daniel Szafir</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/iros-2020-hedayati.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>iros-2020-hedayati.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/XtPepCxWcBg" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/XtPepCxWcBg?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/XtPepCxWcBg/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present PufferBot, an aerial robot with an expandable structure that may expand to protect a drone’s propellers when the robot is close to obstacles or collocated humans. PufferBot is made of a custom 3D printed expandable scissor structure, which utilizes a one degree of freedom actuator with rack and pinion mechanism. We propose four designs for the expandable structure, each with unique charac- terizations which may be useful in different situations. Finally, we present three motivating scenarios in which PufferBot might be useful beyond existing static propeller guard structures.</p></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Hooman Hedayati<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Daniel Leithinger<!-- -->, <!-- -->Daniel Szafir<!-- -->. <b>PufferBot: Actuated Expandable Structures for Aerial Robots</b>. <i>In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS &#x27;20)</i>. <!-- -->IEEE, New York, NY, USA<!-- -->  Page: 1-<!-- -->6<!-- -->.  DOI: <a target="_blank"></a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-suzuki" class="ui large modal"><div class="header"><a href="/publications/chi-2020-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2020-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-suzuki" target="_blank">RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Hooman Hedayati</span> , <span>Clement Zheng</span> , <span>James Bohn</span> , <span>Daniel Szafir</span> , <span>Ellen Yi-Luen Do</span> , <span>Mark D Gross</span> , <span>Daniel Leithinger</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/chi-2020-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2020-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/4OWU60gTOFE" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/4OWU60gTOFE?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/4OWU60gTOFE/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>RoomShift is a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Room Scale Haptics</span><span class="ui brown basic label">Haptic Interfaces</span><span class="ui brown basic label">Swarm Robots</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Hooman Hedayati<!-- -->, <!-- -->Clement Zheng<!-- -->, <!-- -->James Bohn<!-- -->, <!-- -->Daniel Szafir<!-- -->, <!-- -->Ellen Yi-Luen Do<!-- -->, <!-- -->Mark D Gross<!-- -->, <!-- -->Daniel Leithinger<!-- -->. <b>RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->11<!-- -->.  DOI: <a href="https://doi.org/10.1145/3313831.3376523" target="_blank">https://doi.org/10.1145/3313831.3376523</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tei-2020-suzuki" class="ui large modal"><div class="header"><a href="/publications/tei-2020-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>tei-2020-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TEI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2020-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tei-2020-suzuki" target="_blank">LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Ryosuke Nakayama</span> , <span>Dan Liu</span> , <span>Yasuaki Kakehi</span> , <span>Mark D. Gross</span> , <span>Daniel Leithinger</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/tei-2020-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>tei-2020-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/0LHeTkOMR84" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/0LHeTkOMR84?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/0LHeTkOMR84/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Large-scale shape-changing interfaces have great potential, but creating such systems requires substantial time, cost, space, and efforts, which hinders the research community to explore interactions beyond the scale of human hands. We introduce modular inflatable actuators as building blocks for prototyping room-scale shape-changing interfaces. Each actuator can change its height from 15cm to 150cm, actuated and controlled by air pressure. Each unit is low-cost (8 USD), lightweight (10 kg), compact (15 cm), and robust, making it well-suited for prototyping room-scale shape transformations. Moreover, our modular and reconfigurable design allows researchers and designers to quickly construct different geometries and to explore various applications. This paper contributes to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Shape Changing Interfaces</span><span class="ui brown basic label">Inflatables</span><span class="ui brown basic label">Large Scale Interactions</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Ryosuke Nakayama<!-- -->, <!-- -->Dan Liu<!-- -->, <!-- -->Yasuaki Kakehi<!-- -->, <!-- -->Mark D. Gross<!-- -->, <!-- -->Daniel Leithinger<!-- -->. <b>LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces</b>. <i>In Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction (TEI &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->9<!-- -->.  DOI: <a href="https://dl.acm.org/doi/10.1145/3374920.3374941" target="_blank">https://dl.acm.org/doi/10.1145/3374920.3374941</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2019-suzuki" class="ui large modal"><div class="header"><a href="/publications/uist-2019-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2019-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2019-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2019-suzuki" target="_blank">ShapeBots: Shape-changing Swarm Robots</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Clement Zheng</span> , <span>Yasuaki Kakehi</span> , <span>Tom Yeh</span> , <span>Ellen Yi-Luen Do</span> , <span>Mark D. Gross</span> , <span>Daniel Leithinger</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/uist-2019-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2019-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/cwPaof0kKdM" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/cwPaof0kKdM?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/cwPaof0kKdM/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce shape-changing swarm robots. A swarm of self-transformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions. The modular design of each actuator enables various shapes and geometries of self-transformation. We illustrate potential application scenarios and discuss how this type of interface opens up possibilities for the future of ubiquitous and distributed shape-changing interfaces.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Swarm User Interfaces</span><span class="ui brown basic label">Shape Changing User Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Clement Zheng<!-- -->, <!-- -->Yasuaki Kakehi<!-- -->, <!-- -->Tom Yeh<!-- -->, <!-- -->Ellen Yi-Luen Do<!-- -->, <!-- -->Mark D. Gross<!-- -->, <!-- -->Daniel Leithinger<!-- -->. <b>ShapeBots: Shape-changing Swarm Robots</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3332165.3347911" target="_blank">https://doi.org/10.1145/3332165.3347911</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2019-nakayama" class="ui large modal"><div class="header"><a href="/publications/dis-2019-nakayama" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2019-nakayama</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-nakayama.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2019-nakayama" target="_blank">MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction</a></h1><p class="meta"><span>Ryosuke Nakayama</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Satoshi Nakamaru</span> , <span>Ryuma Niiyama</span> , <span>Yoshihiro Kawahara</span> , <span>Yasuaki Kakehi</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/dis-2019-nakayama.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>dis-2019-nakayama.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/ZkCcazfFD-M" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/ZkCcazfFD-M?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/ZkCcazfFD-M/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce MorphIO, entirely soft sensing and actuation modules for programming by demonstration of soft robots and shape-changing interfaces.MorphIO&#x27;s hardware consists of a soft pneumatic actuator containing a conductive sponge sensor.This allows both input and output of three-dimensional deformation of a soft material.Leveraging this capability, MorphIO enables a user to record and later playback physical motion of programmable shape-changing materials.In addition, the modular design of MorphIO&#x27;s unit allows the user to construct various shapes and topologies through magnetic connection.We demonstrate several application scenarios, including tangible character animation, locomotion experiment of a soft robot, and prototyping tools for animated soft objects.Our user study with six participants confirms the benefits of MorphIO, as compared to the existing programming paradigm.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Shape Changing Interfaces</span><span class="ui brown basic label">Programming By Demonstration</span><span class="ui brown basic label">Soft Robots</span><span class="ui brown basic label">Pneumatic Actuation</span><span class="ui brown basic label">Tangible Interactions</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryosuke Nakayama<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Satoshi Nakamaru<!-- -->, <!-- -->Ryuma Niiyama<!-- -->, <!-- -->Yoshihiro Kawahara<!-- -->, <!-- -->Yasuaki Kakehi<!-- -->. <b>MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->11<!-- -->.  DOI: <a href="https://doi.org/10.1145/3322276.3322337" target="_blank">https://doi.org/10.1145/3322276.3322337</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2018-suzuki" class="ui large modal"><div class="header"><a href="/publications/uist-2018-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2018-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2018-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2018-suzuki" target="_blank">Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Junichi Yamaoka</span> , <span>Daniel Leithinger</span> , <span>Tom Yeh</span> , <span>Mark D. Gross</span> , <span>Yoshihiro Kawahara</span> , <span>Yasuaki Kakehi</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/uist-2018-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2018-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/92eGI-gYYc4" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/92eGI-gYYc4?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/92eGI-gYYc4/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces Dynamic 3D Printing, a fast and reconstructable shape formation system. Dynamic 3D Printing can assemble an arbitrary three-dimensional shape from a large number of small physical elements. Also, it can disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbitrary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and implementation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long-term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper, we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Digital Materials</span><span class="ui brown basic label">Dynamic 3 D Printing</span><span class="ui brown basic label">Shape Displays</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Junichi Yamaoka<!-- -->, <!-- -->Daniel Leithinger<!-- -->, <!-- -->Tom Yeh<!-- -->, <!-- -->Mark D. Gross<!-- -->, <!-- -->Yoshihiro Kawahara<!-- -->, <!-- -->Yasuaki Kakehi<!-- -->. <b>Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->16<!-- -->.  DOI: <a href="https://doi.org/10.1145/3242587.3242659" target="_blank">https://doi.org/10.1145/3242587.3242659</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/R3FRUtOIiCQ" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/R3FRUtOIiCQ?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/R3FRUtOIiCQ/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-oh" class="ui large modal"><div class="header"><a href="/publications/chi-2018-oh" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-oh</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-oh.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-oh" target="_blank">PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-Based Electronic Devices</a></h1><p class="meta"><span>Hyunjoo Oh</span> , <span>Tung D. Ta</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Mark D. Gross</span> , <span>Yoshihiro Kawahara</span> , <span>Lining Yao</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/chi-2018-oh.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2018-oh.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/DTd863suDN0" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/DTd863suDN0?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/DTd863suDN0/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present PEP (Printed Electronic Papercrafts), a set of design and fabrication techniques to integrate electronic based interactivities into printed papercrafts via 3D sculpting. We explore the design space of PEP, integrating four functions into 3D paper products: actuation, sensing, display, and communication, leveraging the expressive and technical opportunities enabled by paper-like functional layers with a stack of paper. We outline a seven-step workflow, introduce a design tool we developed as an add-on to an existing CAD environment, and demonstrate example applications that combine the electronic enabled functionality, the capability of 3D sculpting, and the unique creative affordances by the materiality of paper.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Paper Electronics</span><span class="ui brown basic label">3 D Sculpting</span><span class="ui brown basic label">Paper Craft</span><span class="ui brown basic label">Fabrication Techniques</span><span class="ui brown basic label">Prototyping</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Hyunjoo Oh<!-- -->, <!-- -->Tung D. Ta<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Mark D. Gross<!-- -->, <!-- -->Yoshihiro Kawahara<!-- -->, <!-- -->Lining Yao<!-- -->. <b>PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-Based Electronic Devices</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3174015" target="_blank">https://doi.org/10.1145/3173574.3174015</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-suzuki" class="ui large modal"><div class="header"><a href="/publications/chi-2018-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-suzuki" target="_blank">Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Jun Kato</span> , <span>Mark D. Gross</span> , <span>Tom Yeh</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/chi-2018-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2018-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Gb7brajKCVE" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Gb7brajKCVE?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/Gb7brajKCVE/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We explore a new approach to programming swarm user interfaces (Swarm UI) by leveraging direct physical manipulation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high-level interface design. Inspired by current UI programming practices, we introduce a four-step workflow-create elements, abstract attributes, specify behaviors, and propagate changes-for Swarm UI programming. We propose a set of direct physical manipulation techniques to support each step in this workflow. To demonstrate these concepts, we developed Reactile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studies-an in-class survey with 148 students and a lab interview with eight participants-confirm that our approach is intuitive and understandable for programming Swarm UIs.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Direct Manipulation</span><span class="ui brown basic label">Tangible Programming</span><span class="ui brown basic label">Swarm User Interfaces</span><span class="ui brown basic label">Programming By Demonstration</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Jun Kato<!-- -->, <!-- -->Mark D. Gross<!-- -->, <!-- -->Tom Yeh<!-- -->. <b>Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3173773" target="_blank">https://doi.org/10.1145/3173574.3173773</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="assets-2017-suzuki" class="ui large modal"><div class="header"><a href="/publications/assets-2017-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>assets-2017-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">ASSETS 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/assets-2017-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/assets-2017-suzuki" target="_blank">FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Abigale Stangl</span> , <span>Mark D. Gross</span> , <span>Tom Yeh</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/assets-2017-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>assets-2017-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/VbwIZ9V6i_g" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/VbwIZ9V6i_g?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/VbwIZ9V6i_g/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>For people with visual impairments, tactile graphics are an important means to learn and explore information. However, raised line tactile graphics created with traditional materials such as embossing are static. While available refreshable displays can dynamically change the content, they are still too expensive for many users, and are limited in size. These factors limit wide-spread adoption and the representation of large graphics or data sets. In this paper, we present FluxMaker, an inexpensive scalable system that renders dynamic information on top of static tactile graphics with movable tactile markers. These dynamic tactile markers can be easily reconfigured and used to annotate static raised line tactile graphics, including maps, graphs, and diagrams. We developed a hardware prototype that actuates magnetic tactile markers driven by low-cost and scalable electromagnetic coil arrays, which can be fabricated with standard printed circuit board manufacturing. We evaluate our prototype with six participants with visual impairments and found positive results across four application areas: location finding or navigating on tactile maps, data analysis, and physicalization, feature identification for tactile graphics, and drawing support. The user study confirms advantages in application domains such as education and data exploration.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Visual Impairment</span><span class="ui brown basic label">Dynamic Tactile Markers</span><span class="ui brown basic label">Tangible Interfaces</span><span class="ui brown basic label">Interactive Tactile Graphics</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Abigale Stangl<!-- -->, <!-- -->Mark D. Gross<!-- -->, <!-- -->Tom Yeh<!-- -->. <b>FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers</b>. <i>In undefined (ASSETS &#x27;20)</i>. <!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="https://doi.org/10.1145/3132525.3132548" target="_blank">https://doi.org/10.1145/3132525.3132548</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div></div></div></div><div class="one wide column"></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><img style="max-width:180px;margin:30px auto" src="/static/images/logo-6.png"/><div class="content"><img style="max-width:200px;margin:0px auto" src="/static/images/logo-4.png"/><div class="sub header">Department of Computer Science</div></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"dataManager":"[]","props":{"pageProps":{"id":"ryo-suzuki"}},"page":"/person","query":{"id":"ryo-suzuki"},"buildId":"hVbt0deeX6DfTDZPiuYzv","dynamicBuildId":false,"nextExport":true}</script><script async="" id="__NEXT_PAGE__/person" src="/_next/static/hVbt0deeX6DfTDZPiuYzv/pages/person.js"></script><script async="" id="__NEXT_PAGE__/_app" src="/_next/static/hVbt0deeX6DfTDZPiuYzv/pages/_app.js"></script><script src="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" async=""></script><script src="/_next/static/chunks/commons.0c93cb8d3516282dd2c4.js" async=""></script><script src="/_next/static/runtime/main-563fdde58a64bdca21c4.js" async=""></script></body></html>