<!DOCTYPE html><html><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="format-detection" content="telephone=no"/><link href="https://use.fontawesome.com/releases/v5.1.1/css/all.css" rel="stylesheet"/><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.css" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/css/lightbox.css" rel="stylesheet"/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link href="/static/css/style.css" rel="stylesheet"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox-plus-jquery.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              $('.ui.sidebar')
                .sidebar('attach events', '.sidebar.icon')

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1" class="next-head"/><meta charSet="utf-8" class="next-head"/><title class="next-head">Aditya Shekhar Nittala | Interactions Lab - University of Calgary HCI Group</title><meta name="keywords" content="Human-Computer Interaction, HCI, Information Visualization, University of Calgary, CHI, UIST" class="next-head"/><meta name="description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" class="next-head"/><meta property="og:title" content="Aditya Shekhar Nittala | Interactions Lab - University of Calgary HCI Group" class="next-head"/><meta property="og:description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" class="next-head"/><meta property="og:site_name" content="University of Calgary Interactions Lab" class="next-head"/><meta property="og:url" content="https://ilab.ucalgary.ca/" class="next-head"/><meta property="og:image" content="https://ilab.ucalgary.ca/static/images/people/aditya-shekhar-nittala.jpg" class="next-head"/><meta property="og:type" content="website" class="next-head"/><meta name="twitter:title" content="Aditya Shekhar Nittala | Interactions Lab - University of Calgary HCI Group" class="next-head"/><meta name="twitter:description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" class="next-head"/><meta name="twitter:image" content="https://ilab.ucalgary.ca/static/images/people/aditya-shekhar-nittala.jpg" class="next-head"/><meta name="twitter:card" content="summary" class="next-head"/><meta name="twitter:site" content="@ucalgary" class="next-head"/><meta name="twitter:url" content="https://ilab.ucalgary.ca/" class="next-head"/><link rel="preload" href="/_next/static/iKofXNZiAVtv09XJiW4Uc/pages/person.js" as="script"/><link rel="preload" href="/_next/static/iKofXNZiAVtv09XJiW4Uc/pages/_app.js" as="script"/><link rel="preload" href="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.0c93cb8d3516282dd2c4.js" as="script"/><link rel="preload" href="/_next/static/runtime/main-563fdde58a64bdca21c4.js" as="script"/></head><body><div id="__next"><div><div><div class="ui right vertical sidebar menu"><a class="item" href="/">Home</a><a class="item" href="/publications">Publications</a><a class="item active" href="/people">People</a><a class="item" href="/courses">Courses</a><a class="item" href="/facility">Facility</a><a class="item" href="/seminar">Seminar</a><a class="item" href="/location">Location</a></div><div class="ui stackable secondary pointing container menu" style="border-bottom:none;margin-right:15%;font-size:1.1em"><div class="left menu"><a class="item" href="/"><b>UCalgary iLab</b></a></div><div class="right menu"><a class="item" href="/publications">Publications</a><a class="item active" href="/people">People</a><a class="item" href="/courses">Courses</a><a class="item" href="/facility">Facility</a><a class="item" href="/seminar">Seminar</a><a class="item" href="/location">Location</a><div class="toc item"><a href="/"><b>UCalgary iLab</b></a><i style="float:right" class="sidebar icon"></i></div></div></div></div><div class="ui stackable grid"><div class="one wide column"></div><div class="eleven wide column centered"><div id="person" class="category" style="text-align:center"><img class="ui circular image large-profile" src="/static/images/people/aditya-shekhar-nittala.jpg" style="margin:auto"/><h1>Aditya Shekhar Nittala</h1><p>Assistant Professor</p><p><a href="https://sites.google.com/site/adityanittala/" target="_blank"><i class="fas fa-link fa-fw"></i>https://sites.google.com/site/adityanittala/</a></p><p><a href="https://scholar.google.com/citations?user=pDSbjBsAAAAJ" target="_blank"><i class="fas fa-graduation-cap fa-fw"></i>Google Scholar</a></p><div class="ui horizontal small divided link list"><div class="item"><a href="https://www.linkedin.com/in/adityashekharn" target="_blank" style="font-size:1.2em"><i class="fab fa-linkedin-in fa-fw"></i>LinkedIn</a></div><div class="item"><a href="mailto:anittala@ucalgary.ca" target="_blank" style="font-size:1.2em"><i class="far fa-envelope fa-fw"></i>anittala@ucalgary.ca</a></div></div></div><div id="publications" class="category"><h1 class="ui horizontal divider header"><i class="file alternate outline icon"></i>Publications</h1><div class="ui segment" style="margin-top:50px"><div class="publication ui vertical segment stackable grid" data-id="chi-2025-ghaneezabadi"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2025-ghaneezabadi.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2025</span></p><p class="color" style="font-size:1.3em"><b>IntelliLining: Activity Sensing through Textile Interlining Sensors Using TENGs</b></p><p><span>Mahdie GhaneEzabadi</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Aditya Shekhar Nittala</span></a> , <a href="/people/xing-dong-yang"><img src="/static/images/people/no-profile-2.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Xing-Dong Yang</span></a> , <span>Te-yen Wu</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Interactive Textile</span><span class="ui brown basic label">TEN Gs</span><span class="ui brown basic label">Machine Learning</span><span class="ui brown basic label">Vibration Sensing</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2025-madill"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2025-madill.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2025</span></p><p class="color" style="font-size:1.3em"><b>Playing with Robots: Performing Arts Techniques for Designing and Understanding Robot Group Movement</b></p><p><span>Philippa Madill</span> , <span>Matthew Newton</span> , <span>Huanjun Zhao</span> , <span>Yichen Lian</span> , <a href="/people/zachary-mckendrick"><img src="/static/images/people/zachary-mckendrick.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Zachary McKendrick</span></a> , <span>Patrick Finn</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Aditya Shekhar Nittala</span></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Humanities</span><span class="ui brown basic label">Art</span><span class="ui brown basic label">Robots</span><span class="ui brown basic label">Method</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2025-shiokawa"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2025-shiokawa.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2025</span></p><p class="color" style="font-size:1.3em"><b>Beyond Vacuuming: How Can We Exploit Domestic Robots&#x27; Idle Time?</b></p><p><span>Yoshiaki Shiokawa</span> , <span>Winnie Chen</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Aditya Shekhar Nittala</span></a> , <span>Jason Alexander</span> , <span>Adwait Sharma</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Domestic Robots</span><span class="ui brown basic label">Ubiquitous</span><span class="ui brown basic label">Interaction</span><span class="ui brown basic label">Design Space</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2024-roy"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2024-roy.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2024</span></p><p class="color" style="font-size:1.3em"><b>HoloChemie - Sustainable Fabrication of Soft Biochemical Holographic Devices for Ubiquitous Sensing</b></p><p><a href="/people/sutirtha-roy"><img src="/static/images/people/sutirtha-roy.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sutirtha Roy</span></a> , <span>Moshfiq-Us-Saleheen Chowdhury</span> , <span>Jurjaan Onayza Noim</span> , <span>Richa Pandey</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Aditya Shekhar Nittala</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Physiological Sensing</span><span class="ui brown basic label">Biochemical Devices Sensing</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2024-danyluk"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2024-danyluk.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2024</span></p><p class="color" style="font-size:1.3em"><b>Understanding Gesture and Microgesture Inputs for Augmented Reality Maps</b></p><p><a href="/people/kurtis-danyluk"><img src="/static/images/people/kurtis-danyluk.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kurtis Danyluk</span></a> , <span>Simon Klueber</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Aditya Shekhar Nittala</span></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Gestural Input</span><span class="ui brown basic label">Microgestures</span><span class="ui brown basic label">AR</span><span class="ui brown basic label">Maps</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2024-panigrahy"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2024-panigrahy.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2024</span></p><p class="color" style="font-size:1.3em"><b>ecSkin: Low-Cost Fabrication of Epidermal Electrochemical Sensors for Detecting Biomarkers in Sweat</b></p><p><span>Sai Nandan Panigrahy*</span> , <span>Chang Hyeon Lee*</span> , <span>Vrahant Nagoria</span> , <span>Mohammad Janghorban</span> , <span>Richa Pandey</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Aditya Shekhar Nittala</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Physiological Sensing</span><span class="ui brown basic label">Electrochemical Devices</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2023-mukashev"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2023-mukashev.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2023</span></p><p class="color" style="font-size:1.3em"><b>TactTongue: Prototyping ElectroTactile Stimulations on the Tongue</b></p><p><a href="/people/dinmukhammed-mukashev"><img src="/static/images/people/dinmukhammed-mukashev.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Dinmukhammed Mukashev</span></a> , <span>Nimesha Ranasinghe</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Aditya Shekhar Nittala</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Electrotactile Actuation</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tochi-2022-nittala"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tochi-2022-nittala.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TOCHI 2022</span></p><p class="color" style="font-size:1.3em"><b>SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures</b></p><p><span>Adwait Sharma</span> , <span>Christina Salchow-Hömmen</span> , <span>Vimal Suresh Mollyn</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Aditya Shekhar Nittala</span></a> , <span>Michael A. Hedderich</span> , <span>Marion Koelle</span> , <span>Thomas Seel</span> , <span>Jürgen Steimle</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Gesture Recognition</span><span class="ui brown basic label">Hand Gestures</span><span class="ui brown basic label">Sensor Placement</span><span class="ui brown basic label">IMU</span><span class="ui brown basic label">Objects</span><span class="ui brown basic label">Design Tool</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2022-nittala"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2022-nittala.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2022</span><span class="ui big basic pink label"><b><i class="fas fa-trophy"></i> Best Paper</b></span></p><p class="color" style="font-size:1.3em"><b>Prototyping Soft Devices with Interactive Bioplastics</b></p><p><span>Marion Koelle</span> , <span>Madalina Nicolae</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Aditya Shekhar Nittala</span></a> , <span>Marc Teyssier</span> , <span>Jürgen Steimle</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Bioplastics</span><span class="ui brown basic label">Biomaterials</span><span class="ui brown basic label">Do It Yourself</span><span class="ui brown basic label">DIY</span><span class="ui brown basic label">Sustainability</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2022-nittala"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2022-nittala.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2022</span></p><p class="color" style="font-size:1.3em"><b>Next Steps in Epidermal Computing: Opportunities and Challenges for Soft On-Skin Devices</b></p><p><a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Aditya Shekhar Nittala</span></a> , <span>Jürgen Steimle</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Wearable Devices</span><span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Survey</span><span class="ui brown basic label">Soft Wearables</span></div></p></div></div></div><div id="publications-modal"><div id="chi-2025-ghaneezabadi" class="ui large modal"><div class="header"><a href="/publications/chi-2025-ghaneezabadi" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2025-ghaneezabadi</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2025</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2025-ghaneezabadi.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2025-ghaneezabadi" target="_blank">IntelliLining: Activity Sensing through Textile Interlining Sensors Using TENGs</a></h1><p class="meta"><span>Mahdie GhaneEzabadi</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><strong>Aditya Shekhar Nittala</strong></a> , <a href="/people/xing-dong-yang"><img src="/static/images/people/xing-dong-yang.jpg" class="ui circular spaced image mini-profile"/><strong>Xing-Dong Yang</strong></a> , <span>Te-yen Wu</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/chi-2025-ghaneezabadi.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2025-ghaneezabadi.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We introduce a novel component for smart garments: smart interlining, and validate its technical feasibility through a series of experiments. Our work involved the implementation of a prototype that employs a textile vibration sensor based on Triboelectric Nanogenerators (TENGs), commonly used for activity detection. We explore several unique features of smart interlining, including how sensor signals and patterns are influenced by factors such as the size and shape of the interlining sensor, the location of the vibration source within the sensor area, and various propagation media, such as airborne and surface vibrations. We present our study results and discuss how these findings support the feasibility of smart interlining. Additionally, we demonstrate that smart interlinings on a shirt can detect a variety of user activities involving the hand, mouth, and upper body, achieving an accuracy rate of 93.9% in the tested activities.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Interactive Textile</span><span class="ui brown basic label">TEN Gs</span><span class="ui brown basic label">Machine Learning</span><span class="ui brown basic label">Vibration Sensing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Mahdie GhaneEzabadi<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Xing-Dong Yang<!-- -->, <!-- -->Te-yen Wu<!-- -->. <b>IntelliLining: Activity Sensing through Textile Interlining Sensors Using TENGs</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;25)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->.  DOI: <a href="https://doi.org/10.1145/3706598.3713167" target="_blank">https://doi.org/10.1145/3706598.3713167</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2025-madill" class="ui large modal"><div class="header"><a href="/publications/chi-2025-madill" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2025-madill</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2025</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2025-madill.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2025-madill" target="_blank">Playing with Robots: Performing Arts Techniques for Designing and Understanding Robot Group Movement</a></h1><p class="meta"><span>Philippa Madill</span> , <span>Matthew Newton</span> , <span>Huanjun Zhao</span> , <span>Yichen Lian</span> , <a href="/people/zachary-mckendrick"><img src="/static/images/people/zachary-mckendrick.jpg" class="ui circular spaced image mini-profile"/><strong>Zachary McKendrick</strong></a> , <span>Patrick Finn</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><strong>Aditya Shekhar Nittala</strong></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/chi-2025-madill.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2025-madill.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this work, we introduce a formal design approach derived from the performing arts to design robot group behaviour. In our first experiment, we worked with professional actors, directors, and non-specialists using a participatory design approach to identify common group behaviour patterns. In a follow-up studio work, we identified twelve common group movement patterns, transposed them into a performance script, built a scale model to support the performance process, and evaluated the patterns with a senior actor under studio conditions. We evaluated our refined models with 20 volunteers in a user study in the third experiment. Results from our affective circumplex modelling suggest that the patterns elicit positive emotional responses from the users. Also, participants performed better than chance in identifying the motion patterns without prior training. Based on our results, we propose design guidelines for social robots’ behaviour and movement design to improve their overall comprehensibility in interaction.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Humanities</span><span class="ui brown basic label">Art</span><span class="ui brown basic label">Robots</span><span class="ui brown basic label">Method</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Philippa Madill<!-- -->, <!-- -->Matthew Newton<!-- -->, <!-- -->Huanjun Zhao<!-- -->, <!-- -->Yichen Lian<!-- -->, <!-- -->Zachary McKendrick<!-- -->, <!-- -->Patrick Finn<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>Playing with Robots: Performing Arts Techniques for Designing and Understanding Robot Group Movement</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;25)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->.  DOI: <a href="https://dl.acm.org/doi/10.1145/3706598.3713996" target="_blank">https://dl.acm.org/doi/10.1145/3706598.3713996</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2025-shiokawa" class="ui large modal"><div class="header"><a href="/publications/chi-2025-shiokawa" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2025-shiokawa</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2025</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2025-shiokawa.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2025-shiokawa" target="_blank">Beyond Vacuuming: How Can We Exploit Domestic Robots&#x27; Idle Time?</a></h1><p class="meta"><span>Yoshiaki Shiokawa</span> , <span>Winnie Chen</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><strong>Aditya Shekhar Nittala</strong></a> , <span>Jason Alexander</span> , <span>Adwait Sharma</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/chi-2025-shiokawa.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2025-shiokawa.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/npYIDenYb2Y" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/npYIDenYb2Y?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/npYIDenYb2Y/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We are increasingly adopting domestic robots (e.g., Roomba) that provide relief from mundane household tasks. However, these robots usually only spend little time executing their specific task and remain idle for long periods. They typically possess advanced mobility and sensing capabilities, and therefore have significant potential applications beyond their designed use. Our work explores this untapped potential of domestic robots in ubiquitous computing, focusing on how they can improve and support modern lifestyles. We conducted two studies: an online survey (n=50) to understand current usage patterns of these robots within homes and an exploratory study (n=12) with HCI and HRI experts. Our thematic analysis revealed 12 key dimensions for developing interactions with domestic robots and outlined over 100 use cases, illustrating how these robots can offer proactive assistance and provide privacy. Finally, we implemented a proof-of-concept prototype to demonstrate the feasibility of reappropriating domestic robots for diverse ubiquitous computing applications.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Domestic Robots</span><span class="ui brown basic label">Ubiquitous</span><span class="ui brown basic label">Interaction</span><span class="ui brown basic label">Design Space</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Yoshiaki Shiokawa<!-- -->, <!-- -->Winnie Chen<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Jason Alexander<!-- -->, <!-- -->Adwait Sharma<!-- -->. <b>Beyond Vacuuming: How Can We Exploit Domestic Robots&#x27; Idle Time?</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;25)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->.  DOI: <a href="https://dl.acm.org/doi/10.1145/3706598.3714266" target="_blank">https://dl.acm.org/doi/10.1145/3706598.3714266</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2024-roy" class="ui large modal"><div class="header"><a href="/publications/uist-2024-roy" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2024-roy</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2024-roy.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2024-roy" target="_blank">HoloChemie - Sustainable Fabrication of Soft Biochemical Holographic Devices for Ubiquitous Sensing</a></h1><p class="meta"><a href="/people/sutirtha-roy"><img src="/static/images/people/sutirtha-roy.jpg" class="ui circular spaced image mini-profile"/><strong>Sutirtha Roy</strong></a> , <span>Moshfiq-Us-Saleheen Chowdhury</span> , <span>Jurjaan Onayza Noim</span> , <span>Richa Pandey</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><strong>Aditya Shekhar Nittala</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/uist-2024-roy.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2024-roy.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/0FrYD1xInNs" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/0FrYD1xInNs?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/0FrYD1xInNs/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Sustainable fabrication approaches and biomaterials are increasingly being used in HCI to fabricate interactive devices. However, the majority of the work has focused on integrating electronics. This paper takes a sustainable approach to exploring the fabrication of biochemical sensing devices. Firstly, we contribute a set of biochemical formulations for biological and environmental sensing with bio-sourced and environment-friendly substrate materials. Our formulations are based on a combination of enzymes derived from bacteria and fungi, plant extracts and commercially available chemicals to sense both liquid and gaseous analytes: glucose, lactic acid, pH levels and carbon dioxide. Our novel holographic sensing scheme allows for detecting the presence of analytes and enables quantitative estimation of the analyte levels. We present a set of application scenarios that demonstrate the versatility of our approach and discuss the sustainability aspects, its limitations, and the implications for bio-chemical systems in HCI.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Physiological Sensing</span><span class="ui brown basic label">Biochemical Devices Sensing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sutirtha Roy<!-- -->, <!-- -->Moshfiq-Us-Saleheen Chowdhury<!-- -->, <!-- -->Jurjaan Onayza Noim<!-- -->, <!-- -->Richa Pandey<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->. <b>HoloChemie - Sustainable Fabrication of Soft Biochemical Holographic Devices for Ubiquitous Sensing</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;24)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->19<!-- -->.  DOI: <a href="https://doi.org/10.1145/3654777.3676448" target="_blank">https://doi.org/10.1145/3654777.3676448</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2024-danyluk" class="ui large modal"><div class="header"><a href="/publications/dis-2024-danyluk" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2024-danyluk</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2024-danyluk.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2024-danyluk" target="_blank">Understanding Gesture and Microgesture Inputs for Augmented Reality Maps</a></h1><p class="meta"><a href="/people/kurtis-danyluk"><img src="/static/images/people/kurtis-danyluk.jpg" class="ui circular spaced image mini-profile"/><strong>Kurtis Danyluk</strong></a> , <span>Simon Klueber</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><strong>Aditya Shekhar Nittala</strong></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/dis-2024-danyluk.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>dis-2024-danyluk.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We explore the potential for subtle on-hand gesture and microgesture interactions for map navigation with augmented reality (AR) devices. We describe a design exercise and follow-up elicitation study in which we identified on-hand gestures for cartographic interaction primitives. Microgestures and on-hand interactions are a promising space for AR map navigation as they offers always-available, tactile, and memorable spaces for interaction. Our findings show a clear set of microgesture interaction patterns that are well suited for supporting map navigation and manipulation. In particular, we highlight how the properties of various microgestures align with particular cartographic interaction tasks. We also describe our experience creating an exploratory proof-of-concept AR map prototype which helped us identify new opportunities and practical challenges for microgesture control. Finally, we discuss how future AR map systems could benefit from on-hand and microgesture input schemes.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Gestural Input</span><span class="ui brown basic label">Microgestures</span><span class="ui brown basic label">AR</span><span class="ui brown basic label">Maps</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kurtis Danyluk<!-- -->, <!-- -->Simon Klueber<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Understanding Gesture and Microgesture Inputs for Augmented Reality Maps</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;24)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->.  DOI: <a href="https://doi.org/10.1145/3643834.3661630" target="_blank">https://doi.org/10.1145/3643834.3661630</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2024-panigrahy" class="ui large modal"><div class="header"><a href="/publications/chi-2024-panigrahy" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2024-panigrahy</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2024-panigrahy.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2024-panigrahy" target="_blank">ecSkin: Low-Cost Fabrication of Epidermal Electrochemical Sensors for Detecting Biomarkers in Sweat</a></h1><p class="meta"><span>Sai Nandan Panigrahy*</span> , <span>Chang Hyeon Lee*</span> , <span>Vrahant Nagoria</span> , <span>Mohammad Janghorban</span> , <span>Richa Pandey</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><strong>Aditya Shekhar Nittala</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/chi-2024-panigrahy.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2024-panigrahy.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/OqW3owQyMk8" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/OqW3owQyMk8?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/OqW3owQyMk8/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>The development of low-cost and non-invasive biosensors for monitoring electrochemical biomarkers in sweat holds great promise for personalized healthcare and early disease detection. In this work, we present ecSkin, a novel fabrication approach for realizing epidermal electrochemical sensors that can detect two vital biomarkers in sweat: glucose and cortisol. We contribute the synthesis of functional reusable inks, that can be formulated using simple household materials. Electrical characterization of inks indicates that they outperform commercially available carbon inks. Cyclic voltammetry experiments show that our inks are electrochemically active and detect glucose and cortisol at activation voltages of -0.36 V and -0.22 V, respectively. Chronoamperometry experiments show that the sensors can detect the full range of glucose and cortisol levels typically found in sweat. Results from a user evaluation show that ecSkin sensors successfully function on the skin. Finally, we demonstrate three applications to illustrate how ecSkin devices can be deployed for various interactive applications.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Physiological Sensing</span><span class="ui brown basic label">Electrochemical Devices</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sai Nandan Panigrahy*<!-- -->, <!-- -->Chang Hyeon Lee*<!-- -->, <!-- -->Vrahant Nagoria<!-- -->, <!-- -->Mohammad Janghorban<!-- -->, <!-- -->Richa Pandey<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->. <b>ecSkin: Low-Cost Fabrication of Epidermal Electrochemical Sensors for Detecting Biomarkers in Sweat</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;24)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->20<!-- -->.  DOI: <a href="https://doi.org/10.1145/3613904.3642232" target="_blank">https://doi.org/10.1145/3613904.3642232</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2023-mukashev" class="ui large modal"><div class="header"><a href="/publications/uist-2023-mukashev" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2023-mukashev</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2023-mukashev.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2023-mukashev" target="_blank">TactTongue: Prototyping ElectroTactile Stimulations on the Tongue</a></h1><p class="meta"><a href="/people/dinmukhammed-mukashev"><img src="/static/images/people/dinmukhammed-mukashev.jpg" class="ui circular spaced image mini-profile"/><strong>Dinmukhammed Mukashev</strong></a> , <span>Nimesha Ranasinghe</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><strong>Aditya Shekhar Nittala</strong></a></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/uist-2023-mukashev.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2023-mukashev.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/zCUdJNNRz5s" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/zCUdJNNRz5s?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/zCUdJNNRz5s/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>The tongue is a remarkable human organ with a high concentration of taste receptors and an exceptional ability to sense touch. This work uses electro-tactile stimulation to explore the intricate interplay between tactile perception and taste rendering on the tongue. To facilitate this exploration, we utilized a fexible, high-resolution electro-tactile prototyping platform that can be administered in the mouth. We have created a design tool that abstracts users from the low-level stimulation parameters, enabling them to focus on higher-level design objectives. Through this platform, we present the results of three studies. Our frst study evaluates the design tool’s qualitative and formative aspects. In contrast, the second study measures the qualitative attributes of the sensations produced by our device, including tactile sensations and taste. In the third study, we demonstrate the ability of our device to sense touch input through the tongue when placed on the hard palate region in the mouth. Finally, we present a range of application demonstrators that span diverse domains, including accessibility, medical surgeries, and extended reality. These demonstrators showcase the versatility and potential of our platform, highlighting its ability to enable researchers and practitioners to explore new ways of leveraging the tongue’s unique capabilities. Overall, this work presents new opportunities to deploy tongue interfaces and has broad implications for designing interfaces that incorporate the tongue as a sensory organ.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Wearables</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Electrotactile Actuation</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Dinmukhammed Mukashev<!-- -->, <!-- -->Nimesha Ranasinghe<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->. <b>TactTongue: Prototyping ElectroTactile Stimulations on the Tongue</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->14<!-- -->.  DOI: <a href="https://dl.acm.org/doi/10.1145/3586183.3606829" target="_blank">https://dl.acm.org/doi/10.1145/3586183.3606829</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tochi-2022-nittala" class="ui large modal"><div class="header"><a href="/publications/tochi-2022-nittala" target="_blank"><i class="fas fa-link fa-fw"></i>tochi-2022-nittala</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TOCHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tochi-2022-nittala.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tochi-2022-nittala" target="_blank">SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures</a></h1><p class="meta"><span>Adwait Sharma</span> , <span>Christina Salchow-Hömmen</span> , <span>Vimal Suresh Mollyn</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><strong>Aditya Shekhar Nittala</strong></a> , <span>Michael A. Hedderich</span> , <span>Marion Koelle</span> , <span>Thomas Seel</span> , <span>Jürgen Steimle</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/tochi-2022-nittala.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>tochi-2022-nittala.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Gestural interaction with freehands and while grasping an everyday object enables always-available input. To sense such gestures, minimal instrumentation of the user’s hand is desirable. However, the choice of an effective but minimal IMU layout remains challenging, due to the complexity of the multi-factorial space that comprises diverse finger gestures, objects and grasps. We present SparseIMU, a rapid method for selecting minimal inertial sensor-based layouts for effective gesture recognition. Furthermore, we contribute a computational tool to guide designers with optimal sensor placement. Our approach builds on an extensive microgestures dataset that we collected with a dense network of 17 inertial measurement units (IMUs). We performed a series of analyses, including an evaluation of the entire combinatorial space for freehand and grasping microgestures (393K layouts), and quantified the performance across different layout choices, revealing new gesture detection opportunities with IMUs. Finally, we demonstrate the versatility of our method with four scenarios.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Gesture Recognition</span><span class="ui brown basic label">Hand Gestures</span><span class="ui brown basic label">Sensor Placement</span><span class="ui brown basic label">IMU</span><span class="ui brown basic label">Objects</span><span class="ui brown basic label">Design Tool</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Adwait Sharma<!-- -->, <!-- -->Christina Salchow-Hömmen<!-- -->, <!-- -->Vimal Suresh Mollyn<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Michael A. Hedderich<!-- -->, <!-- -->Marion Koelle<!-- -->, <!-- -->Thomas Seel<!-- -->, <!-- -->Jürgen Steimle<!-- -->. <b>SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures</b>. <i>In undefined (TOCHI &#x27;22)</i>. <!-- -->  Page: 1-<!-- -->40<!-- -->.  DOI: <a href="https://doi.org/10.1145/3569894" target="_blank">https://doi.org/10.1145/3569894</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2022-nittala" class="ui large modal"><div class="header"><a href="/publications/uist-2022-nittala" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2022-nittala</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2022-nittala.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2022-nittala" target="_blank">Prototyping Soft Devices with Interactive Bioplastics</a></h1><p class="meta"><span>Marion Koelle</span> , <span>Madalina Nicolae</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><strong>Aditya Shekhar Nittala</strong></a> , <span>Marc Teyssier</span> , <span>Jürgen Steimle</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/uist-2022-nittala.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2022-nittala.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/8Paq3P3EsKQ" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/8Paq3P3EsKQ?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/8Paq3P3EsKQ/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Designers and makers are increasingly interested in leveraging bio-based and bio-degradable ‘do-it-yourself’ (DIY) materials for sustainable prototyping. Their self-produced bioplastics possess compelling properties such as self-adhesion but have so far not been functionalized to create soft interactive devices, due to a lack of DIY techniques for the fabrication of functional electronic circuits and sensors. In this paper, we contribute a DIY approach for creating Interactive Bioplastics that is accessible to a wide audience, making use of easy-to-obtain bio-based raw materials and familiar tools. We present three types of conductive bioplastic materials and their formulation: sheets, pastes and foams. Our materials enable additive and subtractive fabrication of soft circuits and sensors. Furthermore, we demonstrate how these materials can substitute conventional prototyping materials, be combined with off-the-shelf electronics, and be fed into a sustainable material ‘life-cycle’ including disassembly, re-use, and re-melting of materials. A formal characterization of our conductors highlights that they are even on-par with commercially available carbon-based conductive pastes.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Bioplastics</span><span class="ui brown basic label">Biomaterials</span><span class="ui brown basic label">Do It Yourself</span><span class="ui brown basic label">DIY</span><span class="ui brown basic label">Sustainability</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Marion Koelle<!-- -->, <!-- -->Madalina Nicolae<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Marc Teyssier<!-- -->, <!-- -->Jürgen Steimle<!-- -->. <b>Prototyping Soft Devices with Interactive Bioplastics</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->16<!-- -->.  DOI: <a href="https://doi.org/10.1145/3526113.3545623" target="_blank">https://doi.org/10.1145/3526113.3545623</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2022-nittala" class="ui large modal"><div class="header"><a href="/publications/chi-2022-nittala" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2022-nittala</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2022-nittala.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2022-nittala" target="_blank">Next Steps in Epidermal Computing: Opportunities and Challenges for Soft On-Skin Devices</a></h1><p class="meta"><a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><strong>Aditya Shekhar Nittala</strong></a> , <span>Jürgen Steimle</span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/master/static/publications/chi-2022-nittala.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2022-nittala.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Lj9Yk5IQsok" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Lj9Yk5IQsok?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/Lj9Yk5IQsok/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Skin is a promising interaction medium and has been widely explored for mobile, and expressive interaction. Recent research in HCI has seen the development of Epidermal Computing Devices: ultra-thin and non-invasive devices which reside on the user’s skin, offering intimate integration with the curved surfaces of the body, while having physical and mechanical properties that are akin to skin, expanding the horizon of on-body interaction. However, with rapid technological advancements in multiple disciplines, we see a need to synthesize the main open research questions and opportunities for the HCI community to advance future research in this area. By systematically analyzing Epidermal Devices contributed in the HCI community, physical sciences research and from our experiences in designing and building Epidermal Devices, we identify opportunities and challenges for advancing research across five themes. This multi-disciplinary synthesis enables multiple research communities to facilitate progression towards more coordinated endeavors for advancing Epidermal Computing.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Wearable Devices</span><span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Survey</span><span class="ui brown basic label">Soft Wearables</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Aditya Shekhar Nittala<!-- -->, <!-- -->Jürgen Steimle<!-- -->. <b>Next Steps in Epidermal Computing: Opportunities and Challenges for Soft On-Skin Devices</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->22<!-- -->.  DOI: <a href="https://doi.org/10.1145/3491102.3517668" target="_blank">https://doi.org/10.1145/3491102.3517668</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div></div></div></div><div class="one wide column"></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><img style="max-width:180px;margin:30px auto" src="/static/images/logo-6.png"/><div class="content"><img style="max-width:200px;margin:0px auto" src="/static/images/logo-4.png"/><div class="sub header">Department of Computer Science</div></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"dataManager":"[]","props":{"pageProps":{"id":"aditya-shekhar-nittala"}},"page":"/person","query":{"id":"aditya-shekhar-nittala"},"buildId":"iKofXNZiAVtv09XJiW4Uc","dynamicBuildId":false,"nextExport":true}</script><script async="" id="__NEXT_PAGE__/person" src="/_next/static/iKofXNZiAVtv09XJiW4Uc/pages/person.js"></script><script async="" id="__NEXT_PAGE__/_app" src="/_next/static/iKofXNZiAVtv09XJiW4Uc/pages/_app.js"></script><script src="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" async=""></script><script src="/_next/static/chunks/commons.0c93cb8d3516282dd2c4.js" async=""></script><script src="/_next/static/runtime/main-563fdde58a64bdca21c4.js" async=""></script></body></html>