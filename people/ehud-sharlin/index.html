<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><title data-next-head="">Ehud Sharlin | Interactions Lab - University of Calgary HCI Group</title><meta name="keywords" content="Human-Computer Interaction, HCI, Information Visualization, University of Calgary, CHI, UIST" data-next-head=""/><meta name="description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" data-next-head=""/><meta property="og:title" content="Ehud Sharlin | Interactions Lab - University of Calgary HCI Group" data-next-head=""/><meta property="og:description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" data-next-head=""/><meta property="og:site_name" content="University of Calgary Interactions Lab" data-next-head=""/><meta property="og:url" content="https://ilab.ucalgary.ca/" data-next-head=""/><meta property="og:image" content="https://ilab.ucalgary.ca/static/images/people/ehud-sharlin.jpg" data-next-head=""/><meta property="og:type" content="website" data-next-head=""/><meta name="twitter:title" content="Ehud Sharlin | Interactions Lab - University of Calgary HCI Group" data-next-head=""/><meta name="twitter:description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" data-next-head=""/><meta name="twitter:image" content="https://ilab.ucalgary.ca/static/images/people/ehud-sharlin.jpg" data-next-head=""/><meta name="twitter:card" content="summary" data-next-head=""/><meta name="twitter:site" content="@ucalgary" data-next-head=""/><meta name="twitter:url" content="https://ilab.ucalgary.ca/" data-next-head=""/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link rel="preload" href="/_next/static/media/dc84b505c4b06e35-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/e484fdd57b325383.css" as="style"/><link rel="preload" href="/_next/static/css/a1a0497113412518.css" as="style"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              // $('.ui.sidebar')
              //   .sidebar('attach events', '.sidebar.icon')

              $('.sidebar.icon').on('click', function(event) {
                $('.ui.sidebar')
                  .sidebar('toggle')
              })

              $('.project').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })

              $('.thesis').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><link rel="stylesheet" href="/_next/static/css/e484fdd57b325383.css" data-n-g=""/><link rel="stylesheet" href="/_next/static/css/a1a0497113412518.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-a339f4a57035852b.js" defer=""></script><script src="/_next/static/chunks/340-6d79087c62b8c852.js" defer=""></script><script src="/_next/static/chunks/main-f5a95b893c70049f.js" defer=""></script><script src="/_next/static/chunks/vendor-styles-25e9cf206ffc5a28.js" defer=""></script><script src="/_next/static/chunks/505-07248a38b06891a1.js" defer=""></script><script src="/_next/static/chunks/pages/_app-09ca72778dd825e1.js" defer=""></script><script src="/_next/static/chunks/347-6f14004506bc7107.js" defer=""></script><script src="/_next/static/chunks/590-8bb7f72173922b48.js" defer=""></script><script src="/_next/static/chunks/104-79403ca78d551e49.js" defer=""></script><script src="/_next/static/chunks/404-e70bff84362a91b6.js" defer=""></script><script src="/_next/static/chunks/pages/people/%5Bid%5D-e453192cce546b52.js" defer=""></script><script src="/_next/static/gYcQ3cHgcaXKKuycsy64l/_buildManifest.js" defer=""></script><script src="/_next/static/gYcQ3cHgcaXKKuycsy64l/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_7d1f77"><div class="ui center aligned container"><div class="ui secondary huge compact menu"><a class="item" href="/"><img loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui tiny image" style="color:transparent" srcSet="/static/images/ilab-logo-3d.gif 1x" src="/static/images/ilab-logo-3d.gif"/></a><a class="item" href="/people/">People</a><a class="item" href="/publications/">Research</a></div></div><div class="pusher"><div class="ui stackable grid"><div class="one wide column"></div><div class="eleven wide column centered"><div id="person" class="category" style="text-align:center"><img alt="Ehud Sharlin&#x27;s photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular image large-profile" style="color:transparent;margin:auto" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><h1>Ehud Sharlin</h1><p>Professor</p><p><a target="_blank" href="http://contacts.ucalgary.ca/info/cpsc/profiles/102-3264"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>http://contacts.ucalgary.ca/info/cpsc/profiles/102-3264</a></p><p><a target="_blank" href="https://scholar.google.ca/citations?hl=en&amp;user=eAFxlZIAAAAJ"><svg data-prefix="fas" data-icon="graduation-cap" class="svg-inline--fa fa-graduation-cap" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M48 195.8l209.2 86.1c9.8 4 20.2 6.1 30.8 6.1s21-2.1 30.8-6.1l242.4-99.8c9-3.7 14.8-12.4 14.8-22.1s-5.8-18.4-14.8-22.1L318.8 38.1C309 34.1 298.6 32 288 32s-21 2.1-30.8 6.1L14.8 137.9C5.8 141.6 0 150.3 0 160L0 456c0 13.3 10.7 24 24 24s24-10.7 24-24l0-260.2zm48 71.7L96 384c0 53 86 96 192 96s192-43 192-96l0-116.6-142.9 58.9c-15.6 6.4-32.2 9.7-49.1 9.7s-33.5-3.3-49.1-9.7L96 267.4z"></path></svg>Google Scholar</a></p><div class="ui horizontal small divided link list"></div><div class="one wide column"><div class="ui horizontal small divided link list"><a href="/labs/utouch/"><div style="background:#ecaa35;z-index:2;border-radius:50%;min-height:6vw;height:6vw;min-width:6vw;width:6vw;display:flex;align-items:center"><img alt="utouch" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" style="color:transparent;height:auto;width:100%" srcSet="/static/images/labs/utouch.png 1x" src="/static/images/labs/utouch.png"/></div></a></div></div></div><div id="publications" class="category ui container"><h1 class="ui horizontal divider header"><svg data-prefix="far" data-icon="file-lines" class="svg-inline--fa fa-file-lines" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M64 48l112 0 0 88c0 39.8 32.2 72 72 72l88 0 0 240c0 8.8-7.2 16-16 16L64 464c-8.8 0-16-7.2-16-16L48 64c0-8.8 7.2-16 16-16zM224 67.9l92.1 92.1-68.1 0c-13.3 0-24-10.7-24-24l0-68.1zM64 0C28.7 0 0 28.7 0 64L0 448c0 35.3 28.7 64 64 64l256 0c35.3 0 64-28.7 64-64l0-261.5c0-17-6.7-33.3-18.7-45.3L242.7 18.7C230.7 6.7 214.5 0 197.5 0L64 0zm56 256c-13.3 0-24 10.7-24 24s10.7 24 24 24l144 0c13.3 0 24-10.7 24-24s-10.7-24-24-24l-144 0zm0 96c-13.3 0-24 10.7-24 24s10.7 24 24 24l144 0c13.3 0 24-10.7 24-24s-10.7-24-24-24l-144 0z"></path></svg>Publications</h1><div class="ui segment" style="margin-top:50px"><div class="publication ui vertical segment stackable grid" data-id="chi-2025-madill"><div class="three wide column" style="margin:auto"><img alt="chi-2025-madill cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/chi-2025-madill.jpg 1x" src="/static/images/publications/cover/chi-2025-madill.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2025</span></p><p class="color" style="font-size:1.3em"><b>Playing with Robots: Performing Arts Techniques for Designing and Understanding Robot Group Movement</b></p><p><span>Philippa Madill</span><span class="role"></span>, <span>Matthew Newton</span><span class="role"></span>, <span>Huanjun Zhao</span><span class="role"></span>, <span>Yichen Lian</span><span class="role"></span>, <a href="/people/zachary-mckendrick/"><img alt="Zachary McKendrick picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/zachary-mckendrick.jpg 1x" src="/static/images/people/zachary-mckendrick.jpg"/><span class="author-link">Zachary McKendrick</span></a><span class="role"></span>, <span>Patrick Finn</span><span class="role"></span>, <a href="/people/aditya-shekhar-nittala/"><img alt="Aditya Shekhar Nittala picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/static/images/people/aditya-shekhar-nittala.jpg"/><span class="author-link">Aditya Shekhar Nittala</span></a><span class="role"></span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Humanities</span><span class="ui brown basic label">Art</span><span class="ui brown basic label">Robots</span><span class="ui brown basic label">Method</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2021-asha"><div class="three wide column" style="margin:auto"><img alt="dis-2021-asha cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/dis-2021-asha.jpg 1x" src="/static/images/publications/cover/dis-2021-asha.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2021</span></p><p class="color" style="font-size:1.3em"><b>Co-Designing Interactions between Pedestrians in Wheelchairs and Autonomous Vehicles</b></p><p><a href="/people/ashratuz-zavin-asha/"><img alt="Ashratuz Zavin Asha picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ashratuz-zavin-asha.jpg 1x" src="/static/images/people/ashratuz-zavin-asha.jpg"/><span class="author-link">Ashratuz Zavin Asha</span></a><span class="role"></span>, <a href="/people/christopher-smith/"><img alt="Christopher Smith picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/christopher-smith.jpg 1x" src="/static/images/people/christopher-smith.jpg"/><span class="author-link">Christopher Smith</span></a><span class="role"></span>, <a href="/people/georgina-freeman/"><img alt="Georgina Freeman picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/georgina-freeman.jpg 1x" src="/static/images/people/georgina-freeman.jpg"/><span class="author-link">Georgina Freeman</span></a><span class="role"></span>, <span>Sean Crump</span><span class="role"></span>, <a href="/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"></span>, <a href="/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"></span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Pedestrians In Wheelchairs</span><span class="ui brown basic label">Co Design</span><span class="ui brown basic label">Autonomous Vehicles</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2021-hammad"><div class="three wide column" style="margin:auto"><img alt="chi-2021-hammad cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/chi-2021-hammad.jpg 1x" src="/static/images/publications/cover/chi-2021-hammad.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2021</span></p><p class="color" style="font-size:1.3em"><b>Homecoming: Exploring Returns to Long-Term Single Player Games</b></p><p><span>Noor Hammad</span><span class="role"></span>, <span>Owen Brierley</span><span class="role"></span>, <a href="/people/zachary-mckendrick/"><img alt="Zachary McKendrick picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/zachary-mckendrick.jpg 1x" src="/static/images/people/zachary-mckendrick.jpg"/><span class="author-link">Zachary McKendrick</span></a><span class="role"></span>, <a href="/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"></span>, <span>Patrick Finn</span><span class="role"></span>, <span>Jessica Hammer</span><span class="role"></span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Long Term Single Player Game</span><span class="ui brown basic label">Autobiographical Design</span><span class="ui brown basic label">Pivot Point</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="imwut-2020-wang"><div class="three wide column" style="margin:auto"><img alt="imwut-2020-wang cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/imwut-2020-wang.jpg 1x" src="/static/images/publications/cover/imwut-2020-wang.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IMWUT 2020</span></p><p class="color" style="font-size:1.3em"><b>AssessBlocks: Exploring Toy Block Play Features for Assessing Stress in Young Children after Natural Disasters</b></p><p><span>Xiyue Wang</span><span class="role"></span>, <span>Kazuki Takashima</span><span class="role"></span>, <span>Tomoaki Adachi</span><span class="role"></span>, <span>Patrick Finn</span><span class="role"></span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"></span>, <span>Yoshifumi Kitamura</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Well Being</span><span class="ui brown basic label">Toy Blocks</span><span class="ui brown basic label">PTSD</span><span class="ui brown basic label">Tangibles For Health</span><span class="ui brown basic label">Stress Assessment</span><span class="ui brown basic label">Play</span><span class="ui brown basic label">Children</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-asha"><div class="three wide column" style="margin:auto"><img alt="chi-2020-asha cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/chi-2020-asha.jpg 1x" src="/static/images/publications/cover/chi-2020-asha.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020 LBW</span></p><p class="color" style="font-size:1.3em"><b>Views from the Wheelchair: Understanding Interaction between Autonomous Vehicle and Pedestrians with Reduced Mobility</b></p><p><a href="/people/ashratuz-zavin-asha/"><img alt="Ashratuz Zavin Asha picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ashratuz-zavin-asha.jpg 1x" src="/static/images/people/ashratuz-zavin-asha.jpg"/><span class="author-link">Ashratuz Zavin Asha</span></a><span class="role"></span>, <a href="/people/christopher-smith/"><img alt="Christopher Smith picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/christopher-smith.jpg 1x" src="/static/images/people/christopher-smith.jpg"/><span class="author-link">Christopher Smith</span></a><span class="role"></span>, <a href="/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"></span>, <a href="/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"></span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Autonomous Vehicle</span><span class="ui brown basic label">Pedestrian With Reduced Mobility</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-hou"><div class="three wide column" style="margin:auto"><img alt="chi-2020-hou cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/chi-2020-hou.jpg 1x" src="/static/images/publications/cover/chi-2020-hou.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020</span></p><p class="color" style="font-size:1.3em"><b>Autonomous Vehicle-Cyclist Interaction: Peril and Promise</b></p><p><span>Ming Hou</span><span class="role"></span>, <a href="/people/karthik-mahadevan/"><img alt="Karthik Mahadevan picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/karthik-mahadevan.jpg 1x" src="/static/images/people/karthik-mahadevan.jpg"/><span class="author-link">Karthik Mahadevan</span></a><span class="role"></span>, <a href="/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"></span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"></span>, <a href="/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Autonomous Vehicle Cyclist Interaction</span><span class="ui brown basic label">Interfaces For Communicating Intent And Awareness</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="cnc-2019-hammad"><div class="three wide column" style="margin:auto"><img alt="cnc-2019-hammad cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/cnc-2019-hammad.jpg 1x" src="/static/images/publications/cover/cnc-2019-hammad.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">C&amp;C 2019</span></p><p class="color" style="font-size:1.3em"><b>Mutation: Leveraging Performing Arts Practices in Cyborg Transitioning</b></p><p><a href="/people/nour-hammad/"><img alt="Nour Hammad picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/nour-hammad.jpg 1x" src="/static/images/people/nour-hammad.jpg"/><span class="author-link">Nour Hammad</span></a><span class="role"></span>, <span>Elaheh Sanoubari</span><span class="role"></span>, <span>Patrick Finn</span><span class="role"></span>, <a href="/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"></span>, <span>James E. Young</span><span class="role"></span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Interaction Design</span><span class="ui brown basic label">Cyborgs</span><span class="ui brown basic label">User Experience</span><span class="ui brown basic label">Performing Art Techniques</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2019-mahadevan"><div class="three wide column" style="margin:auto"><img alt="dis-2019-mahadevan cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/dis-2019-mahadevan.jpg 1x" src="/static/images/publications/cover/dis-2019-mahadevan.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2019</span></p><p class="color" style="font-size:1.3em"><b>AV-Pedestrian Interaction Design Using a Pedestrian Mixed Traffic Simulator</b></p><p><a href="/people/karthik-mahadevan/"><img alt="Karthik Mahadevan picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/karthik-mahadevan.jpg 1x" src="/static/images/people/karthik-mahadevan.jpg"/><span class="author-link">Karthik Mahadevan</span></a><span class="role"></span>, <span>Elaheh Sanoubari</span><span class="role"></span>, <a href="/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"></span>, <span>James E. Young</span><span class="role"></span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Mixed Traffic</span><span class="ui brown basic label">Pedestrian Simulator</span><span class="ui brown basic label">Autonomous Vehicle Pedestrian Interaction</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2018-ta"><div class="three wide column" style="margin:auto"><img alt="dis-2018-ta cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/dis-2018-ta.jpg 1x" src="/static/images/publications/cover/dis-2018-ta.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2018</span></p><p class="color" style="font-size:1.3em"><b>Bod-IDE: An Augmented Reality Sandbox for eFashion Garments</b></p><p><span>Kevin Ta</span><span class="role"></span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"></span>, <a href="/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Electronic Fashion</span><span class="ui brown basic label">Creativity Support Tool</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="tei-2016-somanath"><div class="three wide column" style="margin:auto"><img alt="tei-2016-somanath cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/tei-2016-somanath.jpg 1x" src="/static/images/publications/cover/tei-2016-somanath.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TEI 2016</span></p><p class="color" style="font-size:1.3em"><b>Engaging &#x27;At-Risk&#x27; Students through Maker Culture Activities</b></p><p><a href="/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"></span>, <span>Laura Morrison</span><span class="role"></span>, <span>Janette Hughes</span><span class="role"></span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"></span>, <span>Mario Costa Sousa</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">DIY</span><span class="ui brown basic label">At Risk Students</span><span class="ui brown basic label">Maker Culture</span><span class="ui brown basic label">Education</span><span class="ui brown basic label">Young Learners</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-feick"><div class="three wide column" style="margin:auto"><img alt="chi-2018-feick cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/chi-2018-feick.jpg 1x" src="/static/images/publications/cover/chi-2018-feick.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span><span class="ui big basic pink label"><b><svg data-prefix="fas" data-icon="award" class="svg-inline--fa fa-award" role="img" viewBox="0 0 448 512" aria-hidden="true"><path fill="currentColor" d="M245.9-25.9c-13.4-8.2-30.3-8.2-43.7 0-24.4 14.9-39.5 18.9-68.1 18.3-15.7-.4-30.3 8.1-37.9 21.9-13.7 25.1-24.8 36.2-49.9 49.9-13.8 7.5-22.2 22.2-21.9 37.9 .7 28.6-3.4 43.7-18.3 68.1-8.2 13.4-8.2 30.3 0 43.7 14.9 24.4 18.9 39.5 18.3 68.1-.4 15.7 8.1 30.3 21.9 37.9 22.1 12.1 33.3 22.1 45.1 41.5L42.7 458.5c-5.9 11.9-1.1 26.3 10.7 32.2l86 43c11.5 5.7 25.5 1.4 31.7-9.8l52.8-95.1 52.8 95.1c6.2 11.2 20.2 15.6 31.7 9.8l86-43c11.9-5.9 16.7-20.3 10.7-32.2l-48.6-97.2c11.7-19.4 23-29.4 45.1-41.5 13.8-7.5 22.2-22.2 21.9-37.9-.7-28.6 3.4-43.7 18.3-68.1 8.2-13.4 8.2-30.3 0-43.7-14.9-24.4-18.9-39.5-18.3-68.1 .4-15.7-8.1-30.3-21.9-37.9-25.1-13.7-36.2-24.8-49.9-49.9-7.5-13.8-22.2-22.2-37.9-21.9-28.6 .7-43.7-3.4-68.1-18.3zM224 96a96 96 0 1 1 0 192 96 96 0 1 1 0-192z"></path></svg> Honorable Mention</b></span></p><p class="color" style="font-size:1.3em"><b>Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration</b></p><p><a href="/people/martin-feick/"><img alt="Martin Feick picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/martin-feick.jpg 1x" src="/static/images/people/martin-feick.jpg"/><span class="author-link">Martin Feick</span></a><span class="role"></span>, <span>Terrance Tin Hoi Mok</span><span class="role"></span>, <a href="/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/anthony-tang.jpg 1x" src="/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a><span class="role"></span>, <a href="/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"></span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Cscw</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Object Focused Collaboration</span><span class="ui brown basic label">Physical Telepresence</span><span class="ui brown basic label">Collaborative Physical Tasks</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-mahadevan"><div class="three wide column" style="margin:auto"><img alt="chi-2018-mahadevan cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/chi-2018-mahadevan.jpg 1x" src="/static/images/publications/cover/chi-2018-mahadevan.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>Communicating Awareness and Intent in Autonomous Vehicle-Pedestrian Interaction</b></p><p><a href="/people/karthik-mahadevan/"><img alt="Karthik Mahadevan picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/karthik-mahadevan.jpg 1x" src="/static/images/people/karthik-mahadevan.jpg"/><span class="author-link">Karthik Mahadevan</span></a><span class="role"></span>, <a href="/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"></span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Autonomous Vehicle Pedestrian Interaction</span><span class="ui brown basic label">Perceived Awareness And Intent In Autonomous Vehicles</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="hri-2018-feick"><div class="three wide column" style="margin:auto"><img alt="hri-2018-feick cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/hri-2018-feick.jpg 1x" src="/static/images/publications/cover/hri-2018-feick.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">HRI 2018</span></p><p class="color" style="font-size:1.3em"><b>The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</b></p><p><a href="/people/martin-feick/"><img alt="Martin Feick picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/martin-feick.jpg 1x" src="/static/images/people/martin-feick.jpg"/><span class="author-link">Martin Feick</span></a><span class="role"></span>, <a href="/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"></span>, <a href="/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/anthony-tang.jpg 1x" src="/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a><span class="role"></span>, <span>Andr√© Miede</span><span class="role"></span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Movement Trajectory Velocity</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Robot Surrogate</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="sui-2017-li"><div class="three wide column" style="margin:auto"><img alt="sui-2017-li cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/sui-2017-li.jpg 1x" src="/static/images/publications/cover/sui-2017-li.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">SUI 2017</span></p><p class="color" style="font-size:1.3em"><b>Visibility Perception and Dynamic Viewsheds for Topographic Maps and Models</b></p><p><span>Nico Li</span><span class="role"></span>, <a href="/people/wesley-willett/"><img alt="Wesley Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley Willett</span></a><span class="role"></span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"></span>, <span>Mario Costa Sousa</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Terrain Visualization</span><span class="ui brown basic label">Geospatial Visualization</span><span class="ui brown basic label">Dynamic Viewshed</span><span class="ui brown basic label">Topographic Maps</span><span class="ui brown basic label">Tangible User Interfaces</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2017-somanath"><div class="three wide column" style="margin:auto"><img alt="chi-2017-somanath cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/chi-2017-somanath.jpg 1x" src="/static/images/publications/cover/chi-2017-somanath.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2017</span></p><p class="color" style="font-size:1.3em"><b>&#x27;Maker&#x27; within Constraints: Exploratory Study of Young Learners using Arduino at a High School in India</b></p><p><a href="/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"></span>, <a href="/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"></span>, <span>Janette Hughes</span><span class="role"></span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"></span>, <span>Mario Costa Sousa</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">India</span><span class="ui brown basic label">HCI 4 D</span><span class="ui brown basic label">Physical Computing</span><span class="ui brown basic label">DIY</span><span class="ui brown basic label">Young Learners</span><span class="ui brown basic label">Maker Culture</span></div></div></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2016-jones"><div class="three wide column" style="margin:auto"><img alt="dis-2016-jones cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/dis-2016-jones.jpg 1x" src="/static/images/publications/cover/dis-2016-jones.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2016</span></p><p class="color" style="font-size:1.3em"><b>Elevating Communication, Collaboration, and Shared Experiences in Mobile Video through Drones</b></p><p><a href="/people/brennan-jones/"><img alt="Brennan Jones picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/brennan-jones.jpg 1x" src="/static/images/people/brennan-jones.jpg"/><span class="author-link">Brennan Jones</span></a><span class="role"></span>, <span>Kody Dillman</span><span class="role"></span>, <span>Richard Tang</span><span class="role"></span>, <a href="/people/anthony-tang/"><img alt="Anthony Tang picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/anthony-tang.jpg 1x" src="/static/images/people/anthony-tang.jpg"/><span class="author-link">Anthony Tang</span></a><span class="role"></span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"></span>, <a href="/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"></span>, <a href="/people/carman-neustaedter/"><img alt="Carman Neustaedter picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/carman-neustaedter.jpg 1x" src="/static/images/people/carman-neustaedter.jpg"/><span class="author-link">Carman Neustaedter</span></a><span class="role"></span>, <span>Scott Bateman</span><span class="role"></span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Cscw</span><span class="ui brown basic label">Telepresence</span><span class="ui brown basic label">Video Communication</span><span class="ui brown basic label">Shared Experiences</span><span class="ui brown basic label">Teleoperation</span><span class="ui brown basic label">Drones</span><span class="ui brown basic label">Collaboration</span><span class="ui brown basic label">Hri</span></div></div></div></div></div><div id="publications-modal"><div id="chi-2025-madill" class="ui large modal"><div class="header"><a target="_blank" href="/publications/chi-2025-madill/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2025-madill</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2025</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2025-madill cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/chi-2025-madill.jpg 1x" src="/static/images/publications/cover/chi-2025-madill.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2025-madill" target="_blank">Playing with Robots: Performing Arts Techniques for Designing and Understanding Robot Group Movement</a></h1><p class="meta"><span>Philippa Madill<!-- --> <span class="role"></span></span>, <span>Matthew Newton<!-- --> <span class="role"></span></span>, <span>Huanjun Zhao<!-- --> <span class="role"></span></span>, <span>Yichen Lian<!-- --> <span class="role"></span></span>, <a href="/people/zachary-mckendrick"><img alt="zachary-mckendrick photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/zachary-mckendrick.jpg 1x" src="/static/images/people/zachary-mckendrick.jpg"/><strong>Zachary McKendrick</strong></a><span class="role"></span>, <span>Patrick Finn<!-- --> <span class="role"></span></span>, <a href="/people/aditya-shekhar-nittala"><img alt="aditya-shekhar-nittala photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/aditya-shekhar-nittala.jpg 1x" src="/static/images/people/aditya-shekhar-nittala.jpg"/><strong>Aditya Shekhar Nittala</strong></a><span class="role"></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2025-madill.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2025-madill.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this work, we introduce a formal design approach derived from the performing arts to design robot group behaviour. In our first experiment, we worked with professional actors, directors, and non-specialists using a participatory design approach to identify common group behaviour patterns. In a follow-up studio work, we identified twelve common group movement patterns, transposed them into a performance script, built a scale model to support the performance process, and evaluated the patterns with a senior actor under studio conditions. We evaluated our refined models with 20 volunteers in a user study in the third experiment. Results from our affective circumplex modelling suggest that the patterns elicit positive emotional responses from the users. Also, participants performed better than chance in identifying the motion patterns without prior training. Based on our results, we propose design guidelines for social robots‚Äô behaviour and movement design to improve their overall comprehensibility in interaction.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Humanities</span><span class="ui brown basic label">Art</span><span class="ui brown basic label">Robots</span><span class="ui brown basic label">Method</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Philippa Madill<!-- -->, <!-- -->Matthew Newton<!-- -->, <!-- -->Huanjun Zhao<!-- -->, <!-- -->Yichen Lian<!-- -->, <!-- -->Zachary McKendrick<!-- -->, <!-- -->Patrick Finn<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Ehud Sharlin<!-- -->.¬†<b>Playing with Robots: Performing Arts Techniques for Designing and Understanding Robot Group Movement</b>.¬†<i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2025<!-- -->)</i>ACM, New York, NY, USA<!-- -->¬†<!-- -->DOI: <a href="https://dl.acm.org/doi/10.1145/3706598.3713996" target="_blank">https://dl.acm.org/doi/10.1145/3706598.3713996</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2021-asha" class="ui large modal"><div class="header"><a target="_blank" href="/publications/dis-2021-asha/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2021-asha</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2021-asha cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/dis-2021-asha.jpg 1x" src="/static/images/publications/cover/dis-2021-asha.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2021-asha" target="_blank">Co-Designing Interactions between Pedestrians in Wheelchairs and Autonomous Vehicles</a></h1><p class="meta"><a href="/people/ashratuz-zavin-asha"><img alt="ashratuz-zavin-asha photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ashratuz-zavin-asha.jpg 1x" src="/static/images/people/ashratuz-zavin-asha.jpg"/><strong>Ashratuz Zavin Asha</strong></a><span class="role"></span>, <a href="/people/christopher-smith"><img alt="christopher-smith photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/christopher-smith.jpg 1x" src="/static/images/people/christopher-smith.jpg"/><strong>Christopher Smith</strong></a><span class="role"></span>, <a href="/people/georgina-freeman"><img alt="georgina-freeman photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/georgina-freeman.jpg 1x" src="/static/images/people/georgina-freeman.jpg"/><strong>Georgina Freeman</strong></a><span class="role"></span>, <span>Sean Crump<!-- --> <span class="role"></span></span>, <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>In the near future, mixed traffic consisting of manual and autonomous vehicles (AVs) will be common. Questions surrounding how vulnerable road users such as pedestrians in wheelchairs (PWs) will make crossing decisions in these new situations are underexplored. We conducted a remote co-design study with one of the researchers of this work who has the lived experience as a powered wheelchair user and applied inclusive design practices. This allowed us to identify and reflect on interface design ideas that can help PWs make safe crossing decisions at intersections. Through an iterative five-week study, we implemented interfaces that can be placed on the vehicle, on the wheelchair, and on the street infrastructure and evaluated them during the co-design sessions using a VR simulator testbed. Informed by our findings, we discuss design insights for implementing inclusive interfaces to improve interactions between autonomous vehicles and vulnerable road users.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Pedestrians In Wheelchairs</span><span class="ui brown basic label">Co Design</span><span class="ui brown basic label">Autonomous Vehicles</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ashratuz Zavin Asha<!-- -->, <!-- -->Christopher Smith<!-- -->, <!-- -->Georgina Freeman<!-- -->, <!-- -->Sean Crump<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Ehud Sharlin<!-- -->.¬†<b>Co-Designing Interactions between Pedestrians in Wheelchairs and Autonomous Vehicles</b>.¬†<i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- --> <!-- -->(<!-- -->DIS 2021<!-- -->)</i>ACM, New York, NY, USA<!-- -->¬†<!-- -->Page: 1-<!-- -->13<!-- -->.¬†<!-- -->DOI: <a href="https://doi.org/10.1145/3461778.3462068" target="_blank">10.1145/3461778.3462068</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2021-hammad" class="ui large modal"><div class="header"><a target="_blank" href="/publications/chi-2021-hammad/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2021-hammad</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2021-hammad cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/chi-2021-hammad.jpg 1x" src="/static/images/publications/cover/chi-2021-hammad.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2021-hammad" target="_blank">Homecoming: Exploring Returns to Long-Term Single Player Games</a></h1><p class="meta"><span>Noor Hammad<!-- --> <span class="role"></span></span>, <span>Owen Brierley<!-- --> <span class="role"></span></span>, <a href="/people/zachary-mckendrick"><img alt="zachary-mckendrick photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/zachary-mckendrick.jpg 1x" src="/static/images/people/zachary-mckendrick.jpg"/><strong>Zachary McKendrick</strong></a><span class="role"></span>, <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"></span>, <span>Patrick Finn<!-- --> <span class="role"></span></span>, <span>Jessica Hammer<!-- --> <span class="role"></span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present an autobiographical design journey exploring the experience of returning to long-term single player games. Continuing progress from a previously saved game, particularly when substantial time has passed, is an understudied area in games research. To begin our exploration in this domain, we investigated what the return experience is like first-hand. By returning to four long-term single player games played extensively in the past, we revealed a phenomenon we call The Pivot Point, a ‚Äòeureka‚Äô moment in return gameplay. The pivot point anchors our design explorations, where we created prototypes to leverage the pivot point in reconnecting with the experience. These return experiences and subsequent prototyping iterations inform our understanding of how to design better returns to gameplay, which can benefit both producers and consumers of long-term single player games.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Long Term Single Player Game</span><span class="ui brown basic label">Autobiographical Design</span><span class="ui brown basic label">Pivot Point</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Noor Hammad<!-- -->, <!-- -->Owen Brierley<!-- -->, <!-- -->Zachary McKendrick<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->Patrick Finn<!-- -->, <!-- -->Jessica Hammer<!-- -->, <!-- -->Ehud Sharlin<!-- -->.¬†<b>Homecoming: Exploring Returns to Long-Term Single Player Games</b>.¬†<i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2021<!-- -->)</i>ACM, New York, NY, USA<!-- -->¬†<!-- -->Page: 1-<!-- -->13<!-- -->.¬†<!-- -->DOI: <a href="https://doi.org/10.1145/3411764.3445357" target="_blank">https://doi.org/10.1145/3411764.3445357</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="imwut-2020-wang" class="ui large modal"><div class="header"><a target="_blank" href="/publications/imwut-2020-wang/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>imwut-2020-wang</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">IMWUT 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="imwut-2020-wang cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/imwut-2020-wang.jpg 1x" src="/static/images/publications/cover/imwut-2020-wang.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/imwut-2020-wang" target="_blank">AssessBlocks: Exploring Toy Block Play Features for Assessing Stress in Young Children after Natural Disasters</a></h1><p class="meta"><span>Xiyue Wang<!-- --> <span class="role"></span></span>, <span>Kazuki Takashima<!-- --> <span class="role"></span></span>, <span>Tomoaki Adachi<!-- --> <span class="role"></span></span>, <span>Patrick Finn<!-- --> <span class="role"></span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"></span>, <span>Yoshifumi Kitamura<!-- --> <span class="role"></span></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/imwut-2020-wang.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>imwut-2020-wang.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/fxxvZBY80ug" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/fxxvZBY80ug?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/fxxvZBY80ug/maxresdefault.jpg src=https://img.youtube.com/vi/fxxvZBY80ug/maxresdefault.jpg&gt;&lt;span&gt;‚ñ∂&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Natural disasters cause long-lasting mental health problems such as PTSD in children. Following the 2011 Earthquake and Tsunami in Japan, we witnessed a shift of toy block play behavior in young children who suffered from stress after the disaster. The behavior reflected their emotional responses to the traumatic event. In this paper, we explore the feasibility of using data captured from block-play to assess children&#x27;s stress after a major natural disaster. We prototyped sets of sensor-embedded toy blocks, AssessBlocks, that automate quantitative play data acquisition. During a three-year period, the blocks were dispatched to fifty-two post-disaster children. Within a free play session, we captured block features, a child&#x27;s playing behavior, and stress evaluated by several methods. The result from our analysis reveal correlations between block play features and stress measurements and show initial promise of using the effectiveness of using AssessBlocks to assess children&#x27;s stress after a disaster. We provide detailed insights into the potential as well as the challenges of our approach and unique conditions. From these insights we summarize guidelines for future research in automated play assessment systems that support children&#x27;s mental health.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Well Being</span><span class="ui brown basic label">Toy Blocks</span><span class="ui brown basic label">PTSD</span><span class="ui brown basic label">Tangibles For Health</span><span class="ui brown basic label">Stress Assessment</span><span class="ui brown basic label">Play</span><span class="ui brown basic label">Children</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Xiyue Wang<!-- -->, <!-- -->Kazuki Takashima<!-- -->, <!-- -->Tomoaki Adachi<!-- -->, <!-- -->Patrick Finn<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Yoshifumi Kitamura<!-- -->.¬†<b>AssessBlocks: Exploring Toy Block Play Features for Assessing Stress in Young Children after Natural Disasters</b>.¬†<i>In <!-- -->Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies<!-- --> <!-- -->(<!-- -->IMWUT 2020<!-- -->)</i>ACM, New York, NY, USA<!-- -->¬†<!-- -->Page: 1-<!-- -->29<!-- -->.¬†<!-- -->DOI: <a href="https://doi.org/10.1145/3381016" target="_blank">https://doi.org/10.1145/3381016</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-asha" class="ui large modal"><div class="header"><a target="_blank" href="/publications/chi-2020-asha/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2020-asha</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2020 LBW</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2020-asha cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/chi-2020-asha.jpg 1x" src="/static/images/publications/cover/chi-2020-asha.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-asha" target="_blank">Views from the Wheelchair: Understanding Interaction between Autonomous Vehicle and Pedestrians with Reduced Mobility</a></h1><p class="meta"><a href="/people/ashratuz-zavin-asha"><img alt="ashratuz-zavin-asha photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ashratuz-zavin-asha.jpg 1x" src="/static/images/people/ashratuz-zavin-asha.jpg"/><strong>Ashratuz Zavin Asha</strong></a><span class="role"></span>, <a href="/people/christopher-smith"><img alt="christopher-smith photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/christopher-smith.jpg 1x" src="/static/images/people/christopher-smith.jpg"/><strong>Christopher Smith</strong></a><span class="role"></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"></span>, <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"></span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/JNc49desa44" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/JNc49desa44?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/JNc49desa44/maxresdefault.jpg src=https://img.youtube.com/vi/JNc49desa44/maxresdefault.jpg&gt;&lt;span&gt;‚ñ∂&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We are interested in the ways pedestrians will interact with autonomous vehicle (AV) in a future AV transportation ecosystem, when nonverbal cues from the driver such as eye movements, hand gestures, etc. are no longer provided. In this work, we examine a subset of this challenge: interaction between pedestrian with reduced mobility (PRM) and AV. This study explores interface designs between AVs and people in a wheelchair to help them interact with AVs by conducting a preliminary design study. We have assessed the data collected from the study using qualitative analysis and presented different findings on AV-PRM interactions. Our findings reflect on the importance of visual interfaces, changes to the wheelchair and the creative use of the street infrastructure.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Autonomous Vehicle</span><span class="ui brown basic label">Pedestrian With Reduced Mobility</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ashratuz Zavin Asha<!-- -->, <!-- -->Christopher Smith<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->Ehud Sharlin<!-- -->.¬†<b>Views from the Wheelchair: Understanding Interaction between Autonomous Vehicle and Pedestrians with Reduced Mobility</b>.¬†<i>(<!-- -->CHI 2020 LBW<!-- -->)</i>¬†<!-- -->Page: 1-<!-- -->8<!-- -->.¬†<!-- -->DOI: <a href="https://doi.org/10.1145/3334480.3383041" target="_blank">10.1145/3334480.3383041</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-hou" class="ui large modal"><div class="header"><a target="_blank" href="/publications/chi-2020-hou/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2020-hou</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2020-hou cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/chi-2020-hou.jpg 1x" src="/static/images/publications/cover/chi-2020-hou.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-hou" target="_blank">Autonomous Vehicle-Cyclist Interaction: Peril and Promise</a></h1><p class="meta"><span>Ming Hou<!-- --> <span class="role"></span></span>, <a href="/people/karthik-mahadevan"><img alt="karthik-mahadevan photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/karthik-mahadevan.jpg 1x" src="/static/images/people/karthik-mahadevan.jpg"/><strong>Karthik Mahadevan</strong></a><span class="role"></span>, <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2020-hou.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2020-hou.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/fsgbUeAaFfI" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/fsgbUeAaFfI?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/fsgbUeAaFfI/maxresdefault.jpg src=https://img.youtube.com/vi/fsgbUeAaFfI/maxresdefault.jpg&gt;&lt;span&gt;‚ñ∂&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Autonomous vehicles (AVs) will redefine interactions between road users. Presently, cyclists and drivers communicate through implicit cues (vehicle motion) and explicit but imprecise signals (hand gestures, horns). Future AVs could consistently communicate awareness and intent and other feedback to cyclists based on their sensor data. We present an exploration of AV-cyclist interaction, starting with preliminary design studies which informed the implementation of an immersive VR AV-cyclist simulator, and the design and evaluation of a number of AV-cyclist interfaces. Our findings suggest that AV-cyclist interfaces can improve rider confidence in lane merging scenarios. We contribute an AV-cyclist immersive simulator, insights on trade-offs of various aspects of AV-cyclist interaction design including modalities, location, and complexity, and positive results suggesting improved rider confidence due to AV-cyclist interaction. While we are encouraged by the potential positive impact AV-cyclist interfaces can have on cyclist culture, we also emphasize the risks over-reliance can pose to cyclists.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Autonomous Vehicle Cyclist Interaction</span><span class="ui brown basic label">Interfaces For Communicating Intent And Awareness</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ming Hou<!-- -->, <!-- -->Karthik Mahadevan<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Lora Oehlberg<!-- -->.¬†<b>Autonomous Vehicle-Cyclist Interaction: Peril and Promise</b>.¬†<i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2020<!-- -->)</i>ACM, New York, NY, USA<!-- -->¬†<!-- -->Page: 1-<!-- -->12<!-- -->.¬†<!-- -->DOI: <a href="https://doi.org/10.1145/3313831.3376884" target="_blank">https://doi.org/10.1145/3313831.3376884</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/DtxkWAW9B1s" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/DtxkWAW9B1s?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/DtxkWAW9B1s/maxresdefault.jpg src=https://img.youtube.com/vi/DtxkWAW9B1s/maxresdefault.jpg&gt;&lt;span&gt;‚ñ∂&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="cnc-2019-hammad" class="ui large modal"><div class="header"><a target="_blank" href="/publications/cnc-2019-hammad/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>cnc-2019-hammad</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">C&amp;C 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="cnc-2019-hammad cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/cnc-2019-hammad.jpg 1x" src="/static/images/publications/cover/cnc-2019-hammad.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/cnc-2019-hammad" target="_blank">Mutation: Leveraging Performing Arts Practices in Cyborg Transitioning</a></h1><p class="meta"><a href="/people/nour-hammad"><img alt="nour-hammad photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/nour-hammad.jpg 1x" src="/static/images/people/nour-hammad.jpg"/><strong>Nour Hammad</strong></a><span class="role"></span>, <span>Elaheh Sanoubari<!-- --> <span class="role"></span></span>, <span>Patrick Finn<!-- --> <span class="role"></span></span>, <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"></span>, <span>James E. Young<!-- --> <span class="role"></span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/cnc-2019-hammad.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>cnc-2019-hammad.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/HFH59__Fkok" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/HFH59__Fkok?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/HFH59__Fkok/maxresdefault.jpg src=https://img.youtube.com/vi/HFH59__Fkok/maxresdefault.jpg&gt;&lt;span&gt;‚ñ∂&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present Mutation: performing arts based approach that can help decrease the cognitive load associated with cyborg transitioning. Cyborgs are human-machine hybrids with organic and mechatronic body parts that can be implanted or worn. The transition into and out of experiencing additional body parts is not fully understood. Our goal is to draw from performing arts techniques in order to help decrease the cognitive load associated with becoming and unbecoming a cyborg. Actors constantly shift between states, whether from one character to another, or from pre- to post- performance. We contribute a straightforward adaptation of classic performing art practices to cyborg transitioning, and a study where actors used these protocols in order to enter a cyborg state, perform as a cyborg, and then exit the cyborg state. Our work on Mutation suggests that classic performing art practices can be useful in cyborg transitioning, as well as in other technology augmented experiences.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Interaction Design</span><span class="ui brown basic label">Cyborgs</span><span class="ui brown basic label">User Experience</span><span class="ui brown basic label">Performing Art Techniques</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nour Hammad<!-- -->, <!-- -->Elaheh Sanoubari<!-- -->, <!-- -->Patrick Finn<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->James E. Young<!-- -->, <!-- -->Ehud Sharlin<!-- -->.¬†<b>Mutation: Leveraging Performing Arts Practices in Cyborg Transitioning</b>.¬†<i>In <!-- -->Proceedings of the ACM on Creativity and Cognition<!-- --> <!-- -->(<!-- -->C&amp;C 2019<!-- -->)</i>ACM, New York, NY, USA<!-- -->¬†<!-- -->DOI: <a href="https://doi.org/10.1145/3325480.3325508" target="_blank">https://doi.org/10.1145/3325480.3325508</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2019-mahadevan" class="ui large modal"><div class="header"><a target="_blank" href="/publications/dis-2019-mahadevan/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2019-mahadevan</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2019-mahadevan cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/dis-2019-mahadevan.jpg 1x" src="/static/images/publications/cover/dis-2019-mahadevan.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2019-mahadevan" target="_blank">AV-Pedestrian Interaction Design Using a Pedestrian Mixed Traffic Simulator</a></h1><p class="meta"><a href="/people/karthik-mahadevan"><img alt="karthik-mahadevan photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/karthik-mahadevan.jpg 1x" src="/static/images/people/karthik-mahadevan.jpg"/><strong>Karthik Mahadevan</strong></a><span class="role"></span>, <span>Elaheh Sanoubari<!-- --> <span class="role"></span></span>, <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"></span>, <span>James E. Young<!-- --> <span class="role"></span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/dis-2019-mahadevan.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>dis-2019-mahadevan.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>AV-pedestrian interaction will impact pedestrian safety, etiquette, and overall acceptance of AV technology. Evaluating AV-pedestrian interaction is challenging given limited availability of AVs and safety concerns. These challenges are compounded by &quot;mixed traffic&quot; conditions: studying AV-pedestrian interaction will be difficult in traffic consisting of vehicles varying in autonomy level. We propose immersive pedestrian simulators as design tools to study AV-pedestrian interaction, allowing rapid prototyping and evaluation of future AV-pedestrian interfaces. We present OnFoot: a VR-based simulator that immerses participants in mixed traffic conditions and allows examination of their behavior while controlling vehicles&#x27; autonomy-level, traffic and street characteristics, behavior of other virtual pedestrians, and integration of novel AV-pedestrian interfaces. We validated OnFoot against prior simulators and Wizard-of-Oz studies, and conducted a user study, manipulating vehicles&#x27; autonomy level, interfaces, and pedestrian group behavior. Our findings highlight the potential to use VR simulators as powerful tools for AV-pedestrian interaction design in mixed traffic.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Mixed Traffic</span><span class="ui brown basic label">Pedestrian Simulator</span><span class="ui brown basic label">Autonomous Vehicle Pedestrian Interaction</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Karthik Mahadevan<!-- -->, <!-- -->Elaheh Sanoubari<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->James E. Young<!-- -->, <!-- -->Ehud Sharlin<!-- -->.¬†<b>AV-Pedestrian Interaction Design Using a Pedestrian Mixed Traffic Simulator</b>.¬†<i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- --> <!-- -->(<!-- -->DIS 2019<!-- -->)</i>ACM, New York, NY, USA<!-- -->¬†<!-- -->Page: 1-<!-- -->12<!-- -->.¬†<!-- -->DOI: <a href="https://doi.org/10.1145/3322276.3322328" target="_blank">https://doi.org/10.1145/3322276.3322328</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2018-ta" class="ui large modal"><div class="header"><a target="_blank" href="/publications/dis-2018-ta/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2018-ta</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2018-ta cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/dis-2018-ta.jpg 1x" src="/static/images/publications/cover/dis-2018-ta.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2018-ta" target="_blank">Bod-IDE: An Augmented Reality Sandbox for eFashion Garments</a></h1><p class="meta"><span>Kevin Ta<!-- --> <span class="role"></span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/dis-2018-ta.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>dis-2018-ta.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Electronic fashion (eFashion) garments use technology to augment the human body with wearable interaction. In developing ideas, eFashion designers need to prototype the role and behavior of the interactive garment in context; however, current wearable prototyping toolkits require semi-permanent construction with physical materials that cannot easily be altered. We present Bod-IDE, an augmented reality &#x27;mirror&#x27; that allows eFashion designers to create virtual interactive garment prototypes. Designers can quickly build, refine, and test on-the-body interactions without the need to connect or program electronics. By envisioning interaction with the body in mind, eFashion designers can focus more on reimagining the relationship between bodies, clothing, and technology.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Electronic Fashion</span><span class="ui brown basic label">Creativity Support Tool</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kevin Ta<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Lora Oehlberg<!-- -->.¬†<b>Bod-IDE: An Augmented Reality Sandbox for eFashion Garments</b>.¬†<i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- --> <!-- -->(<!-- -->DIS 2018<!-- -->)</i>ACM, New York, NY, USA<!-- -->¬†<!-- -->Page: 1-<!-- -->5<!-- -->.¬†<!-- -->DOI: <a href="https://doi.org/10.1145/3197391.3205408" target="_blank">https://doi.org/10.1145/3197391.3205408</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tei-2016-somanath" class="ui large modal"><div class="header"><a target="_blank" href="/publications/tei-2016-somanath/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>tei-2016-somanath</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">TEI 2016</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="tei-2016-somanath cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/tei-2016-somanath.jpg 1x" src="/static/images/publications/cover/tei-2016-somanath.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tei-2016-somanath" target="_blank">Engaging &#x27;At-Risk&#x27; Students through Maker Culture Activities</a></h1><p class="meta"><a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"></span>, <span>Laura Morrison<!-- --> <span class="role"></span></span>, <span>Janette Hughes<!-- --> <span class="role"></span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"></span>, <span>Mario Costa Sousa<!-- --> <span class="role"></span></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/tei-2016-somanath.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>tei-2016-somanath.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>This paper presents a set of lessons learnt from introducing maker culture and DIY paradigms to &#x27;at-risk&#x27; students (age 12-14). Our goal is to engage &#x27;at-risk&#x27; students through maker culture activities. While improved technology literacy is one of the outcomes we also wanted the learners to use technology to realize concepts and ideas, and to gain freedom of thinking similar to creators, artists and designers. We present our study and a set of high level suggestions to enable thinking about how maker culture activities can facilitate engagement and creative use of technology by 1) thinking about creativity in task, 2) facilitating different entry points, 3) the importance of personal relevance, and 4) relevance to education.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">DIY</span><span class="ui brown basic label">At Risk Students</span><span class="ui brown basic label">Maker Culture</span><span class="ui brown basic label">Education</span><span class="ui brown basic label">Young Learners</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sowmya Somanath<!-- -->, <!-- -->Laura Morrison<!-- -->, <!-- -->Janette Hughes<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Mario Costa Sousa<!-- -->.¬†<b>Engaging &#x27;At-Risk&#x27; Students through Maker Culture Activities</b>.¬†<i>In <!-- -->Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction<!-- --> <!-- -->(<!-- -->TEI 2016<!-- -->)</i>ACM, New York, NY, USA<!-- -->¬†<!-- -->Page: 1-<!-- -->9<!-- -->.¬†<!-- -->DOI: <a href="https://doi.org/10.1145/2839462.2839482" target="_blank">https://doi.org/10.1145/2839462.2839482</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-feick" class="ui large modal"><div class="header"><a target="_blank" href="/publications/chi-2018-feick/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2018-feick</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2018-feick cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/chi-2018-feick.jpg 1x" src="/static/images/publications/cover/chi-2018-feick.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-feick" target="_blank">Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration</a></h1><p class="meta"><a href="/people/martin-feick"><img alt="martin-feick photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/martin-feick.jpg 1x" src="/static/images/people/martin-feick.jpg"/><strong>Martin Feick</strong></a><span class="role"></span>, <span>Terrance Tin Hoi Mok<!-- --> <span class="role"></span></span>, <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/anthony-tang.jpg 1x" src="/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a><span class="role"></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2018-feick.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2018-feick.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/sfxTHsPJWHY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/sfxTHsPJWHY?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/sfxTHsPJWHY/maxresdefault.jpg src=https://img.youtube.com/vi/sfxTHsPJWHY/maxresdefault.jpg&gt;&lt;span&gt;‚ñ∂&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Remote collaborators working together on physical objects have difficulty building a shared understanding of what each person is talking about. Conventional video chat systems are insufficient for many situations because they present a single view of the object in a flattened image. To understand how this limited perspective affects collaboration, we designed the Remote Manipulator (ReMa), which can reproduce orientation manipulations on a proxy object at a remote site. We conducted two studies with ReMa, with two main findings. First, a shared perspective is more effective and preferred compared to the opposing perspective offered by conventional video chat systems. Second, the physical proxy and video chat complement one another in a combined system: people used the physical proxy to understand objects, and used video chat to perform gestures and confirm remote actions.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Cscw</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Object Focused Collaboration</span><span class="ui brown basic label">Physical Telepresence</span><span class="ui brown basic label">Collaborative Physical Tasks</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Martin Feick<!-- -->, <!-- -->Terrance Tin Hoi Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Ehud Sharlin<!-- -->.¬†<b>Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration</b>.¬†<i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2018<!-- -->)</i>ACM, New York, NY, USA<!-- -->¬†<!-- -->Page: 1-<!-- -->13<!-- -->.¬†<!-- -->DOI: <a href="https://doi.org/10.1145/3173574.3173855" target="_blank">https://doi.org/10.1145/3173574.3173855</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-mahadevan" class="ui large modal"><div class="header"><a target="_blank" href="/publications/chi-2018-mahadevan/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2018-mahadevan</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2018-mahadevan cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/chi-2018-mahadevan.jpg 1x" src="/static/images/publications/cover/chi-2018-mahadevan.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-mahadevan" target="_blank">Communicating Awareness and Intent in Autonomous Vehicle-Pedestrian Interaction</a></h1><p class="meta"><a href="/people/karthik-mahadevan"><img alt="karthik-mahadevan photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/karthik-mahadevan.jpg 1x" src="/static/images/people/karthik-mahadevan.jpg"/><strong>Karthik Mahadevan</strong></a><span class="role"></span>, <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/chi-2018-mahadevan.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>chi-2018-mahadevan.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/D_hhcGVREGA" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/D_hhcGVREGA?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/D_hhcGVREGA/maxresdefault.jpg src=https://img.youtube.com/vi/D_hhcGVREGA/maxresdefault.jpg&gt;&lt;span&gt;‚ñ∂&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Drivers use nonverbal cues such as vehicle speed, eye gaze, and hand gestures to communicate awareness and intent to pedestrians. Conversely, in autonomous vehicles, drivers can be distracted or absent, leaving pedestrians to infer awareness and intent from the vehicle alone. In this paper, we investigate the usefulness of interfaces (beyond vehicle movement) that explicitly communicate awareness and intent of autonomous vehicles to pedestrians, focusing on crosswalk scenarios. We conducted a preliminary study to gain insight on designing interfaces that communicate autonomous vehicle awareness and intent to pedestrians. Based on study outcomes, we developed four prototype interfaces and deployed them in studies involving a Segway and a car. We found interfaces communicating vehicle awareness and intent: (1) can help pedestrians attempting to cross; (2) are not limited to the vehicle and can exist in the environment; and (3) should use a combination of modalities such as visual, auditory, and physical.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Autonomous Vehicle Pedestrian Interaction</span><span class="ui brown basic label">Perceived Awareness And Intent In Autonomous Vehicles</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Karthik Mahadevan<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->Ehud Sharlin<!-- -->.¬†<b>Communicating Awareness and Intent in Autonomous Vehicle-Pedestrian Interaction</b>.¬†<i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2018<!-- -->)</i>ACM, New York, NY, USA<!-- -->¬†<!-- -->Page: 1-<!-- -->12<!-- -->.¬†<!-- -->DOI: <a href="https://doi.org/10.1145/3173574.3174003" target="_blank">https://doi.org/10.1145/3173574.3174003</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/08OEKuz93dY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/08OEKuz93dY?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/08OEKuz93dY/maxresdefault.jpg src=https://img.youtube.com/vi/08OEKuz93dY/maxresdefault.jpg&gt;&lt;span&gt;‚ñ∂&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="hri-2018-feick" class="ui large modal"><div class="header"><a target="_blank" href="/publications/hri-2018-feick/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>hri-2018-feick</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">HRI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="hri-2018-feick cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/hri-2018-feick.jpg 1x" src="/static/images/publications/cover/hri-2018-feick.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/hri-2018-feick" target="_blank">The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</a></h1><p class="meta"><a href="/people/martin-feick"><img alt="martin-feick photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/martin-feick.jpg 1x" src="/static/images/people/martin-feick.jpg"/><strong>Martin Feick</strong></a><span class="role"></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"></span>, <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/anthony-tang.jpg 1x" src="/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a><span class="role"></span>, <span>Andr√© Miede<!-- --> <span class="role"></span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/hri-2018-feick.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>hri-2018-feick.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this paper, we discuss the role of the movement trajectory and velocity enabled by our tele-robotic system (ReMa) for remote collaboration on physical tasks. Our system reproduces changes in object orientation and position at a remote location using a humanoid robotic arm. However, even minor kinematics differences between robot and human arm can result in awkward or exaggerated robot movements. As a result, user communication with the robotic system can become less efficient, less fluent and more time intensive.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Movement Trajectory Velocity</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Robot Surrogate</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Martin Feick<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Andr√© Miede<!-- -->, <!-- -->Ehud Sharlin<!-- -->.¬†<b>The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</b>.¬†<i>In <!-- -->Adjunct Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction<!-- --> <!-- -->(<!-- -->HRI 2018<!-- -->)</i>ACM, New York, NY, USA<!-- -->¬†<!-- -->Page: 1-<!-- -->2<!-- -->.¬†<!-- -->DOI: <a href="https://doi.org/10.1145/3173386.3176959" target="_blank">https://doi.org/10.1145/3173386.3176959</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="sui-2017-li" class="ui large modal"><div class="header"><a target="_blank" href="/publications/sui-2017-li/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>sui-2017-li</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">SUI 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="sui-2017-li cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/sui-2017-li.jpg 1x" src="/static/images/publications/cover/sui-2017-li.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/sui-2017-li" target="_blank">Visibility Perception and Dynamic Viewsheds for Topographic Maps and Models</a></h1><p class="meta"><span>Nico Li<!-- --> <span class="role"></span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><strong>Wesley Willett</strong></a><span class="role"></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"></span>, <span>Mario Costa Sousa<!-- --> <span class="role"></span></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/sui-2017-li.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>sui-2017-li.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/275404995" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/275404995?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://i.vimeocdn.com/video/707686230_640.webp src=https://i.vimeocdn.com/video/707686230_640.webp&gt;&lt;span&gt;‚ñ∂&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We compare the effectiveness of 2D maps and 3D terrain models for visibility tasks and demonstrate how interactive dynamic viewsheds can improve performance for both types of terrain representations. In general, the two-dimensional nature of classic topographic maps limits their legibility and can make complex yet typical cartographic tasks like determining the visibility between locations difficult. Both 3D physical models and interactive techniques like dynamic viewsheds have the potential to improve viewers&#x27; understanding of topography, but their impact has not been deeply explored. We evaluate the effectiveness of 2D maps, 3D models, and interactive viewsheds for both simple and complex visibility tasks. Our results demonstrate the benefits of the dynamic viewshed technique and highlight opportunities for additional tactile interactions. Based on these findings we present guidelines for improving the design and usability of future topographic maps and models.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Terrain Visualization</span><span class="ui brown basic label">Geospatial Visualization</span><span class="ui brown basic label">Dynamic Viewshed</span><span class="ui brown basic label">Topographic Maps</span><span class="ui brown basic label">Tangible User Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nico Li<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Mario Costa Sousa<!-- -->.¬†<b>Visibility Perception and Dynamic Viewsheds for Topographic Maps and Models</b>.¬†<i>(<!-- -->SUI 2017<!-- -->)</i>¬†<!-- -->Page: 1-<!-- -->9<!-- -->.¬†<!-- -->DOI: <a href="https://doi.org/10.1145/3131277.3132178" target="_blank">https://doi.org/10.1145/3131277.3132178</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/aVXUojoQF60" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/aVXUojoQF60?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/aVXUojoQF60/maxresdefault.jpg src=https://img.youtube.com/vi/aVXUojoQF60/maxresdefault.jpg&gt;&lt;span&gt;‚ñ∂&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2017-somanath" class="ui large modal"><div class="header"><a target="_blank" href="/publications/chi-2017-somanath/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>chi-2017-somanath</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">CHI 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="chi-2017-somanath cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/chi-2017-somanath.jpg 1x" src="/static/images/publications/cover/chi-2017-somanath.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2017-somanath" target="_blank">&#x27;Maker&#x27; within Constraints: Exploratory Study of Young Learners using Arduino at a High School in India</a></h1><p class="meta"><a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"></span>, <span>Janette Hughes<!-- --> <span class="role"></span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"></span>, <span>Mario Costa Sousa<!-- --> <span class="role"></span></span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/NpIME1h1mH8" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/NpIME1h1mH8?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/NpIME1h1mH8/maxresdefault.jpg src=https://img.youtube.com/vi/NpIME1h1mH8/maxresdefault.jpg&gt;&lt;span&gt;‚ñ∂&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Do-it-yourself (DIY) inspired activities have gained popularity as a means of creative expression and self-directed learning. However, DIY culture is difficult to implement in places with limited technology infrastructure and traditional learning cultures. Our goal is to understand how learners in such a setting react to DIY activities. We present observations from a physical computing workshop with 12 students (13-15 years old) conducted at a high school in India. We observed unique challenges for these students when tackling DIY activities: a high monetary and psychological cost to exploration, limited independent learning resources, difficulties with finding intellectual courage and assumed technical language proficiency. Our participants, however, overcome some of these challenges by adopting their own local strategies: resilience, nonverbal and verbal learning techniques, and creating documentation and fallback circuit versions. Based on our findings, we discuss a set of lessons learned about makerspaces in a context with socio-technical challenges.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">India</span><span class="ui brown basic label">HCI 4 D</span><span class="ui brown basic label">Physical Computing</span><span class="ui brown basic label">DIY</span><span class="ui brown basic label">Young Learners</span><span class="ui brown basic label">Maker Culture</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sowmya Somanath<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Janette Hughes<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Mario Costa Sousa<!-- -->.¬†<b>&#x27;Maker&#x27; within Constraints: Exploratory Study of Young Learners using Arduino at a High School in India</b>.¬†<i>In <!-- -->Proceedings of the CHI Conference on Human Factors in Computing Systems<!-- --> <!-- -->(<!-- -->CHI 2017<!-- -->)</i>ACM, New York, NY, USA<!-- -->¬†<!-- -->Page: 1-<!-- -->13<!-- -->.¬†<!-- -->DOI: <a href="https://doi.org/10.1145/3025453.3025849" target="_blank">https://doi.org/10.1145/3025453.3025849</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2016-jones" class="ui large modal"><div class="header"><a target="_blank" href="/publications/dis-2016-jones/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>dis-2016-jones</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications/">Publication</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">DIS 2016</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img alt="dis-2016-jones cover" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="cover" style="color:transparent" srcSet="/static/images/publications/cover/dis-2016-jones.jpg 1x" src="/static/images/publications/cover/dis-2016-jones.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2016-jones" target="_blank">Elevating Communication, Collaboration, and Shared Experiences in Mobile Video through Drones</a></h1><p class="meta"><a href="/people/brennan-jones"><img alt="brennan-jones photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/brennan-jones.jpg 1x" src="/static/images/people/brennan-jones.jpg"/><strong>Brennan Jones</strong></a><span class="role"></span>, <span>Kody Dillman<!-- --> <span class="role"></span></span>, <span>Richard Tang<!-- --> <span class="role"></span></span>, <a href="/people/anthony-tang"><img alt="anthony-tang photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/anthony-tang.jpg 1x" src="/static/images/people/anthony-tang.jpg"/><strong>Anthony Tang</strong></a><span class="role"></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"></span>, <a href="/people/carman-neustaedter"><img alt="carman-neustaedter photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/carman-neustaedter.jpg 1x" src="/static/images/people/carman-neustaedter.jpg"/><strong>Carman Neustaedter</strong></a><span class="role"></span>, <span>Scott Bateman<!-- --> <span class="role"></span></span></p><p><a href="https://raw.githubusercontent.com/ucalgary-ilab/ucalgary-ilab.github.io/main/static/publications/dis-2016-jones.pdf" target="_blank"><svg data-prefix="far" data-icon="file-pdf" class="svg-inline--fa fa-file-pdf" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M208 48L96 48c-8.8 0-16 7.2-16 16l0 384c0 8.8 7.2 16 16 16l80 0 0 48-80 0c-35.3 0-64-28.7-64-64L32 64C32 28.7 60.7 0 96 0L229.5 0c17 0 33.3 6.7 45.3 18.7L397.3 141.3c12 12 18.7 28.3 18.7 45.3l0 149.5-48 0 0-128-88 0c-39.8 0-72-32.2-72-72l0-88zM348.1 160L256 67.9 256 136c0 13.3 10.7 24 24 24l68.1 0zM240 380l32 0c33.1 0 60 26.9 60 60s-26.9 60-60 60l-12 0 0 28c0 11-9 20-20 20s-20-9-20-20l0-128c0-11 9-20 20-20zm32 80c11 0 20-9 20-20s-9-20-20-20l-12 0 0 40 12 0zm96-80l32 0c28.7 0 52 23.3 52 52l0 64c0 28.7-23.3 52-52 52l-32 0c-11 0-20-9-20-20l0-128c0-11 9-20 20-20zm32 128c6.6 0 12-5.4 12-12l0-64c0-6.6-5.4-12-12-12l-12 0 0 88 12 0zm76-108c0-11 9-20 20-20l48 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 24 28 0c11 0 20 9 20 20s-9 20-20 20l-28 0 0 44c0 11-9 20-20 20s-20-9-20-20l0-128z"></path></svg>dis-2016-jones.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/10hbJHIQVX8" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/10hbJHIQVX8?autoplay=1&gt;&lt;Image width={0} height={0} alt=https://img.youtube.com/vi/10hbJHIQVX8/maxresdefault.jpg src=https://img.youtube.com/vi/10hbJHIQVX8/maxresdefault.jpg&gt;&lt;span&gt;‚ñ∂&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowFullScreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>People are increasingly using mobile video to communicate, collaborate, and share experiences while on the go. Yet this presents challenges in adequately sharing camera views with remote users. In this paper, we study the use of semi-autonomous drones for video conferencing, where an outdoor user (using a smartphone) is connected to a desktop user who can explore the environment from the drone&#x27;s perspective. We describe findings from a study where pairs collaborated to complete shared navigation and search tasks. We illustrate the benefits of providing the desktop user with a view that is elevated, manipulable, and decoupled from the outdoor user. In addition, we articulate how participants overcame challenges in communicating environmental information and navigational cues, negotiated control of the view, and used the drone as a tool for sharing experiences. This provides a new way of thinking about mobile video conferencing where cameras that are decoupled from both users play an integral role in communication, collaboration, and sharing experiences.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Cscw</span><span class="ui brown basic label">Telepresence</span><span class="ui brown basic label">Video Communication</span><span class="ui brown basic label">Shared Experiences</span><span class="ui brown basic label">Teleoperation</span><span class="ui brown basic label">Drones</span><span class="ui brown basic label">Collaboration</span><span class="ui brown basic label">Hri</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Brennan Jones<!-- -->, <!-- -->Kody Dillman<!-- -->, <!-- -->Richard Tang<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Carman Neustaedter<!-- -->, <!-- -->Scott Bateman<!-- -->.¬†<b>Elevating Communication, Collaboration, and Shared Experiences in Mobile Video through Drones</b>.¬†<i>In <!-- -->Proceedings of the ACM on Designing Interactive Systems Conference<!-- --> <!-- -->(<!-- -->DIS 2016<!-- -->)</i>ACM, New York, NY, USA<!-- -->¬†<!-- -->Page: 1-<!-- -->13<!-- -->.¬†<!-- -->DOI: <a href="https://doi.org/10.1145/2901790.2901847" target="_blank">https://doi.org/10.1145/2901790.2901847</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div></div></div><div id="theses" class="category ui container"><h1 class="ui horizontal divider header"><svg data-prefix="far" data-icon="file-lines" class="svg-inline--fa fa-file-lines" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M64 48l112 0 0 88c0 39.8 32.2 72 72 72l88 0 0 240c0 8.8-7.2 16-16 16L64 464c-8.8 0-16-7.2-16-16L48 64c0-8.8 7.2-16 16-16zM224 67.9l92.1 92.1-68.1 0c-13.3 0-24-10.7-24-24l0-68.1zM64 0C28.7 0 0 28.7 0 64L0 448c0 35.3 28.7 64 64 64l256 0c35.3 0 64-28.7 64-64l0-261.5c0-17-6.7-33.3-18.7-45.3L242.7 18.7C230.7 6.7 214.5 0 197.5 0L64 0zm56 256c-13.3 0-24 10.7-24 24s10.7 24 24 24l144 0c13.3 0 24-10.7 24-24s-10.7-24-24-24l-144 0zm0 96c-13.3 0-24 10.7-24 24s10.7 24 24 24l144 0c13.3 0 24-10.7 24-24s-10.7-24-24-24l-144 0z"></path></svg>Theses Supervised</h1><div class="ui segment" style="margin-top:50px"><div class="thesis ui vertical segment stackable grid" data-id="phd-2025-cabral-mota"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">PhD 2025</span></p><p class="color" style="font-size:1.3em"><b>Modeling, Designing, and Evaluating Lens Visualizations for 3D and Immersive Analytics</b></p><p><a href="/people/roberta-cabral-mota/"><img alt="Roberta Cabral Mota picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/roberta-cabral-mota.jpg 1x" src="/static/images/people/roberta-cabral-mota.jpg"/><span class="author-link">Roberta Cabral Mota</span></a><span class="role"> (author)</span>, <span>Usman Alim</span><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>Mario Costa Sousa</span><span class="role"> (committee)</span>, <span>Nivan Ferreira</span><span class="role"> (committee)</span>, <a href="/people/fateme-rajabiyazdi/"><img alt="Fateme Rajabiyazdi picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/fateme-rajabiyazdi.jpg 1x" src="/static/images/people/fateme-rajabiyazdi.jpg"/><span class="author-link">Fateme Rajabiyazdi</span></a><span class="role"> (committee)</span>, <span>Parmit K. Chilana</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Focus Context Visualization</span><span class="ui brown basic label">Lens Visualization</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Immersive Analytics</span><span class="ui brown basic label">3 D Data</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2024-mckendrick"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">PhD 2024</span></p><p class="color" style="font-size:1.3em"><b>The Virtual Rehearsal Suite: Drama and Performance Approaches for Virtual Reality and Human-Computer Interaction</b></p><p><a href="/people/zachary-mckendrick/"><img alt="Zachary E. R. McKendrick picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/zachary-mckendrick.jpg 1x" src="/static/images/people/zachary-mckendrick.jpg"/><span class="author-link">Zachary E. R. McKendrick</span></a><span class="role"> (author)</span>, <span>Patrick Finn</span><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>Cosmin Munteanu</span><span class="role"> (committee)</span>, <a href="/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"> (committee)</span>, <span>April Viczko</span><span class="role"> (committee)</span>, <span>Michael Ullyot</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Drama</span><span class="ui brown basic label">Performance</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Extended Reality</span><span class="ui brown basic label">Human Comouter Interaction</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2024-friedel"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2024</span></p><p class="color" style="font-size:1.3em"><b>Large-surface Passive Haptic Interactions using Pantograph Mechanisms</b></p><p><a href="/people/marcus-friedel/"><img alt="Marcus Kenneth Ernst Friedel picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/marcus-friedel.jpg 1x" src="/static/images/people/marcus-friedel.jpg"/><span class="author-link">Marcus Kenneth Ernst Friedel</span></a><span class="role"> (author)</span>, <a href="/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ryo-suzuki.jpg 1x" src="/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>Aditya Nittala</span><span class="role"> (committee)</span>, <span>Richard Zhao</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">HCI</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Passive Haptics</span><span class="ui brown basic label">Pantographs</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2023-smith"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2023</span></p><p class="color" style="font-size:1.3em"><b>Expanding the User Interactions and Design Process of Haptic Experiences in Virtual Reality</b></p><p><a href="/people/christopher-smith/"><img alt="Christopher Geoffrey Smith picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/christopher-smith.jpg 1x" src="/static/images/people/christopher-smith.jpg"/><span class="author-link">Christopher Geoffrey Smith</span></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <a href="/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"> (supervisor)</span>, <a href="/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ryo-suzuki.jpg 1x" src="/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <a href="/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"> (supervisor)</span>, <a href="/people/ryo-suzuki/"><img alt="Ryo Suzuki picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ryo-suzuki.jpg 1x" src="/static/images/people/ryo-suzuki.jpg"/><span class="author-link">Ryo Suzuki</span></a><span class="role"> (supervisor)</span>, <span>Richard Zhao</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Design Tool</span><span class="ui brown basic label">Tactile Feedback</span><span class="ui brown basic label">Design Process</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2023-wei"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2023</span></p><p class="color" style="font-size:1.3em"><b>Design of Anthropomorphic Interfaces for Autonomous Vehicle-Pedestrian Interaction</b></p><p><a href="/people/wei-wei/"><img alt="Wei Wei picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wei-wei.jpg 1x" src="/static/images/people/wei-wei.jpg"/><span class="author-link">Wei Wei</span></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>Zhangxing Chen</span><span class="role"> (committee)</span>, <a href="/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"> (committee)</span>, <a href="/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Anthropomorphism</span><span class="ui brown basic label">AV Pedestrian Interaction</span><span class="ui brown basic label">Immersive Analytics</span><span class="ui brown basic label">VR</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2021-asha"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2021</span></p><p class="color" style="font-size:1.3em"><b>Designing Interaction with Autonomous Vehicles: External Displays and Interfaces for Vulnerable Road Users</b></p><p><a href="/people/ashratuz-zavin-asha/"><img alt="Ashratuz Zavin Asha picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ashratuz-zavin-asha.jpg 1x" src="/static/images/people/ashratuz-zavin-asha.jpg"/><span class="author-link">Ashratuz Zavin Asha</span></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>Michael John Jacobson Jr.</span><span class="role"> (committee)</span>, <span>Barry Wylant</span><span class="role"> (committee)</span>, <span>Patrick Finn</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Autonomous Vehicles</span><span class="ui brown basic label">External Automotive Displays</span><span class="ui brown basic label">Pedestrians With Hearing Aids</span><span class="ui brown basic label">Pedestrians In Wheelchairs</span><span class="ui brown basic label">Co Design</span><span class="ui brown basic label">VR Simulation</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2019-mahadevan"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2019</span></p><p class="color" style="font-size:1.3em"><b>Exploring the Design of Autonomous Vehicle-Pedestrian Interaction</b></p><p><a href="/people/karthik-mahadevan/"><img alt="Karthik Mahadevan picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/karthik-mahadevan.jpg 1x" src="/static/images/people/karthik-mahadevan.jpg"/><span class="author-link">Karthik Mahadevan</span></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <a href="/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Autonomous Vehicle Pedestrian Interaction</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Interaction Design</span><span class="ui brown basic label">Virtual Reality</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2019-li"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">PhD 2019</span></p><p class="color" style="font-size:1.3em"><b>Applications of Interactive Topographic Maps: Tangibility with Improved Spatial Awareness and Readability</b></p><p><span>Hao Li</span><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>Mario Costa Sousa</span><span class="role"> (supervisor)</span>, <span>Kazuki Takashima</span><span class="role"> (committee)</span>, <span>Zhangxing Chen</span><span class="role"> (committee)</span>, <span>Pablo Figueroa</span><span class="role"> (committee)</span>, <a href="/people/wesley-willett/"><img alt="Wesley J. Willett picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><span class="author-link">Wesley J. Willett</span></a><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Tangible User Interface</span><span class="ui brown basic label">Topographic Map</span><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Physicalization</span><span class="ui brown basic label">Physical Visualization</span><span class="ui brown basic label">Spatial Awareness</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2018-cartwright"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2018</span></p><p class="color" style="font-size:1.3em"><b>Secure Collaboration Across the Reality-Virtuality Continuum Using Reservoir Data</b></p><p><span>Stephen Cartwright</span><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>Mario Costa Sousa</span><span class="role"> (supervisor)</span>, <span>Zhangxing Chen</span><span class="role"> (committee)</span>, <span>Naser El-Sheimy</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Extended Reality</span><span class="ui brown basic label">Collaboration</span><span class="ui brown basic label">Information Security</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2018-ta"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2018</span></p><p class="color" style="font-size:1.3em"><b>Exploring Prototyping Tools for Interactive Fashion Design</b></p><p><span>Kevin Ta</span><span class="role"> (author)</span>, <a href="/people/lora-oehlberg/"><img alt="Lora A. Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora A. Oehlberg</span></a><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>Anthony Tony</span><span class="role"> (committee)</span>, <span>Joshua M. Taron</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Prototyping Tools</span><span class="ui brown basic label">Electronic Fashion</span><span class="ui brown basic label">Augmented Reality</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2018-mostafa"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">PhD 2018</span></p><p class="color" style="font-size:1.3em"><b>Mediating Experiential Learning in Interactive Immersive Environments</b></p><p><span>Ahmed Mostafa</span><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>M√°rio Costa Sousa</span><span class="role"> (supervisor)</span>, <span>Sonny Chan</span><span class="role"> (committee)</span>, <span>Kazuki Takashima</span><span class="role"> (committee)</span>, <span>Pierre Boulanger</span><span class="role"> (committee)</span>, <span>Naser El-Sheimy</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Virtual Environments</span><span class="ui brown basic label">Interactive</span><span class="ui brown basic label">Education</span><span class="ui brown basic label">Learning</span><span class="ui brown basic label">Simulation</span><span class="ui brown basic label">Immersion</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2017-somanath"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">PhD 2017</span></p><p class="color" style="font-size:1.3em"><b>&#x27;Making&#x27; within Material, Cultural, and Emotional Constraints</b></p><p><a href="/people/sowmya-somanath/"><img alt="Sowmya Somanath picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><span class="author-link">Sowmya Somanath</span></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>M√°rio Costa Sousa</span><span class="role"> (supervisor)</span>, <a href="/people/lora-oehlberg/"><img alt="Lora Oehlberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><span class="author-link">Lora Oehlberg</span></a><span class="role"> (committee)</span>, <span>Janette Hughes</span><span class="role"> (committee)</span>, <span>Vera Parlac</span><span class="role"> (committee)</span>, <span>Oscar Meruvia Pastor</span><span class="role"> (committee)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Computer Science</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2015-li"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2015</span></p><p class="color" style="font-size:1.3em"><b>Two-Sided Transparent Display as a Collaborative Medium</b></p><p><a href="/people/jiannan-li/"><img alt="Jiannan Li picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/jiannan-li.jpg 1x" src="/static/images/people/jiannan-li.jpg"/><span class="author-link">Jiannan Li</span></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <a href="/people/saul-greenberg/"><img alt="Saul Greenberg picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/saul-greenberg.jpg 1x" src="/static/images/people/saul-greenberg.jpg"/><span class="author-link">Saul Greenberg</span></a><span class="role"> (supervisor)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Computer Science</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="mmus-2012-pon"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MMus 2012</span></p><p class="color" style="font-size:1.3em"><b>Vuzik: Exploring a Medium for Painting Music</b></p><p><span>Aura Pon</span><span class="role"> (author)</span>, <span>David Eagle</span><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span></p><div><div class="ui large basic labels"><span class="ui brown basic label">Music</span></div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2012-lapides"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2012</span></p><p class="color" style="font-size:1.3em"><b>Designing video games with social, physical, and authorship gameplay</b></p><p><a href="/people/paul-lapides/"><img alt="Paul Lapides picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/paul-lapides.jpg 1x" src="/static/images/people/paul-lapides.jpg"/><span class="author-link">Paul Lapides</span></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span>, <span>M√°rio Costa Sousa</span><span class="role"> (supervisor)</span></p><div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2011-harris"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2011</span></p><p class="color" style="font-size:1.3em"><b>Exploring the affect of emotive motion in social human robot interaction</b></p><p><span>John J. R. Harris</span><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span></p><div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2011-sultanum"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2011</span></p><p class="color" style="font-size:1.3em"><b>Exploring novel interfaces for 3d visualization of reservoir simulation post-processing data</b></p><p><span>Nicole Barbosa Sultanum</span><span class="role"> (author)</span>, <span>M√°rio Costa Sousa</span><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span></p><div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="phd-2010-young"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">PhD 2010</span></p><p class="color" style="font-size:1.3em"><b>Exploring social interaction between robots and people</b></p><p><span>James E. Young</span><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span></p><div></div></div></div><div class="thesis ui vertical segment stackable grid" data-id="msc-2008-guo"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MSc 2008</span></p><p class="color" style="font-size:1.3em"><b>New paradigms for human-robot interaction using tangible user interfaces</b></p><p><span>Cheng Guo</span><span class="role"> (author)</span>, <a href="/people/ehud-sharlin/"><img alt="Ehud Sharlin picture" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><span class="author-link">Ehud Sharlin</span></a><span class="role"> (supervisor)</span></p><div></div></div></div></div><div id="theses-modal"><div id="phd-2025-cabral-mota" class="ui large modal"><div class="header"><a target="_blank" href="/theses/phd-2025-cabral-mota/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2025-cabral-mota</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">PhD 2025</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2025-cabral-mota" target="_blank">Modeling, Designing, and Evaluating Lens Visualizations for 3D and Immersive Analytics</a></h1><p class="meta"><a href="/people/roberta-cabral-mota"><img alt="roberta-cabral-mota photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/roberta-cabral-mota.jpg 1x" src="/static/images/people/roberta-cabral-mota.jpg"/><strong>Roberta Cabral Mota</strong></a><span class="role"> (author)</span>, <span>Usman Alim<!-- --> <span class="role"> (supervisor)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>Mario Costa Sousa<!-- --> <span class="role"> (committee)</span></span>, <span>Nivan Ferreira<!-- --> <span class="role"> (committee)</span></span>, <a href="/people/fateme-rajabiyazdi"><img alt="fateme-rajabiyazdi photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/fateme-rajabiyazdi.jpg 1x" src="/static/images/people/fateme-rajabiyazdi.jpg"/><strong>Fateme Rajabiyazdi</strong></a><span class="role"> (committee)</span>, <span>Parmit K. Chilana<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Lens visualization has been a prominent research area in the visualization community, fueled by the continuous need to mitigate visual clutter and occlusion resulting from the increasingly large datasets. Interactive lenses for 3D data, particularly, challenge visualization designers to conceive design strategies that facilitates the analysis of dense, multifaceted data with spatial referents. Given their relevance, the overarching research goal of this dissertation is to investigate how visualization lenses may support 3D data exploration and analysis‚Äîacross both conventional and immersive environments. To this end, we begin with (1) modeling lenses by conducting a systematic review of existing lenses operating within spatial contexts. From this survey, we derive a design space that captures core design dimensions and choices involved in constructing spatially-embedded visualization lenses. Building upon this theoretical foundation, we proceed with (2) designing lenses, through the design, development, and evaluation of four immersive lenses tailored to support distinct forms of 3D data analysis: i) a deformable quadric lens for heterogeneous feature analysis, ii) a dual-imagery lens for multivariate, multi-geometry analysis, iii) a view-dependent lens for reservoir uncertainty analysis, and iv) a shape-conformal lens for spatiotemporal urban data analysis. Afterwards, we probe (3) evaluating lenses by complementing our prior theoretical and practical endeavors with empirical evidence from a controlled study comparing the efficacy of four lens-based designs in supporting time-varying urban data analysis. Throughout the course of our research agenda, we document domain-specific lessons learned and translate them into generalizable design guidelines intended to inform the broader visualization community. Finally, this dissertation concludes by suggesting future research directions for advancing visualization lenses in 3D data exploration and analysis‚Äîacross both traditional and immersive environments. In the long term, we hope that this dissertation serve valuable reference points for visualization researchers and practitioners operating at either a theoretical, design, or empirical level.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Focus Context Visualization</span><span class="ui brown basic label">Lens Visualization</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Immersive Analytics</span><span class="ui brown basic label">3 D Data</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Roberta Cabral Mota<!-- -->.¬†<b>Modeling, Designing, and Evaluating Lens Visualizations for 3D and Immersive Analytics</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2025-09-22<!-- -->. <!-- -->DOI: <a href="https://dx.doi.org/10.11575/PRISM/50555" target="_blank">https://dx.doi.org/10.11575/PRISM/50555</a>URL: <a href="https://hdl.handle.net/1880/122961" target="_blank">https://hdl.handle.net/1880/122961</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2024-mckendrick" class="ui large modal"><div class="header"><a target="_blank" href="/theses/phd-2024-mckendrick/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2024-mckendrick</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">PhD 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2024-mckendrick" target="_blank">The Virtual Rehearsal Suite: Drama and Performance Approaches for Virtual Reality and Human-Computer Interaction</a></h1><p class="meta"><a href="/people/zachary-mckendrick"><img alt="zachary-mckendrick photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/zachary-mckendrick.jpg 1x" src="/static/images/people/zachary-mckendrick.jpg"/><strong>Zachary E. R. McKendrick</strong></a><span class="role"> (author)</span>, <span>Patrick Finn<!-- --> <span class="role"> (supervisor)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>Cosmin Munteanu<!-- --> <span class="role"> (committee)</span></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"> (committee)</span>, <span>April Viczko<!-- --> <span class="role"> (committee)</span></span>, <span>Michael Ullyot<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>This dissertation explores the intersection of Drama, virtual reality (VR), and Human-Computer Interaction (HCI), examining their parallels and collective impact on shaping immersive digital experiences. Throughout our research, we ask: how can VR support traditional performance practices? And how can we leverage existing performance practices to support wholistic user engagement? From our interdisciplinary position we identify gaps in existing research and offer novel solutions and innovative frameworks for navigating and structuring experiences within VR. In pursuit of answers to our research questions, we contribute C1) Performance-Based Multimodal Methodological Approaches for HCI, and C2) A Demonstration of Interdisciplinary Possibilities to the landscape of HCI VR research. We propose a symbiotic relationship between the user&#x27;s cognitive engagement and the digital environment&#x27;s architectural and interaction design can be enhanced through the application of practices and principles from Drama and performance. The practical application of these theoretical constructs is showcased in C3) Thresholding Protocols for Digital State Change, and C4) The Virtual Rehearsal Suite (VRS), an immersive VR environment that supports solo performance training, demonstrating how Drama and performance methodologies can enhance the user&#x27;s experience, offering tested perspectives and techniques that promote interaction, presence, and embodiment. Starting with our related works, we identify our interdisciplinary position with a foundation that draws from both the academic and artistic communities interested in VR as a domain, research, and performance tool. We then cast a wide net to understand the dimensions of virtual technologies and their impact on user experience with subsequent chapters investigating the layers of reality, immersion, embodiment, performance rituals, and thresholding concepts. Each chapter contributes to the identification of gaps and parallels across research domains and the discussion of how Drama and performance can elevate the understanding and advancement of VR and HCI systems. The dissertation concludes that the confluence of Drama and performance practice with Interaction Design holds the potential to shape the future aesthetics and experiential facets of virtual environments. The embrace of VR as both a tool and a medium for creative expression is positioned as a transformative leap forward in both HCI and Drama, heralding a new era of digital interaction that needs to embrace the full spectrum of human experience for success and longevity. Our work positions actors as interaction specialists, capable of existing in multiple realities at once and providing insightful reflections on their experiences in iterative processes. The VRS study demonstrates this ability, while emphasizing that virtual environments are not merely technological constructs but complex experiential spaces where the physical and digital converge, challenging traditional perceptions of reality. It highlights the importance of centralizing the user as key to creating compelling virtual experiences that con only be achieved through meticulously designed interactions that resonate with the user&#x27;s sensory and cognitive faculties. Our study underscores the efficacy of VR in supporting actor training with minimal digital interventions, facilitating a seamless transition into and out of VR, enhancing focus during VR engagement, and addressing issues such as VR sickness. It highlights the centralization of the human element as the pivotal factor in VR creation, emphasizing that VR environments should cater to the nuanced spectrum of human emotions, behaviours, and social interactions.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Drama</span><span class="ui brown basic label">Performance</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Extended Reality</span><span class="ui brown basic label">Human Comouter Interaction</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Zachary E. R. McKendrick<!-- -->.¬†<b>The Virtual Rehearsal Suite: Drama and Performance Approaches for Virtual Reality and Human-Computer Interaction</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2024-05-14<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.11575/PRISM/46371" target="_blank">https://doi.org/10.11575/PRISM/46371</a>URL: <a href="https://hdl.handle.net/1880/118774" target="_blank">https://hdl.handle.net/1880/118774</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2024-friedel" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2024-friedel/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2024-friedel</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2024-friedel" target="_blank">Large-surface Passive Haptic Interactions using Pantograph Mechanisms</a></h1><p class="meta"><a href="/people/marcus-friedel"><img alt="marcus-friedel photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/marcus-friedel.jpg 1x" src="/static/images/people/marcus-friedel.jpg"/><strong>Marcus Kenneth Ernst Friedel</strong></a><span class="role"> (author)</span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ryo-suzuki.jpg 1x" src="/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>Aditya Nittala<!-- --> <span class="role"> (committee)</span></span>, <span>Richard Zhao<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Dexterous and natural haptic interaction with the environment in Virtual Reality promises a new era of embodied and intuitive computing. But among the remaining challenges stands the difficulty of natural wall interactions. Personal haptic devices for natural wall interaction in virtual reality should be portable and should provide passive, body-scale interactions. However, existing techniques fall short: Room-scale proxies lack portability, wearable robotic arms are energy-intensive and induce friction, and existing hand-scale passive interaction techniques are unsuitable for continuous large-scale renders. In this thesis, we introduce PantographHaptics, a technique which uses the scaling properties of a pantograph to passively render body-scale surfaces. A pantograph is a classical linkage mechanism which can enlarge or shrink designs by coordinating nodes to move in scaled, geometrically similar paths. To our knowledge, no prior work has applied the pantograph mechanism to large-scale immersive haptics. PantographHaptics is a novel method for passively achieving body-scale haptics which uses a pantograph to scale up a small positional constraint into an encounterable midair render. We present the conceptual foundation underpinning of PantographHaptics by describing the operation of the pantograph mechanism and detailing how we apply it for haptics. Then we verify the PantographHaptics technique through two prototypes: HapticLever, a grounded system, and Feedbackpack, a wearable device. We detail the designs, implementations, and technical evaluations of both prototypes, and we highlight the challenges and solutions involved in their development. We evaluate these prototypes with user evaluations, which contribute assessments of their interaction fidelity, investigations of their usability, comparisons of their performance against other haptic modalities, and recorded participant experiences of using the devices. By introducing and verifying PantographHaptics, we show that this novel technique is a viable and promising approach for interactions with large surfaces. By documenting the development of our prototype artifacts and reporting user experiences with the devices, we contribute a foundation for future research.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Haptics</span><span class="ui brown basic label">HCI</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Passive Haptics</span><span class="ui brown basic label">Pantographs</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Marcus Kenneth Ernst Friedel<!-- -->.¬†<b>Large-surface Passive Haptic Interactions using Pantograph Mechanisms</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2024-01-17<!-- -->. <!-- -->DOI: <a href="https://doi.org/10.11575/PRISM/42844" target="_blank">https://doi.org/10.11575/PRISM/42844</a>URL: <a href="https://hdl.handle.net/1880/118000" target="_blank">https://hdl.handle.net/1880/118000</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2023-smith" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2023-smith/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2023-smith</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2023-smith" target="_blank">Expanding the User Interactions and Design Process of Haptic Experiences in Virtual Reality</a></h1><p class="meta"><a href="/people/christopher-smith"><img alt="christopher-smith photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/christopher-smith.jpg 1x" src="/static/images/people/christopher-smith.jpg"/><strong>Christopher Geoffrey Smith</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"> (supervisor)</span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ryo-suzuki.jpg 1x" src="/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"> (supervisor)</span>, <a href="/people/ryo-suzuki"><img alt="ryo-suzuki photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ryo-suzuki.jpg 1x" src="/static/images/people/ryo-suzuki.jpg"/><strong>Ryo Suzuki</strong></a><span class="role"> (supervisor)</span>, <span>Richard Zhao<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Virtual reality can be a highly immersive experience due to its realistic visual presentation. This immersive state is useful for applications including education, training, and entertainment. To enhance the state of immersion provided by virtual reality further, devices capable of simulating touch and force have been researched to allow not only a visual and audio experience but a haptic experience as well. Such research has investigated many approaches to generating haptics for virtual reality but often does not explore how to create an immersive haptic experience using them. In this thesis, we present a discussion on four proposed areas of the virtual reality haptic experience design process using a demonstration methodology. To investigate the application of haptic devices, we designed a modular ungrounded haptic system which was used to create a general-purpose device capable of force-based feedback and used it in the three topics of exploration. The first area explored is the application of existing haptic theory for aircraft control to the field of virtual reality drone control. The second area explored is the presence of the size-weight sensory illusion within virtual reality when using a simulated haptic force.  The third area explored is how authoring within a virtual reality medium can be used by a designer to create VR haptic experiences. From these explorations, we begin a higher-level discussion of the broader process of creating a virtual reality haptic experience. Using the results of each project as a representation of our proposed design steps, we discuss not only the broader concepts the steps contribute to the process and their importance, but also draw connections between them. By doing this, we present a more holistic approach to the large-scale design of virtual reality haptic experiences and the benefits we believe it provides.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Design Tool</span><span class="ui brown basic label">Tactile Feedback</span><span class="ui brown basic label">Design Process</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Christopher Geoffrey Smith<!-- -->.¬†<b>Expanding the User Interactions and Design Process of Haptic Experiences in Virtual Reality</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2023-08<!-- -->. <!-- -->DOI: <a href="https://dx.doi.org/10.11575/PRISM/41738" target="_blank">https://dx.doi.org/10.11575/PRISM/41738</a>URL: <a href="https://hdl.handle.net/1880/116896" target="_blank">https://hdl.handle.net/1880/116896</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2023-wei" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2023-wei/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2023-wei</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2023-wei" target="_blank">Design of Anthropomorphic Interfaces for Autonomous Vehicle-Pedestrian Interaction</a></h1><p class="meta"><a href="/people/wei-wei"><img alt="wei-wei photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wei-wei.jpg 1x" src="/static/images/people/wei-wei.jpg"/><strong>Wei Wei</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>Zhangxing Chen<!-- --> <span class="role"> (committee)</span></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"> (committee)</span>, <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"> (committee)</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Autonomous Vehicle (AV) technology promises to revolutionize human life. The promise of AVs includes reduced highway congestion, more efficient energy usage, and cheaper goods and services. However, without careful design, removing human drivers from vehicles will eliminate the natural communication channels which enable pedestrians to navigate safely. This thesis aims to design, present, and study anthropomorphic interfaces for autonomous vehicles, with the objective of enabling AVs to communicate with pedestrians through non-verbal cues. Non-verbal human communication is vital in human relationships. People use non-verbal communication when speech is impractical, such as when interacting with vehicles. When looking into ways in which AVs can use non-verbal communication to interact with pedestrians, we were inspired by the prospect of using anthropomorphic interfaces. This concept is well explored in Human-Robot Interaction (HRI) but has not been investigated in the context of AVs. For this thesis, we explored the design of anthropomorphic interfaces for autonomous vehicles. First, we proposed three types of anthropomorphic interfaces for AVs: facial expressions, hand gestures, and humanoid torsos. We developed a design space for each category using sketches and a low-fi prototype. Then, to research the benefits and limitations of anthropomorphic AVs, we implemented our AV interfaces in a Virtual Reality (VR) environment and developed two testbeds to evaluate their feasibility and scalability. Finally, we conducted two studies using the two testbeds. We investigated the study results using immersive analytics alongside traditional methods and revealed that anthropomorphic AVs could be helpful in AV-pedestrian interaction when designed by specific guidelines. Since we studied anthropomorphic AVs in VR, we were interested in the possibilities of analyzing the data of our study in an immersive environment. We designed a VR prototype specifically to analyze the data collected from the anthropomorphic AV study. The prototype provided basic immersive analytics features for the AV study data. We conducted an expert session with two domain experts to evaluate our immersive analytics prototype. The study contributed insights into the opportunities and challenges of utilizing immersive analytics to analyze AV studies.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Anthropomorphism</span><span class="ui brown basic label">AV Pedestrian Interaction</span><span class="ui brown basic label">Immersive Analytics</span><span class="ui brown basic label">VR</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Wei Wei<!-- -->.¬†<b>Design of Anthropomorphic Interfaces for Autonomous Vehicle-Pedestrian Interaction</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2023-01<!-- -->. <!-- -->DOI: <a href="https://dx.doi.org/10.11575/PRISM/40689" target="_blank">https://dx.doi.org/10.11575/PRISM/40689</a>URL: <a href="http://hdl.handle.net/1880/115776" target="_blank">http://hdl.handle.net/1880/115776</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2021-asha" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2021-asha/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2021-asha</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2021-asha" target="_blank">Designing Interaction with Autonomous Vehicles: External Displays and Interfaces for Vulnerable Road Users</a></h1><p class="meta"><a href="/people/ashratuz-zavin-asha"><img alt="ashratuz-zavin-asha photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ashratuz-zavin-asha.jpg 1x" src="/static/images/people/ashratuz-zavin-asha.jpg"/><strong>Ashratuz Zavin Asha</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>Michael John Jacobson Jr.<!-- --> <span class="role"> (committee)</span></span>, <span>Barry Wylant<!-- --> <span class="role"> (committee)</span></span>, <span>Patrick Finn<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>In the near future, mixed traffic consisting of manual and autonomous vehicles (AVs) will be common. Autonomous vehicles with advanced technology offer opportunities for innovative designs and introduce communication challenges for vulnerable road users such as pedestrians and cyclists. Our goal is to explore the emerging new domain of interaction between different road users and autonomous vehicles in a future AV transportation ecosystem. This led us to conduct the thesis following these two themes: 1) understanding design opportunities for external automotive displays (EADs) of AVs; 2) exploring the design of interactions between vulnerable road users (VRUs) and AVs. In theme 1, our work extends contemporary research into visualizations and related applications for autonomous vehicles. Focusing on external car bodies as a design space we introduce a set of EADs. EADs show visualizations to share context and user-specific information and offer opportunities for interaction between users and AVs. We conducted a design study to explore design concepts for EADs to provide services to different road users: pedestrians, passengers, and drivers of other vehicles. Based on the design study, we prototyped four EADs in virtual reality (VR) to demonstrate the potential of our approach. This exploration contributes to our vision for EADs, a design critique of the prototypes, and a discussion of the possible impact and future usage of external automotive displays. In theme 2, we are interested in the ways pedestrians will interact with autonomous vehicles in the absence of non-verbal cues from the driver (such as eye movements, hand gestures, etc.). Crossing streets in these new situations could be more dangerous for VRUs without a proper communication medium. We examined a subset of this challenge with two groups of pedestrians: interaction between AVs and pedestrians with hearing aids (PHAs), and pedestrians in wheelchairs (PWs). First, we worked with hearing aid users as a preliminary exploration of this research. We conduct a co-design study with a co-designer with hearing impairment who has lived experience of wearing hearing aid enhancements. This study contributes several insights and design recommendations on how potential audio cues can be designed to enhance direct communications between PHAs and AVs. For the second part of our research, we designed interactions between pedestrians in wheelchairs and AVs. From an early exploration of potential interface designs through a design study with interaction designers, we prototyped different interfaces in VR. Then, we evaluated the implemented simulations during a co-design study with a powered wheelchair user following inclusive design practices. We identify and reflect on interface design ideas that can help PWs make safe crossing decisions at intersections and discuss design insights for implementing different inclusive interfaces.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Autonomous Vehicles</span><span class="ui brown basic label">External Automotive Displays</span><span class="ui brown basic label">Pedestrians With Hearing Aids</span><span class="ui brown basic label">Pedestrians In Wheelchairs</span><span class="ui brown basic label">Co Design</span><span class="ui brown basic label">VR Simulation</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ashratuz Zavin Asha<!-- -->.¬†<b>Designing Interaction with Autonomous Vehicles: External Displays and Interfaces for Vulnerable Road Users</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2021-09-02<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/113831" target="_blank">http://hdl.handle.net/1880/113831</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2019-mahadevan" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2019-mahadevan/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2019-mahadevan</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2019-mahadevan" target="_blank">Exploring the Design of Autonomous Vehicle-Pedestrian Interaction</a></h1><p class="meta"><a href="/people/karthik-mahadevan"><img alt="karthik-mahadevan photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/karthik-mahadevan.jpg 1x" src="/static/images/people/karthik-mahadevan.jpg"/><strong>Karthik Mahadevan</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"> (committee)</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Autonomous vehicle research today places an emphasis on developing better sensors and algorithms to enable the vehicle to localize itself in the environment, plan routes, and control its movement. Surveying the general public reveals optimism about the technology but also some skepticism about its ability to communicate with vulnerable road users such as pedestrians and cyclists. In today&#x27;s interaction with vehicles at crosswalks, pedestrians rely on cues originating from the vehicle and the driver. Vehicle cues relate to its kinematics such as speed and stopping distance while driver cues are concerned with communication such as eye gaze and contact, head and body movement, and hand gestures. In autonomous vehicles, however, a driver is not expected to be on-board to provide cues to pedestrians. We attempted to tackle the problem of designing novel ways to facilitate autonomous vehicle-pedestrian interaction at crosswalks. We propose interfaces which communicate an autonomous vehicle&#x27;s awareness and intent as a means of helping pedestrians make safe crossing decisions. Through our exploration, we make several contributions. First, we propose a design space for building interfaces using different cue modalities and cue locations. From an early exploration of this design space, we prototype interfaces designed to facilitate autonomous vehicle-pedestrian interaction. The interaction between vehicles and pedestrians will become more challenging during the transition period until all vehicles on the road are fully autonomous. During this period which we term mixed traffic, vehicles of varying levels of autonomy will occupy roads, some of which will have drivers, others such as semi-autonomous which may have distracted drivers, and fully autonomous vehicles which may or may not have drivers. To study this problem, we contribute a virtual reality-based pedestrian simulator. Our final contribution relates to the evaluation of interfaces in the real and virtual world where we found their inclusion helped pedestrians make safe crossing decisions.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Autonomous Vehicle Pedestrian Interaction</span><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Interaction Design</span><span class="ui brown basic label">Virtual Reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Karthik Mahadevan<!-- -->.¬†<b>Exploring the Design of Autonomous Vehicle-Pedestrian Interaction</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2019-09-12<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/110924" target="_blank">http://hdl.handle.net/1880/110924</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2019-li" class="ui large modal"><div class="header"><a target="_blank" href="/theses/phd-2019-li/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2019-li</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">PhD 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2019-li" target="_blank">Applications of Interactive Topographic Maps: Tangibility with Improved Spatial Awareness and Readability</a></h1><p class="meta"><span>Hao Li<!-- --> <span class="role"> (author)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>Mario Costa Sousa<!-- --> <span class="role"> (supervisor)</span></span>, <span>Kazuki Takashima<!-- --> <span class="role"> (committee)</span></span>, <span>Zhangxing Chen<!-- --> <span class="role"> (committee)</span></span>, <span>Pablo Figueroa<!-- --> <span class="role"> (committee)</span></span>, <a href="/people/wesley-willett"><img alt="wesley-willett photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/wesley-willett.jpg 1x" src="/static/images/people/wesley-willett.jpg"/><strong>Wesley J. Willett</strong></a><span class="role"> (committee)</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Traditional flat topographic maps are difficult to understand due to the distortion and compromise of the 3-dimensional (3D) spatial representation when it is folded into lower-dimension media (e.g. 2D). During the process, the x-y coordinate of a location can be captured but its physical elevation must be transformed using some visualization techniques, resulting in noticeable cognitive effort in comprehending the original geometric and geographic properties of the original terrain. In this manuscript-based dissertation, I present a collection of my past publications that aim to increase the readability of topographic maps by restoring the original spatiality of the terrain - including the elevations - with a physical map representation and then superimpose additional data visualization on top of it. In this way, the entire terrain topology is kept in a scaled physical representation, allowing users to view it with natural human perceptions. Additionally, user gestures can be tracked in real-time as a sketch-based input to allow novel dynamic interaction of the map interface and data manipulation of the spatial information. Through the chapters, I present the aforementioned concept, named interactive topographic interface, along with a few applications of it in different academic and industrial environments. I also report the design and results of a user study that compares the interface with traditional flat topographic maps. In the long-term, I hope that research mentioned in this dissertation inspires future interactive physical cartography to not only improve map comprehension but also facilitate better spatial and situational awareness over the map interface, resulting in an evolved map usefulness.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Tangible User Interface</span><span class="ui brown basic label">Topographic Map</span><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Physicalization</span><span class="ui brown basic label">Physical Visualization</span><span class="ui brown basic label">Spatial Awareness</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Hao Li<!-- -->.¬†<b>Applications of Interactive Topographic Maps: Tangibility with Improved Spatial Awareness and Readability</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2019-07-02<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/110577" target="_blank">http://hdl.handle.net/1880/110577</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2018-cartwright" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2018-cartwright/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2018-cartwright</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2018-cartwright" target="_blank">Secure Collaboration Across the Reality-Virtuality Continuum Using Reservoir Data</a></h1><p class="meta"><span>Stephen Cartwright<!-- --> <span class="role"> (author)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>Mario Costa Sousa<!-- --> <span class="role"> (supervisor)</span></span>, <span>Zhangxing Chen<!-- --> <span class="role"> (committee)</span></span>, <span>Naser El-Sheimy<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Acquiring, storing, processing, synthesizing, visualizing, and interpreting data are core to scientific knowledge discovery. This data life cycle is common to many diverse fields such as medicine and petroleum engineering. A wide range of techniques may be used for interacting with and visualizing data. Immersive technologies such as augmented reality and virtual reality show great potential to enhance important workplace activities such as collaboration. To this end an immersive, collaborative tool for visualizing reservoir data is discussed. Some collaborative scenarios using these technologies are then described. It is important to carefully consider how these technologies will be incorporated into a professional setting to ensure tools based on these technologies will provide a high quality user experience while meeting the security needs of industry. In order to further this goal, some of the architectural considerations of a collaboration tool that uses a variety of technologies from the reality-virtuality continuum are explored. A prototype tool is then presented that has been developed for collaborating over petroleum reservoir scenarios involving sensitive data. This tool incorporates visual protection mechanisms to facilitate collaboration while providing enhanced control over information disclosure. A user feedback session was performed with reservoir engineering subject matter experts, and the results from this exploratory evaluation are reported.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Extended Reality</span><span class="ui brown basic label">Collaboration</span><span class="ui brown basic label">Information Security</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Stephen Cartwright<!-- -->.¬†<b>Secure Collaboration Across the Reality-Virtuality Continuum Using Reservoir Data</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2018-12-20<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/109396" target="_blank">http://hdl.handle.net/1880/109396</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2018-ta" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2018-ta/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2018-ta</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2018-ta" target="_blank">Exploring Prototyping Tools for Interactive Fashion Design</a></h1><p class="meta"><span>Kevin Ta<!-- --> <span class="role"> (author)</span></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora A. Oehlberg</strong></a><span class="role"> (supervisor)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>Anthony Tony<!-- --> <span class="role"> (committee)</span></span>, <span>Joshua M. Taron<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Interactive garments enable new forms of communication between our bodies and with other people. In electronic fashion (eFashion) design, interactive garments on high fashion runways envision how people might use interactive technologies to enhance our clothing with new sensing and output capabilities. Researchers and fashion designers have since explored new interactive textiles that enable aesthetics-driven, interactive, and new material properties to explore on clothing. While there exist physical tools to implement interactive garments and software tools to create the visual aesthetic of a garment, these tools cannot yet enable designers to use new eFashion technologies in their garments because they require engineering expertise and specialized laboratory equipment. In this thesis, I explore the use of computer-aided prototyping tools to develop interactive eFashion garments. I present case studies with makers and two experienced eFashion designers about their design practices and formulate design guidelines for prototyping tools. I then present two prototyping tools for implementation and exploration of interactive garments. Finally, I discuss future work for physical and virtual prototyping tools in eFashion.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Prototyping Tools</span><span class="ui brown basic label">Electronic Fashion</span><span class="ui brown basic label">Augmented Reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kevin Ta<!-- -->.¬†<b>Exploring Prototyping Tools for Interactive Fashion Design</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2018-09-21<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/108718" target="_blank">http://hdl.handle.net/1880/108718</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2018-mostafa" class="ui large modal"><div class="header"><a target="_blank" href="/theses/phd-2018-mostafa/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2018-mostafa</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">PhD 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2018-mostafa" target="_blank">Mediating Experiential Learning in Interactive Immersive Environments</a></h1><p class="meta"><span>Ahmed Mostafa<!-- --> <span class="role"> (author)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>M√°rio Costa Sousa<!-- --> <span class="role"> (supervisor)</span></span>, <span>Sonny Chan<!-- --> <span class="role"> (committee)</span></span>, <span>Kazuki Takashima<!-- --> <span class="role"> (committee)</span></span>, <span>Pierre Boulanger<!-- --> <span class="role"> (committee)</span></span>, <span>Naser El-Sheimy<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Simulation and immersive environments are gaining popularity in various contexts. Arguably, such interactive systems have the potential to benefit many users in a variety of education and training scenarios. However, some of these systems especially with the lack of skilled instructors are still faced by challenges of operational complexity, the incorporation of different technologies and features, and the limited availability of performance measures and feedback. Therefore, the design of these systems would benefit from integrating experiential aspects and essential educational aids. For example, users of such learning systems, especially the novice ones, can be better supported by a smoother learning curve, detailed guidance features, the availability of feedback and performance reporting, and the integration of engaging &amp; reflective capabilities. In essence, we recognize a need to re-explore learning aids and how they impact design, usage, and overall learning experience in interactive immersive environments.<br/>The goal of this dissertation is to mediate experiential learning in interactive immersive environments. This includes exploring existing and novel learning aids that would facilitate learning with improved engagement and immersion, enrich learners with insightful reflections, better support novice users‚Äô learning and training needs, and ultimately enhance the overall experience.<br/>To achieve this goal, we utilized existing learning models and simulation-based training approaches and proposed a framework of learning aids to mediate learning in interactive immersive environments. Working closely with domain expert collaborators, we designed, implemented and evaluated four new interactive immersive prototypes in an attempt to validate the practicality of our aids. The first prototype, NeuroSimVR, is a stereoscopic visualization augmented with educational aids to support how medical users learn about a common back surgery procedure. The second prototype, ReflectiveSpineVR, is an immersive virtual reality surgical simulation with innovative<br/>interaction history capabilities that aim to empower users‚Äô memories and enable deliberate repetitive practice as needed. The third prototype, JackVR, is an interactive immersive training system, utilizing novel gamification elements, and aims to support oil-and-gas experts in the process of landing oil rigs. Our fourth prototype, RoboTeacher, involves a humanoid robot instructor for teaching people industrial assembly tasks. In our prototypes, we presented novel learning aids, visualization, and interaction techniques that are new to many of the current immersive learning tools. We conclude this dissertation with lessons learned and guidelines for designing with learning aids in future research directions that target interactive experiential environments.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Virtual Environments</span><span class="ui brown basic label">Interactive</span><span class="ui brown basic label">Education</span><span class="ui brown basic label">Learning</span><span class="ui brown basic label">Simulation</span><span class="ui brown basic label">Immersion</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ahmed Mostafa<!-- -->.¬†<b>Mediating Experiential Learning in Interactive Immersive Environments</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2018-01-22<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/106342" target="_blank">http://hdl.handle.net/1880/106342</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2017-somanath" class="ui large modal"><div class="header"><a target="_blank" href="/theses/phd-2017-somanath/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2017-somanath</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">PhD 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2017-somanath" target="_blank">&#x27;Making&#x27; within Material, Cultural, and Emotional Constraints</a></h1><p class="meta"><a href="/people/sowmya-somanath"><img alt="sowmya-somanath photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/sowmya-somanath.jpg 1x" src="/static/images/people/sowmya-somanath.jpg"/><strong>Sowmya Somanath</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>M√°rio Costa Sousa<!-- --> <span class="role"> (supervisor)</span></span>, <a href="/people/lora-oehlberg"><img alt="lora-oehlberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/lora-oehlberg.jpg 1x" src="/static/images/people/lora-oehlberg.jpg"/><strong>Lora Oehlberg</strong></a><span class="role"> (committee)</span>, <span>Janette Hughes<!-- --> <span class="role"> (committee)</span></span>, <span>Vera Parlac<!-- --> <span class="role"> (committee)</span></span>, <span>Oscar Meruvia Pastor<!-- --> <span class="role"> (committee)</span></span></p></div></div></div><div class="block"><h1>Abstract</h1><p>The Maker Movement aims to democratize technological practices and promises many benefits for people including improved technical literacy, a means for self-expression and agency, and an opportunity to become more than consumers of technology. As part of the Maker Movement, people build hobbyist and utilitarian projects by themselves using programmable electronics (e.g., microcontroller, sensors, actuators) and software tools. While the Maker Movement is gaining momentum globally, some people are left out. Constraints such as material limitations, educational culture restrictions, and emotional or behavioral difficulties can often limit people from taking part in the Maker Movement. We refer to the systematic investigation of how diverse people respond to making-centered activities within constraints as an exploration of making within constraints.<br/>In this dissertation, we (1) study how people respond to creating physical objects by themselves within constraints and, (2) investigate how to design technology that can help makers within constraints.  We conducted an observational study in an impoverished school in India and identified the students&#x27; challenges and their strategies for making within material and educational culture constraints. We conducted a second study with at-promise youth in Canada and identified a set of lessons learned to engage youth within emotional and behavioral constraints in making-centered activities. Leveraging our observations, we proposed Augmented Reality (AR)-mediated prototyping as a way to address material constraints. AR-mediated prototyping can help makers to build, program, interact with and iterate on physical computing projects that combine both real-world and stand-in virtual electronic components. We designed, implemented, and evaluated a technology probe, Polymorphic Cube (PMC), as an instance of our vision. Our results show that PMC helped participants prototype despite missing I/O electronic components, and highlighted how AR-mediated prototyping extends to exploring project ideas, tinkering with implementation, and making with others.<br/>Informed by our empirical and design explorations, we suggest a set of characteristics of constraints and implications for designing future technologies for makers within constraints. In the long-term, we hope that this research will inspire interaction designers to develop new tools that can help resolve constraints for making.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Computer Science</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sowmya Somanath<!-- -->.¬†<b>&#x27;Making&#x27; within Material, Cultural, and Emotional Constraints</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2017<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/11023/4237" target="_blank">http://hdl.handle.net/11023/4237</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2015-li" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2015-li/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2015-li</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2015</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2015-li" target="_blank">Two-Sided Transparent Display as a Collaborative Medium</a></h1><p class="meta"><a href="/people/jiannan-li"><img alt="jiannan-li photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/jiannan-li.jpg 1x" src="/static/images/people/jiannan-li.jpg"/><strong>Jiannan Li</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <a href="/people/saul-greenberg"><img alt="saul-greenberg photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/saul-greenberg.jpg 1x" src="/static/images/people/saul-greenberg.jpg"/><strong>Saul Greenberg</strong></a><span class="role"> (supervisor)</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Transparent displays are ‚Äòsee-through‚Äô screens: a person can simultaneously view both the graphics on the screen and real-world content visible through the screen. Interactive transparent displays can serve as an important medium supporting face-to-face collaboration, where people interact with both sides of the display and work together. Such displays enhance workspace awareness, which smooths collaboration: when a person is working on one side of a transparent display, the person on the other side can see the other&#x27;s hand gestures, gaze, and what s/he is currently manipulating on the shared screen. Even so, we argue that in order to provide effective support for collaboration, designing such transparent displays must go beyond current offerings. We propose using two-sided transparent displays, which can present different content on both sides. The displays should also accept interactive input on both sides and visually augment users‚Äô actions when display transparency is reduced. We operationalized these design requirements with our two-sided transparent display prototype, FACINGBOARD-II, and devised a palette of supportive interaction techniques. Through empirical studies, we found that the workspace awareness provided by transparent displays is compromised with degrading display transparency, and that visually enhancing user actions can compensate for this awareness loss.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Computer Science</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jiannan Li<!-- -->.¬†<b>Two-Sided Transparent Display as a Collaborative Medium</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2015-01-28<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/11023/2033" target="_blank">http://hdl.handle.net/11023/2033</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="mmus-2012-pon" class="ui large modal"><div class="header"><a target="_blank" href="/theses/mmus-2012-pon/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>mmus-2012-pon</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MMus 2012</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/mmus-2012-pon" target="_blank">Vuzik: Exploring a Medium for Painting Music</a></h1><p class="meta"><span>Aura Pon<!-- --> <span class="role"> (author)</span></span>, <span>David Eagle<!-- --> <span class="role"> (supervisor)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>What if one could paint music? Music is often shaped by the medium we use to interact with it, and thus the development of different ways to compose and experience music can open up new creative possibilities to people. This somewhat whimsical proposition of painting music, and a quest to find a new medium to create, experience, and interact with music, gave impetus and shape to the development of a musical interface known as Vuzik. Vuzik is an interface for creating and visualizing music through painting gestures on a large interactive surface. This thesis presents the vision, design and implementation of Vuzik. It then describes explorations of it as a tool for music education, and for the performance and creation of electronic music. Finally, it presents evaluation efforts of Vuzik in its performance of each of these explorations, and reflects on their implications regarding the nature and potential of Vuzik.</p><div class="ui large basic labels">Keywords: ¬†<span class="ui brown basic label">Music</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Aura Pon<!-- -->.¬†<b>Vuzik: Exploring a Medium for Painting Music</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Music (MMus)<!-- -->. <!-- -->2012-10-04<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/11023/302" target="_blank">http://hdl.handle.net/11023/302</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2012-lapides" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2012-lapides/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2012-lapides</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2012</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2012-lapides" target="_blank">Designing video games with social, physical, and authorship gameplay</a></h1><p class="meta"><a href="/people/paul-lapides"><img alt="paul-lapides photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/paul-lapides.jpg 1x" src="/static/images/people/paul-lapides.jpg"/><strong>Paul Lapides</strong></a><span class="role"> (author)</span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span>, <span>M√°rio Costa Sousa<!-- --> <span class="role"> (supervisor)</span></span></p></div></div></div><div class="block"></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Paul Lapides<!-- -->.¬†<b>Designing video games with social, physical, and authorship gameplay</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2012<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/105630" target="_blank">http://hdl.handle.net/1880/105630</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2011-harris" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2011-harris/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2011-harris</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2011</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2011-harris" target="_blank">Exploring the affect of emotive motion in social human robot interaction</a></h1><p class="meta"><span>John J. R. Harris<!-- --> <span class="role"> (author)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span></p></div></div></div><div class="block"></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">John J. R. Harris<!-- -->.¬†<b>Exploring the affect of emotive motion in social human robot interaction</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2011<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/105508" target="_blank">http://hdl.handle.net/1880/105508</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2011-sultanum" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2011-sultanum/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2011-sultanum</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2011</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2011-sultanum" target="_blank">Exploring novel interfaces for 3d visualization of reservoir simulation post-processing data</a></h1><p class="meta"><span>Nicole Barbosa Sultanum<!-- --> <span class="role"> (author)</span></span>, <span>M√°rio Costa Sousa<!-- --> <span class="role"> (supervisor)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span></p></div></div></div><div class="block"></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nicole Barbosa Sultanum<!-- -->.¬†<b>Exploring novel interfaces for 3d visualization of reservoir simulation post-processing data</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2011<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/105516" target="_blank">http://hdl.handle.net/1880/105516</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="phd-2010-young" class="ui large modal"><div class="header"><a target="_blank" href="/theses/phd-2010-young/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>phd-2010-young</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">PhD 2010</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/phd-2010-young" target="_blank">Exploring social interaction between robots and people</a></h1><p class="meta"><span>James E. Young<!-- --> <span class="role"> (author)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span></p></div></div></div><div class="block"></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">James E. Young<!-- -->.¬†<b>Exploring social interaction between robots and people</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Doctor of Philosophy (PhD)<!-- -->. <!-- -->2010<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/104850" target="_blank">http://hdl.handle.net/1880/104850</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="msc-2008-guo" class="ui large modal"><div class="header"><a target="_blank" href="/theses/msc-2008-guo/"><svg data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link" role="img" viewBox="0 0 576 512" aria-hidden="true"><path fill="currentColor" d="M419.5 96c-16.6 0-32.7 4.5-46.8 12.7-15.8-16-34.2-29.4-54.5-39.5 28.2-24 64.1-37.2 101.3-37.2 86.4 0 156.5 70 156.5 156.5 0 41.5-16.5 81.3-45.8 110.6l-71.1 71.1c-29.3 29.3-69.1 45.8-110.6 45.8-86.4 0-156.5-70-156.5-156.5 0-1.5 0-3 .1-4.5 .5-17.7 15.2-31.6 32.9-31.1s31.6 15.2 31.1 32.9c0 .9 0 1.8 0 2.6 0 51.1 41.4 92.5 92.5 92.5 24.5 0 48-9.7 65.4-27.1l71.1-71.1c17.3-17.3 27.1-40.9 27.1-65.4 0-51.1-41.4-92.5-92.5-92.5zM275.2 173.3c-1.9-.8-3.8-1.9-5.5-3.1-12.6-6.5-27-10.2-42.1-10.2-24.5 0-48 9.7-65.4 27.1L91.1 258.2c-17.3 17.3-27.1 40.9-27.1 65.4 0 51.1 41.4 92.5 92.5 92.5 16.5 0 32.6-4.4 46.7-12.6 15.8 16 34.2 29.4 54.6 39.5-28.2 23.9-64 37.2-101.3 37.2-86.4 0-156.5-70-156.5-156.5 0-41.5 16.5-81.3 45.8-110.6l71.1-71.1c29.3-29.3 69.1-45.8 110.6-45.8 86.6 0 156.5 70.6 156.5 156.9 0 1.3 0 2.6 0 3.9-.4 17.7-15.1 31.6-32.8 31.2s-31.6-15.1-31.2-32.8c0-.8 0-1.5 0-2.3 0-33.7-18-63.3-44.8-79.6z"></path></svg>msc-2008-guo</a><div class="actions" style="float:right;cursor:pointer;color:grey"><svg data-prefix="fas" data-icon="xmark" class="svg-inline--fa fa-xmark" role="img" viewBox="0 0 384 512" aria-hidden="true"><path fill="currentColor" d="M55.1 73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L147.2 256 9.9 393.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L192.5 301.3 329.9 438.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L237.8 256 375.1 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L192.5 210.7 55.1 73.4z"></path></svg></div></div><div class="content"><div id="thesis"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/thesiss/">Thesis</a><svg data-prefix="fas" data-icon="angle-right" class="svg-inline--fa fa-angle-right" role="img" viewBox="0 0 256 512" aria-hidden="true"><path fill="currentColor" d="M247.1 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L179.2 256 41.9 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"></path></svg><a class="active section">MSc 2008</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"></div><div class="thirteen wide column"><h1><a href="/thesiss/msc-2008-guo" target="_blank">New paradigms for human-robot interaction using tangible user interfaces</a></h1><p class="meta"><span>Cheng Guo<!-- --> <span class="role"> (author)</span></span>, <a href="/people/ehud-sharlin"><img alt="ehud-sharlin photo" loading="lazy" width="0" height="0" decoding="async" data-nimg="1" class="ui circular spaced image mini-profile" style="color:transparent" srcSet="/static/images/people/ehud-sharlin.jpg 1x" src="/static/images/people/ehud-sharlin.jpg"/><strong>Ehud Sharlin</strong></a><span class="role"> (supervisor)</span></p></div></div></div><div class="block"></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Cheng Guo<!-- -->.¬†<b>New paradigms for human-robot interaction using tangible user interfaces</b>.¬†<i></i>¬†<!-- -->University of Calgary<!-- -->. <!-- -->Master of Science (MSc)<!-- -->. <!-- -->2008<!-- -->. <!-- -->URL: <a href="http://hdl.handle.net/1880/103564" target="_blank">http://hdl.handle.net/1880/103564</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div></div></div></div></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><div class="content"><a href="https://ucalgary.ca"><img alt="University of Calgary logo" loading="lazy" width="200" height="0" decoding="async" data-nimg="1" style="color:transparent;max-width:200px;margin:0px auto;height:auto" srcSet="/static/images/logo-4.png 1x, /static/images/logo-4.png 2x" src="/static/images/logo-4.png"/></a><div class="sub header"><a class="item" href="https://cpsc.ucalgary.ca">Department of Computer Science</a></div></div></div></footer></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"person":{"name":"Ehud Sharlin","type":"faculty","title":"Professor","labs":["utouch"],"keywords":["HRI","Robots","Drones"],"order":1,"url":"http://contacts.ucalgary.ca/info/cpsc/profiles/102-3264","scholar":"https://scholar.google.ca/citations?hl=en\u0026user=eAFxlZIAAAAJ","dir":"content/output/people","base":"ehud-sharlin.json","ext":".json","sourceBase":"ehud-sharlin.yaml","sourceExt":".yaml","photo":"/static/images/people/ehud-sharlin.jpg"}},"__N_SSG":true},"page":"/people/[id]","query":{"id":"ehud-sharlin"},"buildId":"gYcQ3cHgcaXKKuycsy64l","runtimeConfig":{"basePath":""},"isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>